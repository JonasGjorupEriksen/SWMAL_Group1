{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "We now try building af ML pipeline. The data for this exercise is the same as in L01, meaning that the OECD data from the 'intro.ipynb' have been save into a Python 'pickle' file. \n",
    "\n",
    "The pickle library is a nifty data preservation method in Python, and from L01 the tuple `(X, y)` have been stored to the pickle file `itmal_l01_data.pkl', try reloading it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(29, 1),  y.shape=(29,)\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def LoadDataFromL01():\n",
    "    filename = \"Data/itmal_l01_data.pkl\"\n",
    "    with open(f\"{filename}\", \"rb\") as f:\n",
    "        (X, y) = pickle.load(f)\n",
    "        return X, y\n",
    "\n",
    "X, y = LoadDataFromL01()\n",
    "\n",
    "print(f\"X.shape={X.shape},  y.shape={y.shape}\")\n",
    "\n",
    "assert X.shape[0] == y.shape[0]\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1  # did a y.ravel() before saving to picke file\n",
    "assert X.shape[0] == 29\n",
    "\n",
    "# re-create plot data (not stored in the Pickel file)\n",
    "m = np.linspace(0, 60000, 1000)\n",
    "M = np.empty([m.shape[0], 1])\n",
    "M[:, 0] = m\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Revisiting the problem with the MLP\n",
    "\n",
    "Using the MLP for the QECD data in Qd) from `intro.ipynb` produced a negative $R^2$, meaning that it was unable to fit the data, and the MPL model was actually _worse_ than the naive $\\hat y$ (mean value of y).\n",
    "\n",
    "Let's just revisit this fact. When running the next cell you should now see an OK $~R^2_{lin.reg}~$ score and a negative $~R^2_{mlp}~$ score.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\n",
      "\n",
      "lin.reg.score(X, y)=0.73\n",
      "    MLP.score(X, y)=-3.94\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Setup the MLP and lin. regression again..\n",
    "\n",
    "def isNumpyData(t: np.ndarray, expected_ndim: int):\n",
    "    assert isinstance(expected_ndim, int), f\"input parameter 'expected_ndim' is not an integer but a '{type(expected_ndim)}'\"\n",
    "    assert expected_ndim>=0, f\"expected input parameter 'expected_ndim' to be >=0, got {expected_ndim}\"\n",
    "    if t is None:\n",
    "        print(\"input parameter 't' is None\", file=sys.stderr)\n",
    "        return False\n",
    "    if not isinstance(t, np.ndarray):\n",
    "        print(\"excepted numpy.ndarray got type '{type(t)}'\", file=sys.stderr)\n",
    "        return False\n",
    "    if not t.ndim==expected_ndim:\n",
    "        print(\"expected ndim={expected_ndim} but found {t.ndim}\", file=sys.stderr)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def PlotModels(model1, model2, X: np.ndarray, y: np.ndarray, name_model1: str, name_model2: str):\n",
    "    \n",
    "    # NOTE: local function is such a nifty feature of Python!\n",
    "    def CalcPredAndScore(model, X: np.ndarray, y: np.ndarray,):\n",
    "        assert isNumpyData(X, 2) and isNumpyData(y, 1) and X.shape[0]==y.shape[0]\n",
    "        y_pred_model = model.predict(X)\n",
    "        score_model = r2_score(y, y_pred_model) # call r2\n",
    "        return y_pred_model, score_model    \n",
    "\n",
    "    assert isinstance(name_model1, str) and isinstance(name_model2, str)\n",
    "\n",
    "    y_pred_model1, score_model1 = CalcPredAndScore(model1, X, y)\n",
    "    y_pred_model2, score_model2 = CalcPredAndScore(model2, X, y)\n",
    "\n",
    "    plt.plot(X, y_pred_model1, \"r.-\")\n",
    "    plt.plot(X, y_pred_model2, \"kx-\")\n",
    "    plt.scatter(X, y)\n",
    "    plt.xlabel(\"GDP per capita\")\n",
    "    plt.ylabel(\"Life satisfaction\")\n",
    "    plt.legend([name_model1, name_model2, \"X OECD data\"])\n",
    "\n",
    "    l = max(len(name_model1), len(name_model2))\n",
    "    \n",
    "    print(f\"{(name_model1).rjust(l)}.score(X, y)={score_model1:0.2f}\")\n",
    "    print(f\"{(name_model2).rjust(l)}.score(X, y)={score_model2:0.2f}\")\n",
    "\n",
    "# lets make a linear and MLP regressor and redo the plots\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10, ),\n",
    "                   solver='adam',\n",
    "                   activation='relu',\n",
    "                   tol=1E-5,\n",
    "                   max_iter=100000,\n",
    "                   verbose=False)\n",
    "linreg = LinearRegression()\n",
    "\n",
    "mlp.fit(X, y)\n",
    "linreg.fit(X, y)\n",
    "\n",
    "print(\"The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\\n\")\n",
    "PlotModels(linreg, mlp, X, y, \"lin.reg\", \"MLP\")\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa) Create a Min/max scaler for the MLP\n",
    "\n",
    "Now, the neurons in neural networks normally expect input data in the range `[0;1]` or sometimes in the range `[-1;1]`, meaning that for value outside this range then the neuron will saturate to its min or max value (also typical `0` or `1`). \n",
    "\n",
    "A concrete value of `X` is, say 22.000 USD, that is far away from what the MLP expects. Af fix to the problem in Qd), from `intro.ipynb`, is to preprocess data by scaling it down to something more sensible.\n",
    "\n",
    "Try to manually scale X to a range of `[0;1]`, re-train the MLP, re-plot and find the new score from the rescaled input. Any better?\n",
    "\n",
    "(If you already made exercise \"Qe) Neural Network with pre-scaling\" in L01, then reuse Your work here!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_min=9055, X_max=55805, s=46750\n",
      "X_scaled.shape=(29, 1)\n",
      "np.min(X_scaled)=0.0\n",
      "np.max(X_scaled)=1.0\n",
      "mlp.score=0.73\n"
     ]
    }
   ],
   "source": [
    "# TODO: add your code here..\n",
    "#assert False, \"TODO: rescale X and refit the model(s)..\"\n",
    "\n",
    "X_min = np.min(X)\n",
    "X_max = np.max(X)\n",
    "s = X_max - X_min\n",
    "\n",
    "print(f\"X_min={X_min:.0f}, X_max={X_max:.0f}, s={s:.0f}\")\n",
    "\n",
    "X_scaled = (X - X_min) / s\n",
    "print(f\"X_scaled.shape={X_scaled.shape}\")\n",
    "print(f\"np.min(X_scaled)={np.min(X_scaled)}\")\n",
    "print(f\"np.max(X_scaled)={np.max(X_scaled)}\")\n",
    "\n",
    "mlp.fit(X_scaled, y)\n",
    "y_pred_mlp = mlp.predict((M-X_min)/s)\n",
    "\n",
    "#plt.plot(m, y_pred_lin, \"r\")\n",
    "plt.plot(m, y_pred_mlp, \"k.\")\n",
    "\n",
    "print(f\"mlp.score={mlp.score(X_scaled, y):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb) Scikit-learn Pipelines\n",
    "\n",
    "Now, rescale again, but use the `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "When this works put both the MLP and the scaler into a composite construction via `sklearn.pipeline.Pipeline`. This composite is just a new Scikit-learn estimator, and can be used just like any other `fit-predict` models, try it, and document it for the journal.\n",
    "\n",
    "(You could reuse the `PlotModels()` function by also retraining the linear regressor on the scaled data, or just write your own plot code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe.score(..)=0.72\n"
     ]
    }
   ],
   "source": [
    "# TODO: add your code here..\n",
    "#assert False, \"TODO: put everything into a pipeline..\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('scaler',MinMaxScaler()),\n",
    "        ('mlp',mlp)\n",
    "    ]\n",
    ")\n",
    "pipe.fit(X,y)\n",
    "\n",
    "print(f\"pipe.score(..)={pipe.score(X,y):0.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc) Outliers and the Min-max Scaler vs. the Standard Scaler\n",
    "\n",
    "Explain the fundamental problem with a min-max scaler and outliers. \n",
    "\n",
    "Will a `sklearn.preprocessing.StandardScaler` do better here, in the case of abnormal feature values/outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general problem with normal scalers are that they are largely affected by outliers. lets say we have a system that generally swings between 1 and 10, but then you have an outlier of 100 then the max value would be 100 and all the other number would range from 0-0.1 which is not a lot. So this looses some sensitivity. \n",
    "\n",
    "The standard scaler is a litte better at this but still affected because it centers around the mean instead of the min and max. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_test: [  1   2   3   4   5   6   7   8   9  10 100]\n",
      "Transformed X_test: [-0.47944098 -0.44281701 -0.40619305 -0.36956909 -0.33294512 -0.29632116\n",
      " -0.2596972  -0.22307323 -0.18644927 -0.14982531  3.14633142]\n",
      "Mean of Scaled Data: 0.0\n",
      "Standard Deviation of Scaled Data: 1.0\n",
      "Transformed X_test: [0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n",
      " 0.06060606 0.07070707 0.08080808 0.09090909 1.        ]\n",
      "Mean of Scaled Data: 0.1322314049586777\n",
      "Standard Deviation of Scaled Data: 0.275803302177393\n"
     ]
    }
   ],
   "source": [
    "# TODO: research the problem here..\n",
    "#assert False, \"TODO: investigate outlier problems and try a StandardScaler..\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define test data (reshaped for sklearn compatibility)\n",
    "X_test = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100]).reshape(-1, 1)\n",
    "\n",
    "# Define a pipeline with StandardScaler\n",
    "pipe_standard = Pipeline([\n",
    "    ('scaler', StandardScaler())  # StandardScaler is a transformer\n",
    "])\n",
    "\n",
    "# Define a pipeline with StandardScaler\n",
    "pipe_minmax = Pipeline([\n",
    "    ('scaler', MinMaxScaler())  # StandardScaler is a transformer\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled_standard = pipe_standard.fit_transform(X_test)\n",
    "X_scaled_minmax = pipe_minmax.fit_transform(X_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Original X_test:\", X_test.ravel())  # Flatten for readability\n",
    "print(\"Transformed X_test:\", X_scaled_standard.ravel())  # Scaled values\n",
    "print(\"Mean of Scaled Data:\", X_scaled_standard.mean())  # Should be ~0\n",
    "print(\"Standard Deviation of Scaled Data:\", X_scaled_standard.std())  # Should be ~1\n",
    "\n",
    "print(\"Transformed X_test:\", X_scaled_minmax.ravel())  # Scaled values\n",
    "print(\"Mean of Scaled Data:\", X_scaled_minmax.mean())  # Should be ~0\n",
    "print(\"Standard Deviation of Scaled Data:\", X_scaled_minmax.std())  # Should be ~1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that the standard ranges in the non outliers from -0.48 to -0.14 which is a range of 0.34 and the minmax ranges from 0-0.09 which is a way smaller range. But the outlier is outside the range of -1 - 1 so that could be a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd) Modify the MLP Hyperparameters\n",
    "\n",
    "Finally, try out some of the hyperparameters associated with the MLP.\n",
    "\n",
    "Specifically, test how few neurons the MLP can do with---still producing a sensible output, i.e. high $R^2$. \n",
    "\n",
    "Also try-out some other activation functions, ala sigmoid, and solvers, like `sgd`.\n",
    "\n",
    "Notice, that the Scikit-learn MLP does not have as many adjustable parameters, as a Keras MLP, for example, the Scikit-learn MLP misses neurons initialization parameters (pp.333-334 2nd./pp.358-359 3rd. [HOML]) and the ELU activation function (p.336 2nd./ p.363 3rd. [HOML]).\n",
    "\n",
    "[OPTIONAL 1]: use a Keras MLP regressor instead of the Scikit-learn MLP (You need to install the  Keras if its not installed as default).\n",
    "\n",
    "[OPTIONAL 2]: try out the `early_stopping` hyperparameter on the `MLPRegressor`. \n",
    "\n",
    "[OPTIONAL 3]: try putting all score-calculations into K-fold cross-validation  methods readily available in Scikit-learn using\n",
    "\n",
    "* `sklearn.model_selection.cross_val_predict`\n",
    "* `sklearn.model_selection.cross_val_score` \n",
    "\n",
    "or similar (this is, in theory, the correct method, but can be hard to use due to the  extremely small number of data points, `n=29`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.9597\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters: {'mlp__activation': 'tanh', 'mlp__hidden_layer_sizes': (10,), 'mlp__solver': 'lbfgs'}\n",
      "Best Model R² Score: 0.9597\n",
      "Cross-Validation R² Scores: [0.97338243 0.9714233  0.96517035 0.98015593 0.96384068]\n",
      "Mean R²: 0.9708\n",
      "R² Score with Early Stopping: 0.9597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Create synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(128, 1) * 10  # Feature\n",
    "y = np.sin(X).ravel() + np.random.normal(scale=0.1, size=128)  # Target with noise\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Feature scaling\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(10,), activation='tanh', solver='lbfgs',\n",
    "                         max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(5,), (10,), (50,)],  # Different neuron counts\n",
    "    'mlp__activation': ['relu', 'tanh', 'logistic'],  # Test activation functions\n",
    "    'mlp__solver': ['sgd', 'adam', 'lbfgs'],  # Compare solvers\n",
    "}\n",
    "\n",
    "# Set up GridSearch\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='r2', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(f\"Best Model R² Score: {r2_score(y_test, y_pred_best):.4f}\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "print(f\"Cross-Validation R² Scores: {cv_scores}\")\n",
    "print(f\"Mean R²: {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "pipeline_early_stop = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(10,), activation='tanh', solver='lbfgs',\n",
    "                         max_iter=5000, random_state=42, early_stopping=True))\n",
    "])\n",
    "\n",
    "pipeline_early_stop.fit(X_train, y_train)\n",
    "y_pred_early_stop = pipeline_early_stop.predict(X_test)\n",
    "print(f\"R² Score with Early Stopping: {r2_score(y_test, y_pred_early_stop):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-|\n",
    "2020-10-15| CEF, initial. \n",
    "2020-10-21| CEF, added Standard Scaler Q.\n",
    "2020-11-17| CEF, removed orhpant text in Qa (moded to Qc).\n",
    "2021-02-10| CEF, updated for ITMAL F21.\n",
    "2021-11-08| CEF, updated print info.\n",
    "2021-02-10| CEF, updated for SWMAL F22.\n",
    "2023-02-19| CEF, updated for SWMAL F23, adjuste page numbers for 3rd.ed.\n",
    "2023-02-21| CEF, added types, rewrote CalcPredAndScore and added isNumpyData.\n",
    "2024-09-11| CEF, updated page refefences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
