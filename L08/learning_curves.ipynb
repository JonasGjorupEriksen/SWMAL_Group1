{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Learning Curves\n",
    "\n",
    "### The Generalization Error, Over- and Underfitting, Early Stopping, and Optimal Capacity\n",
    "\n",
    "In this exercise, we need to explain all important overall concepts in training. Let's begin with Figure 5.3 from Deep Learning (Ian Goodfellow, et. al. [DL]), which pretty much sums it all up\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L08/Figs/dl_generalization_error.png\" alt=\"WARNING: could not get image from server.\" style=\"height:500px\">\n",
    "\n",
    "\n",
    "### Qa) The Generalization Error\n",
    "\n",
    "Write a detailed description of figure 5.3 (above) for your hand-in.\n",
    " \n",
    "All concepts in the figure must be explained \n",
    "\n",
    "* training/generalization error, \n",
    "* underfit/overfit zone, \n",
    "* optimal capacity, \n",
    "* generalization gab, \n",
    "* and the two axes: x/capacity, y/error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: ...in text\n",
    "assert False, \"TODO: write some text..\"\n",
    "\n",
    "Training and generalization error:\n",
    "On the graph it shows the behavior of training and generalization error, when compared to the capacity. At the lowest capacity both errors are high, and it is called the underfitting zone. Here the error is high, as it does not have many iterations in the model. If the data is trained on different points on a graph, it will output a straight line if the capacity is low, and it will probably be far off the right function of the original graph of the points. If the capacity is increased, the model will have a lower error for the training data. But for the generalization, it can be seen as unseen data, and if the model has been trained very complex to the training data, it will be hard to correctly asses random/new/unseen data.\n",
    "\n",
    "\n",
    "Underfit and overfit zones\n",
    "The underfit zone is the zone in the left most part of the graph. Here the capacity is too low, to provide enough complexity to the training data, and therefore has a high error. The same applies for the generalization. At some point, the generalization and training error flattens out, and the gap between the 2 increases. At some point the generalization error will increase again, this area is called the overfitting zone. If the complexity is too high, then when training the data, it will get to complex to look at other data than what is what trained on. The training error will still decrease as the capacity increases as with more capacity, it can easily fit all the complex data.\n",
    "\n",
    "Optimal capacity\n",
    "The optimal capacity is the point where the generalization error is at the lowest point. This is where the training doesn't get to complex, but it still is a good fit for looking at the general data. It is possible to find the optimal capacity by increasing the capacity, and look at the error of the generalization. If the error keeps getting lower as the complexity increases, you can keep increasing the capacity. As the error starts to increase, its time to stop increasing the complexity and use the model with the lowest error.\n",
    "\n",
    "Generalization gap\n",
    "This is the gap between the training and generalization. As the capacity increases, the gap will also increase. This happens as the training error will decrease more rapidly than the generalization error, and at some point the generalization data will also increase. \n",
    "\n",
    "Y axis: error\n",
    "On the y-axis it shows error. The higher on the graph, the model performs worse. And at lower values it means the model is working better. It is prefered to get the error as low as possible. \n",
    "\n",
    "X axis: model capacity\n",
    "On the X-axis is the model capacity. The capacity can also be called complexity. The bigger the number, the more complex it will get, and it will be easier to reduce the error for the training model. At some point the complexity gets too high for general data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Learning Curves\n",
    "\n",
    "Next, produce a loss vs epoch graph ala (from `04_training_linear_models.ipynb` [GITHOML]) \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L08/Figs/training_curve.png\" alt=\"WARNING: could not get image from server.\">\n",
    "\n",
    "\n",
    "for your linear regressor, `MyLinReg`, from an earlier lesson. You need a function \n",
    "\n",
    ">```LossHistory()```\n",
    "\n",
    "on your regressor class, that accumulates the internally calculated $J$'s. Once `LossHistory` is ready, plot it using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"TODO: implement a J-history function on your linear regressor..\"\n",
    "\n",
    "r = MyLinReg()\n",
    "\n",
    "assert False, \"TODO: fit on some data here..\"\n",
    "\n",
    "h = r.LossHistory()\n",
    "\n",
    "# Plot of J vs. epoch (or perhaps iteration)...\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(h)\n",
    "\n",
    "ax.set_title(\"Loss history\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"J\")\n",
    "\n",
    "zoom_axis = False\n",
    "if zoom_axis:\n",
    "    limits_x = ax.get_xlim()\n",
    "    limits_y = ax.get_ylim()\n",
    "    ax.set_ylim([0, limits_y[1]/10]) # zoom on y axis\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc)  Early Stopping\n",
    "\n",
    "Then implement ___early stopping___, in your `MyLinReg` estimator. Below is a graphical view of early stopping similar to Figure 4-20 p.162 [HOML] (for a non-linear model?)\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L08/Figs/early_stopping.png\" alt=\"WARNING: could not get image from server.\">\n",
    "\n",
    "Write an explanation of the early stopping concept in text, implement it in your linear regressor and write some test/demo code, that via some learning-curve graphs demonstrates how it works when training your linear regressor in both _batch-gradient descent_ and \n",
    "_stochastic gradient descent_ modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"TODO: implement early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd:  [OPTIONAL]  Using a Polynomial Regressor to Produce an Error-vs-Capacity Graph\n",
    "\n",
    "Finally, create a polynomial estimator based on your `MyLinReg`, see details of\n",
    "how to expend a linear regressor to a polynomial fitting in [HOLM] or in\n",
    "\n",
    "> https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    " \n",
    "that contains most of the code you will need. \n",
    " \n",
    "With a polynomial regressor, you should be able to reproduce a graph similar to Figure 5.3 from Deep Learning [DL], where you notice that the _x-axis_ is capacity and not _epoch_ as the learning curves you just produced in Qb/c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False \"TODO: implement a polynomial-fit pipeline, and create an Error-vs-Capacity plot..\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      (\"polynomial_features\", polynomial_features),\n",
    "      (\"linear_regression\",   MyLinReg())\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qe Conclusion\n",
    "\n",
    "Sum it all up in a nice, well-written conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-\n",
    "2018-12-19| CEF, initial.                  \n",
    "2018-02-14| CEF, major update and put in sync with under/overfitting exe.         \n",
    "2018-02-20| CEF, fixed revision table malformatting.\n",
    "2018-02-25| CEF, minor text updates, and made Qc optional.\n",
    "2018-02-25| CEF, updated code, made more functions.\n",
    "2018-03-11| CEF, corrected RSME to RMSE.\n",
    "2019-10-08| CEF, updated to ITMAL E19.\n",
    "2020-03-14| CEF, updated to ITMAL F20.\n",
    "2020-10-15| CEF, updated to ITMAL E20.\n",
    "2020-11-17| CEF, added a comment on 90-degree polynomial, made early stopping a pseudo code exe.\n",
    "2021-03-22| CEF, changed crossref from \"capacity_under_overfitting.ipynb Qc\" to Qa+b in QdExplain the Polynomial RMSE-Capacity Plot. \n",
    "2021-03-23| CEF, changed 'cv RMSE' legend to 'validation RMSE'.\n",
    "2021-10-31| CEF, updated to ITMAL E21.\n",
    "2022-03-25| CEF, updated to SWMAL F22.\n",
    "2023-03-16| CEF, minor update to SWMAL F23.\n",
    "2024-09-25| CEF, major update, combined  generalization_error.ipynb and capacity_under_overfitting.ipynb, removed review parts, added graphs plots for MyLinReg.\n",
    "2024-10-14| CEF, minor text updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.517px",
    "left": "1228px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
