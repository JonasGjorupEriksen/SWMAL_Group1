{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "(In the following you need not present your journal in the Qa+b+c+ etc. order. You could just present the final code with test and comments.)\n",
    "\n",
    "## Training Your Own Linear Regressor\n",
    "\n",
    "Create a linear regressor, with a Scikit-learn compatible fit-predict interface. You should implement every detail of the linear regressor in Python, using whatever libraries, say `numpy`, you want (except a linear regressor itself).\n",
    "\n",
    "Below is a primitive _get-started_ skeleton for your implementation. Keep the class name `MyLinReg`, which is used in the test sequence later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class MyLinReg():\n",
    "    def __init__(self, eta0=0.01, max_iter=10, tol=1e-3, n_iter_no_change=5, verbose=True):\n",
    "        self.eta0 = eta0\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MyLinReg.__str__(): hi!\"\n",
    "    \n",
    "    def update_weights(self): \n",
    "        # Pick one random sample (stochastic)\n",
    "        i = np.random.randint(0, self.m)  # Random index\n",
    "        X_i = self.X[i].reshape(1, -1)  # Make it a row vector\n",
    "        Y_i = self.Y[i]  # Corresponding target\n",
    "\n",
    "        # Compute prediction\n",
    "        Y_pred = self.predict(X_i)\n",
    "\n",
    "        # Compute gradient (SGD formula)\n",
    "        dW = X_i.T.dot(Y_pred - Y_i)  # Gradient for weights\n",
    "        db = Y_pred - Y_i\n",
    "\n",
    "        # Update weights\n",
    "        self.W = self.W - self.eta0 * dW\n",
    "        self.b = self.b - self.eta0 * db\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.m, self.n = X.shape\n",
    "         # weight initialization           \n",
    "        self.W = np.zeros( self.n )           \n",
    "        self.b = 0          \n",
    "        self.X = X           \n",
    "        self.Y = y \n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_change_count = 0\n",
    "                    \n",
    "        # gradient descent learning                  \n",
    "        for i in range(self.max_iter):\n",
    "            Y_pred = self.predict(self.X)\n",
    "            loss = np.mean((self.Y - Y_pred) ** 2)  # Mean Squared Error (MSE)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Iteration {i+1}, Loss: {loss}\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if abs(best_loss - loss) < self.tol:\n",
    "                no_change_count += 1\n",
    "                if no_change_count >= self.n_iter_no_change:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Stopping early at iteration {i+1} due to no improvement.\")\n",
    "                    break\n",
    "            else:\n",
    "                no_change_count = 0  # Reset count if loss decreases\n",
    "\n",
    "            best_loss = loss\n",
    "\n",
    "            self.update_weights()               \n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the output for given input X.\"\"\"\n",
    "        return (X.dot(self.W) + self.b).flatten()\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        \"\"\"Compute R^2 (coefficient of determination) score.\"\"\"\n",
    "        y_pred = self.predict(X)  # Ensure predictions are 1D\n",
    "        y_true = y_true.flatten()  # Ensure true values are also 1D\n",
    "\n",
    "        ss_total = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "        ss_residual = np.sum((y_true - y_pred) ** 2)  # Residual sum of squares\n",
    "        \n",
    "        r2_score = 1 - (ss_residual / ss_total)  # Compute R^2\n",
    "        return r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TODO list\n",
    "\n",
    "You must investigate and describe all major details for a linear regressor, and implement at least the following concepts (MUST):\n",
    "\n",
    "### Qa: Concepts and Implementations MUSTS\n",
    "\n",
    "* Implement: the `fit-predict` interface, for a one-dimensional output only, \n",
    "* Implement: a $R^2$ score function (re-use existing code or perhaps just inherit it), \n",
    "* Implement: loss function based on (R)MSE,\n",
    "* Implement: setting of the number of iterations and learning rate ($\\eta$) via parameters in the constructor (the signature of your `__init__` must include the named parameters `eta0` and `max_iter`),\n",
    "* (in a later exercise we will also add `tol`, `n_iter_no_change` and `verbose` to the constructor),\n",
    "* Implement: the batch-gradient decent algorithm (GD),\n",
    "* Implement: constant learning rate (maybe also adaptive learning rate if you are brave),\n",
    "* Implement: stochastic gradient descent (SGD),\n",
    "* Describe in text: epochs vs iterations,\n",
    "* Describe in text: compare the numerical optimization with the Closed-form solution.\n",
    "\n",
    "### Qb: [OPTIONAL] Additional Concepts and Implementations\n",
    "\n",
    "And perhaps you could include (SHOULD/COULD):\n",
    "\n",
    "* (stochastic) mini-bach gradient decent, \n",
    "* interface to your bias and weights via `intercept_` and `coef_` attributes on your linear regressor `class`,\n",
    "* get/set functionality of your regressor, such that it is fully compatible with other Scikit-learn algorithms, try it out in say a `cross_val_score()` call from Scikit-learn,\n",
    "* test in via the smoke tests at the end of this Notebook,\n",
    "* testing it on MNIST data.\n",
    "\n",
    "With the following no-no's (WONT):\n",
    "\n",
    "* no learning graphs, no early stopping (we will do this in a later exercise),\n",
    "* no multi-linear regression,\n",
    "* no reuse of the Scikit-learn regressor,\n",
    "* no `C/C++` optimized implementation with a _thin_ Python interface (nifty, but out-of-scope for this cause),\n",
    "* no copy-paste of code from other sources WITHOUT a clear cite/reference for your source.\n",
    "\n",
    "### Qc: Testing and Test Data\n",
    "\n",
    "Use mainly very low-dimensional data for testing, say the IRIS set, since it might be very slow. Or create a simple low-dimensionality data generator.\n",
    "\n",
    "(There is a _micro_ data set in the function `GenerateData` in the smoke tests functions below, but better is to opt for an realistic data set.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 5901425308.702746\n",
      "Iteration 2, Loss: 4308168641.239727\n",
      "Iteration 3, Loss: 1974530525.327931\n",
      "Iteration 4, Loss: 1940546827.9886749\n",
      "Iteration 5, Loss: 2096286109.3489108\n",
      "Iteration 6, Loss: 2281358737.453598\n",
      "Iteration 7, Loss: 1912532455.122107\n",
      "Iteration 8, Loss: 1949583836.4428751\n",
      "Iteration 9, Loss: 2089302397.25674\n",
      "Iteration 10, Loss: 2213618689.1997104\n",
      "Iteration 11, Loss: 2415441566.05527\n",
      "Iteration 12, Loss: 2675489206.820121\n",
      "Iteration 13, Loss: 2656613657.070831\n",
      "Iteration 14, Loss: 2808467705.8382754\n",
      "Iteration 15, Loss: 2774445092.397732\n",
      "Iteration 16, Loss: 2782945870.898328\n",
      "Iteration 17, Loss: 1948734739.9479706\n",
      "Iteration 18, Loss: 2158667814.450154\n",
      "Iteration 19, Loss: 2107291439.3259034\n",
      "Iteration 20, Loss: 2371045347.1506505\n",
      "Iteration 21, Loss: 2741607398.731968\n",
      "Iteration 22, Loss: 1892511600.4918408\n",
      "Iteration 23, Loss: 1945584528.3174045\n",
      "Iteration 24, Loss: 2213579887.12436\n",
      "Iteration 25, Loss: 1923139054.1015415\n",
      "Iteration 26, Loss: 1987267227.9596038\n",
      "Iteration 27, Loss: 1923816018.865073\n",
      "Iteration 28, Loss: 2030575675.8464246\n",
      "Iteration 29, Loss: 2113532293.5707915\n",
      "Iteration 30, Loss: 2277567016.7173753\n",
      "Iteration 31, Loss: 1876599273.5739212\n",
      "Iteration 32, Loss: 2130813479.1506767\n",
      "Iteration 33, Loss: 1934065312.775702\n",
      "Iteration 34, Loss: 2322489936.0135617\n",
      "Iteration 35, Loss: 2060175952.638042\n",
      "Iteration 36, Loss: 2225319132.0862255\n",
      "Iteration 37, Loss: 2066312230.9837997\n",
      "Iteration 38, Loss: 2427565182.455845\n",
      "Iteration 39, Loss: 1876222201.668197\n",
      "Iteration 40, Loss: 2094800528.7890105\n",
      "Iteration 41, Loss: 2341112014.6683254\n",
      "Iteration 42, Loss: 1874446630.0002897\n",
      "Iteration 43, Loss: 1962927723.0853143\n",
      "Iteration 44, Loss: 2078050948.5731292\n",
      "Iteration 45, Loss: 1930527486.775665\n",
      "Iteration 46, Loss: 2099496960.3676188\n",
      "Iteration 47, Loss: 2246345700.786012\n",
      "Iteration 48, Loss: 1821370228.1644862\n",
      "Iteration 49, Loss: 1861287842.280319\n",
      "Iteration 50, Loss: 1995648286.7994237\n",
      "Iteration 51, Loss: 2370771400.5522895\n",
      "Iteration 52, Loss: 2533497433.6184177\n",
      "Iteration 53, Loss: 2771214905.62645\n",
      "Iteration 54, Loss: 4066031631.069226\n",
      "Iteration 55, Loss: 1898891955.4916723\n",
      "Iteration 56, Loss: 1815138489.085161\n",
      "Iteration 57, Loss: 2246852099.3321958\n",
      "Iteration 58, Loss: 2455938341.7364817\n",
      "Iteration 59, Loss: 1951236629.9440224\n",
      "Iteration 60, Loss: 2073029179.5027745\n",
      "Iteration 61, Loss: 2198917185.8196282\n",
      "Iteration 62, Loss: 3291646551.721518\n",
      "Iteration 63, Loss: 3992889861.0596566\n",
      "Iteration 64, Loss: 1815759803.2652717\n",
      "Iteration 65, Loss: 1797754751.3621066\n",
      "Iteration 66, Loss: 1921955885.662304\n",
      "Iteration 67, Loss: 1960613815.4315555\n",
      "Iteration 68, Loss: 2254732132.5431075\n",
      "Iteration 69, Loss: 1771691917.9622335\n",
      "Iteration 70, Loss: 1899454239.163883\n",
      "Iteration 71, Loss: 2066236702.6826656\n",
      "Iteration 72, Loss: 2420051513.663825\n",
      "Iteration 73, Loss: 1787925281.871057\n",
      "Iteration 74, Loss: 1831983321.2848258\n",
      "Iteration 75, Loss: 2094816563.1572096\n",
      "Iteration 76, Loss: 1823977423.7490544\n",
      "Iteration 77, Loss: 2027809424.0671592\n",
      "Iteration 78, Loss: 1766296165.4945817\n",
      "Iteration 79, Loss: 1760347892.954901\n",
      "Iteration 80, Loss: 1844389438.9502928\n",
      "Iteration 81, Loss: 1922294391.8637946\n",
      "Iteration 82, Loss: 2074856476.8648582\n",
      "Iteration 83, Loss: 2257912670.1934323\n",
      "Iteration 84, Loss: 2305375326.378732\n",
      "Iteration 85, Loss: 2398218936.728523\n",
      "Iteration 86, Loss: 1759424701.211059\n",
      "Iteration 87, Loss: 1829296808.2140117\n",
      "Iteration 88, Loss: 1954926820.8318198\n",
      "Iteration 89, Loss: 2145882325.601862\n",
      "Iteration 90, Loss: 2355852495.4815693\n",
      "Iteration 91, Loss: 2552918053.337781\n",
      "Iteration 92, Loss: 5163424463.413274\n",
      "Iteration 93, Loss: 31751726000.515064\n",
      "Iteration 94, Loss: 3285337644.7823977\n",
      "Iteration 95, Loss: 2502983560.1880336\n",
      "Iteration 96, Loss: 1796348266.8524714\n",
      "Iteration 97, Loss: 1731946611.6818824\n",
      "Iteration 98, Loss: 1850625725.0749693\n",
      "Iteration 99, Loss: 1989745101.196895\n",
      "Iteration 100, Loss: 2126673258.6694255\n",
      "Iteration 101, Loss: 2307852790.3496923\n",
      "Iteration 102, Loss: 2537363632.4298506\n",
      "Iteration 103, Loss: 2323146778.5060897\n",
      "Iteration 104, Loss: 2462941849.79446\n",
      "Iteration 105, Loss: 2729213039.2053523\n",
      "Iteration 106, Loss: 2563747414.898575\n",
      "Iteration 107, Loss: 2802449342.8585334\n",
      "Iteration 108, Loss: 2982269415.1078734\n",
      "Iteration 109, Loss: 3165175551.9024076\n",
      "Iteration 110, Loss: 1713671034.2881398\n",
      "Iteration 111, Loss: 1676080002.5493932\n",
      "Iteration 112, Loss: 1706226891.8038485\n",
      "Iteration 113, Loss: 1676559061.3581104\n",
      "Iteration 114, Loss: 2048865786.32295\n",
      "Iteration 115, Loss: 1699440531.7757814\n",
      "Iteration 116, Loss: 1813736559.4994853\n",
      "Iteration 117, Loss: 1723976515.8172388\n",
      "Iteration 118, Loss: 1846787407.7702303\n",
      "Iteration 119, Loss: 1814611876.233535\n",
      "Iteration 120, Loss: 1758463716.7958739\n",
      "Iteration 121, Loss: 1676435232.1883085\n",
      "Iteration 122, Loss: 1824451081.8685017\n",
      "Iteration 123, Loss: 1711836326.217743\n",
      "Iteration 124, Loss: 1795256426.5897384\n",
      "Iteration 125, Loss: 2119814506.1386745\n",
      "Iteration 126, Loss: 2284098571.393919\n",
      "Iteration 127, Loss: 2316169171.25727\n",
      "Iteration 128, Loss: 2475060429.6078825\n",
      "Iteration 129, Loss: 2343778781.7265167\n",
      "Iteration 130, Loss: 2297403177.6193423\n",
      "Iteration 131, Loss: 2083224085.1309617\n",
      "Iteration 132, Loss: 1855050994.5241666\n",
      "Iteration 133, Loss: 1845579884.5270922\n",
      "Iteration 134, Loss: 1955522367.596843\n",
      "Iteration 135, Loss: 2056630183.733116\n",
      "Iteration 136, Loss: 2740478837.428238\n",
      "Iteration 137, Loss: 2104044486.5735614\n",
      "Iteration 138, Loss: 1723328667.9861858\n",
      "Iteration 139, Loss: 1784937288.985676\n",
      "Iteration 140, Loss: 1982067388.236708\n",
      "Iteration 141, Loss: 1838331263.89865\n",
      "Iteration 142, Loss: 1937207460.8863668\n",
      "Iteration 143, Loss: 1845575150.4559207\n",
      "Iteration 144, Loss: 1709265129.7629738\n",
      "Iteration 145, Loss: 1642320166.6568716\n",
      "Iteration 146, Loss: 1849389844.0680802\n",
      "Iteration 147, Loss: 2160657078.048325\n",
      "Iteration 148, Loss: 2405209482.87739\n",
      "Iteration 149, Loss: 2306554148.7392774\n",
      "Iteration 150, Loss: 1622207151.1971886\n",
      "Iteration 151, Loss: 1830264593.4203103\n",
      "Iteration 152, Loss: 2140216951.6815448\n",
      "Iteration 153, Loss: 2311952625.4623375\n",
      "Iteration 154, Loss: 2294010378.664538\n",
      "Iteration 155, Loss: 1711941517.8807173\n",
      "Iteration 156, Loss: 1621355798.4733014\n",
      "Iteration 157, Loss: 1675621315.0321956\n",
      "Iteration 158, Loss: 1665299348.7738936\n",
      "Iteration 159, Loss: 1747725884.3512921\n",
      "Iteration 160, Loss: 1877135282.528462\n",
      "Iteration 161, Loss: 1993224820.6359246\n",
      "Iteration 162, Loss: 2237039939.262897\n",
      "Iteration 163, Loss: 1893364122.3992987\n",
      "Iteration 164, Loss: 1712366270.4107397\n",
      "Iteration 165, Loss: 1636454121.125446\n",
      "Iteration 166, Loss: 1729328327.4286375\n",
      "Iteration 167, Loss: 1842269533.53112\n",
      "Iteration 168, Loss: 1786838674.670882\n",
      "Iteration 169, Loss: 1876473969.539733\n",
      "Iteration 170, Loss: 2031299955.6500123\n",
      "Iteration 171, Loss: 2146694367.3357654\n",
      "Iteration 172, Loss: 2268706970.9514685\n",
      "Iteration 173, Loss: 2426433933.465081\n",
      "Iteration 174, Loss: 1649046652.3354497\n",
      "Iteration 175, Loss: 1751326536.995616\n",
      "Iteration 176, Loss: 1872484506.0707998\n",
      "Iteration 177, Loss: 1669390957.7261405\n",
      "Iteration 178, Loss: 1891267570.9925382\n",
      "Iteration 179, Loss: 2015886263.937777\n",
      "Iteration 180, Loss: 1756735058.3173988\n",
      "Iteration 181, Loss: 1632496502.615567\n",
      "Iteration 182, Loss: 1730683946.1268973\n",
      "Iteration 183, Loss: 1685336822.2153003\n",
      "Iteration 184, Loss: 1656024194.152624\n",
      "Iteration 185, Loss: 1566706629.2289548\n",
      "Iteration 186, Loss: 1605476134.2621927\n",
      "Iteration 187, Loss: 1561618380.4446182\n",
      "Iteration 188, Loss: 1584671211.7624958\n",
      "Iteration 189, Loss: 1847077783.2210135\n",
      "Iteration 190, Loss: 2115991937.8138049\n",
      "Iteration 191, Loss: 1822584534.298785\n",
      "Iteration 192, Loss: 1576940911.5713484\n",
      "Iteration 193, Loss: 1742740388.5280743\n",
      "Iteration 194, Loss: 1561046722.7196689\n",
      "Iteration 195, Loss: 1703290168.703509\n",
      "Iteration 196, Loss: 1791454488.8032568\n",
      "Iteration 197, Loss: 1929470656.1311274\n",
      "Iteration 198, Loss: 1547814235.5904672\n",
      "Iteration 199, Loss: 1561768156.79279\n",
      "Iteration 200, Loss: 1852549957.3192031\n",
      "Iteration 201, Loss: 1946034032.4292786\n",
      "Iteration 202, Loss: 2153488401.4773374\n",
      "Iteration 203, Loss: 2249292061.199736\n",
      "Iteration 204, Loss: 2425607391.3427076\n",
      "Iteration 205, Loss: 2481562547.8918395\n",
      "Iteration 206, Loss: 2603320389.359191\n",
      "Iteration 207, Loss: 2721103677.302571\n",
      "Iteration 208, Loss: 2560284430.828959\n",
      "Iteration 209, Loss: 2571560991.554509\n",
      "Iteration 210, Loss: 1609061104.1849365\n",
      "Iteration 211, Loss: 1763248115.594854\n",
      "Iteration 212, Loss: 1592959969.973765\n",
      "Iteration 213, Loss: 1781001193.595456\n",
      "Iteration 214, Loss: 2002389109.906154\n",
      "Iteration 215, Loss: 2137488533.0507193\n",
      "Iteration 216, Loss: 2301170719.545208\n",
      "Iteration 217, Loss: 2401996325.683838\n",
      "Iteration 218, Loss: 1942619629.2532394\n",
      "Iteration 219, Loss: 1945314664.2677445\n",
      "Iteration 220, Loss: 1553019371.4994588\n",
      "Iteration 221, Loss: 1619212263.4816737\n",
      "Iteration 222, Loss: 1545044085.4639373\n",
      "Iteration 223, Loss: 2041819743.4173303\n",
      "Iteration 224, Loss: 2071208451.6295397\n",
      "Iteration 225, Loss: 2857870776.9041524\n",
      "Iteration 226, Loss: 8481807377.940632\n",
      "Iteration 227, Loss: 1536477131.255351\n",
      "Iteration 228, Loss: 1730356714.7496147\n",
      "Iteration 229, Loss: 1678327841.3191557\n",
      "Iteration 230, Loss: 1904031811.3283222\n",
      "Iteration 231, Loss: 2036750985.8948715\n",
      "Iteration 232, Loss: 2068870043.8349895\n",
      "Iteration 233, Loss: 2202556706.235205\n",
      "Iteration 234, Loss: 1559564020.022432\n",
      "Iteration 235, Loss: 1558653173.163586\n",
      "Iteration 236, Loss: 1810537606.880672\n",
      "Iteration 237, Loss: 1656235615.6974847\n",
      "Iteration 238, Loss: 1555501589.5495336\n",
      "Iteration 239, Loss: 1584003579.4395182\n",
      "Iteration 240, Loss: 1768807419.888832\n",
      "Iteration 241, Loss: 1818982070.016657\n",
      "Iteration 242, Loss: 1853504550.8954394\n",
      "Iteration 243, Loss: 2065380349.9005086\n",
      "Iteration 244, Loss: 2151936859.7347045\n",
      "Iteration 245, Loss: 1545537068.1263049\n",
      "Iteration 246, Loss: 1519034423.5154552\n",
      "Iteration 247, Loss: 1631088472.0587373\n",
      "Iteration 248, Loss: 1901009038.984621\n",
      "Iteration 249, Loss: 1508924142.7806134\n",
      "Iteration 250, Loss: 1625646208.661451\n",
      "Iteration 251, Loss: 1872854295.5194125\n",
      "Iteration 252, Loss: 1562292205.7436554\n",
      "Iteration 253, Loss: 1608209253.8478909\n",
      "Iteration 254, Loss: 1779525154.0910428\n",
      "Iteration 255, Loss: 2027949647.4290383\n",
      "Iteration 256, Loss: 2195729384.873487\n",
      "Iteration 257, Loss: 2255172405.1573305\n",
      "Iteration 258, Loss: 2339264433.8525114\n",
      "Iteration 259, Loss: 3871842475.414825\n",
      "Iteration 260, Loss: 1679400608.9127412\n",
      "Iteration 261, Loss: 1475926071.3023653\n",
      "Iteration 262, Loss: 1559132966.3440332\n",
      "Iteration 263, Loss: 1697560780.2002504\n",
      "Iteration 264, Loss: 1934667301.2353852\n",
      "Iteration 265, Loss: 2047544235.8574936\n",
      "Iteration 266, Loss: 1858617611.1577287\n",
      "Iteration 267, Loss: 2141500626.3599732\n",
      "Iteration 268, Loss: 2425648832.5347314\n",
      "Iteration 269, Loss: 2204128081.8710203\n",
      "Iteration 270, Loss: 3398519373.902087\n",
      "Iteration 271, Loss: 2154032769.7478733\n",
      "Iteration 272, Loss: 2065975614.460564\n",
      "Iteration 273, Loss: 1716681110.4674017\n",
      "Iteration 274, Loss: 1677364122.036394\n",
      "Iteration 275, Loss: 1868920916.0301948\n",
      "Iteration 276, Loss: 1462579884.5496008\n",
      "Iteration 277, Loss: 1520459022.2716165\n",
      "Iteration 278, Loss: 1604229185.7798202\n",
      "Iteration 279, Loss: 1545975941.5130568\n",
      "Iteration 280, Loss: 1698649653.5061185\n",
      "Iteration 281, Loss: 1788680948.6126432\n",
      "Iteration 282, Loss: 1896449501.1282587\n",
      "Iteration 283, Loss: 1974920525.9964037\n",
      "Iteration 284, Loss: 1448937320.1773484\n",
      "Iteration 285, Loss: 1916541100.7045166\n",
      "Iteration 286, Loss: 2083716546.2810075\n",
      "Iteration 287, Loss: 2229113638.357041\n",
      "Iteration 288, Loss: 1570974154.8884437\n",
      "Iteration 289, Loss: 1938644026.508933\n",
      "Iteration 290, Loss: 1804476886.9992597\n",
      "Iteration 291, Loss: 1482940817.2247732\n",
      "Iteration 292, Loss: 1539618732.0122576\n",
      "Iteration 293, Loss: 1775134494.209744\n",
      "Iteration 294, Loss: 1665521565.1408277\n",
      "Iteration 295, Loss: 1507814545.9864106\n",
      "Iteration 296, Loss: 2094209601.4626162\n",
      "Iteration 297, Loss: 2220336555.7553887\n",
      "Iteration 298, Loss: 2290459744.563831\n",
      "Iteration 299, Loss: 1821537349.7898946\n",
      "Iteration 300, Loss: 1626726847.9717271\n",
      "Iteration 301, Loss: 1718285786.1540585\n",
      "Iteration 302, Loss: 1778063633.3434067\n",
      "Iteration 303, Loss: 1876208552.262501\n",
      "Iteration 304, Loss: 1495792776.5278735\n",
      "Iteration 305, Loss: 1524691374.3920517\n",
      "Iteration 306, Loss: 1766719101.818997\n",
      "Iteration 307, Loss: 1527989263.6297798\n",
      "Iteration 308, Loss: 1432536972.2864149\n",
      "Iteration 309, Loss: 1544229090.9928427\n",
      "Iteration 310, Loss: 1499593447.3627787\n",
      "Iteration 311, Loss: 1423711738.9359524\n",
      "Iteration 312, Loss: 1499106612.3327036\n",
      "Iteration 313, Loss: 1557893873.5696867\n",
      "Iteration 314, Loss: 1675046541.114243\n",
      "Iteration 315, Loss: 1452102217.0441475\n",
      "Iteration 316, Loss: 1422556305.2544231\n",
      "Iteration 317, Loss: 1587480657.036608\n",
      "Iteration 318, Loss: 1589155125.6176136\n",
      "Iteration 319, Loss: 1817737561.4622161\n",
      "Iteration 320, Loss: 1725675719.6315727\n",
      "Iteration 321, Loss: 1586869615.1830893\n",
      "Iteration 322, Loss: 2196608794.60016\n",
      "Iteration 323, Loss: 2266876283.6771183\n",
      "Iteration 324, Loss: 2359279457.8359394\n",
      "Iteration 325, Loss: 1954173962.7720318\n",
      "Iteration 326, Loss: 2035868266.3726752\n",
      "Iteration 327, Loss: 2216730947.6531215\n",
      "Iteration 328, Loss: 2314186244.6954174\n",
      "Iteration 329, Loss: 2020304302.528366\n",
      "Iteration 330, Loss: 2083052797.7499526\n",
      "Iteration 331, Loss: 2799065890.64035\n",
      "Iteration 332, Loss: 5608407866.634805\n",
      "Iteration 333, Loss: 1970509550.271299\n",
      "Iteration 334, Loss: 5514030304.1053915\n",
      "Iteration 335, Loss: 5461379471.635851\n",
      "Iteration 336, Loss: 5435030881.595777\n",
      "Iteration 337, Loss: 1393532319.39688\n",
      "Iteration 338, Loss: 1381790976.1214027\n",
      "Iteration 339, Loss: 1398639235.9356656\n",
      "Iteration 340, Loss: 1434776772.1278005\n",
      "Iteration 341, Loss: 1483842627.096312\n",
      "Iteration 342, Loss: 1516459346.203465\n",
      "Iteration 343, Loss: 1658430775.2030516\n",
      "Iteration 344, Loss: 1411421924.6697483\n",
      "Iteration 345, Loss: 1469576205.8345375\n",
      "Iteration 346, Loss: 1518755008.5377476\n",
      "Iteration 347, Loss: 1578550127.6560545\n",
      "Iteration 348, Loss: 1591116256.125148\n",
      "Iteration 349, Loss: 1719012890.070898\n",
      "Iteration 350, Loss: 1794912693.7332606\n",
      "Iteration 351, Loss: 1729897476.20719\n",
      "Iteration 352, Loss: 1470372902.2336884\n",
      "Iteration 353, Loss: 1373015506.5547462\n",
      "Iteration 354, Loss: 1394070979.1003983\n",
      "Iteration 355, Loss: 1703502689.1734807\n",
      "Iteration 356, Loss: 1737316838.1550264\n",
      "Iteration 357, Loss: 1471021666.3728852\n",
      "Iteration 358, Loss: 1684739267.6258273\n",
      "Iteration 359, Loss: 1369315019.9867904\n",
      "Iteration 360, Loss: 1376777379.7958007\n",
      "Iteration 361, Loss: 1413784046.2655041\n",
      "Iteration 362, Loss: 1441425575.299243\n",
      "Iteration 363, Loss: 1606407678.5363045\n",
      "Iteration 364, Loss: 1757599190.025241\n",
      "Iteration 365, Loss: 1854113880.0610483\n",
      "Iteration 366, Loss: 1837375770.0478497\n",
      "Iteration 367, Loss: 1902660738.9875302\n",
      "Iteration 368, Loss: 2034585269.709768\n",
      "Iteration 369, Loss: 2112107665.3164384\n",
      "Iteration 370, Loss: 1937470165.9556246\n",
      "Iteration 371, Loss: 1397636782.6638541\n",
      "Iteration 372, Loss: 1446816576.9200077\n",
      "Iteration 373, Loss: 1374331372.3326151\n",
      "Iteration 374, Loss: 1569453918.8809416\n",
      "Iteration 375, Loss: 1662092335.102875\n",
      "Iteration 376, Loss: 1378811900.3390646\n",
      "Iteration 377, Loss: 1364736691.2353342\n",
      "Iteration 378, Loss: 1364130422.0302327\n",
      "Iteration 379, Loss: 1453972438.5286186\n",
      "Iteration 380, Loss: 1430918543.419492\n",
      "Iteration 381, Loss: 1473658236.6534421\n",
      "Iteration 382, Loss: 1527201176.6823528\n",
      "Iteration 383, Loss: 1561390166.4389036\n",
      "Iteration 384, Loss: 1371189930.9373949\n",
      "Iteration 385, Loss: 1575499604.0888515\n",
      "Iteration 386, Loss: 1489784894.4249167\n",
      "Iteration 387, Loss: 1543741384.1151845\n",
      "Iteration 388, Loss: 1355080996.8637671\n",
      "Iteration 389, Loss: 1893688257.696972\n",
      "Iteration 390, Loss: 1941002857.5192404\n",
      "Iteration 391, Loss: 1372127118.3189273\n",
      "Iteration 392, Loss: 1461731949.7995877\n",
      "Iteration 393, Loss: 1445599950.6103122\n",
      "Iteration 394, Loss: 1557754256.6381216\n",
      "Iteration 395, Loss: 1558998367.4785879\n",
      "Iteration 396, Loss: 1355464886.6148372\n",
      "Iteration 397, Loss: 1809616329.2850854\n",
      "Iteration 398, Loss: 1515135689.1162791\n",
      "Iteration 399, Loss: 1513330641.3998096\n",
      "Iteration 400, Loss: 1674756199.2952983\n",
      "Iteration 401, Loss: 1834412803.3430905\n",
      "Iteration 402, Loss: 1887709963.0535545\n",
      "Iteration 403, Loss: 1933995405.5150166\n",
      "Iteration 404, Loss: 1631695773.9558725\n",
      "Iteration 405, Loss: 5285143162.645079\n",
      "Iteration 406, Loss: 3434111996.0550966\n",
      "Iteration 407, Loss: 1820366746.684797\n",
      "Iteration 408, Loss: 1606984686.4317791\n",
      "Iteration 409, Loss: 1669973774.2246742\n",
      "Iteration 410, Loss: 1371075960.958099\n",
      "Iteration 411, Loss: 1469483350.4997919\n",
      "Iteration 412, Loss: 1537079787.8536181\n",
      "Iteration 413, Loss: 1690598949.4312875\n",
      "Iteration 414, Loss: 1433846066.774636\n",
      "Iteration 415, Loss: 1460120576.9740047\n",
      "Iteration 416, Loss: 1434680457.9737625\n",
      "Iteration 417, Loss: 1335093662.501656\n",
      "Iteration 418, Loss: 1785683814.3456523\n",
      "Iteration 419, Loss: 1639070880.8646479\n",
      "Iteration 420, Loss: 1375319778.5329118\n",
      "Iteration 421, Loss: 1621126678.2794583\n",
      "Iteration 422, Loss: 1644782899.1699522\n",
      "Iteration 423, Loss: 1351274176.55411\n",
      "Iteration 424, Loss: 1494284266.3955095\n",
      "Iteration 425, Loss: 1590502589.7732425\n",
      "Iteration 426, Loss: 1751094756.434079\n",
      "Iteration 427, Loss: 1323413638.3414795\n",
      "Iteration 428, Loss: 1632589131.654814\n",
      "Iteration 429, Loss: 1356617462.7159743\n",
      "Iteration 430, Loss: 1577723277.0216527\n",
      "Iteration 431, Loss: 1670604085.534701\n",
      "Iteration 432, Loss: 1817569156.29051\n",
      "Iteration 433, Loss: 1850064557.6384387\n",
      "Iteration 434, Loss: 1939802767.9870164\n",
      "Iteration 435, Loss: 1331403352.3322756\n",
      "Iteration 436, Loss: 1863401552.70608\n",
      "Iteration 437, Loss: 1332107748.6369112\n",
      "Iteration 438, Loss: 1526164384.2959518\n",
      "Iteration 439, Loss: 1646541305.4826596\n",
      "Iteration 440, Loss: 1470051389.2978208\n",
      "Iteration 441, Loss: 1448636069.2500682\n",
      "Iteration 442, Loss: 1645355305.0665593\n",
      "Iteration 443, Loss: 1660727859.4318452\n",
      "Iteration 444, Loss: 1377994925.416593\n",
      "Iteration 445, Loss: 1583140439.0414984\n",
      "Iteration 446, Loss: 1388451732.8713298\n",
      "Iteration 447, Loss: 1769033253.527607\n",
      "Iteration 448, Loss: 1406342631.987512\n",
      "Iteration 449, Loss: 1313783737.2231252\n",
      "Iteration 450, Loss: 1334228885.5626144\n",
      "Iteration 451, Loss: 1446433156.030185\n",
      "Iteration 452, Loss: 1447669645.3157325\n",
      "Iteration 453, Loss: 1417707227.2961352\n",
      "Iteration 454, Loss: 1530453912.359969\n",
      "Iteration 455, Loss: 1464714800.6650953\n",
      "Iteration 456, Loss: 1654588584.053723\n",
      "Iteration 457, Loss: 1805892561.958743\n",
      "Iteration 458, Loss: 1968134064.0080192\n",
      "Iteration 459, Loss: 1488074251.4623947\n",
      "Iteration 460, Loss: 4814057988.024047\n",
      "Iteration 461, Loss: 4163372711.1710205\n",
      "Iteration 462, Loss: 3985445873.1825414\n",
      "Iteration 463, Loss: 2648096318.0956593\n",
      "Iteration 464, Loss: 1310018480.967863\n",
      "Iteration 465, Loss: 1313202464.6740344\n",
      "Iteration 466, Loss: 1473812374.6341531\n",
      "Iteration 467, Loss: 1591769050.832712\n",
      "Iteration 468, Loss: 1438918971.1328049\n",
      "Iteration 469, Loss: 1406756257.7774327\n",
      "Iteration 470, Loss: 1458970502.0864766\n",
      "Iteration 471, Loss: 1630384750.0908718\n",
      "Iteration 472, Loss: 1422984329.9670172\n",
      "Iteration 473, Loss: 1476346247.5312052\n",
      "Iteration 474, Loss: 1552881927.4691253\n",
      "Iteration 475, Loss: 1703051891.3382068\n",
      "Iteration 476, Loss: 1846626085.383419\n",
      "Iteration 477, Loss: 1877879415.3493438\n",
      "Iteration 478, Loss: 1903610601.1629488\n",
      "Iteration 479, Loss: 1459108791.540729\n",
      "Iteration 480, Loss: 1483772757.963938\n",
      "Iteration 481, Loss: 1319705303.44256\n",
      "Iteration 482, Loss: 1385257525.5279841\n",
      "Iteration 483, Loss: 1537381353.3903756\n",
      "Iteration 484, Loss: 1328147803.9841626\n",
      "Iteration 485, Loss: 1581496578.938718\n",
      "Iteration 486, Loss: 1418064625.9577298\n",
      "Iteration 487, Loss: 1325670114.9181027\n",
      "Iteration 488, Loss: 1308196910.1477304\n",
      "Iteration 489, Loss: 1322656383.3118126\n",
      "Iteration 490, Loss: 1601317788.8059204\n",
      "Iteration 491, Loss: 1653532015.981996\n",
      "Iteration 492, Loss: 1288435430.3680048\n",
      "Iteration 493, Loss: 1704993965.1970549\n",
      "Iteration 494, Loss: 1800506668.3981626\n",
      "Iteration 495, Loss: 1871963125.0820422\n",
      "Iteration 496, Loss: 1831578762.254755\n",
      "Iteration 497, Loss: 1890717577.1023562\n",
      "Iteration 498, Loss: 1563823355.8680592\n",
      "Iteration 499, Loss: 1971600759.971235\n",
      "Iteration 500, Loss: 1970781268.3442957\n",
      "Iteration 501, Loss: 1997647538.7347212\n",
      "Iteration 502, Loss: 1289594648.350147\n",
      "Iteration 503, Loss: 1350159007.8030605\n",
      "Iteration 504, Loss: 1537109174.9152458\n",
      "Iteration 505, Loss: 1297341624.5284202\n",
      "Iteration 506, Loss: 1325816409.340333\n",
      "Iteration 507, Loss: 1435962483.0251448\n",
      "Iteration 508, Loss: 1464034636.2612493\n",
      "Iteration 509, Loss: 1484745607.042567\n",
      "Iteration 510, Loss: 1652857127.5624695\n",
      "Iteration 511, Loss: 1307010569.077149\n",
      "Iteration 512, Loss: 1332240344.0593257\n",
      "Iteration 513, Loss: 1360900143.5686927\n",
      "Iteration 514, Loss: 1460722507.0246675\n",
      "Iteration 515, Loss: 1362317592.6000934\n",
      "Iteration 516, Loss: 1547211390.9871647\n",
      "Iteration 517, Loss: 1555399661.7165504\n",
      "Iteration 518, Loss: 1482514466.1590183\n",
      "Iteration 519, Loss: 1503979529.6067972\n",
      "Iteration 520, Loss: 1392418982.3038027\n",
      "Iteration 521, Loss: 1331108847.7758777\n",
      "Iteration 522, Loss: 1427316006.9459772\n",
      "Iteration 523, Loss: 1478084425.9306495\n",
      "Iteration 524, Loss: 1296948459.2948556\n",
      "Iteration 525, Loss: 1538423955.2460256\n",
      "Iteration 526, Loss: 1639393058.3406296\n",
      "Iteration 527, Loss: 1668294053.8285396\n",
      "Iteration 528, Loss: 1776132484.1828942\n",
      "Iteration 529, Loss: 1799480789.8361228\n",
      "Iteration 530, Loss: 1867353519.287761\n",
      "Iteration 531, Loss: 1268233930.6690154\n",
      "Iteration 532, Loss: 1272783098.4087076\n",
      "Iteration 533, Loss: 1298069600.7800798\n",
      "Iteration 534, Loss: 1399397909.5758317\n",
      "Iteration 535, Loss: 1263299365.6530166\n",
      "Iteration 536, Loss: 1265997739.4187524\n",
      "Iteration 537, Loss: 2236207027.892606\n",
      "Iteration 538, Loss: 2244093151.4843297\n",
      "Iteration 539, Loss: 2369577860.2330704\n",
      "Iteration 540, Loss: 2660241128.0536375\n",
      "Iteration 541, Loss: 2361360573.731074\n",
      "Iteration 542, Loss: 1308892577.060962\n",
      "Iteration 543, Loss: 1273689267.8231215\n",
      "Iteration 544, Loss: 1245552916.7443168\n",
      "Iteration 545, Loss: 1249094892.0739646\n",
      "Iteration 546, Loss: 1539267188.019732\n",
      "Iteration 547, Loss: 1578981908.593165\n",
      "Iteration 548, Loss: 1618700352.2738922\n",
      "Iteration 549, Loss: 1735519753.284797\n",
      "Iteration 550, Loss: 1263246833.9617252\n",
      "Iteration 551, Loss: 1576356549.94963\n",
      "Iteration 552, Loss: 1288451480.9944456\n",
      "Iteration 553, Loss: 1482405771.7208936\n",
      "Iteration 554, Loss: 1515220408.57957\n",
      "Iteration 555, Loss: 1277993260.3041184\n",
      "Iteration 556, Loss: 1288416303.2165084\n",
      "Iteration 557, Loss: 1481256046.1675403\n",
      "Iteration 558, Loss: 1512960829.2390862\n",
      "Iteration 559, Loss: 1589088559.9585314\n",
      "Iteration 560, Loss: 1627338296.2397048\n",
      "Iteration 561, Loss: 1292149536.6012216\n",
      "Iteration 562, Loss: 1318307762.17697\n",
      "Iteration 563, Loss: 1252299276.6473393\n",
      "Iteration 564, Loss: 1586493324.6572723\n",
      "Iteration 565, Loss: 1702870043.4963136\n",
      "Iteration 566, Loss: 1322630359.801198\n",
      "Iteration 567, Loss: 1351794294.348754\n",
      "Iteration 568, Loss: 1354963969.369011\n",
      "Iteration 569, Loss: 1336991175.7134116\n",
      "Iteration 570, Loss: 1373217214.3575864\n",
      "Iteration 571, Loss: 1443751294.9938228\n",
      "Iteration 572, Loss: 1513246769.866629\n",
      "Iteration 573, Loss: 1640546917.257699\n",
      "Iteration 574, Loss: 1322943315.2661703\n",
      "Iteration 575, Loss: 1351837129.4193344\n",
      "Iteration 576, Loss: 1517107039.7702074\n",
      "Iteration 577, Loss: 1634215266.8980072\n",
      "Iteration 578, Loss: 1376405803.6295266\n",
      "Iteration 579, Loss: 1304928985.8412728\n",
      "Iteration 580, Loss: 1223511602.8080306\n",
      "Iteration 581, Loss: 1219269524.4510298\n",
      "Iteration 582, Loss: 1268750233.0579684\n",
      "Iteration 583, Loss: 1368263609.4675722\n",
      "Iteration 584, Loss: 1524881574.4148283\n",
      "Iteration 585, Loss: 1507735524.8218012\n",
      "Iteration 586, Loss: 1353653116.9802234\n",
      "Iteration 587, Loss: 1653422284.8345723\n",
      "Iteration 588, Loss: 1659167125.7233377\n",
      "Iteration 589, Loss: 1525451364.6392343\n",
      "Iteration 590, Loss: 1558515893.57506\n",
      "Iteration 591, Loss: 1381040190.6426096\n",
      "Iteration 592, Loss: 2061928614.8839424\n",
      "Iteration 593, Loss: 1452199400.050356\n",
      "Iteration 594, Loss: 1245561205.6021924\n",
      "Iteration 595, Loss: 1448457676.152389\n",
      "Iteration 596, Loss: 1452268095.5942774\n",
      "Iteration 597, Loss: 1293285364.9020214\n",
      "Iteration 598, Loss: 1317816144.769894\n",
      "Iteration 599, Loss: 1308848064.9160268\n",
      "Iteration 600, Loss: 1325869836.194789\n",
      "Iteration 601, Loss: 1409014055.997082\n",
      "Iteration 602, Loss: 1276895972.6064165\n",
      "Iteration 603, Loss: 1492639005.6626387\n",
      "Iteration 604, Loss: 1321434267.0071337\n",
      "Iteration 605, Loss: 1396984967.6365604\n",
      "Iteration 606, Loss: 1306134157.570057\n",
      "Iteration 607, Loss: 1475590012.974699\n",
      "Iteration 608, Loss: 1358462319.981431\n",
      "Iteration 609, Loss: 1366575539.2903721\n",
      "Iteration 610, Loss: 1415519754.856926\n",
      "Iteration 611, Loss: 1273199555.800928\n",
      "Iteration 612, Loss: 1366676388.2313612\n",
      "Iteration 613, Loss: 1270972546.076708\n",
      "Iteration 614, Loss: 1284389554.5297425\n",
      "Iteration 615, Loss: 1450752098.6365778\n",
      "Iteration 616, Loss: 1518989767.613382\n",
      "Iteration 617, Loss: 1335749061.2504985\n",
      "Iteration 618, Loss: 1344561527.3891232\n",
      "Iteration 619, Loss: 1319659206.2963285\n",
      "Iteration 620, Loss: 1509984078.9299662\n",
      "Iteration 621, Loss: 1335977594.6807048\n",
      "Iteration 622, Loss: 1224252772.0062265\n",
      "Iteration 623, Loss: 1239309883.7014346\n",
      "Iteration 624, Loss: 1393807832.6920369\n",
      "Iteration 625, Loss: 1503922237.7558515\n",
      "Iteration 626, Loss: 1522343993.2948375\n",
      "Iteration 627, Loss: 1538571281.288442\n",
      "Iteration 628, Loss: 1267840049.556869\n",
      "Iteration 629, Loss: 1226077412.645996\n",
      "Iteration 630, Loss: 1200741596.3811123\n",
      "Iteration 631, Loss: 1508715789.5190663\n",
      "Iteration 632, Loss: 1424593978.1497717\n",
      "Iteration 633, Loss: 1298773692.3585901\n",
      "Iteration 634, Loss: 1334125334.9764738\n",
      "Iteration 635, Loss: 1324174131.0874295\n",
      "Iteration 636, Loss: 1354765575.0284266\n",
      "Iteration 637, Loss: 1431767161.92544\n",
      "Iteration 638, Loss: 1381128857.675795\n",
      "Iteration 639, Loss: 1282433919.2264419\n",
      "Iteration 640, Loss: 1293627367.1285613\n",
      "Iteration 641, Loss: 1317254791.865057\n",
      "Iteration 642, Loss: 1341939796.1516235\n",
      "Iteration 643, Loss: 1349201032.9655616\n",
      "Iteration 644, Loss: 1319777793.0709763\n",
      "Iteration 645, Loss: 1370619505.8864748\n",
      "Iteration 646, Loss: 1312039967.4383895\n",
      "Iteration 647, Loss: 1463492519.9755194\n",
      "Iteration 648, Loss: 1395059336.7597377\n",
      "Iteration 649, Loss: 1276785286.7220016\n",
      "Iteration 650, Loss: 1190994234.9946306\n",
      "Iteration 651, Loss: 1323113640.9969485\n",
      "Iteration 652, Loss: 1312228463.6994977\n",
      "Iteration 653, Loss: 1187294794.430734\n",
      "Iteration 654, Loss: 1197643030.8973086\n",
      "Iteration 655, Loss: 1263925872.886729\n",
      "Iteration 656, Loss: 1274830304.6708477\n",
      "Iteration 657, Loss: 1297451161.0500383\n",
      "Iteration 658, Loss: 1231934258.9347143\n",
      "Iteration 659, Loss: 1496092448.5188856\n",
      "Iteration 660, Loss: 1272458641.4076145\n",
      "Iteration 661, Loss: 1186695924.5471447\n",
      "Iteration 662, Loss: 3094326390.2394085\n",
      "Iteration 663, Loss: 1927332196.146962\n",
      "Iteration 664, Loss: 1677858641.5125668\n",
      "Iteration 665, Loss: 1269866250.6515296\n",
      "Iteration 666, Loss: 1426755473.0952704\n",
      "Iteration 667, Loss: 1490036369.0508022\n",
      "Iteration 668, Loss: 1314163298.9193037\n",
      "Iteration 669, Loss: 1460431052.9084663\n",
      "Iteration 670, Loss: 1557799801.5540593\n",
      "Iteration 671, Loss: 1608540449.2028491\n",
      "Iteration 672, Loss: 1232399691.8642032\n",
      "Iteration 673, Loss: 1195695652.3419564\n",
      "Iteration 674, Loss: 1375715799.0005736\n",
      "Iteration 675, Loss: 1505929730.6872048\n",
      "Iteration 676, Loss: 1240778512.2426848\n",
      "Iteration 677, Loss: 1874565901.8319256\n",
      "Iteration 678, Loss: 1848218824.7397444\n",
      "Iteration 679, Loss: 1226983531.5997758\n",
      "Iteration 680, Loss: 1428434853.3564584\n",
      "Iteration 681, Loss: 1451555378.8554666\n",
      "Iteration 682, Loss: 1516238599.4877038\n",
      "Iteration 683, Loss: 1300919021.8476155\n",
      "Iteration 684, Loss: 1446091944.3924673\n",
      "Iteration 685, Loss: 1515063733.345735\n",
      "Iteration 686, Loss: 1596847527.2772515\n",
      "Iteration 687, Loss: 1243550292.235611\n",
      "Iteration 688, Loss: 1560713962.9435656\n",
      "Iteration 689, Loss: 1558185794.4813287\n",
      "Iteration 690, Loss: 1435238901.4989743\n",
      "Iteration 691, Loss: 1506053268.5295722\n",
      "Iteration 692, Loss: 1595943173.5391707\n",
      "Iteration 693, Loss: 1270258212.68174\n",
      "Iteration 694, Loss: 1313171434.925293\n",
      "Iteration 695, Loss: 1188711001.130959\n",
      "Iteration 696, Loss: 1366692820.7303004\n",
      "Iteration 697, Loss: 1239391064.0595675\n",
      "Iteration 698, Loss: 1393016999.5120053\n",
      "Iteration 699, Loss: 1376423741.0368605\n",
      "Iteration 700, Loss: 1383686907.562141\n",
      "Iteration 701, Loss: 1346333410.9457345\n",
      "Iteration 702, Loss: 1256218562.7515807\n",
      "Iteration 703, Loss: 1291935891.6195376\n",
      "Iteration 704, Loss: 1468216618.5210032\n",
      "Iteration 705, Loss: 1562938011.1573148\n",
      "Iteration 706, Loss: 1305083206.056746\n",
      "Iteration 707, Loss: 1251880023.7650235\n",
      "Iteration 708, Loss: 1314401205.6679897\n",
      "Iteration 709, Loss: 1470142220.413002\n",
      "Iteration 710, Loss: 1209379446.9820962\n",
      "Iteration 711, Loss: 1337575633.4049726\n",
      "Iteration 712, Loss: 1174721741.7380555\n",
      "Iteration 713, Loss: 1296464439.8086336\n",
      "Iteration 714, Loss: 1320041360.684574\n",
      "Iteration 715, Loss: 1456425966.5819356\n",
      "Iteration 716, Loss: 1309886123.6048229\n",
      "Iteration 717, Loss: 1265248273.2722487\n",
      "Iteration 718, Loss: 1259482487.2980964\n",
      "Iteration 719, Loss: 1444305357.877651\n",
      "Iteration 720, Loss: 1204962778.2222073\n",
      "Iteration 721, Loss: 1524595803.8798635\n",
      "Iteration 722, Loss: 1569005299.5712945\n",
      "Iteration 723, Loss: 1497661259.8850892\n",
      "Iteration 724, Loss: 1499805928.3903406\n",
      "Iteration 725, Loss: 1365394187.0297868\n",
      "Iteration 726, Loss: 1386289972.6627111\n",
      "Iteration 727, Loss: 1404069481.123763\n",
      "Iteration 728, Loss: 1345438287.2531297\n",
      "Iteration 729, Loss: 1472921844.2798681\n",
      "Iteration 730, Loss: 1475830039.4611766\n",
      "Iteration 731, Loss: 1565186488.7798738\n",
      "Iteration 732, Loss: 1192031758.3448865\n",
      "Iteration 733, Loss: 1452531489.4130478\n",
      "Iteration 734, Loss: 1345280982.5569847\n",
      "Iteration 735, Loss: 1411675087.4687195\n",
      "Iteration 736, Loss: 1421332645.834263\n",
      "Iteration 737, Loss: 1430274373.6153316\n",
      "Iteration 738, Loss: 1434665123.7206216\n",
      "Iteration 739, Loss: 1187543140.7502375\n",
      "Iteration 740, Loss: 1212260016.3473594\n",
      "Iteration 741, Loss: 1340840127.2149029\n",
      "Iteration 742, Loss: 1380923990.5897555\n",
      "Iteration 743, Loss: 1478865608.0266747\n",
      "Iteration 744, Loss: 1225679264.5488307\n",
      "Iteration 745, Loss: 1218957960.456207\n",
      "Iteration 746, Loss: 1347732600.998471\n",
      "Iteration 747, Loss: 1380038844.6964464\n",
      "Iteration 748, Loss: 1254270588.2711525\n",
      "Iteration 749, Loss: 1404372611.3573074\n",
      "Iteration 750, Loss: 1509747731.2030227\n",
      "Iteration 751, Loss: 1217619548.2370095\n",
      "Iteration 752, Loss: 1238162260.0726058\n",
      "Iteration 753, Loss: 1287398390.4351528\n",
      "Iteration 754, Loss: 1332883367.0570202\n",
      "Iteration 755, Loss: 1227853710.473731\n",
      "Iteration 756, Loss: 1236018496.8114445\n",
      "Iteration 757, Loss: 1286013392.3751059\n",
      "Iteration 758, Loss: 1180790418.7529836\n",
      "Iteration 759, Loss: 1394113429.6372259\n",
      "Iteration 760, Loss: 1488080830.5423093\n",
      "Iteration 761, Loss: 1177278225.4684842\n",
      "Iteration 762, Loss: 1195538507.8252733\n",
      "Iteration 763, Loss: 1344538026.0891423\n",
      "Iteration 764, Loss: 1345540471.3256624\n",
      "Iteration 765, Loss: 1371985014.447484\n",
      "Iteration 766, Loss: 1160812060.9584262\n",
      "Iteration 767, Loss: 2433071124.7802176\n",
      "Iteration 768, Loss: 2649848222.19501\n",
      "Iteration 769, Loss: 1616147541.0236354\n",
      "Iteration 770, Loss: 1163920790.2064543\n",
      "Iteration 771, Loss: 1157271301.5123475\n",
      "Iteration 772, Loss: 1432179203.4829361\n",
      "Iteration 773, Loss: 1323428321.4262664\n",
      "Iteration 774, Loss: 1446977706.4828706\n",
      "Iteration 775, Loss: 1507429895.0286865\n",
      "Iteration 776, Loss: 1158622089.9917057\n",
      "Iteration 777, Loss: 1160581191.6869144\n",
      "Iteration 778, Loss: 1231100636.8779888\n",
      "Iteration 779, Loss: 1205391615.577277\n",
      "Iteration 780, Loss: 1237176940.5720384\n",
      "Iteration 781, Loss: 1238873636.4592044\n",
      "Iteration 782, Loss: 1234064202.851381\n",
      "Iteration 783, Loss: 1159578949.7010272\n",
      "Iteration 784, Loss: 1176667315.6303618\n",
      "Iteration 785, Loss: 1401224648.548295\n",
      "Iteration 786, Loss: 1392624599.0330346\n",
      "Iteration 787, Loss: 1362292311.122321\n",
      "Iteration 788, Loss: 1348342500.64985\n",
      "Iteration 789, Loss: 1357241186.6612122\n",
      "Iteration 790, Loss: 1443296182.565665\n",
      "Iteration 791, Loss: 1447426207.863769\n",
      "Iteration 792, Loss: 1404752418.030037\n",
      "Iteration 793, Loss: 1457399784.8878317\n",
      "Iteration 794, Loss: 1154551456.5864172\n",
      "Iteration 795, Loss: 1146407606.2556648\n",
      "Iteration 796, Loss: 1971234917.546083\n",
      "Iteration 797, Loss: 1519628931.441515\n",
      "Iteration 798, Loss: 1388788702.758225\n",
      "Iteration 799, Loss: 1242408594.3634176\n",
      "Iteration 800, Loss: 1435581019.2861915\n",
      "Iteration 801, Loss: 1159564109.6708603\n",
      "Iteration 802, Loss: 2339833389.1247463\n",
      "Iteration 803, Loss: 1937322699.0156608\n",
      "Iteration 804, Loss: 2112591807.2758362\n",
      "Iteration 805, Loss: 1677163161.6947408\n",
      "Iteration 806, Loss: 1142571078.1437237\n",
      "Iteration 807, Loss: 1638145494.4737556\n",
      "Iteration 808, Loss: 1616953167.4916422\n",
      "Iteration 809, Loss: 1231476449.845718\n",
      "Iteration 810, Loss: 1256604979.6008694\n",
      "Iteration 811, Loss: 1257047638.9037683\n",
      "Iteration 812, Loss: 1207467367.7700891\n",
      "Iteration 813, Loss: 1281536944.6850324\n",
      "Iteration 814, Loss: 1291009779.6390538\n",
      "Iteration 815, Loss: 1360963389.098642\n",
      "Iteration 816, Loss: 1228384683.7277079\n",
      "Iteration 817, Loss: 1267768839.7407665\n",
      "Iteration 818, Loss: 1271828337.2808113\n",
      "Iteration 819, Loss: 1317512200.204156\n",
      "Iteration 820, Loss: 1374385801.1508825\n",
      "Iteration 821, Loss: 1143184105.0644271\n",
      "Iteration 822, Loss: 1477462527.0618997\n",
      "Iteration 823, Loss: 1255887117.739588\n",
      "Iteration 824, Loss: 1366451064.9571579\n",
      "Iteration 825, Loss: 1417662743.7403226\n",
      "Iteration 826, Loss: 1461672267.2732265\n",
      "Iteration 827, Loss: 1413262208.2803714\n",
      "Iteration 828, Loss: 1187101616.1121664\n",
      "Iteration 829, Loss: 1270957519.2938778\n",
      "Iteration 830, Loss: 1286851252.8771827\n",
      "Iteration 831, Loss: 1230324997.0279438\n",
      "Iteration 832, Loss: 1234932906.1626995\n",
      "Iteration 833, Loss: 1318732122.0447607\n",
      "Iteration 834, Loss: 1435749121.6508842\n",
      "Iteration 835, Loss: 1503614326.655973\n",
      "Iteration 836, Loss: 1258890768.4541936\n",
      "Iteration 837, Loss: 1375337282.9870837\n",
      "Iteration 838, Loss: 1388473803.640476\n",
      "Iteration 839, Loss: 1139883963.747963\n",
      "Iteration 840, Loss: 1408720443.2546978\n",
      "Iteration 841, Loss: 1451895266.1576798\n",
      "Iteration 842, Loss: 1443636274.6325848\n",
      "Iteration 843, Loss: 1487475818.7027051\n",
      "Iteration 844, Loss: 1253692382.6270173\n",
      "Iteration 845, Loss: 1257094046.2428756\n",
      "Iteration 846, Loss: 1425643513.4788456\n",
      "Iteration 847, Loss: 1172049216.6817803\n",
      "Iteration 848, Loss: 1238066883.7769578\n",
      "Iteration 849, Loss: 1251729980.7845333\n",
      "Iteration 850, Loss: 1133134222.1024709\n",
      "Iteration 851, Loss: 1170351099.46332\n",
      "Iteration 852, Loss: 1183433425.231757\n",
      "Iteration 853, Loss: 1435618269.8626447\n",
      "Iteration 854, Loss: 1472125134.9468222\n",
      "Iteration 855, Loss: 1532137273.2311337\n",
      "Iteration 856, Loss: 1437571688.4118564\n",
      "Iteration 857, Loss: 1151269737.6381326\n",
      "Iteration 858, Loss: 1281134768.4980779\n",
      "Iteration 859, Loss: 1210786625.0765846\n",
      "Iteration 860, Loss: 1344669191.6671052\n",
      "Iteration 861, Loss: 1443963297.0858138\n",
      "Iteration 862, Loss: 1432113707.634737\n",
      "Iteration 863, Loss: 1272534621.46058\n",
      "Iteration 864, Loss: 1353421636.5942786\n",
      "Iteration 865, Loss: 1355782995.3855982\n",
      "Iteration 866, Loss: 1353453510.3608487\n",
      "Iteration 867, Loss: 1307076491.1822758\n",
      "Iteration 868, Loss: 1263550271.7315593\n",
      "Iteration 869, Loss: 1270934281.2280874\n",
      "Iteration 870, Loss: 1168268670.9202125\n",
      "Iteration 871, Loss: 1220182807.9413025\n",
      "Iteration 872, Loss: 1254095636.0399897\n",
      "Iteration 873, Loss: 1380772438.575298\n",
      "Iteration 874, Loss: 1204242704.5065548\n",
      "Iteration 875, Loss: 1265122774.4381664\n",
      "Iteration 876, Loss: 1226218971.8837354\n",
      "Iteration 877, Loss: 1282914005.934643\n",
      "Iteration 878, Loss: 1204054538.8034813\n",
      "Iteration 879, Loss: 1220796822.6047688\n",
      "Iteration 880, Loss: 1236521820.0088089\n",
      "Iteration 881, Loss: 1234909425.7634888\n",
      "Iteration 882, Loss: 1265825984.965923\n",
      "Iteration 883, Loss: 1333484568.6803727\n",
      "Iteration 884, Loss: 1128496874.6076255\n",
      "Iteration 885, Loss: 1159321253.5463638\n",
      "Iteration 886, Loss: 1561696122.3380392\n",
      "Iteration 887, Loss: 1570859381.3532803\n",
      "Iteration 888, Loss: 1543197383.5466633\n",
      "Iteration 889, Loss: 1445555149.3312454\n",
      "Iteration 890, Loss: 1147590119.693699\n",
      "Iteration 891, Loss: 1128940506.939659\n",
      "Iteration 892, Loss: 1248214087.8385\n",
      "Iteration 893, Loss: 1408011613.7183685\n",
      "Iteration 894, Loss: 1442479453.3279388\n",
      "Iteration 895, Loss: 1229294098.3044622\n",
      "Iteration 896, Loss: 1240024996.418899\n",
      "Iteration 897, Loss: 1251541946.8344686\n",
      "Iteration 898, Loss: 1291410193.6703255\n",
      "Iteration 899, Loss: 1297139099.4035587\n",
      "Iteration 900, Loss: 1206677320.527914\n",
      "Iteration 901, Loss: 1220705465.7732003\n",
      "Iteration 902, Loss: 1287780623.4911177\n",
      "Iteration 903, Loss: 1238646678.858519\n",
      "Iteration 904, Loss: 1361314892.211253\n",
      "Iteration 905, Loss: 1449265498.6086814\n",
      "Iteration 906, Loss: 1299383816.7853055\n",
      "Iteration 907, Loss: 1127089327.026022\n",
      "Iteration 908, Loss: 1136589853.226701\n",
      "Iteration 909, Loss: 1153032060.6275551\n",
      "Iteration 910, Loss: 1201481177.1426816\n",
      "Iteration 911, Loss: 1166709801.963984\n",
      "Iteration 912, Loss: 1294731382.7642105\n",
      "Iteration 913, Loss: 1402449540.2887743\n",
      "Iteration 914, Loss: 1392164336.9687433\n",
      "Iteration 915, Loss: 1271903475.4559395\n",
      "Iteration 916, Loss: 1251834297.914432\n",
      "Iteration 917, Loss: 1241572279.172969\n",
      "Iteration 918, Loss: 1208554372.7074\n",
      "Iteration 919, Loss: 1238747677.0966623\n",
      "Iteration 920, Loss: 1276844787.3963218\n",
      "Iteration 921, Loss: 1184758901.5688875\n",
      "Iteration 922, Loss: 1317681512.5028176\n",
      "Iteration 923, Loss: 1163290329.9883819\n",
      "Iteration 924, Loss: 1318088296.1842268\n",
      "Iteration 925, Loss: 1204656647.8189082\n",
      "Iteration 926, Loss: 1493912763.9058764\n",
      "Iteration 927, Loss: 1507061117.9385457\n",
      "Iteration 928, Loss: 1430286331.131922\n",
      "Iteration 929, Loss: 1433949974.0515673\n",
      "Iteration 930, Loss: 1380898124.0129676\n",
      "Iteration 931, Loss: 1367899007.4099221\n",
      "Iteration 932, Loss: 1450616410.1519415\n",
      "Iteration 933, Loss: 1222793750.621023\n",
      "Iteration 934, Loss: 1246875937.720677\n",
      "Iteration 935, Loss: 1323558085.758869\n",
      "Iteration 936, Loss: 1415933587.8599062\n",
      "Iteration 937, Loss: 1404455368.4974465\n",
      "Iteration 938, Loss: 1391074280.5788887\n",
      "Iteration 939, Loss: 1366723650.4199407\n",
      "Iteration 940, Loss: 1120064405.7788472\n",
      "Iteration 941, Loss: 1132522182.8893347\n",
      "Iteration 942, Loss: 1143047183.5700574\n",
      "Iteration 943, Loss: 1250222847.306578\n",
      "Iteration 944, Loss: 1233224857.8679564\n",
      "Iteration 945, Loss: 1719739397.9922261\n",
      "Iteration 946, Loss: 1123852565.846003\n",
      "Iteration 947, Loss: 2367982583.1282377\n",
      "Iteration 948, Loss: 1216728621.6219304\n",
      "Iteration 949, Loss: 1340412795.7578256\n",
      "Iteration 950, Loss: 1426609666.3461018\n",
      "Iteration 951, Loss: 1476010240.2636323\n",
      "Iteration 952, Loss: 1454526465.624165\n",
      "Iteration 953, Loss: 1176607833.416768\n",
      "Iteration 954, Loss: 1588415809.020991\n",
      "Iteration 955, Loss: 1543103270.6531441\n",
      "Iteration 956, Loss: 1516549438.4097648\n",
      "Iteration 957, Loss: 1219181392.8525853\n",
      "Iteration 958, Loss: 1226345492.4569519\n",
      "Iteration 959, Loss: 1248015131.2908921\n",
      "Iteration 960, Loss: 1242740134.0096927\n",
      "Iteration 961, Loss: 1228042693.1117501\n",
      "Iteration 962, Loss: 1247732556.0248263\n",
      "Iteration 963, Loss: 1324060535.1511166\n",
      "Iteration 964, Loss: 1154115830.3912368\n",
      "Iteration 965, Loss: 1168438992.1289234\n",
      "Iteration 966, Loss: 1281311378.163453\n",
      "Iteration 967, Loss: 1231492229.6023538\n",
      "Iteration 968, Loss: 1218602438.755316\n",
      "Iteration 969, Loss: 1244346687.894216\n",
      "Iteration 970, Loss: 1278788652.7332327\n",
      "Iteration 971, Loss: 1353578957.7657197\n",
      "Iteration 972, Loss: 1398511146.532995\n",
      "Iteration 973, Loss: 1402163112.9678044\n",
      "Iteration 974, Loss: 1362360861.071321\n",
      "Iteration 975, Loss: 1273741324.6648448\n",
      "Iteration 976, Loss: 1593751147.2005334\n",
      "Iteration 977, Loss: 1546579162.175389\n",
      "Iteration 978, Loss: 1328952957.3327157\n",
      "Iteration 979, Loss: 1400820679.1740103\n",
      "Iteration 980, Loss: 1451209905.0152283\n",
      "Iteration 981, Loss: 1433536181.016332\n",
      "Iteration 982, Loss: 1476583217.8670392\n",
      "Iteration 983, Loss: 1197866427.6472287\n",
      "Iteration 984, Loss: 1120255796.7294612\n",
      "Iteration 985, Loss: 1120322735.55264\n",
      "Iteration 986, Loss: 1126381178.1467671\n",
      "Iteration 987, Loss: 1480796395.4327037\n",
      "Iteration 988, Loss: 1221446819.4423046\n",
      "Iteration 989, Loss: 1163528764.7858436\n",
      "Iteration 990, Loss: 1259377300.2437751\n",
      "Iteration 991, Loss: 1259147078.8681252\n",
      "Iteration 992, Loss: 1264695118.103699\n",
      "Iteration 993, Loss: 1310025167.1332874\n",
      "Iteration 994, Loss: 1310860548.7419229\n",
      "Iteration 995, Loss: 1299954646.183521\n",
      "Iteration 996, Loss: 1309932432.7048287\n",
      "Iteration 997, Loss: 1497285783.0598168\n",
      "Iteration 998, Loss: 1464239670.916817\n",
      "Iteration 999, Loss: 1136719924.252556\n",
      "Iteration 1000, Loss: 1198423757.064121\n",
      "Iteration 1001, Loss: 1189260248.2614129\n",
      "Iteration 1002, Loss: 1377649741.0862775\n",
      "Iteration 1003, Loss: 1362378818.788447\n",
      "Iteration 1004, Loss: 1121392292.981518\n",
      "Iteration 1005, Loss: 1247137513.0206077\n",
      "Iteration 1006, Loss: 1201058406.390349\n",
      "Iteration 1007, Loss: 1196887261.5025346\n",
      "Iteration 1008, Loss: 1207407391.281664\n",
      "Iteration 1009, Loss: 1269848697.3492482\n",
      "Iteration 1010, Loss: 1189522165.74784\n",
      "Iteration 1011, Loss: 1242256998.9262002\n",
      "Iteration 1012, Loss: 1247678899.7295341\n",
      "Iteration 1013, Loss: 1332138136.06762\n",
      "Iteration 1014, Loss: 1279658083.055602\n",
      "Iteration 1015, Loss: 1293513223.970409\n",
      "Iteration 1016, Loss: 1220356460.0909123\n",
      "Iteration 1017, Loss: 1255843791.2049303\n",
      "Iteration 1018, Loss: 1323537923.1611125\n",
      "Iteration 1019, Loss: 1412136426.6535883\n",
      "Iteration 1020, Loss: 1100679258.274497\n",
      "Iteration 1021, Loss: 1099097416.04219\n",
      "Iteration 1022, Loss: 2860028416.610818\n",
      "Iteration 1023, Loss: 1150911503.760866\n",
      "Iteration 1024, Loss: 1430208623.8662426\n",
      "Iteration 1025, Loss: 1300049041.769597\n",
      "Iteration 1026, Loss: 1340223384.4579828\n",
      "Iteration 1027, Loss: 1299383419.2401128\n",
      "Iteration 1028, Loss: 1392739455.5465536\n",
      "Iteration 1029, Loss: 1375795828.5750883\n",
      "Iteration 1030, Loss: 1126616883.6664536\n",
      "Iteration 1031, Loss: 3573382169.004231\n",
      "Iteration 1032, Loss: 2301603226.785888\n",
      "Iteration 1033, Loss: 2244333178.6961446\n",
      "Iteration 1034, Loss: 1719088744.7245939\n",
      "Iteration 1035, Loss: 1385634151.1095986\n",
      "Iteration 1036, Loss: 1280021278.0248945\n",
      "Iteration 1037, Loss: 1104880450.6841338\n",
      "Iteration 1038, Loss: 1113515834.5184417\n",
      "Iteration 1039, Loss: 2486239767.1099057\n",
      "Iteration 1040, Loss: 2255992508.572081\n",
      "Iteration 1041, Loss: 1687759581.1278417\n",
      "Iteration 1042, Loss: 1188687384.2661166\n",
      "Iteration 1043, Loss: 1861714664.710387\n",
      "Iteration 1044, Loss: 1547306674.5013938\n",
      "Iteration 1045, Loss: 1232810554.3411837\n",
      "Iteration 1046, Loss: 1187715113.6657429\n",
      "Iteration 1047, Loss: 1337560791.0719557\n",
      "Iteration 1048, Loss: 1189532351.8691313\n",
      "Iteration 1049, Loss: 1328555572.8800187\n",
      "Iteration 1050, Loss: 1138373690.0059352\n",
      "Iteration 1051, Loss: 1240953018.2612407\n",
      "Iteration 1052, Loss: 1222486513.9078376\n",
      "Iteration 1053, Loss: 1735005677.890298\n",
      "Iteration 1054, Loss: 1622261035.9187605\n",
      "Iteration 1055, Loss: 1598347076.8923845\n",
      "Iteration 1056, Loss: 1150167171.2493942\n",
      "Iteration 1057, Loss: 1352068778.1095572\n",
      "Iteration 1058, Loss: 1416005815.638789\n",
      "Iteration 1059, Loss: 1142987625.2793674\n",
      "Iteration 1060, Loss: 1102612742.9205053\n",
      "Iteration 1061, Loss: 1238130405.1138804\n",
      "Iteration 1062, Loss: 1268211216.7503574\n",
      "Iteration 1063, Loss: 1274761776.9300478\n",
      "Iteration 1064, Loss: 1122691407.5236084\n",
      "Iteration 1065, Loss: 1323962861.5779438\n",
      "Iteration 1066, Loss: 1232666935.249158\n",
      "Iteration 1067, Loss: 1180484048.6668308\n",
      "Iteration 1068, Loss: 1334581515.3997312\n",
      "Iteration 1069, Loss: 1116462403.6507156\n",
      "Iteration 1070, Loss: 1268195259.5270464\n",
      "Iteration 1071, Loss: 1211463230.205917\n",
      "Iteration 1072, Loss: 1332352040.947884\n",
      "Iteration 1073, Loss: 1322707479.4619977\n",
      "Iteration 1074, Loss: 1389851716.4721792\n",
      "Iteration 1075, Loss: 1374963520.658605\n",
      "Iteration 1076, Loss: 1291122096.2635512\n",
      "Iteration 1077, Loss: 1360686041.598792\n",
      "Iteration 1078, Loss: 1119517164.2929184\n",
      "Iteration 1079, Loss: 1097944077.9122999\n",
      "Iteration 1080, Loss: 1129984380.4663587\n",
      "Iteration 1081, Loss: 1130903239.682455\n",
      "Iteration 1082, Loss: 1174484438.303202\n",
      "Iteration 1083, Loss: 1328298021.2268364\n",
      "Iteration 1084, Loss: 1140206313.0937395\n",
      "Iteration 1085, Loss: 1140667418.2545419\n",
      "Iteration 1086, Loss: 1261885538.6529796\n",
      "Iteration 1087, Loss: 1235354852.645265\n",
      "Iteration 1088, Loss: 1213891676.3399994\n",
      "Iteration 1089, Loss: 1330053457.856716\n",
      "Iteration 1090, Loss: 1369893441.5864506\n",
      "Iteration 1091, Loss: 1401254362.427084\n",
      "Iteration 1092, Loss: 1257319640.2828698\n",
      "Iteration 1093, Loss: 1359590963.0446763\n",
      "Iteration 1094, Loss: 1254440314.6064327\n",
      "Iteration 1095, Loss: 1207566363.6378608\n",
      "Iteration 1096, Loss: 1214292491.6591394\n",
      "Iteration 1097, Loss: 1223936492.4398522\n",
      "Iteration 1098, Loss: 1312816180.7434254\n",
      "Iteration 1099, Loss: 1276269714.8993087\n",
      "Iteration 1100, Loss: 1166701270.0009825\n",
      "Iteration 1101, Loss: 1287558943.1378322\n",
      "Iteration 1102, Loss: 1378305886.2315457\n",
      "Iteration 1103, Loss: 1434990011.4856892\n",
      "Iteration 1104, Loss: 1446658662.5716522\n",
      "Iteration 1105, Loss: 1266981769.0771139\n",
      "Iteration 1106, Loss: 1211540442.8356125\n",
      "Iteration 1107, Loss: 1326734834.7759497\n",
      "Iteration 1108, Loss: 1400585586.4762456\n",
      "Iteration 1109, Loss: 1234863164.7968948\n",
      "Iteration 1110, Loss: 1306825700.75641\n",
      "Iteration 1111, Loss: 1173188082.5688097\n",
      "Iteration 1112, Loss: 1164024466.1414988\n",
      "Iteration 1113, Loss: 1167277290.6557796\n",
      "Iteration 1114, Loss: 1165768614.3707385\n",
      "Iteration 1115, Loss: 1378403110.6223426\n",
      "Iteration 1116, Loss: 1118426652.6631253\n",
      "Iteration 1117, Loss: 1115561015.3079674\n",
      "Iteration 1118, Loss: 1089174605.416794\n",
      "Iteration 1119, Loss: 1093436889.5786676\n",
      "Iteration 1120, Loss: 1203868386.7473915\n",
      "Iteration 1121, Loss: 1366021484.9220943\n",
      "Iteration 1122, Loss: 1347433235.460265\n",
      "Iteration 1123, Loss: 1332034662.9681034\n",
      "Iteration 1124, Loss: 1332218716.259107\n",
      "Iteration 1125, Loss: 1087899315.5441823\n",
      "Iteration 1126, Loss: 1563203383.7360725\n",
      "Iteration 1127, Loss: 1529340205.2002122\n",
      "Iteration 1128, Loss: 1091356802.1022515\n",
      "Iteration 1129, Loss: 1265572366.9290226\n",
      "Iteration 1130, Loss: 1261306996.0061877\n",
      "Iteration 1131, Loss: 1271093029.3245773\n",
      "Iteration 1132, Loss: 1317947054.836893\n",
      "Iteration 1133, Loss: 1127077730.9316037\n",
      "Iteration 1134, Loss: 1239751695.796934\n",
      "Iteration 1135, Loss: 1237575508.1851103\n",
      "Iteration 1136, Loss: 1086303317.8077772\n",
      "Iteration 1137, Loss: 1087454736.665678\n",
      "Iteration 1138, Loss: 1093789014.6253004\n",
      "Iteration 1139, Loss: 1126145621.6500432\n",
      "Iteration 1140, Loss: 1250646557.2723663\n",
      "Iteration 1141, Loss: 1287281961.9115076\n",
      "Iteration 1142, Loss: 1193848086.744054\n",
      "Iteration 1143, Loss: 1083682428.7839134\n",
      "Iteration 1144, Loss: 1098005274.2965114\n",
      "Iteration 1145, Loss: 1096591024.824484\n",
      "Iteration 1146, Loss: 1215085884.645658\n",
      "Iteration 1147, Loss: 1238403618.464517\n",
      "Iteration 1148, Loss: 1226012761.6675003\n",
      "Iteration 1149, Loss: 1220491447.9701002\n",
      "Iteration 1150, Loss: 1204589628.221038\n",
      "Iteration 1151, Loss: 1205480742.550579\n",
      "Iteration 1152, Loss: 1212471124.2216425\n",
      "Iteration 1153, Loss: 1189121513.8882477\n",
      "Iteration 1154, Loss: 1158003211.3188875\n",
      "Iteration 1155, Loss: 1200772310.1669574\n",
      "Iteration 1156, Loss: 1197772753.5886915\n",
      "Iteration 1157, Loss: 1192926777.43197\n",
      "Iteration 1158, Loss: 1187264514.1551561\n",
      "Iteration 1159, Loss: 1187271060.8934336\n",
      "Iteration 1160, Loss: 1222382134.2897408\n",
      "Iteration 1161, Loss: 1256517318.0325701\n",
      "Iteration 1162, Loss: 1284880208.8479216\n",
      "Iteration 1163, Loss: 1497690835.6974435\n",
      "Iteration 1164, Loss: 1486395697.8735642\n",
      "Iteration 1165, Loss: 1533070450.3809464\n",
      "Iteration 1166, Loss: 1373413100.7721176\n",
      "Iteration 1167, Loss: 1078956845.222716\n",
      "Iteration 1168, Loss: 1357053112.73228\n",
      "Iteration 1169, Loss: 1083700731.8738666\n",
      "Iteration 1170, Loss: 1232397652.742997\n",
      "Iteration 1171, Loss: 1285681651.9676964\n",
      "Iteration 1172, Loss: 1263730683.0820854\n",
      "Iteration 1173, Loss: 1253765955.9416018\n",
      "Iteration 1174, Loss: 1192983933.4021614\n",
      "Iteration 1175, Loss: 1224105156.3373504\n",
      "Iteration 1176, Loss: 1298988473.4250064\n",
      "Iteration 1177, Loss: 1283632216.9938605\n",
      "Iteration 1178, Loss: 1271867393.4697201\n",
      "Iteration 1179, Loss: 1301135262.3641381\n",
      "Iteration 1180, Loss: 1334145367.8780081\n",
      "Iteration 1181, Loss: 1265011392.7486012\n",
      "Iteration 1182, Loss: 1189401546.43346\n",
      "Iteration 1183, Loss: 1188450697.549129\n",
      "Iteration 1184, Loss: 1181747931.3001432\n",
      "Iteration 1185, Loss: 1084325692.6260257\n",
      "Iteration 1186, Loss: 1087326382.372521\n",
      "Iteration 1187, Loss: 1094961279.5675015\n",
      "Iteration 1188, Loss: 1776408349.9078999\n",
      "Iteration 1189, Loss: 1215530982.9213004\n",
      "Iteration 1190, Loss: 1173310261.2765384\n",
      "Iteration 1191, Loss: 1186611615.6194293\n",
      "Iteration 1192, Loss: 1157767541.6859844\n",
      "Iteration 1193, Loss: 1141638933.1316059\n",
      "Iteration 1194, Loss: 1992581072.2034967\n",
      "Iteration 1195, Loss: 1078807260.0921373\n",
      "Iteration 1196, Loss: 1086445994.920848\n",
      "Iteration 1197, Loss: 2517634743.6602416\n",
      "Iteration 1198, Loss: 2001013900.6166\n",
      "Iteration 1199, Loss: 1929538012.721354\n",
      "Iteration 1200, Loss: 1102042091.86374\n",
      "Iteration 1201, Loss: 1213214067.5080583\n",
      "Iteration 1202, Loss: 1206333876.6358962\n",
      "Iteration 1203, Loss: 1321179803.8219495\n",
      "Iteration 1204, Loss: 1302705920.7413127\n",
      "Iteration 1205, Loss: 1302046797.2500362\n",
      "Iteration 1206, Loss: 1147822011.1207426\n",
      "Iteration 1207, Loss: 1153186260.0956655\n",
      "Iteration 1208, Loss: 1225176403.7700303\n",
      "Iteration 1209, Loss: 1325060577.0351353\n",
      "Iteration 1210, Loss: 1274078111.9822643\n",
      "Iteration 1211, Loss: 1315896165.8820186\n",
      "Iteration 1212, Loss: 1356326043.9116786\n",
      "Iteration 1213, Loss: 1398464919.1058185\n",
      "Iteration 1214, Loss: 1093600374.5584452\n",
      "Iteration 1215, Loss: 1097072799.113115\n",
      "Iteration 1216, Loss: 1108405216.209969\n",
      "Iteration 1217, Loss: 1425823554.669214\n",
      "Iteration 1218, Loss: 1396221718.2564957\n",
      "Iteration 1219, Loss: 1367994020.1814284\n",
      "Iteration 1220, Loss: 1417003448.727025\n",
      "Iteration 1221, Loss: 1408576580.3782494\n",
      "Iteration 1222, Loss: 1385722530.7251425\n",
      "Iteration 1223, Loss: 1432395116.7524388\n",
      "Iteration 1224, Loss: 1404876650.228338\n",
      "Iteration 1225, Loss: 1332539284.4659202\n",
      "Iteration 1226, Loss: 1247575221.814683\n",
      "Iteration 1227, Loss: 1240162337.2279613\n",
      "Iteration 1228, Loss: 1287519854.0644336\n",
      "Iteration 1229, Loss: 1154894210.2686694\n",
      "Iteration 1230, Loss: 1210732328.6654453\n",
      "Iteration 1231, Loss: 1700759410.0995219\n",
      "Iteration 1232, Loss: 1612039122.812699\n",
      "Iteration 1233, Loss: 1238814119.062937\n",
      "Iteration 1234, Loss: 1186933656.8579228\n",
      "Iteration 1235, Loss: 1091363339.6966338\n",
      "Iteration 1236, Loss: 1335090919.1305535\n",
      "Iteration 1237, Loss: 1094083308.930645\n",
      "Iteration 1238, Loss: 1098233361.3585577\n",
      "Iteration 1239, Loss: 2231084333.8604503\n",
      "Iteration 1240, Loss: 2131562406.46733\n",
      "Iteration 1241, Loss: 2021942147.1547318\n",
      "Iteration 1242, Loss: 1595021288.6435769\n",
      "Iteration 1243, Loss: 1551418469.9304903\n",
      "Iteration 1244, Loss: 1090383553.7392914\n",
      "Iteration 1245, Loss: 1801291044.30446\n",
      "Iteration 1246, Loss: 1616348564.9960356\n",
      "Iteration 1247, Loss: 1149038124.3279335\n",
      "Iteration 1248, Loss: 1134483576.9012702\n",
      "Iteration 1249, Loss: 1127094830.3253374\n",
      "Iteration 1250, Loss: 1180776974.5087948\n",
      "Iteration 1251, Loss: 1216459354.3686106\n",
      "Iteration 1252, Loss: 1349327769.0953984\n",
      "Iteration 1253, Loss: 1364682426.083299\n",
      "Iteration 1254, Loss: 1310425163.8103957\n",
      "Iteration 1255, Loss: 1292829005.7346961\n",
      "Iteration 1256, Loss: 1348357041.707567\n",
      "Iteration 1257, Loss: 1087557293.2757308\n",
      "Iteration 1258, Loss: 1090622627.46513\n",
      "Iteration 1259, Loss: 1146052994.2670517\n",
      "Iteration 1260, Loss: 1305591441.2419095\n",
      "Iteration 1261, Loss: 1150367877.9427183\n",
      "Iteration 1262, Loss: 1211385061.1997647\n",
      "Iteration 1263, Loss: 1212999961.350674\n",
      "Iteration 1264, Loss: 1260434567.3073971\n",
      "Iteration 1265, Loss: 1244325009.2146156\n",
      "Iteration 1266, Loss: 1232839074.2065058\n",
      "Iteration 1267, Loss: 1350061412.5852685\n",
      "Iteration 1268, Loss: 1174678229.092397\n",
      "Iteration 1269, Loss: 1148360892.915188\n",
      "Iteration 1270, Loss: 1260958727.9142299\n",
      "Iteration 1271, Loss: 1296993719.6561058\n",
      "Iteration 1272, Loss: 1160119789.0177925\n",
      "Iteration 1273, Loss: 1116607726.4059725\n",
      "Iteration 1274, Loss: 1111148343.5006783\n",
      "Iteration 1275, Loss: 1106877630.1226568\n",
      "Iteration 1276, Loss: 1207193393.151238\n",
      "Iteration 1277, Loss: 1310608796.7808604\n",
      "Iteration 1278, Loss: 1257719656.056594\n",
      "Iteration 1279, Loss: 1247066782.1392288\n",
      "Iteration 1280, Loss: 1236409065.671176\n",
      "Iteration 1281, Loss: 1163806245.4185314\n",
      "Iteration 1282, Loss: 1167259844.932488\n",
      "Iteration 1283, Loss: 1206908149.9556649\n",
      "Iteration 1284, Loss: 1203984929.7328465\n",
      "Iteration 1285, Loss: 1081891289.1413975\n",
      "Iteration 1286, Loss: 1228702788.241184\n",
      "Iteration 1287, Loss: 1296058983.284994\n",
      "Iteration 1288, Loss: 1122958773.0481281\n",
      "Iteration 1289, Loss: 1175968419.242644\n",
      "Iteration 1290, Loss: 1120965726.9275835\n",
      "Iteration 1291, Loss: 1263602652.792347\n",
      "Iteration 1292, Loss: 1269239828.8461823\n",
      "Iteration 1293, Loss: 1087870319.7524557\n",
      "Iteration 1294, Loss: 1085865517.466423\n",
      "Iteration 1295, Loss: 1086043523.254697\n",
      "Iteration 1296, Loss: 1091696038.0066454\n",
      "Iteration 1297, Loss: 1766195994.9929852\n",
      "Iteration 1298, Loss: 1599011339.2110298\n",
      "Iteration 1299, Loss: 1343601362.3194108\n",
      "Iteration 1300, Loss: 1120523362.111772\n",
      "Iteration 1301, Loss: 1534839749.4259508\n",
      "Iteration 1302, Loss: 1068758083.4668496\n",
      "Iteration 1303, Loss: 1525510116.2567139\n",
      "Iteration 1304, Loss: 1477430593.3564193\n",
      "Iteration 1305, Loss: 1520910838.7314126\n",
      "Iteration 1306, Loss: 1492945120.7642136\n",
      "Iteration 1307, Loss: 1071554766.649536\n",
      "Iteration 1308, Loss: 1073711391.2827834\n",
      "Iteration 1309, Loss: 1070432563.2219929\n",
      "Iteration 1310, Loss: 1106889939.6133347\n",
      "Iteration 1311, Loss: 1071552959.1278625\n",
      "Iteration 1312, Loss: 1236716506.166178\n",
      "Iteration 1313, Loss: 1235072425.7222667\n",
      "Iteration 1314, Loss: 1223281162.1317284\n",
      "Iteration 1315, Loss: 1189746071.6463704\n",
      "Iteration 1316, Loss: 1167694146.4380708\n",
      "Iteration 1317, Loss: 1190437274.4699996\n",
      "Iteration 1318, Loss: 1183962659.2646654\n",
      "Iteration 1319, Loss: 1286825527.6461272\n",
      "Iteration 1320, Loss: 1354277377.2404394\n",
      "Iteration 1321, Loss: 1095128963.7410822\n",
      "Iteration 1322, Loss: 1098759744.616958\n",
      "Iteration 1323, Loss: 1422188752.3682396\n",
      "Iteration 1324, Loss: 1268607664.9705718\n",
      "Iteration 1325, Loss: 1223508233.6809654\n",
      "Iteration 1326, Loss: 1266103675.2970333\n",
      "Iteration 1327, Loss: 1250916567.3625603\n",
      "Iteration 1328, Loss: 1247555294.485083\n",
      "Iteration 1329, Loss: 1235955543.7651734\n",
      "Iteration 1330, Loss: 1324004701.087373\n",
      "Iteration 1331, Loss: 1348870403.7182653\n",
      "Iteration 1332, Loss: 1153964288.6246505\n",
      "Iteration 1333, Loss: 1151456167.0311143\n",
      "Iteration 1334, Loss: 1110836569.4051344\n",
      "Iteration 1335, Loss: 1115326450.8522809\n",
      "Iteration 1336, Loss: 1136451402.229837\n",
      "Iteration 1337, Loss: 1332991788.7883523\n",
      "Iteration 1338, Loss: 1101074466.9746785\n",
      "Iteration 1339, Loss: 1119670431.7066157\n",
      "Iteration 1340, Loss: 1236762061.295621\n",
      "Iteration 1341, Loss: 1268036550.520054\n",
      "Iteration 1342, Loss: 1257375669.7272854\n",
      "Iteration 1343, Loss: 1241760271.4208658\n",
      "Iteration 1344, Loss: 1137305351.9025607\n",
      "Iteration 1345, Loss: 1246397517.4113827\n",
      "Iteration 1346, Loss: 1096087783.8367436\n",
      "Iteration 1347, Loss: 1270673747.5089786\n",
      "Iteration 1348, Loss: 1155991286.368947\n",
      "Iteration 1349, Loss: 1192477812.7700942\n",
      "Iteration 1350, Loss: 1289938993.5701454\n",
      "Iteration 1351, Loss: 1325703715.201603\n",
      "Iteration 1352, Loss: 1343025629.9531877\n",
      "Iteration 1353, Loss: 1078819365.6367118\n",
      "Iteration 1354, Loss: 1063952027.7130749\n",
      "Iteration 1355, Loss: 1056506728.8086755\n",
      "Iteration 1356, Loss: 1067677328.0914198\n",
      "Iteration 1357, Loss: 1072721033.0671474\n",
      "Iteration 1358, Loss: 1326360828.4943154\n",
      "Iteration 1359, Loss: 1053247423.0125364\n",
      "Iteration 1360, Loss: 1183759059.9677715\n",
      "Iteration 1361, Loss: 1175500813.3939767\n",
      "Iteration 1362, Loss: 1147901573.8422701\n",
      "Iteration 1363, Loss: 1332615640.8342602\n",
      "Iteration 1364, Loss: 1377716747.2846751\n",
      "Iteration 1365, Loss: 1068080123.8767563\n",
      "Iteration 1366, Loss: 1206044167.8238971\n",
      "Iteration 1367, Loss: 1203124912.3802648\n",
      "Iteration 1368, Loss: 1152911366.9930673\n",
      "Iteration 1369, Loss: 1123769152.5758522\n",
      "Iteration 1370, Loss: 1125771419.8665903\n",
      "Iteration 1371, Loss: 1970537259.9868035\n",
      "Iteration 1372, Loss: 1055138216.2698355\n",
      "Iteration 1373, Loss: 3060287084.372997\n",
      "Iteration 1374, Loss: 1747579829.9008005\n",
      "Iteration 1375, Loss: 1111570191.2561321\n",
      "Iteration 1376, Loss: 1166613907.798756\n",
      "Iteration 1377, Loss: 1223946117.3062155\n",
      "Iteration 1378, Loss: 1101636654.7821186\n",
      "Iteration 1379, Loss: 1104209972.9247854\n",
      "Iteration 1380, Loss: 1951457480.2633507\n",
      "Iteration 1381, Loss: 1063909518.320471\n",
      "Iteration 1382, Loss: 1060182241.5104606\n",
      "Iteration 1383, Loss: 1759215914.7442858\n",
      "Iteration 1384, Loss: 1108339465.235207\n",
      "Iteration 1385, Loss: 1253435605.593971\n",
      "Iteration 1386, Loss: 1241247024.8608685\n",
      "Iteration 1387, Loss: 1160761599.8665805\n",
      "Iteration 1388, Loss: 1158968337.2237132\n",
      "Iteration 1389, Loss: 1326625714.276632\n",
      "Iteration 1390, Loss: 1160001868.7015767\n",
      "Iteration 1391, Loss: 1189305819.817728\n",
      "Iteration 1392, Loss: 1232577027.5761535\n",
      "Iteration 1393, Loss: 1157214137.660197\n",
      "Iteration 1394, Loss: 1189698969.2421432\n",
      "Iteration 1395, Loss: 1329139351.1908789\n",
      "Iteration 1396, Loss: 1055903409.8441466\n",
      "Iteration 1397, Loss: 1142781436.278754\n",
      "Iteration 1398, Loss: 1142657575.7332635\n",
      "Iteration 1399, Loss: 1119038950.0621805\n",
      "Iteration 1400, Loss: 1290590279.5669496\n",
      "Iteration 1401, Loss: 1353115110.0305722\n",
      "Iteration 1402, Loss: 1206368598.1629775\n",
      "Iteration 1403, Loss: 1236726603.002342\n",
      "Iteration 1404, Loss: 1193342397.4390986\n",
      "Iteration 1405, Loss: 1164272453.4437902\n",
      "Iteration 1406, Loss: 1160015923.2778113\n",
      "Iteration 1407, Loss: 1177804364.21625\n",
      "Iteration 1408, Loss: 1164194537.9409735\n",
      "Iteration 1409, Loss: 1324042580.1572387\n",
      "Iteration 1410, Loss: 1373495696.2971876\n",
      "Iteration 1411, Loss: 1411111685.1207807\n",
      "Iteration 1412, Loss: 1112599856.6097782\n",
      "Iteration 1413, Loss: 1114405341.410826\n",
      "Iteration 1414, Loss: 1060213213.2796923\n",
      "Iteration 1415, Loss: 1164412236.4044216\n",
      "Iteration 1416, Loss: 1263348037.2083814\n",
      "Iteration 1417, Loss: 1242056903.5517569\n",
      "Iteration 1418, Loss: 1273327045.839585\n",
      "Iteration 1419, Loss: 1240085588.6494749\n",
      "Iteration 1420, Loss: 1166476860.2554626\n",
      "Iteration 1421, Loss: 1180667924.649885\n",
      "Iteration 1422, Loss: 1104641013.9335\n",
      "Iteration 1423, Loss: 1106560473.6013262\n",
      "Iteration 1424, Loss: 1315139102.1145406\n",
      "Iteration 1425, Loss: 1133114016.1845062\n",
      "Iteration 1426, Loss: 1131523994.9013438\n",
      "Iteration 1427, Loss: 1190137336.0280027\n",
      "Iteration 1428, Loss: 1181817196.717454\n",
      "Iteration 1429, Loss: 1295421472.887532\n",
      "Iteration 1430, Loss: 1158906465.7755306\n",
      "Iteration 1431, Loss: 1214808064.76268\n",
      "Iteration 1432, Loss: 1325001824.6620996\n",
      "Iteration 1433, Loss: 1372558309.583029\n",
      "Iteration 1434, Loss: 1343123642.613129\n",
      "Iteration 1435, Loss: 1051695750.6220087\n",
      "Iteration 1436, Loss: 1051140295.7335007\n",
      "Iteration 1437, Loss: 1070300676.4433103\n",
      "Iteration 1438, Loss: 1133130103.0406358\n",
      "Iteration 1439, Loss: 1241512188.807764\n",
      "Iteration 1440, Loss: 1060957454.9915656\n",
      "Iteration 1441, Loss: 1071300854.9269809\n",
      "Iteration 1442, Loss: 1074628947.3069153\n",
      "Iteration 1443, Loss: 1272148955.9138258\n",
      "Iteration 1444, Loss: 1133417354.2620583\n",
      "Iteration 1445, Loss: 1162244987.4217985\n",
      "Iteration 1446, Loss: 1779714935.6995656\n",
      "Iteration 1447, Loss: 1723567768.5497766\n",
      "Iteration 1448, Loss: 1587551516.3931422\n",
      "Iteration 1449, Loss: 1500111611.684719\n",
      "Iteration 1450, Loss: 1089082316.0489392\n",
      "Iteration 1451, Loss: 1390128346.279642\n",
      "Iteration 1452, Loss: 1211620054.4581137\n",
      "Iteration 1453, Loss: 1292160011.106793\n",
      "Iteration 1454, Loss: 1053449962.4191935\n",
      "Iteration 1455, Loss: 2693049858.0460215\n",
      "Iteration 1456, Loss: 1667604476.3454626\n",
      "Iteration 1457, Loss: 1313005131.7746234\n",
      "Iteration 1458, Loss: 1287051520.597942\n",
      "Iteration 1459, Loss: 1333119864.219005\n",
      "Iteration 1460, Loss: 1194060724.6407692\n",
      "Iteration 1461, Loss: 1156886308.8337495\n",
      "Iteration 1462, Loss: 1824412727.542237\n",
      "Iteration 1463, Loss: 1755222057.0062313\n",
      "Iteration 1464, Loss: 1357339162.3420134\n",
      "Iteration 1465, Loss: 1331194821.6008554\n",
      "Iteration 1466, Loss: 1071630146.940445\n",
      "Iteration 1467, Loss: 1286130875.000934\n",
      "Iteration 1468, Loss: 1352902405.9371781\n",
      "Iteration 1469, Loss: 1106135230.9251783\n",
      "Iteration 1470, Loss: 1205869660.9282908\n",
      "Iteration 1471, Loss: 1146897053.3518481\n",
      "Iteration 1472, Loss: 1182034693.6597073\n",
      "Iteration 1473, Loss: 1210846881.1195195\n",
      "Iteration 1474, Loss: 1218142900.4139132\n",
      "Iteration 1475, Loss: 1206388947.0567765\n",
      "Iteration 1476, Loss: 1297290717.5394194\n",
      "Iteration 1477, Loss: 1064539337.6370779\n",
      "Iteration 1478, Loss: 1058204862.0777022\n",
      "Iteration 1479, Loss: 1105418237.4151025\n",
      "Iteration 1480, Loss: 1354902147.998407\n",
      "Iteration 1481, Loss: 1058362876.318241\n",
      "Iteration 1482, Loss: 1052280532.0636268\n",
      "Iteration 1483, Loss: 3646951451.5932245\n",
      "Iteration 1484, Loss: 5001704845.867584\n",
      "Iteration 1485, Loss: 35683414702.17208\n",
      "Iteration 1486, Loss: 15869006541.31606\n",
      "Iteration 1487, Loss: 3442368524.624668\n",
      "Iteration 1488, Loss: 1912312892.7656999\n",
      "Iteration 1489, Loss: 1274429286.5075512\n",
      "Iteration 1490, Loss: 1300420724.643635\n",
      "Iteration 1491, Loss: 1283338102.0753074\n",
      "Iteration 1492, Loss: 1180958101.6759126\n",
      "Iteration 1493, Loss: 1079377072.7605793\n",
      "Iteration 1494, Loss: 1092355332.7820513\n",
      "Iteration 1495, Loss: 1325805202.8609717\n",
      "Iteration 1496, Loss: 1236906163.155575\n",
      "Iteration 1497, Loss: 1118629233.6664398\n",
      "Iteration 1498, Loss: 1238206102.7914689\n",
      "Iteration 1499, Loss: 1203029519.56497\n",
      "Iteration 1500, Loss: 1698835267.4854558\n",
      "Iteration 1501, Loss: 1603430611.3467474\n",
      "Iteration 1502, Loss: 1112698657.0501206\n",
      "Iteration 1503, Loss: 1114281887.8692648\n",
      "Iteration 1504, Loss: 1337213116.4418418\n",
      "Iteration 1505, Loss: 1088313323.5439675\n",
      "Iteration 1506, Loss: 1086298378.525698\n",
      "Iteration 1507, Loss: 1453000331.056505\n",
      "Iteration 1508, Loss: 1349640424.813039\n",
      "Iteration 1509, Loss: 1069999087.0700455\n",
      "Iteration 1510, Loss: 1086670898.958296\n",
      "Iteration 1511, Loss: 1192866382.0850778\n",
      "Iteration 1512, Loss: 1285079069.8057098\n",
      "Iteration 1513, Loss: 1203001337.2234366\n",
      "Iteration 1514, Loss: 1190607077.3322644\n",
      "Iteration 1515, Loss: 1180359204.8859217\n",
      "Iteration 1516, Loss: 1209813400.216335\n",
      "Iteration 1517, Loss: 1163709943.9143577\n",
      "Iteration 1518, Loss: 1166255865.4623647\n",
      "Iteration 1519, Loss: 1226987015.4403281\n",
      "Iteration 1520, Loss: 1616057552.7610404\n",
      "Iteration 1521, Loss: 1107113937.8454227\n",
      "Iteration 1522, Loss: 1414027336.1380599\n",
      "Iteration 1523, Loss: 1110757006.3700702\n",
      "Iteration 1524, Loss: 1398165702.661313\n",
      "Iteration 1525, Loss: 1352948793.9602838\n",
      "Iteration 1526, Loss: 1392482688.8889813\n",
      "Iteration 1527, Loss: 1433096002.872534\n",
      "Iteration 1528, Loss: 1400341349.225681\n",
      "Iteration 1529, Loss: 1370565514.582403\n",
      "Iteration 1530, Loss: 1346244001.59326\n",
      "Iteration 1531, Loss: 1338105120.9288068\n",
      "Iteration 1532, Loss: 1137613953.6308782\n",
      "Iteration 1533, Loss: 1106856715.3360708\n",
      "Iteration 1534, Loss: 1199937847.767556\n",
      "Iteration 1535, Loss: 1234585949.8651123\n",
      "Iteration 1536, Loss: 1130093568.03548\n",
      "Iteration 1537, Loss: 1243621409.9461076\n",
      "Iteration 1538, Loss: 1277830079.233678\n",
      "Iteration 1539, Loss: 1261343463.0242736\n",
      "Iteration 1540, Loss: 1265974991.6171386\n",
      "Iteration 1541, Loss: 1513556027.7584527\n",
      "Iteration 1542, Loss: 1475155793.7175148\n",
      "Iteration 1543, Loss: 1397685412.5912485\n",
      "Iteration 1544, Loss: 1362821966.4001749\n",
      "Iteration 1545, Loss: 1135657473.3845034\n",
      "Iteration 1546, Loss: 1138029293.4402485\n",
      "Iteration 1547, Loss: 1189897899.6161382\n",
      "Iteration 1548, Loss: 1225376529.7741995\n",
      "Iteration 1549, Loss: 1216412191.0630157\n",
      "Iteration 1550, Loss: 1259835063.1611996\n",
      "Iteration 1551, Loss: 1294853820.023877\n",
      "Iteration 1552, Loss: 1275372328.074324\n",
      "Iteration 1553, Loss: 1255860697.4541698\n",
      "Iteration 1554, Loss: 1242333422.1671367\n",
      "Iteration 1555, Loss: 1173937781.0877833\n",
      "Iteration 1556, Loss: 1115188767.858111\n",
      "Iteration 1557, Loss: 1072378456.4459128\n",
      "Iteration 1558, Loss: 1496954057.0979826\n",
      "Iteration 1559, Loss: 1452081813.6034281\n",
      "Iteration 1560, Loss: 1076285785.3510616\n",
      "Iteration 1561, Loss: 1318265224.0476887\n",
      "Iteration 1562, Loss: 1348361386.6565177\n",
      "Iteration 1563, Loss: 1323107115.5287628\n",
      "Iteration 1564, Loss: 1316660814.7660196\n",
      "Iteration 1565, Loss: 1297381278.572919\n",
      "Iteration 1566, Loss: 1123241484.0804136\n",
      "Iteration 1567, Loss: 1145679871.9506369\n",
      "Iteration 1568, Loss: 1295259725.207003\n",
      "Iteration 1569, Loss: 1206137136.6252246\n",
      "Iteration 1570, Loss: 1199867888.9521408\n",
      "Iteration 1571, Loss: 1245830270.7175176\n",
      "Iteration 1572, Loss: 1206495911.3412879\n",
      "Iteration 1573, Loss: 1160999536.91946\n",
      "Iteration 1574, Loss: 1112993474.1478984\n",
      "Iteration 1575, Loss: 1196775007.7622168\n",
      "Iteration 1576, Loss: 1340255068.1023598\n",
      "Iteration 1577, Loss: 1137353748.5199544\n",
      "Iteration 1578, Loss: 1086131893.5496433\n",
      "Iteration 1579, Loss: 1277909086.9728017\n",
      "Iteration 1580, Loss: 1123869560.90334\n",
      "Iteration 1581, Loss: 1375668485.7153554\n",
      "Iteration 1582, Loss: 1348450833.5387995\n",
      "Iteration 1583, Loss: 1155003507.8987067\n",
      "Iteration 1584, Loss: 1264258990.5166245\n",
      "Iteration 1585, Loss: 1319492352.1542628\n",
      "Iteration 1586, Loss: 1097217283.5255277\n",
      "Iteration 1587, Loss: 1100936060.4826589\n",
      "Iteration 1588, Loss: 1098946421.6837409\n",
      "Iteration 1589, Loss: 1219778124.740197\n",
      "Iteration 1590, Loss: 1208290353.4421837\n",
      "Iteration 1591, Loss: 1253100086.597727\n",
      "Iteration 1592, Loss: 1198053980.400357\n",
      "Iteration 1593, Loss: 1195281532.63702\n",
      "Iteration 1594, Loss: 1216344236.314136\n",
      "Iteration 1595, Loss: 1185799667.9914548\n",
      "Iteration 1596, Loss: 1161685405.7014592\n",
      "Iteration 1597, Loss: 1163908884.0742192\n",
      "Iteration 1598, Loss: 1154246667.5954635\n",
      "Iteration 1599, Loss: 1154502644.3440084\n",
      "Iteration 1600, Loss: 1131198243.8821037\n",
      "Iteration 1601, Loss: 1120723141.8581948\n",
      "Iteration 1602, Loss: 1230324771.0325718\n",
      "Iteration 1603, Loss: 1185851498.0139534\n",
      "Iteration 1604, Loss: 1339353748.7367978\n",
      "Iteration 1605, Loss: 1137497105.0116017\n",
      "Iteration 1606, Loss: 1086049836.0763118\n",
      "Iteration 1607, Loss: 1213414003.9607365\n",
      "Iteration 1608, Loss: 1309905607.5577328\n",
      "Iteration 1609, Loss: 1254430306.6619682\n",
      "Iteration 1610, Loss: 1241115855.3333097\n",
      "Iteration 1611, Loss: 1230385621.3237114\n",
      "Iteration 1612, Loss: 1123541558.2796273\n",
      "Iteration 1613, Loss: 1129405999.8230767\n",
      "Iteration 1614, Loss: 1189571352.0432556\n",
      "Iteration 1615, Loss: 1187281698.730681\n",
      "Iteration 1616, Loss: 1151403694.766451\n",
      "Iteration 1617, Loss: 1278433779.944166\n",
      "Iteration 1618, Loss: 1231408118.8862884\n",
      "Iteration 1619, Loss: 1149597181.414565\n",
      "Iteration 1620, Loss: 1150747013.9593728\n",
      "Iteration 1621, Loss: 1142603238.191126\n",
      "Iteration 1622, Loss: 1145895347.1945212\n",
      "Iteration 1623, Loss: 1149598570.0825295\n",
      "Iteration 1624, Loss: 1906160072.3007643\n",
      "Iteration 1625, Loss: 1953249047.4246457\n",
      "Iteration 1626, Loss: 1377330824.6339262\n",
      "Iteration 1627, Loss: 1418404842.901292\n",
      "Iteration 1628, Loss: 1376925477.556159\n",
      "Iteration 1629, Loss: 1166371195.323065\n",
      "Iteration 1630, Loss: 1227332529.798948\n",
      "Iteration 1631, Loss: 1269952321.8891082\n",
      "Iteration 1632, Loss: 1267386583.2458122\n",
      "Iteration 1633, Loss: 1267883126.5062566\n",
      "Iteration 1634, Loss: 1079357661.5274756\n",
      "Iteration 1635, Loss: 2479740339.466702\n",
      "Iteration 1636, Loss: 1646153369.1782746\n",
      "Iteration 1637, Loss: 1502492768.378831\n",
      "Iteration 1638, Loss: 1459827691.8132653\n",
      "Iteration 1639, Loss: 1091529657.1815262\n",
      "Iteration 1640, Loss: 1118137621.3337138\n",
      "Iteration 1641, Loss: 1122135501.591544\n",
      "Iteration 1642, Loss: 1308715100.3250961\n",
      "Iteration 1643, Loss: 1369592646.9758637\n",
      "Iteration 1644, Loss: 1135171489.0961337\n",
      "Iteration 1645, Loss: 1107922698.7710848\n",
      "Iteration 1646, Loss: 1411439201.1909454\n",
      "Iteration 1647, Loss: 1381293342.5401511\n",
      "Iteration 1648, Loss: 1212838320.3937478\n",
      "Iteration 1649, Loss: 1133197180.9891045\n",
      "Iteration 1650, Loss: 1414943009.1225095\n",
      "Iteration 1651, Loss: 1375268531.5649638\n",
      "Iteration 1652, Loss: 1136404772.5449672\n",
      "Iteration 1653, Loss: 1368347925.2914205\n",
      "Iteration 1654, Loss: 1368362969.3258488\n",
      "Iteration 1655, Loss: 1169788669.9119444\n",
      "Iteration 1656, Loss: 1195789088.9194546\n",
      "Iteration 1657, Loss: 1125192251.596083\n",
      "Iteration 1658, Loss: 1340082008.7487001\n",
      "Iteration 1659, Loss: 1124457650.6760042\n",
      "Iteration 1660, Loss: 1338064375.3984468\n",
      "Iteration 1661, Loss: 1124456236.7991722\n",
      "Iteration 1662, Loss: 1077856361.7634974\n",
      "Iteration 1663, Loss: 1091196048.18605\n",
      "Iteration 1664, Loss: 1096990967.657345\n",
      "Iteration 1665, Loss: 1102721681.2594235\n",
      "Iteration 1666, Loss: 1104103416.1373694\n",
      "Iteration 1667, Loss: 1158290209.6088703\n",
      "Iteration 1668, Loss: 1219265316.6875722\n",
      "Iteration 1669, Loss: 1252397680.1541672\n",
      "Iteration 1670, Loss: 1335387393.741763\n",
      "Iteration 1671, Loss: 1206934964.259063\n",
      "Iteration 1672, Loss: 1067760938.9255114\n",
      "Iteration 1673, Loss: 1071013406.8589346\n",
      "Iteration 1674, Loss: 1071100622.9139255\n",
      "Iteration 1675, Loss: 1072355643.4450918\n",
      "Iteration 1676, Loss: 1085432527.9901226\n",
      "Iteration 1677, Loss: 1084750903.9230974\n",
      "Iteration 1678, Loss: 1461191513.9390907\n",
      "Iteration 1679, Loss: 1122419532.673484\n",
      "Iteration 1680, Loss: 1098326878.2373939\n",
      "Iteration 1681, Loss: 1330414669.6150634\n",
      "Iteration 1682, Loss: 1170933417.345504\n",
      "Iteration 1683, Loss: 1338639515.5618646\n",
      "Iteration 1684, Loss: 1066511657.8533634\n",
      "Iteration 1685, Loss: 1073577809.9368656\n",
      "Iteration 1686, Loss: 1490266999.994631\n",
      "Iteration 1687, Loss: 1070624133.6038362\n",
      "Iteration 1688, Loss: 2608013025.0353284\n",
      "Iteration 1689, Loss: 1395837464.5045109\n",
      "Iteration 1690, Loss: 1428465690.890764\n",
      "Iteration 1691, Loss: 1356058929.8014178\n",
      "Iteration 1692, Loss: 1073680211.7735554\n",
      "Iteration 1693, Loss: 1335016970.2838056\n",
      "Iteration 1694, Loss: 1348609228.3382874\n",
      "Iteration 1695, Loss: 1222599119.5844207\n",
      "Iteration 1696, Loss: 1342018300.9234157\n",
      "Iteration 1697, Loss: 1394490144.4029696\n",
      "Iteration 1698, Loss: 1431485357.117966\n",
      "Iteration 1699, Loss: 1071412194.5259435\n",
      "Iteration 1700, Loss: 1125249132.8214417\n",
      "Iteration 1701, Loss: 1241565909.5108883\n",
      "Iteration 1702, Loss: 1338529282.034969\n",
      "Iteration 1703, Loss: 1366292195.4024959\n",
      "Iteration 1704, Loss: 1370932505.991115\n",
      "Iteration 1705, Loss: 1106100850.2468925\n",
      "Iteration 1706, Loss: 1269407911.5015788\n",
      "Iteration 1707, Loss: 1144573046.348579\n",
      "Iteration 1708, Loss: 1257183024.7862113\n",
      "Iteration 1709, Loss: 1183887570.8243377\n",
      "Iteration 1710, Loss: 1202436840.1757324\n",
      "Iteration 1711, Loss: 1335494974.2583838\n",
      "Iteration 1712, Loss: 1075850683.4350886\n",
      "Iteration 1713, Loss: 1453092991.2165315\n",
      "Iteration 1714, Loss: 1494689259.87617\n",
      "Iteration 1715, Loss: 1457932225.322985\n",
      "Iteration 1716, Loss: 1499538596.1294627\n",
      "Iteration 1717, Loss: 1145349249.3579855\n",
      "Iteration 1718, Loss: 1182442103.198096\n",
      "Iteration 1719, Loss: 1113127641.6637511\n",
      "Iteration 1720, Loss: 1161024613.48737\n",
      "Iteration 1721, Loss: 1203589683.3456435\n",
      "Iteration 1722, Loss: 1297044517.2816665\n",
      "Iteration 1723, Loss: 1254574671.9662378\n",
      "Iteration 1724, Loss: 1127908201.2869277\n",
      "Iteration 1725, Loss: 1099330315.0171046\n",
      "Iteration 1726, Loss: 1117964959.4215858\n",
      "Iteration 1727, Loss: 2042972131.374965\n",
      "Iteration 1728, Loss: 2085948489.0889466\n",
      "Iteration 1729, Loss: 1277896008.0893333\n",
      "Iteration 1730, Loss: 1117378611.6898115\n",
      "Iteration 1731, Loss: 1164132354.907357\n",
      "Iteration 1732, Loss: 1194588854.6561131\n",
      "Iteration 1733, Loss: 1227187825.9676344\n",
      "Iteration 1734, Loss: 1279343437.735377\n",
      "Iteration 1735, Loss: 1260217388.6775775\n",
      "Iteration 1736, Loss: 1337995771.6344252\n",
      "Iteration 1737, Loss: 1347952703.337613\n",
      "Iteration 1738, Loss: 1215258622.5731564\n",
      "Iteration 1739, Loss: 1181043187.6145134\n",
      "Iteration 1740, Loss: 1164077991.370536\n",
      "Iteration 1741, Loss: 1131157637.81927\n",
      "Iteration 1742, Loss: 1068168623.9663028\n",
      "Iteration 1743, Loss: 1490492384.380514\n",
      "Iteration 1744, Loss: 1165925560.4731877\n",
      "Iteration 1745, Loss: 1207491247.0838215\n",
      "Iteration 1746, Loss: 1272648381.2804332\n",
      "Iteration 1747, Loss: 1307688758.2071261\n",
      "Iteration 1748, Loss: 1059128153.2037963\n",
      "Iteration 1749, Loss: 1337866399.9011219\n",
      "Iteration 1750, Loss: 1072452442.659434\n",
      "Iteration 1751, Loss: 1102658051.4080422\n",
      "Iteration 1752, Loss: 1243076116.0437293\n",
      "Iteration 1753, Loss: 1276042995.784361\n",
      "Iteration 1754, Loss: 1254707425.1667187\n",
      "Iteration 1755, Loss: 1134265018.172288\n",
      "Iteration 1756, Loss: 1173248617.90623\n",
      "Iteration 1757, Loss: 1184950768.559449\n",
      "Iteration 1758, Loss: 1171268346.0642083\n",
      "Iteration 1759, Loss: 1228472318.567952\n",
      "Iteration 1760, Loss: 1188819178.3651228\n",
      "Iteration 1761, Loss: 1182627248.3537667\n",
      "Iteration 1762, Loss: 1194715315.449222\n",
      "Iteration 1763, Loss: 1329343459.3033006\n",
      "Iteration 1764, Loss: 1344445852.5899441\n",
      "Iteration 1765, Loss: 1077557191.2747884\n",
      "Iteration 1766, Loss: 1279567028.1500454\n",
      "Iteration 1767, Loss: 1464733309.4587295\n",
      "Iteration 1768, Loss: 1224607049.5753474\n",
      "Iteration 1769, Loss: 1302281792.0627508\n",
      "Iteration 1770, Loss: 1277551503.3889027\n",
      "Iteration 1771, Loss: 1252762712.4107172\n",
      "Iteration 1772, Loss: 1257457226.584482\n",
      "Iteration 1773, Loss: 1252733161.5009968\n",
      "Iteration 1774, Loss: 1082635064.5993042\n",
      "Iteration 1775, Loss: 1212896056.5047774\n",
      "Iteration 1776, Loss: 1240991652.841218\n",
      "Iteration 1777, Loss: 1207445261.3352973\n",
      "Iteration 1778, Loss: 1153011819.6270947\n",
      "Iteration 1779, Loss: 1151579705.8148315\n",
      "Iteration 1780, Loss: 1131449170.7006733\n",
      "Iteration 1781, Loss: 1133067789.0608447\n",
      "Iteration 1782, Loss: 1126568030.9746501\n",
      "Iteration 1783, Loss: 1228745855.38818\n",
      "Iteration 1784, Loss: 1214383873.106122\n",
      "Iteration 1785, Loss: 1330107204.7393463\n",
      "Iteration 1786, Loss: 1243570250.627556\n",
      "Iteration 1787, Loss: 1332083942.9310036\n",
      "Iteration 1788, Loss: 1243166942.4826865\n",
      "Iteration 1789, Loss: 1161955673.2447164\n",
      "Iteration 1790, Loss: 1208220060.4563806\n",
      "Iteration 1791, Loss: 1301517505.4320133\n",
      "Iteration 1792, Loss: 1335423854.179749\n",
      "Iteration 1793, Loss: 1361891271.072728\n",
      "Iteration 1794, Loss: 1338909648.5252247\n",
      "Iteration 1795, Loss: 1375827552.551496\n",
      "Iteration 1796, Loss: 1210565683.940843\n",
      "Iteration 1797, Loss: 1114826549.7104023\n",
      "Iteration 1798, Loss: 1293123165.6130404\n",
      "Iteration 1799, Loss: 1233621164.2215836\n",
      "Iteration 1800, Loss: 1218172749.1193678\n",
      "Iteration 1801, Loss: 1204249010.0056598\n",
      "Iteration 1802, Loss: 1195509208.3829663\n",
      "Iteration 1803, Loss: 1183770098.615966\n",
      "Iteration 1804, Loss: 1066916046.0577646\n",
      "Iteration 1805, Loss: 1174541040.832612\n",
      "Iteration 1806, Loss: 1760142214.508008\n",
      "Iteration 1807, Loss: 1528385526.4979725\n",
      "Iteration 1808, Loss: 1462158303.4256449\n",
      "Iteration 1809, Loss: 1437521892.8881469\n",
      "Iteration 1810, Loss: 1340512956.710651\n",
      "Iteration 1811, Loss: 1154617561.4852853\n",
      "Iteration 1812, Loss: 1062534346.0589072\n",
      "Iteration 1813, Loss: 1205477138.081209\n",
      "Iteration 1814, Loss: 1174201911.372465\n",
      "Iteration 1815, Loss: 1104337726.824473\n",
      "Iteration 1816, Loss: 1150685728.2735295\n",
      "Iteration 1817, Loss: 1844914626.6644347\n",
      "Iteration 1818, Loss: 1772947501.4356377\n",
      "Iteration 1819, Loss: 1424522941.4908915\n",
      "Iteration 1820, Loss: 1389833721.393361\n",
      "Iteration 1821, Loss: 1064609440.6325556\n",
      "Iteration 1822, Loss: 1060135222.5782913\n",
      "Iteration 1823, Loss: 1162076787.9564316\n",
      "Iteration 1824, Loss: 1200664490.455468\n",
      "Iteration 1825, Loss: 1191601333.6577215\n",
      "Iteration 1826, Loss: 1220911460.6865633\n",
      "Iteration 1827, Loss: 1207630163.3687084\n",
      "Iteration 1828, Loss: 1237946103.728146\n",
      "Iteration 1829, Loss: 1125819289.725102\n",
      "Iteration 1830, Loss: 1355088603.7715738\n",
      "Iteration 1831, Loss: 1325032788.5894918\n",
      "Iteration 1832, Loss: 1090262902.454641\n",
      "Iteration 1833, Loss: 1141127228.491975\n",
      "Iteration 1834, Loss: 1278658887.3431044\n",
      "Iteration 1835, Loss: 1254655760.345558\n",
      "Iteration 1836, Loss: 1075606594.3577309\n",
      "Iteration 1837, Loss: 1310065297.3948364\n",
      "Iteration 1838, Loss: 1063427233.1963855\n",
      "Iteration 1839, Loss: 1376742031.3156452\n",
      "Iteration 1840, Loss: 1115056842.6462662\n",
      "Iteration 1841, Loss: 2019702779.47064\n",
      "Iteration 1842, Loss: 1053818557.9088755\n",
      "Iteration 1843, Loss: 1053602175.4211054\n",
      "Iteration 1844, Loss: 1111268504.0947607\n",
      "Iteration 1845, Loss: 1158812732.9333513\n",
      "Iteration 1846, Loss: 1190865551.3436437\n",
      "Iteration 1847, Loss: 1157682990.2658148\n",
      "Iteration 1848, Loss: 1133542175.8945923\n",
      "Iteration 1849, Loss: 1128673302.686953\n",
      "Iteration 1850, Loss: 1339890984.4721303\n",
      "Iteration 1851, Loss: 1306390750.570551\n",
      "Iteration 1852, Loss: 1327925949.6959026\n",
      "Iteration 1853, Loss: 1315947868.2661262\n",
      "Iteration 1854, Loss: 1051248845.2731259\n",
      "Iteration 1855, Loss: 1076877316.8357074\n",
      "Iteration 1856, Loss: 1124245483.7677875\n",
      "Iteration 1857, Loss: 1119797698.4446652\n",
      "Iteration 1858, Loss: 1398118502.612451\n",
      "Iteration 1859, Loss: 1304119068.2110634\n",
      "Iteration 1860, Loss: 1357083290.5287511\n",
      "Iteration 1861, Loss: 1331019695.701713\n",
      "Iteration 1862, Loss: 1329599016.4197023\n",
      "Iteration 1863, Loss: 1048805125.6930516\n",
      "Iteration 1864, Loss: 1061812857.2985865\n",
      "Iteration 1865, Loss: 1062787642.2658772\n",
      "Iteration 1866, Loss: 1091780204.5985076\n",
      "Iteration 1867, Loss: 1087160301.1825607\n",
      "Iteration 1868, Loss: 1137114932.675584\n",
      "Iteration 1869, Loss: 1241635430.985339\n",
      "Iteration 1870, Loss: 1191949579.4698453\n",
      "Iteration 1871, Loss: 1174752273.613988\n",
      "Iteration 1872, Loss: 1190383580.598804\n",
      "Iteration 1873, Loss: 1178564614.1951902\n",
      "Iteration 1874, Loss: 1231270300.590678\n",
      "Iteration 1875, Loss: 1220487484.3356514\n",
      "Iteration 1876, Loss: 1157714915.0291865\n",
      "Iteration 1877, Loss: 1484874335.2751274\n",
      "Iteration 1878, Loss: 1101279382.7014053\n",
      "Iteration 1879, Loss: 1120098414.028553\n",
      "Iteration 1880, Loss: 1267277807.6641693\n",
      "Iteration 1881, Loss: 1055843056.2274611\n",
      "Iteration 1882, Loss: 1054311262.4683864\n",
      "Iteration 1883, Loss: 1102396525.559055\n",
      "Iteration 1884, Loss: 1147779257.8786247\n",
      "Iteration 1885, Loss: 1191196529.0882533\n",
      "Iteration 1886, Loss: 1275562749.1764965\n",
      "Iteration 1887, Loss: 1050428068.5386715\n",
      "Iteration 1888, Loss: 1301800468.0146916\n",
      "Iteration 1889, Loss: 1189985885.400146\n",
      "Iteration 1890, Loss: 1331876153.5690114\n",
      "Iteration 1891, Loss: 1056156600.4846736\n",
      "Iteration 1892, Loss: 1440139062.2137754\n",
      "Iteration 1893, Loss: 1257778284.697679\n",
      "Iteration 1894, Loss: 1272467649.4188871\n",
      "Iteration 1895, Loss: 1262137721.803035\n",
      "Iteration 1896, Loss: 1305713783.92769\n",
      "Iteration 1897, Loss: 1276175175.181521\n",
      "Iteration 1898, Loss: 1050150906.2164713\n",
      "Iteration 1899, Loss: 1244359611.5198843\n",
      "Iteration 1900, Loss: 1235961072.480943\n",
      "Iteration 1901, Loss: 1187353847.2382007\n",
      "Iteration 1902, Loss: 1278891960.2397485\n",
      "Iteration 1903, Loss: 1111051210.1573646\n",
      "Iteration 1904, Loss: 1214602036.6271334\n",
      "Iteration 1905, Loss: 1198118299.8775911\n",
      "Iteration 1906, Loss: 1275468839.3033397\n",
      "Iteration 1907, Loss: 1052365752.9651031\n",
      "Iteration 1908, Loss: 1050331925.7141986\n",
      "Iteration 1909, Loss: 2617068045.5807023\n",
      "Iteration 1910, Loss: 1058582186.348008\n",
      "Iteration 1911, Loss: 1053642242.5338856\n",
      "Iteration 1912, Loss: 1044597726.5651277\n",
      "Iteration 1913, Loss: 1879437419.601554\n",
      "Iteration 1914, Loss: 1659822387.2645977\n",
      "Iteration 1915, Loss: 1108948212.1954458\n",
      "Iteration 1916, Loss: 1219159072.6289709\n",
      "Iteration 1917, Loss: 1201649415.270247\n",
      "Iteration 1918, Loss: 1316029316.5978017\n",
      "Iteration 1919, Loss: 1043637735.6363027\n",
      "Iteration 1920, Loss: 1547261457.0158062\n",
      "Iteration 1921, Loss: 1514871556.164378\n",
      "Iteration 1922, Loss: 1473751595.8850574\n",
      "Iteration 1923, Loss: 1481666301.3578978\n",
      "Iteration 1924, Loss: 1050061929.9739552\n",
      "Iteration 1925, Loss: 1096337186.0797162\n",
      "Iteration 1926, Loss: 1097191320.90876\n",
      "Iteration 1927, Loss: 1230073054.7678962\n",
      "Iteration 1928, Loss: 1210499059.7082539\n",
      "Iteration 1929, Loss: 1207875368.6201427\n",
      "Iteration 1930, Loss: 1192540307.831595\n",
      "Iteration 1931, Loss: 1313875869.0850296\n",
      "Iteration 1932, Loss: 1121901871.073596\n",
      "Iteration 1933, Loss: 1156930588.8961816\n",
      "Iteration 1934, Loss: 1186926004.5796545\n",
      "Iteration 1935, Loss: 1172235403.4839008\n",
      "Iteration 1936, Loss: 1052957254.613618\n",
      "Iteration 1937, Loss: 1063013330.4181849\n",
      "Iteration 1938, Loss: 1048771956.3652586\n",
      "Iteration 1939, Loss: 1048747293.937084\n",
      "Iteration 1940, Loss: 2174171268.102981\n",
      "Iteration 1941, Loss: 1983411682.4600413\n",
      "Iteration 1942, Loss: 1830381885.42497\n",
      "Iteration 1943, Loss: 1472893774.9281776\n",
      "Iteration 1944, Loss: 1409068696.9873831\n",
      "Iteration 1945, Loss: 1251983663.6379378\n",
      "Iteration 1946, Loss: 1244877238.8078144\n",
      "Iteration 1947, Loss: 1129498536.0814366\n",
      "Iteration 1948, Loss: 1124808449.2054362\n",
      "Iteration 1949, Loss: 1715086902.1962147\n",
      "Iteration 1950, Loss: 1349013089.0197096\n",
      "Iteration 1951, Loss: 1384076204.2428505\n",
      "Iteration 1952, Loss: 1238036622.3339095\n",
      "Iteration 1953, Loss: 1052027702.7964916\n",
      "Iteration 1954, Loss: 1056390582.1702704\n",
      "Iteration 1955, Loss: 1058107230.6366932\n",
      "Iteration 1956, Loss: 1220721234.77414\n",
      "Iteration 1957, Loss: 1250884606.7254536\n",
      "Iteration 1958, Loss: 1250519914.3181992\n",
      "Iteration 1959, Loss: 1248430995.4665508\n",
      "Iteration 1960, Loss: 1229445816.4583182\n",
      "Iteration 1961, Loss: 1223954322.9912634\n",
      "Iteration 1962, Loss: 1168267655.0870967\n",
      "Iteration 1963, Loss: 1125829228.3054893\n",
      "Iteration 1964, Loss: 1316535451.778029\n",
      "Iteration 1965, Loss: 1365970072.2836106\n",
      "Iteration 1966, Loss: 1285659992.450117\n",
      "Iteration 1967, Loss: 1329519760.4946127\n",
      "Iteration 1968, Loss: 1071080996.1522791\n",
      "Iteration 1969, Loss: 1441952632.5536213\n",
      "Iteration 1970, Loss: 1403797355.3003988\n",
      "Iteration 1971, Loss: 1150487657.951461\n",
      "Iteration 1972, Loss: 1252449052.4566505\n",
      "Iteration 1973, Loss: 1236730829.6500177\n",
      "Iteration 1974, Loss: 1084523635.2136157\n",
      "Iteration 1975, Loss: 1089812189.6420462\n",
      "Iteration 1976, Loss: 1312214293.785567\n",
      "Iteration 1977, Loss: 1327461151.326491\n",
      "Iteration 1978, Loss: 1210509375.1096087\n",
      "Iteration 1979, Loss: 1263346474.6660812\n",
      "Iteration 1980, Loss: 1164508325.1928618\n",
      "Iteration 1981, Loss: 1154205213.6478834\n",
      "Iteration 1982, Loss: 1123359878.447765\n",
      "Iteration 1983, Loss: 1158562011.0370803\n",
      "Iteration 1984, Loss: 1185162028.0851731\n",
      "Iteration 1985, Loss: 1061309043.0782895\n",
      "Iteration 1986, Loss: 1060507570.8482754\n",
      "Iteration 1987, Loss: 1086905928.9933958\n",
      "Iteration 1988, Loss: 1089173511.7085392\n",
      "Iteration 1989, Loss: 1091376687.6420252\n",
      "Iteration 1990, Loss: 1270533538.9972937\n",
      "Iteration 1991, Loss: 1063247706.1668295\n",
      "Iteration 1992, Loss: 1306206916.0232012\n",
      "Iteration 1993, Loss: 1221017371.9052582\n",
      "Iteration 1994, Loss: 1140335062.3156855\n",
      "Iteration 1995, Loss: 1131867534.063861\n",
      "Iteration 1996, Loss: 1165868865.4222567\n",
      "Iteration 1997, Loss: 1223877190.0894864\n",
      "Iteration 1998, Loss: 1208887361.0603611\n",
      "Iteration 1999, Loss: 1194251487.4223614\n",
      "Iteration 2000, Loss: 1233752152.7816427\n",
      "Iteration 2001, Loss: 1226154165.076201\n",
      "Iteration 2002, Loss: 1103713769.1845539\n",
      "Iteration 2003, Loss: 1289770554.5959268\n",
      "Iteration 2004, Loss: 1174121936.293397\n",
      "Iteration 2005, Loss: 1320775617.8946984\n",
      "Iteration 2006, Loss: 1292285096.063171\n",
      "Iteration 2007, Loss: 1351058142.706618\n",
      "Iteration 2008, Loss: 1385614636.110904\n",
      "Iteration 2009, Loss: 1354577911.782553\n",
      "Iteration 2010, Loss: 1149952409.2480917\n",
      "Iteration 2011, Loss: 1319247837.8690119\n",
      "Iteration 2012, Loss: 1154241031.7516856\n",
      "Iteration 2013, Loss: 1144823061.1555398\n",
      "Iteration 2014, Loss: 1060093167.7476918\n",
      "Iteration 2015, Loss: 1060018960.6438708\n",
      "Iteration 2016, Loss: 1171291514.877398\n",
      "Iteration 2017, Loss: 1061590898.3359472\n",
      "Iteration 2018, Loss: 1157043584.0295534\n",
      "Iteration 2019, Loss: 1159189855.8540745\n",
      "Iteration 2020, Loss: 1155481692.8389065\n",
      "Iteration 2021, Loss: 1143446484.5502014\n",
      "Iteration 2022, Loss: 1168404884.7721243\n",
      "Iteration 2023, Loss: 1130216820.4849553\n",
      "Iteration 2024, Loss: 1130954117.328095\n",
      "Iteration 2025, Loss: 1092106420.3924484\n",
      "Iteration 2026, Loss: 1096792423.696246\n",
      "Iteration 2027, Loss: 1388138069.2323236\n",
      "Iteration 2028, Loss: 1189005623.8356469\n",
      "Iteration 2029, Loss: 1289211705.8890197\n",
      "Iteration 2030, Loss: 1310369340.9208329\n",
      "Iteration 2031, Loss: 1050954267.2805018\n",
      "Iteration 2032, Loss: 1053578988.6035264\n",
      "Iteration 2033, Loss: 1099820383.1939535\n",
      "Iteration 2034, Loss: 1055118546.4044292\n",
      "Iteration 2035, Loss: 1302198383.6724098\n",
      "Iteration 2036, Loss: 1336296120.5195878\n",
      "Iteration 2037, Loss: 1097673904.1863813\n",
      "Iteration 2038, Loss: 1193578843.1398265\n",
      "Iteration 2039, Loss: 1331036178.0795455\n",
      "Iteration 2040, Loss: 1299536470.773516\n",
      "Iteration 2041, Loss: 1288454699.97233\n",
      "Iteration 2042, Loss: 1307224910.8114882\n",
      "Iteration 2043, Loss: 1121537880.4046745\n",
      "Iteration 2044, Loss: 1102818574.205567\n",
      "Iteration 2045, Loss: 1103457466.5524182\n",
      "Iteration 2046, Loss: 1095909565.6953776\n",
      "Iteration 2047, Loss: 1089292127.3513691\n",
      "Iteration 2048, Loss: 1099657876.4787335\n",
      "Iteration 2049, Loss: 1090984591.8823347\n",
      "Iteration 2050, Loss: 1055391155.5707923\n",
      "Iteration 2051, Loss: 1379468396.6701586\n",
      "Iteration 2052, Loss: 1416219706.61921\n",
      "Iteration 2053, Loss: 1222069394.0777\n",
      "Iteration 2054, Loss: 1272529959.2228441\n",
      "Iteration 2055, Loss: 1140904561.282499\n",
      "Iteration 2056, Loss: 1140465723.4411597\n",
      "Iteration 2057, Loss: 1222811937.4663355\n",
      "Iteration 2058, Loss: 1253494102.3013368\n",
      "Iteration 2059, Loss: 1124626516.4780972\n",
      "Iteration 2060, Loss: 1170993328.8344405\n",
      "Iteration 2061, Loss: 1296319154.8349075\n",
      "Iteration 2062, Loss: 1268537216.9686825\n",
      "Iteration 2063, Loss: 1105586429.131402\n",
      "Iteration 2064, Loss: 1310661839.6755807\n",
      "Iteration 2065, Loss: 1127935141.291105\n",
      "Iteration 2066, Loss: 1931245085.1032243\n",
      "Iteration 2067, Loss: 1837970263.5157397\n",
      "Iteration 2068, Loss: 1074414915.0921059\n",
      "Iteration 2069, Loss: 1217359847.989734\n",
      "Iteration 2070, Loss: 1048943479.0623693\n",
      "Iteration 2071, Loss: 1556041594.1657412\n",
      "Iteration 2072, Loss: 1475103750.096112\n",
      "Iteration 2073, Loss: 1433758282.5284998\n",
      "Iteration 2074, Loss: 1261882393.4570487\n",
      "Iteration 2075, Loss: 1309623497.4781945\n",
      "Iteration 2076, Loss: 1279596460.7120724\n",
      "Iteration 2077, Loss: 1257063096.519238\n",
      "Iteration 2078, Loss: 1329130818.4077203\n",
      "Iteration 2079, Loss: 1212999745.4320998\n",
      "Iteration 2080, Loss: 1062460945.845379\n",
      "Iteration 2081, Loss: 1169200481.9039478\n",
      "Iteration 2082, Loss: 1168160622.9763508\n",
      "Iteration 2083, Loss: 1166926514.9617848\n",
      "Iteration 2084, Loss: 1172049881.529732\n",
      "Iteration 2085, Loss: 1301360441.8496735\n",
      "Iteration 2086, Loss: 1126657918.0376823\n",
      "Iteration 2087, Loss: 1198594253.2190423\n",
      "Iteration 2088, Loss: 1323424049.4173477\n",
      "Iteration 2089, Loss: 1271299184.0662277\n",
      "Iteration 2090, Loss: 1251151982.9928663\n",
      "Iteration 2091, Loss: 1230298312.4739811\n",
      "Iteration 2092, Loss: 1264355052.312466\n",
      "Iteration 2093, Loss: 1064674063.4595711\n",
      "Iteration 2094, Loss: 1304645343.6528106\n",
      "Iteration 2095, Loss: 1360455120.7206542\n",
      "Iteration 2096, Loss: 1335461005.595716\n",
      "Iteration 2097, Loss: 1067018164.1114178\n",
      "Iteration 2098, Loss: 1166227831.0398788\n",
      "Iteration 2099, Loss: 1130213749.6341236\n",
      "Iteration 2100, Loss: 1318857882.7543247\n",
      "Iteration 2101, Loss: 1080910004.8213048\n",
      "Iteration 2102, Loss: 1409038423.9136765\n",
      "Iteration 2103, Loss: 1151690163.310808\n",
      "Iteration 2104, Loss: 1142854355.6342688\n",
      "Iteration 2105, Loss: 1177397617.0907729\n",
      "Iteration 2106, Loss: 1305079820.2122054\n",
      "Iteration 2107, Loss: 1224069810.5283542\n",
      "Iteration 2108, Loss: 1165545614.530801\n",
      "Iteration 2109, Loss: 1182000736.9754345\n",
      "Iteration 2110, Loss: 1054514602.4593393\n",
      "Iteration 2111, Loss: 1060517041.6148775\n",
      "Iteration 2112, Loss: 1062013381.878311\n",
      "Iteration 2113, Loss: 1065752682.1147432\n",
      "Iteration 2114, Loss: 1204539163.819968\n",
      "Iteration 2115, Loss: 1061959313.1052353\n",
      "Iteration 2116, Loss: 1064086137.4190702\n",
      "Iteration 2117, Loss: 1284032419.1430385\n",
      "Iteration 2118, Loss: 1260960884.635406\n",
      "Iteration 2119, Loss: 1114207563.9786518\n",
      "Iteration 2120, Loss: 1135719707.3269641\n",
      "Iteration 2121, Loss: 1092927460.5071037\n",
      "Iteration 2122, Loss: 1087130809.7776515\n",
      "Iteration 2123, Loss: 1137250373.940842\n",
      "Iteration 2124, Loss: 1136333291.1791441\n",
      "Iteration 2125, Loss: 1186489680.7727215\n",
      "Iteration 2126, Loss: 1321300656.6138525\n",
      "Iteration 2127, Loss: 1253064874.1815403\n",
      "Iteration 2128, Loss: 1281558270.5174618\n",
      "Iteration 2129, Loss: 1295831283.9478161\n",
      "Iteration 2130, Loss: 1113265586.4516664\n",
      "Iteration 2131, Loss: 2027174970.592508\n",
      "Iteration 2132, Loss: 1100624850.4909687\n",
      "Iteration 2133, Loss: 1959394474.0576909\n",
      "Iteration 2134, Loss: 1508540212.1966734\n",
      "Iteration 2135, Loss: 1484330133.3413384\n",
      "Iteration 2136, Loss: 1131201741.4010615\n",
      "Iteration 2137, Loss: 1053704043.7325319\n",
      "Iteration 2138, Loss: 1247445074.9801831\n",
      "Iteration 2139, Loss: 1201683519.7277033\n",
      "Iteration 2140, Loss: 1231745280.3391132\n",
      "Iteration 2141, Loss: 1090834638.8619702\n",
      "Iteration 2142, Loss: 1260392407.1073945\n",
      "Iteration 2143, Loss: 1210587928.2803698\n",
      "Iteration 2144, Loss: 1197737230.3290732\n",
      "Iteration 2145, Loss: 1189045763.260987\n",
      "Iteration 2146, Loss: 1204790808.006257\n",
      "Iteration 2147, Loss: 1170392162.0700622\n",
      "Iteration 2148, Loss: 1269584387.6268451\n",
      "Iteration 2149, Loss: 1126468253.429709\n",
      "Iteration 2150, Loss: 1120384438.7139263\n",
      "Iteration 2151, Loss: 1303437260.7932405\n",
      "Iteration 2152, Loss: 1278953894.9277496\n",
      "Iteration 2153, Loss: 1065404197.3381215\n",
      "Iteration 2154, Loss: 1294246020.1777267\n",
      "Iteration 2155, Loss: 1068838348.0664982\n",
      "Iteration 2156, Loss: 1070603171.0318406\n",
      "Iteration 2157, Loss: 1080678654.5182645\n",
      "Iteration 2158, Loss: 1077816379.5593724\n",
      "Iteration 2159, Loss: 1267078942.195846\n",
      "Iteration 2160, Loss: 1070670403.7127377\n",
      "Iteration 2161, Loss: 1069689046.5567305\n",
      "Iteration 2162, Loss: 1099479902.7205508\n",
      "Iteration 2163, Loss: 1253415806.7234719\n",
      "Iteration 2164, Loss: 1132964955.592466\n",
      "Iteration 2165, Loss: 1113674485.4463022\n",
      "Iteration 2166, Loss: 1117236182.3506277\n",
      "Iteration 2167, Loss: 1105584365.6509778\n",
      "Iteration 2168, Loss: 1377253060.4356265\n",
      "Iteration 2169, Loss: 1296899203.4894803\n",
      "Iteration 2170, Loss: 1142641602.9989598\n",
      "Iteration 2171, Loss: 1199642762.108569\n",
      "Iteration 2172, Loss: 1254969496.2639518\n",
      "Iteration 2173, Loss: 1066724848.5548348\n",
      "Iteration 2174, Loss: 1077586030.303894\n",
      "Iteration 2175, Loss: 1198223115.6206982\n",
      "Iteration 2176, Loss: 1185411433.2424028\n",
      "Iteration 2177, Loss: 1177597683.7864041\n",
      "Iteration 2178, Loss: 1276757488.9559755\n",
      "Iteration 2179, Loss: 1146422188.5198991\n",
      "Iteration 2180, Loss: 1193272443.35282\n",
      "Iteration 2181, Loss: 1288332616.8919413\n",
      "Iteration 2182, Loss: 1331701769.2449536\n",
      "Iteration 2183, Loss: 1334230689.82649\n",
      "Iteration 2184, Loss: 1212494638.3558884\n",
      "Iteration 2185, Loss: 1199120962.334326\n",
      "Iteration 2186, Loss: 1183410492.5670497\n",
      "Iteration 2187, Loss: 1140429381.3593254\n",
      "Iteration 2188, Loss: 1192497085.8370092\n",
      "Iteration 2189, Loss: 1159586150.5987806\n",
      "Iteration 2190, Loss: 1216162260.4858775\n",
      "Iteration 2191, Loss: 1242879597.5974836\n",
      "Iteration 2192, Loss: 1233120313.3070333\n",
      "Iteration 2193, Loss: 1257917408.4500096\n",
      "Iteration 2194, Loss: 1117188690.4569921\n",
      "Iteration 2195, Loss: 1139089000.6691256\n",
      "Iteration 2196, Loss: 1094855557.080388\n",
      "Iteration 2197, Loss: 1142204890.9835808\n",
      "Iteration 2198, Loss: 1188512914.9460669\n",
      "Iteration 2199, Loss: 1175029359.586844\n",
      "Iteration 2200, Loss: 1156818604.9571588\n",
      "Iteration 2201, Loss: 1256618779.3408434\n",
      "Iteration 2202, Loss: 1323966685.8374915\n",
      "Iteration 2203, Loss: 1298449953.4157572\n",
      "Iteration 2204, Loss: 1329343413.6589112\n",
      "Iteration 2205, Loss: 1299691101.7041306\n",
      "Iteration 2206, Loss: 1349553139.0987644\n",
      "Iteration 2207, Loss: 1384396170.9967282\n",
      "Iteration 2208, Loss: 1210006198.7843425\n",
      "Iteration 2209, Loss: 1062014219.2616493\n",
      "Iteration 2210, Loss: 1056379737.9501182\n",
      "Iteration 2211, Loss: 1090041339.5316048\n",
      "Iteration 2212, Loss: 1086433042.5288897\n",
      "Iteration 2213, Loss: 1132717429.5779128\n",
      "Iteration 2214, Loss: 1122608065.57393\n",
      "Iteration 2215, Loss: 1116000248.9650805\n",
      "Iteration 2216, Loss: 1173289701.5037534\n",
      "Iteration 2217, Loss: 1193387078.8114543\n",
      "Iteration 2218, Loss: 1051458314.4101065\n",
      "Iteration 2219, Loss: 1429490050.9761043\n",
      "Iteration 2220, Loss: 1467702210.0373354\n",
      "Iteration 2221, Loss: 1160701121.9216917\n",
      "Iteration 2222, Loss: 1159796289.495862\n",
      "Iteration 2223, Loss: 1149779921.4419043\n",
      "Iteration 2224, Loss: 1147381658.8674796\n",
      "Iteration 2225, Loss: 1566805328.6525674\n",
      "Iteration 2226, Loss: 1518982832.4060104\n",
      "Iteration 2227, Loss: 1480129179.3870645\n",
      "Iteration 2228, Loss: 1457156950.5612183\n",
      "Iteration 2229, Loss: 1450605962.4069755\n",
      "Iteration 2230, Loss: 1114158069.8956716\n",
      "Iteration 2231, Loss: 1087035672.7417238\n",
      "Iteration 2232, Loss: 2808094343.0772295\n",
      "Iteration 2233, Loss: 1295427486.4862\n",
      "Iteration 2234, Loss: 1050905949.0606599\n",
      "Iteration 2235, Loss: 1051425740.3641636\n",
      "Iteration 2236, Loss: 1052423079.7280843\n",
      "Iteration 2237, Loss: 2708705612.027952\n",
      "Iteration 2238, Loss: 2736534561.449154\n",
      "Iteration 2239, Loss: 2499163761.5491624\n",
      "Iteration 2240, Loss: 1750158305.9887438\n",
      "Iteration 2241, Loss: 1606943370.85173\n",
      "Iteration 2242, Loss: 1349407825.855079\n",
      "Iteration 2243, Loss: 1103654618.6766713\n",
      "Iteration 2244, Loss: 1150995063.8766782\n",
      "Iteration 2245, Loss: 1185503887.1769726\n",
      "Iteration 2246, Loss: 1202569921.745417\n",
      "Iteration 2247, Loss: 1326639787.651877\n",
      "Iteration 2248, Loss: 1373350680.915286\n",
      "Iteration 2249, Loss: 1197590573.760956\n",
      "Iteration 2250, Loss: 1127917043.7967224\n",
      "Iteration 2251, Loss: 1346392963.791747\n",
      "Iteration 2252, Loss: 1341564392.9846997\n",
      "Iteration 2253, Loss: 1085954727.3576007\n",
      "Iteration 2254, Loss: 1461148590.0622044\n",
      "Iteration 2255, Loss: 1440118989.0257118\n",
      "Iteration 2256, Loss: 1479393526.3544536\n",
      "Iteration 2257, Loss: 1519094339.9226794\n",
      "Iteration 2258, Loss: 1479983849.1597817\n",
      "Iteration 2259, Loss: 1285356925.196818\n",
      "Iteration 2260, Loss: 1348542985.65909\n",
      "Iteration 2261, Loss: 1067660490.2217084\n",
      "Iteration 2262, Loss: 1462917287.4947653\n",
      "Iteration 2263, Loss: 1502528671.4681776\n",
      "Iteration 2264, Loss: 1435239289.8797822\n",
      "Iteration 2265, Loss: 1107965091.106573\n",
      "Iteration 2266, Loss: 1052253857.8015392\n",
      "Iteration 2267, Loss: 1355708934.0893154\n",
      "Iteration 2268, Loss: 1141716457.1010776\n",
      "Iteration 2269, Loss: 1136632086.8475697\n",
      "Iteration 2270, Loss: 1645050280.0752394\n",
      "Iteration 2271, Loss: 1300278771.3549395\n",
      "Iteration 2272, Loss: 1388523114.4858098\n",
      "Iteration 2273, Loss: 1416951726.3276744\n",
      "Iteration 2274, Loss: 1105081315.0513875\n",
      "Iteration 2275, Loss: 1095222523.9101162\n",
      "Iteration 2276, Loss: 2013704398.4319289\n",
      "Iteration 2277, Loss: 1737348400.7766345\n",
      "Iteration 2278, Loss: 1049828720.8445286\n",
      "Iteration 2279, Loss: 1048412086.3609127\n",
      "Iteration 2280, Loss: 1049809505.1043586\n",
      "Iteration 2281, Loss: 1079396534.7794218\n",
      "Iteration 2282, Loss: 1200712905.7064717\n",
      "Iteration 2283, Loss: 1141049060.4513063\n",
      "Iteration 2284, Loss: 1245504536.1870477\n",
      "Iteration 2285, Loss: 1075600422.6458535\n",
      "Iteration 2286, Loss: 1185758791.231121\n",
      "Iteration 2287, Loss: 1281743120.7237341\n",
      "Iteration 2288, Loss: 1441453136.3403764\n",
      "Iteration 2289, Loss: 1108588894.9459271\n",
      "Iteration 2290, Loss: 1084059938.0539157\n",
      "Iteration 2291, Loss: 5388679309.5790205\n",
      "Iteration 2292, Loss: 1548568782.210277\n",
      "Iteration 2293, Loss: 1046885641.8478925\n",
      "Iteration 2294, Loss: 1047595896.5911851\n",
      "Iteration 2295, Loss: 1046420929.1487142\n",
      "Iteration 2296, Loss: 1045790719.1739779\n",
      "Iteration 2297, Loss: 1247166940.3942018\n",
      "Iteration 2298, Loss: 1130560870.7578635\n",
      "Iteration 2299, Loss: 1237342596.2450168\n",
      "Iteration 2300, Loss: 1215933921.9415584\n",
      "Iteration 2301, Loss: 1200480065.6966114\n",
      "Iteration 2302, Loss: 1168545219.7426896\n",
      "Iteration 2303, Loss: 1149579937.078158\n",
      "Iteration 2304, Loss: 1181234688.21222\n",
      "Iteration 2305, Loss: 1220328199.819144\n",
      "Iteration 2306, Loss: 1127277761.623506\n",
      "Iteration 2307, Loss: 1182902337.7337427\n",
      "Iteration 2308, Loss: 1273218551.196034\n",
      "Iteration 2309, Loss: 1321530766.7252223\n",
      "Iteration 2310, Loss: 1327057490.2462306\n",
      "Iteration 2311, Loss: 1290295322.4468753\n",
      "Iteration 2312, Loss: 1122290031.0248716\n",
      "Iteration 2313, Loss: 1159052332.988991\n",
      "Iteration 2314, Loss: 1054973771.6979934\n",
      "Iteration 2315, Loss: 1300171817.7503848\n",
      "Iteration 2316, Loss: 1091390814.9806397\n",
      "Iteration 2317, Loss: 1222506030.3660553\n",
      "Iteration 2318, Loss: 1156657369.454819\n",
      "Iteration 2319, Loss: 1265294838.292067\n",
      "Iteration 2320, Loss: 1241121930.3481114\n",
      "Iteration 2321, Loss: 1219447136.9710758\n",
      "Iteration 2322, Loss: 1057597235.8863219\n",
      "Iteration 2323, Loss: 1214793981.7862785\n",
      "Iteration 2324, Loss: 1107953080.412838\n",
      "Iteration 2325, Loss: 1357732064.1541624\n",
      "Iteration 2326, Loss: 1229119607.9508002\n",
      "Iteration 2327, Loss: 1159187695.337714\n",
      "Iteration 2328, Loss: 1183097462.8541272\n",
      "Iteration 2329, Loss: 1057789973.8779492\n",
      "Iteration 2330, Loss: 1068273866.3861426\n",
      "Iteration 2331, Loss: 1112581830.969818\n",
      "Iteration 2332, Loss: 1226713096.862902\n",
      "Iteration 2333, Loss: 1125513983.8568707\n",
      "Iteration 2334, Loss: 1120625810.0120666\n",
      "Iteration 2335, Loss: 1277906620.5717254\n",
      "Iteration 2336, Loss: 1334218898.4738252\n",
      "Iteration 2337, Loss: 1047785271.8920603\n",
      "Iteration 2338, Loss: 3752564851.107082\n",
      "Iteration 2339, Loss: 1110791889.6773381\n",
      "Iteration 2340, Loss: 1244420185.2788432\n",
      "Iteration 2341, Loss: 1052430105.6488047\n",
      "Iteration 2342, Loss: 1559909425.4824045\n",
      "Iteration 2343, Loss: 1136860973.6419768\n",
      "Iteration 2344, Loss: 1161091367.2870598\n",
      "Iteration 2345, Loss: 1204695353.1409056\n",
      "Iteration 2346, Loss: 1141721115.8121328\n",
      "Iteration 2347, Loss: 1259257782.475725\n",
      "Iteration 2348, Loss: 1305756514.0048294\n",
      "Iteration 2349, Loss: 1219370354.7059383\n",
      "Iteration 2350, Loss: 1162073625.6861832\n",
      "Iteration 2351, Loss: 1184921589.6254828\n",
      "Iteration 2352, Loss: 1169205480.4070518\n",
      "Iteration 2353, Loss: 1299720134.8061075\n",
      "Iteration 2354, Loss: 1340901336.2325833\n",
      "Iteration 2355, Loss: 1378861181.7753637\n",
      "Iteration 2356, Loss: 1081887280.934806\n",
      "Iteration 2357, Loss: 1471107426.751848\n",
      "Iteration 2358, Loss: 1447048432.413569\n",
      "Iteration 2359, Loss: 1137098658.7272904\n",
      "Iteration 2360, Loss: 1209352640.6307237\n",
      "Iteration 2361, Loss: 1108693629.984368\n",
      "Iteration 2362, Loss: 1110063446.1300576\n",
      "Iteration 2363, Loss: 1163612318.0682504\n",
      "Iteration 2364, Loss: 1466005916.6858618\n",
      "Iteration 2365, Loss: 1074666792.7165754\n",
      "Iteration 2366, Loss: 1186740097.5613708\n",
      "Iteration 2367, Loss: 1692438658.7425525\n",
      "Iteration 2368, Loss: 1609861427.9199107\n",
      "Iteration 2369, Loss: 1200769310.425877\n",
      "Iteration 2370, Loss: 1186583847.1885176\n",
      "Iteration 2371, Loss: 1240473399.282218\n",
      "Iteration 2372, Loss: 1255752778.0571916\n",
      "Iteration 2373, Loss: 1504626426.241186\n",
      "Iteration 2374, Loss: 1462182060.9724464\n",
      "Iteration 2375, Loss: 1160271218.1376684\n",
      "Iteration 2376, Loss: 1147725400.764385\n",
      "Iteration 2377, Loss: 1276850140.784484\n",
      "Iteration 2378, Loss: 1329464784.7838216\n",
      "Iteration 2379, Loss: 1332564530.763423\n",
      "Iteration 2380, Loss: 1211118577.0319493\n",
      "Iteration 2381, Loss: 1264039916.5367727\n",
      "Iteration 2382, Loss: 1111520781.0625253\n",
      "Iteration 2383, Loss: 1058284176.399993\n",
      "Iteration 2384, Loss: 1106522371.4504666\n",
      "Iteration 2385, Loss: 2070679270.529693\n",
      "Iteration 2386, Loss: 1069501779.2997609\n",
      "Iteration 2387, Loss: 1099977635.2510967\n",
      "Iteration 2388, Loss: 1956690527.6783288\n",
      "Iteration 2389, Loss: 1873644996.0306156\n",
      "Iteration 2390, Loss: 1508249106.0502357\n",
      "Iteration 2391, Loss: 1124888312.2450943\n",
      "Iteration 2392, Loss: 1287570245.8802214\n",
      "Iteration 2393, Loss: 1321826945.8977385\n",
      "Iteration 2394, Loss: 1064513268.1765152\n",
      "Iteration 2395, Loss: 1463471066.8998928\n",
      "Iteration 2396, Loss: 1439860314.842389\n",
      "Iteration 2397, Loss: 1080014459.2442787\n",
      "Iteration 2398, Loss: 1453746130.6136503\n",
      "Iteration 2399, Loss: 1167668027.3792536\n",
      "Iteration 2400, Loss: 1211059557.696816\n",
      "Iteration 2401, Loss: 1323511778.395134\n",
      "Iteration 2402, Loss: 1213242256.0582867\n",
      "Iteration 2403, Loss: 1061189257.9780966\n",
      "Iteration 2404, Loss: 1473248700.1492755\n",
      "Iteration 2405, Loss: 1447051168.7567093\n",
      "Iteration 2406, Loss: 1401515952.4174662\n",
      "Iteration 2407, Loss: 1394971446.161096\n",
      "Iteration 2408, Loss: 1378303098.3502347\n",
      "Iteration 2409, Loss: 1374248354.4449031\n",
      "Iteration 2410, Loss: 1056262973.0976267\n",
      "Iteration 2411, Loss: 1102986201.6886735\n",
      "Iteration 2412, Loss: 1289394037.6723223\n",
      "Iteration 2413, Loss: 1279783235.752362\n",
      "Iteration 2414, Loss: 1232688093.7265127\n",
      "Iteration 2415, Loss: 1162824477.036192\n",
      "Iteration 2416, Loss: 1186919524.8544075\n",
      "Iteration 2417, Loss: 1104911394.6919618\n",
      "Iteration 2418, Loss: 1196483748.1873403\n",
      "Iteration 2419, Loss: 1050947885.1595445\n",
      "Iteration 2420, Loss: 1510495063.960909\n",
      "Iteration 2421, Loss: 1475033962.7609444\n",
      "Iteration 2422, Loss: 1364434885.9777453\n",
      "Iteration 2423, Loss: 1079810524.8914092\n",
      "Iteration 2424, Loss: 4995093697.246085\n",
      "Iteration 2425, Loss: 1137985128.6201396\n",
      "Iteration 2426, Loss: 1195368157.5101626\n",
      "Iteration 2427, Loss: 1142528162.7498517\n",
      "Iteration 2428, Loss: 1196159345.3477988\n",
      "Iteration 2429, Loss: 1226117417.426962\n",
      "Iteration 2430, Loss: 1252733217.56892\n",
      "Iteration 2431, Loss: 1275051982.0175877\n",
      "Iteration 2432, Loss: 1188782513.0539956\n",
      "Iteration 2433, Loss: 1271761654.0683696\n",
      "Iteration 2434, Loss: 1306090092.445443\n",
      "Iteration 2435, Loss: 1056622135.7543309\n",
      "Iteration 2436, Loss: 1056102816.8464537\n",
      "Iteration 2437, Loss: 1525009905.0698018\n",
      "Iteration 2438, Loss: 1151884283.5130386\n",
      "Iteration 2439, Loss: 1268532741.9160829\n",
      "Iteration 2440, Loss: 1248252831.3240404\n",
      "Iteration 2441, Loss: 1255918833.9881232\n",
      "Iteration 2442, Loss: 1238072867.5504637\n",
      "Iteration 2443, Loss: 1316477544.7713284\n",
      "Iteration 2444, Loss: 1293198975.7201848\n",
      "Iteration 2445, Loss: 1269499419.591885\n",
      "Iteration 2446, Loss: 1069619618.3978826\n",
      "Iteration 2447, Loss: 1312479607.6277544\n",
      "Iteration 2448, Loss: 1285525022.2697794\n",
      "Iteration 2449, Loss: 1309616715.7185204\n",
      "Iteration 2450, Loss: 1129381517.4550567\n",
      "Iteration 2451, Loss: 1169058906.3627043\n",
      "Iteration 2452, Loss: 1196686180.7435904\n",
      "Iteration 2453, Loss: 1181361905.6753306\n",
      "Iteration 2454, Loss: 1153317609.5667236\n",
      "Iteration 2455, Loss: 1179374588.2050908\n",
      "Iteration 2456, Loss: 1275468829.1134431\n",
      "Iteration 2457, Loss: 1170772527.918151\n",
      "Iteration 2458, Loss: 1193582377.488001\n",
      "Iteration 2459, Loss: 1138419542.959023\n",
      "Iteration 2460, Loss: 1128500924.4376807\n",
      "Iteration 2461, Loss: 1219128760.275269\n",
      "Iteration 2462, Loss: 1206885825.3223205\n",
      "Iteration 2463, Loss: 1147486082.0856357\n",
      "Iteration 2464, Loss: 1188650602.0738776\n",
      "Iteration 2465, Loss: 1703164044.0178967\n",
      "Iteration 2466, Loss: 1745276766.978927\n",
      "Iteration 2467, Loss: 1103741594.2543218\n",
      "Iteration 2468, Loss: 1225289440.4379826\n",
      "Iteration 2469, Loss: 1135259084.3103886\n",
      "Iteration 2470, Loss: 1169595599.4359558\n",
      "Iteration 2471, Loss: 1104079179.2657564\n",
      "Iteration 2472, Loss: 1225454959.771719\n",
      "Iteration 2473, Loss: 1185571081.4272425\n",
      "Iteration 2474, Loss: 1241470685.7215455\n",
      "Iteration 2475, Loss: 1160267377.8062398\n",
      "Iteration 2476, Loss: 1272425445.0047636\n",
      "Iteration 2477, Loss: 1241430392.1930614\n",
      "Iteration 2478, Loss: 1273799024.001358\n",
      "Iteration 2479, Loss: 1251047640.42448\n",
      "Iteration 2480, Loss: 1203773319.364646\n",
      "Iteration 2481, Loss: 1653081986.9298131\n",
      "Iteration 2482, Loss: 1202388854.910229\n",
      "Iteration 2483, Loss: 1123126341.824512\n",
      "Iteration 2484, Loss: 1123386507.911531\n",
      "Iteration 2485, Loss: 1174950718.5328457\n",
      "Iteration 2486, Loss: 1207227176.3272772\n",
      "Iteration 2487, Loss: 1196272756.935406\n",
      "Iteration 2488, Loss: 1151655888.6248758\n",
      "Iteration 2489, Loss: 1164450582.5695395\n",
      "Iteration 2490, Loss: 1264626554.3160365\n",
      "Iteration 2491, Loss: 1493012081.145977\n",
      "Iteration 2492, Loss: 1389891698.305666\n",
      "Iteration 2493, Loss: 1357467056.6391442\n",
      "Iteration 2494, Loss: 1394833092.0750542\n",
      "Iteration 2495, Loss: 1151045235.774572\n",
      "Iteration 2496, Loss: 1130801593.61607\n",
      "Iteration 2497, Loss: 1074385858.899068\n",
      "Iteration 2498, Loss: 1186613959.5620847\n",
      "Iteration 2499, Loss: 1161467154.9731872\n",
      "Iteration 2500, Loss: 1194892147.3819172\n",
      "Iteration 2501, Loss: 1180712427.0726693\n",
      "Iteration 2502, Loss: 1728403308.4352872\n",
      "Iteration 2503, Loss: 1353581466.1910584\n",
      "Iteration 2504, Loss: 1054762530.0512102\n",
      "Iteration 2505, Loss: 2127011205.1954684\n",
      "Iteration 2506, Loss: 1926842121.0195067\n",
      "Iteration 2507, Loss: 1848939242.6917186\n",
      "Iteration 2508, Loss: 1488793099.0681236\n",
      "Iteration 2509, Loss: 1464388989.3546395\n",
      "Iteration 2510, Loss: 1168987577.565785\n",
      "Iteration 2511, Loss: 1280703600.6714895\n",
      "Iteration 2512, Loss: 1067516033.8125331\n",
      "Iteration 2513, Loss: 2015342811.9766023\n",
      "Iteration 2514, Loss: 2058612399.97226\n",
      "Iteration 2515, Loss: 1930365291.8679125\n",
      "Iteration 2516, Loss: 1186283886.1283603\n",
      "Iteration 2517, Loss: 1160115611.0825226\n",
      "Iteration 2518, Loss: 1070470231.5680755\n",
      "Iteration 2519, Loss: 1121606790.187464\n",
      "Iteration 2520, Loss: 1205100349.0858183\n",
      "Iteration 2521, Loss: 1193713236.0809844\n",
      "Iteration 2522, Loss: 1192499570.5713549\n",
      "Iteration 2523, Loss: 1250727045.7035806\n",
      "Iteration 2524, Loss: 1540222839.614798\n",
      "Iteration 2525, Loss: 1185820609.6357121\n",
      "Iteration 2526, Loss: 1215336126.3937135\n",
      "Iteration 2527, Loss: 1290772645.1557386\n",
      "Iteration 2528, Loss: 1317468587.0062888\n",
      "Iteration 2529, Loss: 1341513530.268734\n",
      "Iteration 2530, Loss: 1274261361.7813764\n",
      "Iteration 2531, Loss: 1149962679.585218\n",
      "Iteration 2532, Loss: 1337352508.7932575\n",
      "Iteration 2533, Loss: 1353611702.7077844\n",
      "Iteration 2534, Loss: 1161278994.5324776\n",
      "Iteration 2535, Loss: 1278743694.473093\n",
      "Iteration 2536, Loss: 1345746444.2188473\n",
      "Iteration 2537, Loss: 1392147297.60891\n",
      "Iteration 2538, Loss: 1058852261.9046966\n",
      "Iteration 2539, Loss: 1345237034.8693902\n",
      "Iteration 2540, Loss: 1320844081.9442444\n",
      "Iteration 2541, Loss: 1332925318.7309172\n",
      "Iteration 2542, Loss: 1144349191.8434532\n",
      "Iteration 2543, Loss: 1181914435.135083\n",
      "Iteration 2544, Loss: 1210901795.698386\n",
      "Iteration 2545, Loss: 1208976285.0365849\n",
      "Iteration 2546, Loss: 1289763292.045663\n",
      "Iteration 2547, Loss: 1350162731.6577947\n",
      "Iteration 2548, Loss: 1072775166.9390063\n",
      "Iteration 2549, Loss: 1449671927.7792723\n",
      "Iteration 2550, Loss: 1068915153.3677139\n",
      "Iteration 2551, Loss: 1660025495.66339\n",
      "Iteration 2552, Loss: 1065605165.020561\n",
      "Iteration 2553, Loss: 1183687253.7205021\n",
      "Iteration 2554, Loss: 1241557154.4600604\n",
      "Iteration 2555, Loss: 1213169629.7188761\n",
      "Iteration 2556, Loss: 1180047337.9618225\n",
      "Iteration 2557, Loss: 1747248751.7366621\n",
      "Iteration 2558, Loss: 1370971117.8542805\n",
      "Iteration 2559, Loss: 1318296384.6562345\n",
      "Iteration 2560, Loss: 1106245320.595581\n",
      "Iteration 2561, Loss: 1056513283.5113766\n",
      "Iteration 2562, Loss: 1058260353.4812381\n",
      "Iteration 2563, Loss: 1150443409.7392068\n",
      "Iteration 2564, Loss: 1255280619.3833027\n",
      "Iteration 2565, Loss: 1174578313.8110046\n",
      "Iteration 2566, Loss: 1054171116.1686577\n",
      "Iteration 2567, Loss: 1058435852.8053137\n",
      "Iteration 2568, Loss: 2059411549.4107327\n",
      "Iteration 2569, Loss: 1265172928.9176853\n",
      "Iteration 2570, Loss: 1330780708.4160788\n",
      "Iteration 2571, Loss: 1334703829.4580216\n",
      "Iteration 2572, Loss: 1359820539.9205778\n",
      "Iteration 2573, Loss: 1228713226.9564662\n",
      "Iteration 2574, Loss: 1226485944.619999\n",
      "Iteration 2575, Loss: 1268113673.068497\n",
      "Iteration 2576, Loss: 1286896151.0402737\n",
      "Iteration 2577, Loss: 1332863638.7031195\n",
      "Iteration 2578, Loss: 1299803653.0376751\n",
      "Iteration 2579, Loss: 1139311784.7396545\n",
      "Iteration 2580, Loss: 1192964789.8882701\n",
      "Iteration 2581, Loss: 1326169889.6816905\n",
      "Iteration 2582, Loss: 1298479497.2324295\n",
      "Iteration 2583, Loss: 1057437260.1173924\n",
      "Iteration 2584, Loss: 1788843596.7575614\n",
      "Iteration 2585, Loss: 1669159001.2036302\n",
      "Iteration 2586, Loss: 1598471898.832374\n",
      "Iteration 2587, Loss: 1076683943.3411233\n",
      "Iteration 2588, Loss: 2389288885.7971935\n",
      "Iteration 2589, Loss: 2026676121.446659\n",
      "Iteration 2590, Loss: 1444408916.687167\n",
      "Iteration 2591, Loss: 1229988966.0941885\n",
      "Iteration 2592, Loss: 1250139418.500081\n",
      "Iteration 2593, Loss: 1544955342.0562255\n",
      "Iteration 2594, Loss: 1414633154.350986\n",
      "Iteration 2595, Loss: 1123390955.0329878\n",
      "Iteration 2596, Loss: 1126016613.2151334\n",
      "Iteration 2597, Loss: 1219165920.770237\n",
      "Iteration 2598, Loss: 1157953750.7727933\n",
      "Iteration 2599, Loss: 1333157854.5032284\n",
      "Iteration 2600, Loss: 1311315618.9357455\n",
      "Iteration 2601, Loss: 1213202675.4474926\n",
      "Iteration 2602, Loss: 1123833616.871775\n",
      "Iteration 2603, Loss: 1224275481.5757363\n",
      "Iteration 2604, Loss: 1232275524.4818022\n",
      "Iteration 2605, Loss: 1265543871.2185748\n",
      "Iteration 2606, Loss: 1176175856.430245\n",
      "Iteration 2607, Loss: 1456933716.9961345\n",
      "Iteration 2608, Loss: 1422510958.7828665\n",
      "Iteration 2609, Loss: 1081954699.4304338\n",
      "Iteration 2610, Loss: 1085301825.0309887\n",
      "Iteration 2611, Loss: 1326412539.9860928\n",
      "Iteration 2612, Loss: 1079333521.2867362\n",
      "Iteration 2613, Loss: 1351900232.1453867\n",
      "Iteration 2614, Loss: 1350485192.3063047\n",
      "Iteration 2615, Loss: 1354218681.0705554\n",
      "Iteration 2616, Loss: 1069628365.4594139\n",
      "Iteration 2617, Loss: 2911594964.7991724\n",
      "Iteration 2618, Loss: 1606513196.303365\n",
      "Iteration 2619, Loss: 1099626901.3992872\n",
      "Iteration 2620, Loss: 1299561899.072072\n",
      "Iteration 2621, Loss: 1280355108.1083715\n",
      "Iteration 2622, Loss: 1301727716.960394\n",
      "Iteration 2623, Loss: 1246263074.0060165\n",
      "Iteration 2624, Loss: 1098397501.531806\n",
      "Iteration 2625, Loss: 1105167547.8993244\n",
      "Iteration 2626, Loss: 1100290164.200688\n",
      "Iteration 2627, Loss: 1136934746.542324\n",
      "Iteration 2628, Loss: 1187494375.3680737\n",
      "Iteration 2629, Loss: 1209242016.3616073\n",
      "Iteration 2630, Loss: 1303222910.60432\n",
      "Iteration 2631, Loss: 1279534741.0089343\n",
      "Iteration 2632, Loss: 1351075464.8869233\n",
      "Iteration 2633, Loss: 1091527448.857321\n",
      "Iteration 2634, Loss: 1092932970.041279\n",
      "Iteration 2635, Loss: 1066809671.4441478\n",
      "Iteration 2636, Loss: 1070804347.3600527\n",
      "Iteration 2637, Loss: 1099662755.9422464\n",
      "Iteration 2638, Loss: 1066095764.9143723\n",
      "Iteration 2639, Loss: 1103267227.1830623\n",
      "Iteration 2640, Loss: 1102001481.7119303\n",
      "Iteration 2641, Loss: 1258262666.6766331\n",
      "Iteration 2642, Loss: 1253599348.9420846\n",
      "Iteration 2643, Loss: 1167622503.1050763\n",
      "Iteration 2644, Loss: 1281138872.8194938\n",
      "Iteration 2645, Loss: 1255964291.0490618\n",
      "Iteration 2646, Loss: 1239815939.8614466\n",
      "Iteration 2647, Loss: 1173817016.2460313\n",
      "Iteration 2648, Loss: 1135473766.0247726\n",
      "Iteration 2649, Loss: 1185480097.2026262\n",
      "Iteration 2650, Loss: 1179797621.4265459\n",
      "Iteration 2651, Loss: 1174910897.1852286\n",
      "Iteration 2652, Loss: 1769794466.4456592\n",
      "Iteration 2653, Loss: 1356765817.6212733\n",
      "Iteration 2654, Loss: 1109796407.4584236\n",
      "Iteration 2655, Loss: 1304182165.1983616\n",
      "Iteration 2656, Loss: 1236467571.5179944\n",
      "Iteration 2657, Loss: 1269631470.9068236\n",
      "Iteration 2658, Loss: 1189604447.4965198\n",
      "Iteration 2659, Loss: 1115279903.193993\n",
      "Iteration 2660, Loss: 1120414349.2961612\n",
      "Iteration 2661, Loss: 1142609931.0956256\n",
      "Iteration 2662, Loss: 1136196530.468718\n",
      "Iteration 2663, Loss: 1193920452.1892269\n",
      "Iteration 2664, Loss: 1180132355.4525898\n",
      "Iteration 2665, Loss: 1282248401.7631795\n",
      "Iteration 2666, Loss: 1346498135.4013305\n",
      "Iteration 2667, Loss: 1073498741.4666646\n",
      "Iteration 2668, Loss: 2459617917.109087\n",
      "Iteration 2669, Loss: 1093697184.3778565\n",
      "Iteration 2670, Loss: 1089964335.0683544\n",
      "Iteration 2671, Loss: 1197622638.9885614\n",
      "Iteration 2672, Loss: 1255271953.5767894\n",
      "Iteration 2673, Loss: 1333985429.146666\n",
      "Iteration 2674, Loss: 1112938143.2620242\n",
      "Iteration 2675, Loss: 1387191242.649281\n",
      "Iteration 2676, Loss: 1239279441.0565825\n",
      "Iteration 2677, Loss: 1225065787.883698\n",
      "Iteration 2678, Loss: 1277182730.5737848\n",
      "Iteration 2679, Loss: 1170599350.3615303\n",
      "Iteration 2680, Loss: 1281225789.7611802\n",
      "Iteration 2681, Loss: 1072824685.9066792\n",
      "Iteration 2682, Loss: 1072194658.1559763\n",
      "Iteration 2683, Loss: 1101723769.3271914\n",
      "Iteration 2684, Loss: 1210881341.3480306\n",
      "Iteration 2685, Loss: 1150249685.2121675\n",
      "Iteration 2686, Loss: 1255987702.4560385\n",
      "Iteration 2687, Loss: 1239484137.6440933\n",
      "Iteration 2688, Loss: 1099536561.938245\n",
      "Iteration 2689, Loss: 1407973072.5989993\n",
      "Iteration 2690, Loss: 1191662421.6349\n",
      "Iteration 2691, Loss: 1706376464.594838\n",
      "Iteration 2692, Loss: 1750155418.5752983\n",
      "Iteration 2693, Loss: 1709945050.9692671\n",
      "Iteration 2694, Loss: 1455127931.5807068\n",
      "Iteration 2695, Loss: 1076768859.0121124\n",
      "Iteration 2696, Loss: 1443637714.4599602\n",
      "Iteration 2697, Loss: 1071286994.8395922\n",
      "Iteration 2698, Loss: 1122903772.0645235\n",
      "Iteration 2699, Loss: 1256805901.181857\n",
      "Iteration 2700, Loss: 1334118740.8923893\n",
      "Iteration 2701, Loss: 1219348305.380971\n",
      "Iteration 2702, Loss: 1207071861.6973464\n",
      "Iteration 2703, Loss: 1287092261.360261\n",
      "Iteration 2704, Loss: 1197085432.198305\n",
      "Iteration 2705, Loss: 1059846757.6792237\n",
      "Iteration 2706, Loss: 1239478327.7287428\n",
      "Iteration 2707, Loss: 1263341554.2461467\n",
      "Iteration 2708, Loss: 1336556052.893965\n",
      "Iteration 2709, Loss: 1218561348.0583234\n",
      "Iteration 2710, Loss: 1216288383.5230362\n",
      "Iteration 2711, Loss: 1203278546.2818096\n",
      "Iteration 2712, Loss: 1328011970.1710033\n",
      "Iteration 2713, Loss: 1263653109.1114254\n",
      "Iteration 2714, Loss: 1078952674.2030776\n",
      "Iteration 2715, Loss: 2383988140.1382627\n",
      "Iteration 2716, Loss: 1615383062.8848486\n",
      "Iteration 2717, Loss: 1086219501.818692\n",
      "Iteration 2718, Loss: 1083138318.599979\n",
      "Iteration 2719, Loss: 1293049306.6918914\n",
      "Iteration 2720, Loss: 1287256338.6932948\n",
      "Iteration 2721, Loss: 1352246479.8598528\n",
      "Iteration 2722, Loss: 1390977027.208264\n",
      "Iteration 2723, Loss: 1250328338.323415\n",
      "Iteration 2724, Loss: 1276091665.2552233\n",
      "Iteration 2725, Loss: 1250181317.813578\n",
      "Iteration 2726, Loss: 1543199871.3703673\n",
      "Iteration 2727, Loss: 1155918957.1310387\n",
      "Iteration 2728, Loss: 1276674698.2293975\n",
      "Iteration 2729, Loss: 1074145903.6147676\n",
      "Iteration 2730, Loss: 1077001325.7177353\n",
      "Iteration 2731, Loss: 1081804346.9765832\n",
      "Iteration 2732, Loss: 1129099228.3286636\n",
      "Iteration 2733, Loss: 1238049903.9259334\n",
      "Iteration 2734, Loss: 1274549908.6754045\n",
      "Iteration 2735, Loss: 1247777306.6246338\n",
      "Iteration 2736, Loss: 1232984494.962249\n",
      "Iteration 2737, Loss: 1235990442.4525418\n",
      "Iteration 2738, Loss: 1095329644.1785495\n",
      "Iteration 2739, Loss: 1088575706.3917894\n",
      "Iteration 2740, Loss: 2199264271.1276565\n",
      "Iteration 2741, Loss: 1831712277.6488535\n",
      "Iteration 2742, Loss: 1784706933.273221\n",
      "Iteration 2743, Loss: 1739943961.015021\n",
      "Iteration 2744, Loss: 1577855446.5007005\n",
      "Iteration 2745, Loss: 1536163647.1424925\n",
      "Iteration 2746, Loss: 1476664719.6129296\n",
      "Iteration 2747, Loss: 1117379109.7101986\n",
      "Iteration 2748, Loss: 1138976963.977437\n",
      "Iteration 2749, Loss: 1940625069.4687889\n",
      "Iteration 2750, Loss: 1985783092.7028043\n",
      "Iteration 2751, Loss: 1063963382.2374834\n",
      "Iteration 2752, Loss: 1340695454.3895905\n",
      "Iteration 2753, Loss: 1136898497.6424422\n",
      "Iteration 2754, Loss: 1224428789.5300577\n",
      "Iteration 2755, Loss: 1254289705.1901073\n",
      "Iteration 2756, Loss: 1239175045.5899296\n",
      "Iteration 2757, Loss: 1238511556.2073343\n",
      "Iteration 2758, Loss: 1224636798.237835\n",
      "Iteration 2759, Loss: 1314313567.4461727\n",
      "Iteration 2760, Loss: 1293912205.1404252\n",
      "Iteration 2761, Loss: 1128760228.1830015\n",
      "Iteration 2762, Loss: 1413278389.1107783\n",
      "Iteration 2763, Loss: 1080034877.0947504\n",
      "Iteration 2764, Loss: 1069572437.8047731\n",
      "Iteration 2765, Loss: 1320958603.7878847\n",
      "Iteration 2766, Loss: 1281352946.2321754\n",
      "Iteration 2767, Loss: 1145023288.9568193\n",
      "Iteration 2768, Loss: 1170493888.4325652\n",
      "Iteration 2769, Loss: 1068079274.2699281\n",
      "Iteration 2770, Loss: 1067934172.0623966\n",
      "Iteration 2771, Loss: 1188420047.9469454\n",
      "Iteration 2772, Loss: 1290041549.54612\n",
      "Iteration 2773, Loss: 1268733723.028446\n",
      "Iteration 2774, Loss: 1502044221.8507173\n",
      "Iteration 2775, Loss: 1191271509.7166102\n",
      "Iteration 2776, Loss: 1647586956.5539975\n",
      "Iteration 2777, Loss: 1221720836.4366045\n",
      "Iteration 2778, Loss: 2024850207.8245313\n",
      "Iteration 2779, Loss: 1180243347.171437\n",
      "Iteration 2780, Loss: 1177337787.3860853\n",
      "Iteration 2781, Loss: 1281610573.4924521\n",
      "Iteration 2782, Loss: 1180640620.4257486\n",
      "Iteration 2783, Loss: 1168663434.0735521\n",
      "Iteration 2784, Loss: 1163452779.2191324\n",
      "Iteration 2785, Loss: 1158838700.2613387\n",
      "Iteration 2786, Loss: 1215045540.28122\n",
      "Iteration 2787, Loss: 1061986085.5672756\n",
      "Iteration 2788, Loss: 1139191519.3987684\n",
      "Iteration 2789, Loss: 1124687229.3234055\n",
      "Iteration 2790, Loss: 1162336595.423403\n",
      "Iteration 2791, Loss: 1517617010.8455172\n",
      "Iteration 2792, Loss: 1472656902.7011251\n",
      "Iteration 2793, Loss: 1428895936.8248217\n",
      "Iteration 2794, Loss: 1112450838.3732634\n",
      "Iteration 2795, Loss: 5543744464.860847\n",
      "Iteration 2796, Loss: 4410670807.756346\n",
      "Iteration 2797, Loss: 1932690436.9467726\n",
      "Iteration 2798, Loss: 3118244798.435545\n",
      "Iteration 2799, Loss: 2251073839.4244514\n",
      "Iteration 2800, Loss: 2103030696.0357478\n",
      "Iteration 2801, Loss: 1294814343.5516808\n",
      "Iteration 2802, Loss: 1077156544.7301283\n",
      "Iteration 2803, Loss: 1493295807.5497003\n",
      "Iteration 2804, Loss: 1178709792.3513472\n",
      "Iteration 2805, Loss: 1196900083.2006924\n",
      "Iteration 2806, Loss: 1234470911.6448674\n",
      "Iteration 2807, Loss: 1223582325.4227023\n",
      "Iteration 2808, Loss: 1320612596.1490633\n",
      "Iteration 2809, Loss: 1372085525.8871012\n",
      "Iteration 2810, Loss: 1219481837.2184765\n",
      "Iteration 2811, Loss: 1297678510.5625231\n",
      "Iteration 2812, Loss: 1353291846.4806468\n",
      "Iteration 2813, Loss: 1372715340.9853032\n",
      "Iteration 2814, Loss: 1417797113.2747653\n",
      "Iteration 2815, Loss: 1086880184.9547493\n",
      "Iteration 2816, Loss: 1069488940.3250164\n",
      "Iteration 2817, Loss: 1081239495.154893\n",
      "Iteration 2818, Loss: 1184211274.77371\n",
      "Iteration 2819, Loss: 1324054072.2887077\n",
      "Iteration 2820, Loss: 1350657089.638099\n",
      "Iteration 2821, Loss: 1285248542.7707827\n",
      "Iteration 2822, Loss: 1339063879.321584\n",
      "Iteration 2823, Loss: 1154136709.2868872\n",
      "Iteration 2824, Loss: 1152199934.1223803\n",
      "Iteration 2825, Loss: 1205795238.9078588\n",
      "Iteration 2826, Loss: 1306018651.9865937\n",
      "Iteration 2827, Loss: 1343930681.494072\n",
      "Iteration 2828, Loss: 1065689542.8345462\n",
      "Iteration 2829, Loss: 1068795064.2101616\n",
      "Iteration 2830, Loss: 1204793707.9095385\n",
      "Iteration 2831, Loss: 1264254763.1425798\n",
      "Iteration 2832, Loss: 1178912179.7276475\n",
      "Iteration 2833, Loss: 1185465503.1723275\n",
      "Iteration 2834, Loss: 1173947797.2684085\n",
      "Iteration 2835, Loss: 1204334469.4210334\n",
      "Iteration 2836, Loss: 1065849842.353458\n",
      "Iteration 2837, Loss: 1160590774.556495\n",
      "Iteration 2838, Loss: 1195001336.0884244\n",
      "Iteration 2839, Loss: 1215355839.9554095\n",
      "Iteration 2840, Loss: 1273328503.4254057\n",
      "Iteration 2841, Loss: 1253755476.400101\n",
      "Iteration 2842, Loss: 1132479647.9857953\n",
      "Iteration 2843, Loss: 1104001090.2312593\n",
      "Iteration 2844, Loss: 1413056325.204747\n",
      "Iteration 2845, Loss: 1377303878.9532244\n",
      "Iteration 2846, Loss: 1366687107.8705637\n",
      "Iteration 2847, Loss: 1073135108.0094154\n",
      "Iteration 2848, Loss: 1064800441.6275786\n",
      "Iteration 2849, Loss: 1317649725.7534387\n",
      "Iteration 2850, Loss: 1294797014.893312\n",
      "Iteration 2851, Loss: 1273312143.339021\n",
      "Iteration 2852, Loss: 1256800675.136251\n",
      "Iteration 2853, Loss: 1311436139.870776\n",
      "Iteration 2854, Loss: 1077247251.0917695\n",
      "Iteration 2855, Loss: 1120056420.3916953\n",
      "Iteration 2856, Loss: 1385088449.086235\n",
      "Iteration 2857, Loss: 1300303640.138157\n",
      "Iteration 2858, Loss: 1141586677.3529916\n",
      "Iteration 2859, Loss: 1229669825.7544596\n",
      "Iteration 2860, Loss: 1220000372.5949054\n",
      "Iteration 2861, Loss: 1063291540.4622333\n",
      "Iteration 2862, Loss: 1063549126.2629255\n",
      "Iteration 2863, Loss: 1065446442.804186\n",
      "Iteration 2864, Loss: 1065459342.3446558\n",
      "Iteration 2865, Loss: 1317896178.7340019\n",
      "Iteration 2866, Loss: 1344058133.1767151\n",
      "Iteration 2867, Loss: 1382488848.6493545\n",
      "Iteration 2868, Loss: 1347422220.3415382\n",
      "Iteration 2869, Loss: 1352568894.3196826\n",
      "Iteration 2870, Loss: 1064947448.9974797\n",
      "Iteration 2871, Loss: 1360756754.4357123\n",
      "Iteration 2872, Loss: 1106543540.7017894\n",
      "Iteration 2873, Loss: 1143335685.271308\n",
      "Iteration 2874, Loss: 1340471020.6298108\n",
      "Iteration 2875, Loss: 1245796819.0798092\n",
      "Iteration 2876, Loss: 1093330062.5277708\n",
      "Iteration 2877, Loss: 1090861651.1249535\n",
      "Iteration 2878, Loss: 1125905484.2126694\n",
      "Iteration 2879, Loss: 1182542563.2278945\n",
      "Iteration 2880, Loss: 1177590131.1238377\n",
      "Iteration 2881, Loss: 1328591421.890403\n",
      "Iteration 2882, Loss: 1093956466.193413\n",
      "Iteration 2883, Loss: 1099788176.682442\n",
      "Iteration 2884, Loss: 1135616262.9843743\n",
      "Iteration 2885, Loss: 1173891419.1778266\n",
      "Iteration 2886, Loss: 1159755603.0281234\n",
      "Iteration 2887, Loss: 1187566851.8473651\n",
      "Iteration 2888, Loss: 1271237416.2251983\n",
      "Iteration 2889, Loss: 1264812940.6114688\n",
      "Iteration 2890, Loss: 1117114641.0967946\n",
      "Iteration 2891, Loss: 1092437817.6254601\n",
      "Iteration 2892, Loss: 1127454613.4239035\n",
      "Iteration 2893, Loss: 1239841414.2897394\n",
      "Iteration 2894, Loss: 1137927646.6890264\n",
      "Iteration 2895, Loss: 1910813307.0213182\n",
      "Iteration 2896, Loss: 1853588001.080665\n",
      "Iteration 2897, Loss: 1801650642.1272826\n",
      "Iteration 2898, Loss: 1708955796.3009324\n",
      "Iteration 2899, Loss: 1651238591.2723508\n",
      "Iteration 2900, Loss: 1587943342.4206946\n",
      "Iteration 2901, Loss: 1534695481.3990903\n",
      "Iteration 2902, Loss: 1061316162.4125093\n",
      "Iteration 2903, Loss: 1082632697.0955784\n",
      "Iteration 2904, Loss: 1079303666.3910217\n",
      "Iteration 2905, Loss: 1183646572.6843064\n",
      "Iteration 2906, Loss: 1184361950.3946412\n",
      "Iteration 2907, Loss: 1274337429.0481465\n",
      "Iteration 2908, Loss: 1255003992.754788\n",
      "Iteration 2909, Loss: 1306543094.833157\n",
      "Iteration 2910, Loss: 1098770920.7775123\n",
      "Iteration 2911, Loss: 1402380538.0852063\n",
      "Iteration 2912, Loss: 1316266780.3174832\n",
      "Iteration 2913, Loss: 1210989071.4564075\n",
      "Iteration 2914, Loss: 1177238789.7863042\n",
      "Iteration 2915, Loss: 1235113832.1212695\n",
      "Iteration 2916, Loss: 1233124896.0868344\n",
      "Iteration 2917, Loss: 1316259706.3018057\n",
      "Iteration 2918, Loss: 1058094704.7661155\n",
      "Iteration 2919, Loss: 1059003541.0690472\n",
      "Iteration 2920, Loss: 1065336824.2280495\n",
      "Iteration 2921, Loss: 1686842376.8562\n",
      "Iteration 2922, Loss: 1058692713.5314333\n",
      "Iteration 2923, Loss: 1592504059.8669941\n",
      "Iteration 2924, Loss: 1187448672.023612\n",
      "Iteration 2925, Loss: 1138833149.0563488\n",
      "Iteration 2926, Loss: 1120050744.2684052\n",
      "Iteration 2927, Loss: 4175674915.838475\n",
      "Iteration 2928, Loss: 6236077137.325553\n",
      "Iteration 2929, Loss: 3406790584.4025025\n",
      "Iteration 2930, Loss: 2818149532.3267307\n",
      "Iteration 2931, Loss: 20029016431.444202\n",
      "Iteration 2932, Loss: 15446299137.297646\n",
      "Iteration 2933, Loss: 13219901156.755362\n",
      "Iteration 2934, Loss: 3386110876.1877136\n",
      "Iteration 2935, Loss: 2918760273.528425\n",
      "Iteration 2936, Loss: 1427420695.5602257\n",
      "Iteration 2937, Loss: 1134876844.4645405\n",
      "Iteration 2938, Loss: 1132152554.6408958\n",
      "Iteration 2939, Loss: 1155652011.7482765\n",
      "Iteration 2940, Loss: 1137258621.803758\n",
      "Iteration 2941, Loss: 1128409365.5137813\n",
      "Iteration 2942, Loss: 1123478781.3798847\n",
      "Iteration 2943, Loss: 1128768417.4956086\n",
      "Iteration 2944, Loss: 1338595889.1340353\n",
      "Iteration 2945, Loss: 1091385467.0633483\n",
      "Iteration 2946, Loss: 1097459254.6283994\n",
      "Iteration 2947, Loss: 1113862144.8430145\n",
      "Iteration 2948, Loss: 1112180958.4768863\n",
      "Iteration 2949, Loss: 1164218820.8857808\n",
      "Iteration 2950, Loss: 1340813276.4778228\n",
      "Iteration 2951, Loss: 1232246324.0859334\n",
      "Iteration 2952, Loss: 1070054618.669983\n",
      "Iteration 2953, Loss: 1091338950.7031605\n",
      "Iteration 2954, Loss: 1126400292.8810906\n",
      "Iteration 2955, Loss: 1130220970.8941271\n",
      "Iteration 2956, Loss: 1120246167.9531763\n",
      "Iteration 2957, Loss: 1395820606.6214845\n",
      "Iteration 2958, Loss: 1084373576.2312493\n",
      "Iteration 2959, Loss: 1084503250.9207804\n",
      "Iteration 2960, Loss: 1468841581.2732773\n",
      "Iteration 2961, Loss: 1295329553.0056853\n",
      "Iteration 2962, Loss: 1075188227.225326\n",
      "Iteration 2963, Loss: 1075273150.3037803\n",
      "Iteration 2964, Loss: 1504438090.2559917\n",
      "Iteration 2965, Loss: 1120263326.1562068\n",
      "Iteration 2966, Loss: 1259308612.2273686\n",
      "Iteration 2967, Loss: 1316465654.4518332\n",
      "Iteration 2968, Loss: 1220259091.6416411\n",
      "Iteration 2969, Loss: 1643976978.1565988\n",
      "Iteration 2970, Loss: 1581788561.3124256\n",
      "Iteration 2971, Loss: 1301764727.5045197\n",
      "Iteration 2972, Loss: 1181091803.2339056\n",
      "Iteration 2973, Loss: 1123434258.7479591\n",
      "Iteration 2974, Loss: 1117165708.6429062\n",
      "Iteration 2975, Loss: 1106314659.9425962\n",
      "Iteration 2976, Loss: 1443575944.0605302\n",
      "Iteration 2977, Loss: 1447489802.8217428\n",
      "Iteration 2978, Loss: 1413064854.2182407\n",
      "Iteration 2979, Loss: 1432034102.8506756\n",
      "Iteration 2980, Loss: 1343227945.1537247\n",
      "Iteration 2981, Loss: 1088057592.1596773\n",
      "Iteration 2982, Loss: 1865396310.7912505\n",
      "Iteration 2983, Loss: 1915123685.8045924\n",
      "Iteration 2984, Loss: 1711308180.3448148\n",
      "Iteration 2985, Loss: 1577829549.590146\n",
      "Iteration 2986, Loss: 1311608478.9459379\n",
      "Iteration 2987, Loss: 1138757385.6731215\n",
      "Iteration 2988, Loss: 1328417162.9778633\n",
      "Iteration 2989, Loss: 1322859045.4208877\n",
      "Iteration 2990, Loss: 1239847282.438631\n",
      "Iteration 2991, Loss: 1081884938.7138326\n",
      "Iteration 2992, Loss: 1483745835.463125\n",
      "Iteration 2993, Loss: 1107682929.0534518\n",
      "Iteration 2994, Loss: 1094661279.2145882\n",
      "Iteration 2995, Loss: 1096952260.2391622\n",
      "Iteration 2996, Loss: 1127214689.05402\n",
      "Iteration 2997, Loss: 1179387944.7650821\n",
      "Iteration 2998, Loss: 1142954776.8581076\n",
      "Iteration 2999, Loss: 1135444739.30586\n",
      "Iteration 3000, Loss: 1396570983.909374\n",
      "Iteration 3001, Loss: 1359458938.4251182\n",
      "Iteration 3002, Loss: 1331892606.6912906\n",
      "Iteration 3003, Loss: 1141667129.4996727\n",
      "Iteration 3004, Loss: 1185616083.551687\n",
      "Iteration 3005, Loss: 1213690029.8556812\n",
      "Iteration 3006, Loss: 1203562655.4782887\n",
      "Iteration 3007, Loss: 1303682486.1043227\n",
      "Iteration 3008, Loss: 1077017020.074515\n",
      "Iteration 3009, Loss: 1127333732.3353682\n",
      "Iteration 3010, Loss: 1127242390.353598\n",
      "Iteration 3011, Loss: 1093071867.3053718\n",
      "Iteration 3012, Loss: 1095273541.6748226\n",
      "Iteration 3013, Loss: 1476232187.5175202\n",
      "Iteration 3014, Loss: 1434804314.637915\n",
      "Iteration 3015, Loss: 1125905854.7664182\n",
      "Iteration 3016, Loss: 1249896376.7570596\n",
      "Iteration 3017, Loss: 1236560933.3823664\n",
      "Iteration 3018, Loss: 1221392202.3283823\n",
      "Iteration 3019, Loss: 1221588880.1649632\n",
      "Iteration 3020, Loss: 1190313889.3541327\n",
      "Iteration 3021, Loss: 1070674456.5440099\n",
      "Iteration 3022, Loss: 2205061515.0201426\n",
      "Iteration 3023, Loss: 1079138795.0177171\n",
      "Iteration 3024, Loss: 1069099307.6155541\n",
      "Iteration 3025, Loss: 1354553878.178436\n",
      "Iteration 3026, Loss: 1346995820.2963226\n",
      "Iteration 3027, Loss: 1139970646.5209546\n",
      "Iteration 3028, Loss: 1257495724.0763166\n",
      "Iteration 3029, Loss: 1314321834.3886294\n",
      "Iteration 3030, Loss: 1111523666.3841186\n",
      "Iteration 3031, Loss: 1390889969.2712834\n",
      "Iteration 3032, Loss: 1426446910.5064697\n",
      "Iteration 3033, Loss: 1383963826.67678\n",
      "Iteration 3034, Loss: 1084798580.6298523\n",
      "Iteration 3035, Loss: 1279492158.7721288\n",
      "Iteration 3036, Loss: 1263544156.3322246\n",
      "Iteration 3037, Loss: 1346754840.7217855\n",
      "Iteration 3038, Loss: 1325251979.0688384\n",
      "Iteration 3039, Loss: 1148098345.1326568\n",
      "Iteration 3040, Loss: 1151756523.1964748\n",
      "Iteration 3041, Loss: 1076562130.1534657\n",
      "Iteration 3042, Loss: 1376190838.6173284\n",
      "Iteration 3043, Loss: 1417904993.989096\n",
      "Iteration 3044, Loss: 1239157198.3483062\n",
      "Iteration 3045, Loss: 1156701269.533503\n",
      "Iteration 3046, Loss: 1234297705.6341443\n",
      "Iteration 3047, Loss: 1150521336.5149596\n",
      "Iteration 3048, Loss: 1332997568.0089536\n",
      "Iteration 3049, Loss: 1124701041.76207\n",
      "Iteration 3050, Loss: 1309097070.105418\n",
      "Iteration 3051, Loss: 1263216474.3979023\n",
      "Iteration 3052, Loss: 1261321222.9880319\n",
      "Iteration 3053, Loss: 1289330046.1352773\n",
      "Iteration 3054, Loss: 1074765667.2636795\n",
      "Iteration 3055, Loss: 1186996366.2248094\n",
      "Iteration 3056, Loss: 1341960726.9462287\n",
      "Iteration 3057, Loss: 1315154031.3397725\n",
      "Iteration 3058, Loss: 1082534012.1977777\n",
      "Iteration 3059, Loss: 1235176955.6562486\n",
      "Iteration 3060, Loss: 1294069853.6677337\n",
      "Iteration 3061, Loss: 1361235291.1861465\n",
      "Iteration 3062, Loss: 1115832487.1749744\n",
      "Iteration 3063, Loss: 1105608752.081707\n",
      "Iteration 3064, Loss: 1216976535.5086877\n",
      "Iteration 3065, Loss: 1125399707.975355\n",
      "Iteration 3066, Loss: 1074493360.12913\n",
      "Iteration 3067, Loss: 1102535531.3280492\n",
      "Iteration 3068, Loss: 1104234211.0981288\n",
      "Iteration 3069, Loss: 1097020163.301585\n",
      "Iteration 3070, Loss: 1317055656.47118\n",
      "Iteration 3071, Loss: 1218009336.5550497\n",
      "Iteration 3072, Loss: 1295947604.474168\n",
      "Iteration 3073, Loss: 1247017865.9139893\n",
      "Iteration 3074, Loss: 1230602529.4414067\n",
      "Iteration 3075, Loss: 1174829486.769246\n",
      "Iteration 3076, Loss: 1326050822.6459146\n",
      "Iteration 3077, Loss: 1303390015.4030657\n",
      "Iteration 3078, Loss: 1117030260.806134\n",
      "Iteration 3079, Loss: 1258648336.9721663\n",
      "Iteration 3080, Loss: 1092452714.5206718\n",
      "Iteration 3081, Loss: 2241776111.991102\n",
      "Iteration 3082, Loss: 2140187622.167897\n",
      "Iteration 3083, Loss: 9833538139.624065\n",
      "Iteration 3084, Loss: 9158507984.298153\n",
      "Iteration 3085, Loss: 31284967755.98158\n",
      "Iteration 3086, Loss: 29633901675.04269\n",
      "Iteration 3087, Loss: 30587135607.46924\n",
      "Iteration 3088, Loss: 28304629874.012173\n",
      "Iteration 3089, Loss: 17123424228.924347\n",
      "Iteration 3090, Loss: 9886049060.83692\n",
      "Iteration 3091, Loss: 9041719501.607912\n",
      "Iteration 3092, Loss: 7160907990.279179\n",
      "Iteration 3093, Loss: 1163012360.5843506\n",
      "Iteration 3094, Loss: 1126881212.4552548\n",
      "Iteration 3095, Loss: 1196908009.532329\n",
      "Iteration 3096, Loss: 1328608057.5898898\n",
      "Iteration 3097, Loss: 1331328291.1576188\n",
      "Iteration 3098, Loss: 1121266353.15984\n",
      "Iteration 3099, Loss: 1191351320.2365396\n",
      "Iteration 3100, Loss: 1060869500.228438\n",
      "Iteration 3101, Loss: 2682481893.6521044\n",
      "Iteration 3102, Loss: 2533381886.2832446\n",
      "Iteration 3103, Loss: 1055246886.1165622\n",
      "Iteration 3104, Loss: 2852560662.2218337\n",
      "Iteration 3105, Loss: 2520122004.6231284\n",
      "Iteration 3106, Loss: 2388991446.0168633\n",
      "Iteration 3107, Loss: 1562566196.8831148\n",
      "Iteration 3108, Loss: 1389011808.2699778\n",
      "Iteration 3109, Loss: 1424518962.6414757\n",
      "Iteration 3110, Loss: 1078336964.542454\n",
      "Iteration 3111, Loss: 1226312366.5475824\n",
      "Iteration 3112, Loss: 1146015761.8291361\n",
      "Iteration 3113, Loss: 1137263488.1546805\n",
      "Iteration 3114, Loss: 1123994131.6113932\n",
      "Iteration 3115, Loss: 1243229659.2559774\n",
      "Iteration 3116, Loss: 1340148080.5375075\n",
      "Iteration 3117, Loss: 1382637168.8728976\n",
      "Iteration 3118, Loss: 1204621681.1431587\n",
      "Iteration 3119, Loss: 1158606943.5842063\n",
      "Iteration 3120, Loss: 1071556311.7936057\n",
      "Iteration 3121, Loss: 1320367753.9473114\n",
      "Iteration 3122, Loss: 1196841894.9250402\n",
      "Iteration 3123, Loss: 1183763957.9619164\n",
      "Iteration 3124, Loss: 1177344066.0151963\n",
      "Iteration 3125, Loss: 1177888995.0682411\n",
      "Iteration 3126, Loss: 1237402602.178942\n",
      "Iteration 3127, Loss: 1277304299.08454\n",
      "Iteration 3128, Loss: 1120249352.6814022\n",
      "Iteration 3129, Loss: 1177126806.2114756\n",
      "Iteration 3130, Loss: 1224303282.2379117\n",
      "Iteration 3131, Loss: 1060838596.7642897\n",
      "Iteration 3132, Loss: 1071138690.1564264\n",
      "Iteration 3133, Loss: 1077556422.895881\n",
      "Iteration 3134, Loss: 1319443182.1169248\n",
      "Iteration 3135, Loss: 1277094851.0791748\n",
      "Iteration 3136, Loss: 1257595682.4338934\n",
      "Iteration 3137, Loss: 1261640703.486559\n",
      "Iteration 3138, Loss: 1260852656.882922\n",
      "Iteration 3139, Loss: 1259107941.4147666\n",
      "Iteration 3140, Loss: 1243165922.711692\n",
      "Iteration 3141, Loss: 1176308006.645546\n",
      "Iteration 3142, Loss: 1332606656.5456479\n",
      "Iteration 3143, Loss: 1114744138.480373\n",
      "Iteration 3144, Loss: 1117704090.9709284\n",
      "Iteration 3145, Loss: 1113572360.3843217\n",
      "Iteration 3146, Loss: 1389187325.8580894\n",
      "Iteration 3147, Loss: 1157154122.2126825\n",
      "Iteration 3148, Loss: 1193826073.093191\n",
      "Iteration 3149, Loss: 1238477255.376109\n",
      "Iteration 3150, Loss: 1126129685.5040004\n",
      "Iteration 3151, Loss: 1127987796.5523605\n",
      "Iteration 3152, Loss: 1120676288.2059479\n",
      "Iteration 3153, Loss: 1244070525.8719895\n",
      "Iteration 3154, Loss: 1229482144.7806606\n",
      "Iteration 3155, Loss: 1151978233.0290158\n",
      "Iteration 3156, Loss: 1263135076.9414122\n",
      "Iteration 3157, Loss: 1132133011.1648717\n",
      "Iteration 3158, Loss: 1973941001.0911252\n",
      "Iteration 3159, Loss: 1185636254.435058\n",
      "Iteration 3160, Loss: 1116508978.1753964\n",
      "Iteration 3161, Loss: 1154529241.5870812\n",
      "Iteration 3162, Loss: 1332673411.2705379\n",
      "Iteration 3163, Loss: 1134624797.4832125\n",
      "Iteration 3164, Loss: 1137332230.6464705\n",
      "Iteration 3165, Loss: 1690876355.5160017\n",
      "Iteration 3166, Loss: 1080260114.0680175\n",
      "Iteration 3167, Loss: 1079488656.4152186\n",
      "Iteration 3168, Loss: 1085158485.9457798\n",
      "Iteration 3169, Loss: 1292660770.23838\n",
      "Iteration 3170, Loss: 1312400175.6330984\n",
      "Iteration 3171, Loss: 1255497634.983302\n",
      "Iteration 3172, Loss: 1212730836.126684\n",
      "Iteration 3173, Loss: 1146023759.401005\n",
      "Iteration 3174, Loss: 1203322380.771791\n",
      "Iteration 3175, Loss: 1074355241.0077636\n",
      "Iteration 3176, Loss: 1466262064.4300988\n",
      "Iteration 3177, Loss: 1367663959.131679\n",
      "Iteration 3178, Loss: 1067012718.3743964\n",
      "Iteration 3179, Loss: 1261066977.8164523\n",
      "Iteration 3180, Loss: 1076391154.8565674\n",
      "Iteration 3181, Loss: 1129095470.7722847\n",
      "Iteration 3182, Loss: 1152482900.275412\n",
      "Iteration 3183, Loss: 1196259925.3859262\n",
      "Iteration 3184, Loss: 1153288877.2222383\n",
      "Iteration 3185, Loss: 1130219925.2032561\n",
      "Iteration 3186, Loss: 1295611905.921731\n",
      "Iteration 3187, Loss: 1274232628.1085317\n",
      "Iteration 3188, Loss: 1076785965.2770054\n",
      "Iteration 3189, Loss: 1351711304.7530036\n",
      "Iteration 3190, Loss: 1389747436.388507\n",
      "Iteration 3191, Loss: 1294716904.937387\n",
      "Iteration 3192, Loss: 1113527193.3148763\n",
      "Iteration 3193, Loss: 1231333409.7484734\n",
      "Iteration 3194, Loss: 1594486314.3653076\n",
      "Iteration 3195, Loss: 1511308820.521602\n",
      "Iteration 3196, Loss: 1368647466.2525768\n",
      "Iteration 3197, Loss: 1408948061.7636428\n",
      "Iteration 3198, Loss: 1406040466.550943\n",
      "Iteration 3199, Loss: 1230403407.482976\n",
      "Iteration 3200, Loss: 1601364920.7152853\n",
      "Iteration 3201, Loss: 1142462751.751394\n",
      "Iteration 3202, Loss: 1252057554.918807\n",
      "Iteration 3203, Loss: 1234503281.369468\n",
      "Iteration 3204, Loss: 1164756450.1720006\n",
      "Iteration 3205, Loss: 1198158503.7066793\n",
      "Iteration 3206, Loss: 1232506778.734858\n",
      "Iteration 3207, Loss: 1222786871.6376593\n",
      "Iteration 3208, Loss: 1207935698.8081577\n",
      "Iteration 3209, Loss: 1189024761.673353\n",
      "Iteration 3210, Loss: 1292363113.056681\n",
      "Iteration 3211, Loss: 1260834437.8034146\n",
      "Iteration 3212, Loss: 1341462736.289529\n",
      "Iteration 3213, Loss: 1314825452.3799527\n",
      "Iteration 3214, Loss: 1291105318.9941595\n",
      "Iteration 3215, Loss: 1078402315.7821736\n",
      "Iteration 3216, Loss: 1294333140.1025136\n",
      "Iteration 3217, Loss: 1116583608.3252752\n",
      "Iteration 3218, Loss: 1235138945.5580087\n",
      "Iteration 3219, Loss: 1222272747.4375541\n",
      "Iteration 3220, Loss: 1213792723.4409525\n",
      "Iteration 3221, Loss: 1156919763.7780478\n",
      "Iteration 3222, Loss: 1140328570.3342445\n",
      "Iteration 3223, Loss: 1256193687.9699862\n",
      "Iteration 3224, Loss: 1088380056.9262865\n",
      "Iteration 3225, Loss: 1520543483.1202333\n",
      "Iteration 3226, Loss: 1114728140.4480126\n",
      "Iteration 3227, Loss: 1214295348.9779668\n",
      "Iteration 3228, Loss: 1200465284.6333601\n",
      "Iteration 3229, Loss: 1303893760.9926848\n",
      "Iteration 3230, Loss: 1153072800.3662016\n",
      "Iteration 3231, Loss: 1599725465.4710076\n",
      "Iteration 3232, Loss: 1109028150.0803475\n",
      "Iteration 3233, Loss: 1313252667.2968912\n",
      "Iteration 3234, Loss: 1351407072.8344612\n",
      "Iteration 3235, Loss: 1329055876.9428344\n",
      "Iteration 3236, Loss: 1322350679.8903317\n",
      "Iteration 3237, Loss: 1345944997.5873585\n",
      "Iteration 3238, Loss: 1168831101.9249625\n",
      "Iteration 3239, Loss: 1076119623.2040453\n",
      "Iteration 3240, Loss: 2638077119.4577074\n",
      "Iteration 3241, Loss: 1071546273.6173756\n",
      "Iteration 3242, Loss: 2732622710.1596584\n",
      "Iteration 3243, Loss: 1096880168.0566576\n",
      "Iteration 3244, Loss: 1099781452.4709847\n",
      "Iteration 3245, Loss: 1219355643.5773032\n",
      "Iteration 3246, Loss: 1317088163.3182297\n",
      "Iteration 3247, Loss: 1296117141.3777256\n",
      "Iteration 3248, Loss: 1076508518.735679\n",
      "Iteration 3249, Loss: 1075763404.2884653\n",
      "Iteration 3250, Loss: 1077190215.3244393\n",
      "Iteration 3251, Loss: 1116569631.3527782\n",
      "Iteration 3252, Loss: 1393354347.654495\n",
      "Iteration 3253, Loss: 1376258375.206707\n",
      "Iteration 3254, Loss: 1252968117.377199\n",
      "Iteration 3255, Loss: 1070271918.7658867\n",
      "Iteration 3256, Loss: 1402730187.1174045\n",
      "Iteration 3257, Loss: 1132159325.5650578\n",
      "Iteration 3258, Loss: 1224907705.9429898\n",
      "Iteration 3259, Loss: 1080786128.9761634\n",
      "Iteration 3260, Loss: 1228923005.2290497\n",
      "Iteration 3261, Loss: 1189685870.4875534\n",
      "Iteration 3262, Loss: 1188415459.5624006\n",
      "Iteration 3263, Loss: 1218684310.4852085\n",
      "Iteration 3264, Loss: 1164589590.6708913\n",
      "Iteration 3265, Loss: 1293278246.3165321\n",
      "Iteration 3266, Loss: 1315253339.2205584\n",
      "Iteration 3267, Loss: 1067959110.6491516\n",
      "Iteration 3268, Loss: 1166037158.125551\n",
      "Iteration 3269, Loss: 1136282013.3621285\n",
      "Iteration 3270, Loss: 1073875685.4433858\n",
      "Iteration 3271, Loss: 1308471395.5391445\n",
      "Iteration 3272, Loss: 1079894440.6312957\n",
      "Iteration 3273, Loss: 1083120686.7805111\n",
      "Iteration 3274, Loss: 1197710786.7911322\n",
      "Iteration 3275, Loss: 1232288291.8871095\n",
      "Iteration 3276, Loss: 1222290197.1926012\n",
      "Iteration 3277, Loss: 1126833345.991148\n",
      "Iteration 3278, Loss: 1263655379.778481\n",
      "Iteration 3279, Loss: 1245440347.186954\n",
      "Iteration 3280, Loss: 1343409807.091236\n",
      "Iteration 3281, Loss: 1395166558.485039\n",
      "Iteration 3282, Loss: 1368797338.081614\n",
      "Iteration 3283, Loss: 1380922559.3489418\n",
      "Iteration 3284, Loss: 1309735173.8164768\n",
      "Iteration 3285, Loss: 1239377006.7834892\n",
      "Iteration 3286, Loss: 1228700185.1671455\n",
      "Iteration 3287, Loss: 1261888692.8878322\n",
      "Iteration 3288, Loss: 1149562820.1691022\n",
      "Iteration 3289, Loss: 1225133047.8539107\n",
      "Iteration 3290, Loss: 1181273648.549179\n",
      "Iteration 3291, Loss: 1229224069.7250457\n",
      "Iteration 3292, Loss: 1120521479.3206434\n",
      "Iteration 3293, Loss: 1375638087.3315954\n",
      "Iteration 3294, Loss: 1161989097.7431931\n",
      "Iteration 3295, Loss: 1160800853.0738952\n",
      "Iteration 3296, Loss: 1198105335.6863863\n",
      "Iteration 3297, Loss: 1296188437.5774417\n",
      "Iteration 3298, Loss: 1141195551.675552\n",
      "Iteration 3299, Loss: 1191894387.8877814\n",
      "Iteration 3300, Loss: 1290821442.8569376\n",
      "Iteration 3301, Loss: 1138438692.4176545\n",
      "Iteration 3302, Loss: 1682000522.15423\n",
      "Iteration 3303, Loss: 1210106153.2908812\n",
      "Iteration 3304, Loss: 1196082704.0393133\n",
      "Iteration 3305, Loss: 1183409791.8194633\n",
      "Iteration 3306, Loss: 1203205592.944484\n",
      "Iteration 3307, Loss: 1165280172.920586\n",
      "Iteration 3308, Loss: 1214422131.9662547\n",
      "Iteration 3309, Loss: 1276587354.2392094\n",
      "Iteration 3310, Loss: 1077623266.11162\n",
      "Iteration 3311, Loss: 1324390498.6918366\n",
      "Iteration 3312, Loss: 1395312437.4532309\n",
      "Iteration 3313, Loss: 1101758317.6501884\n",
      "Iteration 3314, Loss: 1063214850.1087528\n",
      "Iteration 3315, Loss: 1063102265.7995392\n",
      "Iteration 3316, Loss: 2512284513.185077\n",
      "Iteration 3317, Loss: 1096878107.986572\n",
      "Iteration 3318, Loss: 1221968471.4046495\n",
      "Iteration 3319, Loss: 1280477802.704118\n",
      "Iteration 3320, Loss: 1263705323.341387\n",
      "Iteration 3321, Loss: 1136150557.623772\n",
      "Iteration 3322, Loss: 1139374970.270584\n",
      "Iteration 3323, Loss: 1182338041.097139\n",
      "Iteration 3324, Loss: 1176699868.0017953\n",
      "Iteration 3325, Loss: 1171705059.832936\n",
      "Iteration 3326, Loss: 1221184420.0733945\n",
      "Iteration 3327, Loss: 1168459539.7068214\n",
      "Iteration 3328, Loss: 1075017496.0286188\n",
      "Iteration 3329, Loss: 1085455648.6489232\n",
      "Iteration 3330, Loss: 1325549648.7367504\n",
      "Iteration 3331, Loss: 1304895216.1665905\n",
      "Iteration 3332, Loss: 1299237901.1965659\n",
      "Iteration 3333, Loss: 1346460684.5118182\n",
      "Iteration 3334, Loss: 1252182356.1878512\n",
      "Iteration 3335, Loss: 1180877625.4956787\n",
      "Iteration 3336, Loss: 1323254551.336737\n",
      "Iteration 3337, Loss: 1316488493.280124\n",
      "Iteration 3338, Loss: 1292813109.652007\n",
      "Iteration 3339, Loss: 1358336106.5745873\n",
      "Iteration 3340, Loss: 1214317117.813528\n",
      "Iteration 3341, Loss: 1653075913.1722581\n",
      "Iteration 3342, Loss: 1328943357.8201966\n",
      "Iteration 3343, Loss: 1406953503.522042\n",
      "Iteration 3344, Loss: 1447763810.0888093\n",
      "Iteration 3345, Loss: 1183006500.6582718\n",
      "Iteration 3346, Loss: 1146248571.1653156\n",
      "Iteration 3347, Loss: 1077080598.0596035\n",
      "Iteration 3348, Loss: 1079312356.6086464\n",
      "Iteration 3349, Loss: 1123521626.3896646\n",
      "Iteration 3350, Loss: 1258833823.1727235\n",
      "Iteration 3351, Loss: 1062884965.0643289\n",
      "Iteration 3352, Loss: 1356842835.9821713\n",
      "Iteration 3353, Loss: 1065269220.5792271\n",
      "Iteration 3354, Loss: 1065822568.9283854\n",
      "Iteration 3355, Loss: 1341645883.932205\n",
      "Iteration 3356, Loss: 1348876094.4059007\n",
      "Iteration 3357, Loss: 1160447888.4327345\n",
      "Iteration 3358, Loss: 1158040085.6516027\n",
      "Iteration 3359, Loss: 1838573853.1509671\n",
      "Iteration 3360, Loss: 1101684438.4749596\n",
      "Iteration 3361, Loss: 1323904115.3211408\n",
      "Iteration 3362, Loss: 1225547110.8192856\n",
      "Iteration 3363, Loss: 1174973173.729262\n",
      "Iteration 3364, Loss: 1165921758.7993786\n",
      "Iteration 3365, Loss: 1280498050.8982222\n",
      "Iteration 3366, Loss: 1229266764.0047088\n",
      "Iteration 3367, Loss: 1231890838.4125478\n",
      "Iteration 3368, Loss: 1267432581.8732703\n",
      "Iteration 3369, Loss: 1260274538.4422386\n",
      "Iteration 3370, Loss: 1174105541.738575\n",
      "Iteration 3371, Loss: 1162506340.4811087\n",
      "Iteration 3372, Loss: 1160340180.5693808\n",
      "Iteration 3373, Loss: 1335337199.6870086\n",
      "Iteration 3374, Loss: 1344138583.9460487\n",
      "Iteration 3375, Loss: 1062731315.2015064\n",
      "Iteration 3376, Loss: 2177132188.7345414\n",
      "Iteration 3377, Loss: 1972071047.0381012\n",
      "Iteration 3378, Loss: 1883281157.4122975\n",
      "Iteration 3379, Loss: 1642020722.7144125\n",
      "Iteration 3380, Loss: 1254694520.5409682\n",
      "Iteration 3381, Loss: 1378512260.2788343\n",
      "Iteration 3382, Loss: 1076189663.3786821\n",
      "Iteration 3383, Loss: 1064019872.5408618\n",
      "Iteration 3384, Loss: 1061041181.4851942\n",
      "Iteration 3385, Loss: 2133060920.4075906\n",
      "Iteration 3386, Loss: 1288053365.8473625\n",
      "Iteration 3387, Loss: 1264270131.069961\n",
      "Iteration 3388, Loss: 1258576058.2875779\n",
      "Iteration 3389, Loss: 1075309754.4610577\n",
      "Iteration 3390, Loss: 1477140863.0799296\n",
      "Iteration 3391, Loss: 1173449139.3074114\n",
      "Iteration 3392, Loss: 1335291660.4580405\n",
      "Iteration 3393, Loss: 1353611416.0806468\n",
      "Iteration 3394, Loss: 1395516916.0721943\n",
      "Iteration 3395, Loss: 1362799576.230342\n",
      "Iteration 3396, Loss: 1329502363.6504261\n",
      "Iteration 3397, Loss: 1267444487.9292104\n",
      "Iteration 3398, Loss: 1258917250.977809\n",
      "Iteration 3399, Loss: 1266735280.6808627\n",
      "Iteration 3400, Loss: 1259292172.9527278\n",
      "Iteration 3401, Loss: 1183516794.9961576\n",
      "Iteration 3402, Loss: 1067205322.0287617\n",
      "Iteration 3403, Loss: 2914080684.970717\n",
      "Iteration 3404, Loss: 1419567964.4233031\n",
      "Iteration 3405, Loss: 1199804374.3595743\n",
      "Iteration 3406, Loss: 1343577114.0366833\n",
      "Iteration 3407, Loss: 1228418647.644312\n",
      "Iteration 3408, Loss: 1131487321.3077583\n",
      "Iteration 3409, Loss: 1137267323.2987607\n",
      "Iteration 3410, Loss: 1394718079.7958546\n",
      "Iteration 3411, Loss: 1369913112.0383365\n",
      "Iteration 3412, Loss: 1071944296.9712986\n",
      "Iteration 3413, Loss: 1534087192.958488\n",
      "Iteration 3414, Loss: 1283292145.1313834\n",
      "Iteration 3415, Loss: 1338546446.1000779\n",
      "Iteration 3416, Loss: 1105741278.5855622\n",
      "Iteration 3417, Loss: 1223412071.0055282\n",
      "Iteration 3418, Loss: 1268027945.4838543\n",
      "Iteration 3419, Loss: 1253716243.0182304\n",
      "Iteration 3420, Loss: 1240702967.9011972\n",
      "Iteration 3421, Loss: 1272162023.0366044\n",
      "Iteration 3422, Loss: 1349404083.7962356\n",
      "Iteration 3423, Loss: 1285549578.51216\n",
      "Iteration 3424, Loss: 1269888409.3811657\n",
      "Iteration 3425, Loss: 1197777629.40328\n",
      "Iteration 3426, Loss: 1198204712.7231922\n",
      "Iteration 3427, Loss: 1247057504.4573042\n",
      "Iteration 3428, Loss: 1236006566.9520497\n",
      "Iteration 3429, Loss: 1184560541.160609\n",
      "Iteration 3430, Loss: 1174395325.7475593\n",
      "Iteration 3431, Loss: 1080852469.8640394\n",
      "Iteration 3432, Loss: 1181886177.51177\n",
      "Iteration 3433, Loss: 1212422285.167402\n",
      "Iteration 3434, Loss: 1670527735.4294844\n",
      "Iteration 3435, Loss: 1095839737.0805342\n",
      "Iteration 3436, Loss: 1213358366.2390099\n",
      "Iteration 3437, Loss: 1167216682.995812\n",
      "Iteration 3438, Loss: 1275979773.6335793\n",
      "Iteration 3439, Loss: 1261213909.0809793\n",
      "Iteration 3440, Loss: 1296956789.386161\n",
      "Iteration 3441, Loss: 1279882973.5398066\n",
      "Iteration 3442, Loss: 1264252799.261181\n",
      "Iteration 3443, Loss: 1300097351.7091446\n",
      "Iteration 3444, Loss: 1364069873.540763\n",
      "Iteration 3445, Loss: 1409319447.7221756\n",
      "Iteration 3446, Loss: 1125227298.7085748\n",
      "Iteration 3447, Loss: 1164297027.0025723\n",
      "Iteration 3448, Loss: 1160807082.4437122\n",
      "Iteration 3449, Loss: 1160568201.5174346\n",
      "Iteration 3450, Loss: 1134546369.4261913\n",
      "Iteration 3451, Loss: 1137124657.2493958\n",
      "Iteration 3452, Loss: 1087049193.1422577\n",
      "Iteration 3453, Loss: 1091243470.7364976\n",
      "Iteration 3454, Loss: 1145319409.2271862\n",
      "Iteration 3455, Loss: 1207416319.254926\n",
      "Iteration 3456, Loss: 1199202183.6840608\n",
      "Iteration 3457, Loss: 1233938370.360828\n",
      "Iteration 3458, Loss: 1315171712.0678325\n",
      "Iteration 3459, Loss: 1353075950.9523053\n",
      "Iteration 3460, Loss: 1165148257.5042453\n",
      "Iteration 3461, Loss: 1283372343.163971\n",
      "Iteration 3462, Loss: 1156874390.6540964\n",
      "Iteration 3463, Loss: 1153414521.2306724\n",
      "Iteration 3464, Loss: 1203756696.1173592\n",
      "Iteration 3465, Loss: 1236761153.186976\n",
      "Iteration 3466, Loss: 1173930887.1912115\n",
      "Iteration 3467, Loss: 1280117928.6538105\n",
      "Iteration 3468, Loss: 1072194077.234488\n",
      "Iteration 3469, Loss: 1463940449.1780963\n",
      "Iteration 3470, Loss: 1250225437.1483\n",
      "Iteration 3471, Loss: 1235914420.8895047\n",
      "Iteration 3472, Loss: 1237409881.4875689\n",
      "Iteration 3473, Loss: 1108381640.4733427\n",
      "Iteration 3474, Loss: 1306282685.0953484\n",
      "Iteration 3475, Loss: 1169471728.7421257\n",
      "Iteration 3476, Loss: 1164341296.022645\n",
      "Iteration 3477, Loss: 1154097306.1117187\n",
      "Iteration 3478, Loss: 1180602227.615401\n",
      "Iteration 3479, Loss: 1114098339.588803\n",
      "Iteration 3480, Loss: 1380763331.338283\n",
      "Iteration 3481, Loss: 1420463875.7451391\n",
      "Iteration 3482, Loss: 1423799892.780441\n",
      "Iteration 3483, Loss: 1115266349.9210908\n",
      "Iteration 3484, Loss: 1484579937.9971426\n",
      "Iteration 3485, Loss: 1080049431.6333425\n",
      "Iteration 3486, Loss: 2406450517.781512\n",
      "Iteration 3487, Loss: 1160877575.9504728\n",
      "Iteration 3488, Loss: 1267687206.5738566\n",
      "Iteration 3489, Loss: 1251773380.8440213\n",
      "Iteration 3490, Loss: 1307079853.0136352\n",
      "Iteration 3491, Loss: 1341043504.377611\n",
      "Iteration 3492, Loss: 1077571803.3154857\n",
      "Iteration 3493, Loss: 1130474545.0187304\n",
      "Iteration 3494, Loss: 1101697277.8784912\n",
      "Iteration 3495, Loss: 1099195942.7217913\n",
      "Iteration 3496, Loss: 1198763366.983033\n",
      "Iteration 3497, Loss: 1297438099.6330948\n",
      "Iteration 3498, Loss: 1240535452.5088134\n",
      "Iteration 3499, Loss: 1073976486.7318895\n",
      "Iteration 3500, Loss: 1327176416.510202\n",
      "Iteration 3501, Loss: 1227859096.9890542\n",
      "Iteration 3502, Loss: 1119786536.921397\n",
      "Iteration 3503, Loss: 1166667894.2223523\n",
      "Iteration 3504, Loss: 1110656499.5832317\n",
      "Iteration 3505, Loss: 1148107418.2862635\n",
      "Iteration 3506, Loss: 1209472557.7963886\n",
      "Iteration 3507, Loss: 1301075934.9320815\n",
      "Iteration 3508, Loss: 1336285170.0386655\n",
      "Iteration 3509, Loss: 1114145088.1687992\n",
      "Iteration 3510, Loss: 1110397672.9327338\n",
      "Iteration 3511, Loss: 2106598589.3376753\n",
      "Iteration 3512, Loss: 1917419986.1857774\n",
      "Iteration 3513, Loss: 1918714480.926539\n",
      "Iteration 3514, Loss: 1341860822.3167408\n",
      "Iteration 3515, Loss: 1146917171.4750051\n",
      "Iteration 3516, Loss: 1300915143.1060517\n",
      "Iteration 3517, Loss: 1160389890.0297012\n",
      "Iteration 3518, Loss: 1185484304.8768952\n",
      "Iteration 3519, Loss: 1714056385.0602403\n",
      "Iteration 3520, Loss: 1631198401.1043882\n",
      "Iteration 3521, Loss: 1205674789.8355362\n",
      "Iteration 3522, Loss: 1154526727.7430341\n",
      "Iteration 3523, Loss: 1148853901.122455\n",
      "Iteration 3524, Loss: 1252959574.9880557\n",
      "Iteration 3525, Loss: 1056672495.2231851\n",
      "Iteration 3526, Loss: 1302254756.9703388\n",
      "Iteration 3527, Loss: 1099552989.4844682\n",
      "Iteration 3528, Loss: 2135586378.1809824\n",
      "Iteration 3529, Loss: 1959830714.4877334\n",
      "Iteration 3530, Loss: 1056008879.4705498\n",
      "Iteration 3531, Loss: 1071314605.4896905\n",
      "Iteration 3532, Loss: 1585663006.2614162\n",
      "Iteration 3533, Loss: 1286958112.666419\n",
      "Iteration 3534, Loss: 1122440083.7423043\n",
      "Iteration 3535, Loss: 1123179631.158929\n",
      "Iteration 3536, Loss: 1179107977.337534\n",
      "Iteration 3537, Loss: 1173718941.457882\n",
      "Iteration 3538, Loss: 1273750202.4418778\n",
      "Iteration 3539, Loss: 1475713926.4917417\n",
      "Iteration 3540, Loss: 1452055087.8814363\n",
      "Iteration 3541, Loss: 1432394651.8487597\n",
      "Iteration 3542, Loss: 1080027783.0163174\n",
      "Iteration 3543, Loss: 3672933860.5279365\n",
      "Iteration 3544, Loss: 2467594807.953837\n",
      "Iteration 3545, Loss: 2082506515.7548115\n",
      "Iteration 3546, Loss: 1548333335.3565562\n",
      "Iteration 3547, Loss: 1554425167.9475558\n",
      "Iteration 3548, Loss: 1597679282.7017426\n",
      "Iteration 3549, Loss: 1516199603.5605094\n",
      "Iteration 3550, Loss: 1165657077.6962967\n",
      "Iteration 3551, Loss: 1283831600.176217\n",
      "Iteration 3552, Loss: 1073829263.9887931\n",
      "Iteration 3553, Loss: 1309890210.4700656\n",
      "Iteration 3554, Loss: 1286725027.7118824\n",
      "Iteration 3555, Loss: 1075279655.462414\n",
      "Iteration 3556, Loss: 1073520912.2113066\n",
      "Iteration 3557, Loss: 1078205970.5955656\n",
      "Iteration 3558, Loss: 1077476656.9994886\n",
      "Iteration 3559, Loss: 1324255154.8665633\n",
      "Iteration 3560, Loss: 1362003948.136173\n",
      "Iteration 3561, Loss: 1400897506.6475196\n",
      "Iteration 3562, Loss: 1386942578.3700871\n",
      "Iteration 3563, Loss: 1307322981.5969563\n",
      "Iteration 3564, Loss: 1253868365.7711518\n",
      "Iteration 3565, Loss: 1088506735.7829347\n",
      "Iteration 3566, Loss: 1085001945.4917126\n",
      "Iteration 3567, Loss: 1131348088.5510798\n",
      "Iteration 3568, Loss: 1262427330.8414323\n",
      "Iteration 3569, Loss: 1335353333.0312612\n",
      "Iteration 3570, Loss: 1220243705.7221231\n",
      "Iteration 3571, Loss: 1210219389.1246648\n",
      "Iteration 3572, Loss: 1299704455.829979\n",
      "Iteration 3573, Loss: 1339648003.7289999\n",
      "Iteration 3574, Loss: 1312591910.4419749\n",
      "Iteration 3575, Loss: 1291466841.4820158\n",
      "Iteration 3576, Loss: 1340273624.0121348\n",
      "Iteration 3577, Loss: 1360815818.3068013\n",
      "Iteration 3578, Loss: 1331263649.5361104\n",
      "Iteration 3579, Loss: 1376371751.1134415\n",
      "Iteration 3580, Loss: 1378201190.1485972\n",
      "Iteration 3581, Loss: 1243633845.166781\n",
      "Iteration 3582, Loss: 1134666690.3282778\n",
      "Iteration 3583, Loss: 1201093437.9649112\n",
      "Iteration 3584, Loss: 1680991650.4315944\n",
      "Iteration 3585, Loss: 1612478334.140724\n",
      "Iteration 3586, Loss: 1411110790.6143634\n",
      "Iteration 3587, Loss: 1451797474.5661705\n",
      "Iteration 3588, Loss: 1121719332.70342\n",
      "Iteration 3589, Loss: 1060091309.7417468\n",
      "Iteration 3590, Loss: 1062664554.1564239\n",
      "Iteration 3591, Loss: 1070069752.3319116\n",
      "Iteration 3592, Loss: 1170115789.6679742\n",
      "Iteration 3593, Loss: 1774296759.162765\n",
      "Iteration 3594, Loss: 1080860608.6724215\n",
      "Iteration 3595, Loss: 1087360316.087862\n",
      "Iteration 3596, Loss: 1091376660.017039\n",
      "Iteration 3597, Loss: 1194433382.5856314\n",
      "Iteration 3598, Loss: 1355273522.6732233\n",
      "Iteration 3599, Loss: 1359318245.7804143\n",
      "Iteration 3600, Loss: 1125868261.9434128\n",
      "Iteration 3601, Loss: 1986525887.0510921\n",
      "Iteration 3602, Loss: 1179500853.9378858\n",
      "Iteration 3603, Loss: 1281042681.7767222\n",
      "Iteration 3604, Loss: 1150280036.575735\n",
      "Iteration 3605, Loss: 1578295058.5042264\n",
      "Iteration 3606, Loss: 1122940901.1314046\n",
      "Iteration 3607, Loss: 1324080902.9043982\n",
      "Iteration 3608, Loss: 1136303915.1178303\n",
      "Iteration 3609, Loss: 1325125323.6515677\n",
      "Iteration 3610, Loss: 1335274668.2470062\n",
      "Iteration 3611, Loss: 1056441271.6839827\n",
      "Iteration 3612, Loss: 1243407698.4568043\n",
      "Iteration 3613, Loss: 1138714165.5485678\n",
      "Iteration 3614, Loss: 1338606444.1480997\n",
      "Iteration 3615, Loss: 1344764523.5956185\n",
      "Iteration 3616, Loss: 1144248202.065953\n",
      "Iteration 3617, Loss: 1217472573.3261032\n",
      "Iteration 3618, Loss: 1160717885.702961\n",
      "Iteration 3619, Loss: 1183447079.7056484\n",
      "Iteration 3620, Loss: 1169976295.960317\n",
      "Iteration 3621, Loss: 1313914920.87336\n",
      "Iteration 3622, Loss: 1288369038.816588\n",
      "Iteration 3623, Loss: 1124899067.4832048\n",
      "Iteration 3624, Loss: 1293016735.769368\n",
      "Iteration 3625, Loss: 1166057965.9671717\n",
      "Iteration 3626, Loss: 1212852784.0039434\n",
      "Iteration 3627, Loss: 1158507972.19842\n",
      "Iteration 3628, Loss: 1138008850.413726\n",
      "Iteration 3629, Loss: 1162553130.4958127\n",
      "Iteration 3630, Loss: 1189636014.8213196\n",
      "Iteration 3631, Loss: 1056908431.3457928\n",
      "Iteration 3632, Loss: 1086461248.0836241\n",
      "Iteration 3633, Loss: 1290806496.8592112\n",
      "Iteration 3634, Loss: 1233392670.0939257\n",
      "Iteration 3635, Loss: 1304368378.2636147\n",
      "Iteration 3636, Loss: 1316541085.6206956\n",
      "Iteration 3637, Loss: 1208056790.5238302\n",
      "Iteration 3638, Loss: 1265205179.0138795\n",
      "Iteration 3639, Loss: 1332894841.443564\n",
      "Iteration 3640, Loss: 1336021105.3760235\n",
      "Iteration 3641, Loss: 1336196110.303038\n",
      "Iteration 3642, Loss: 1140378783.8072252\n",
      "Iteration 3643, Loss: 1141463067.0742428\n",
      "Iteration 3644, Loss: 1134155881.7398312\n",
      "Iteration 3645, Loss: 1158195672.9889035\n",
      "Iteration 3646, Loss: 1317718106.7396903\n",
      "Iteration 3647, Loss: 1271223690.6367013\n",
      "Iteration 3648, Loss: 1332017648.154801\n",
      "Iteration 3649, Loss: 1125184636.6920912\n",
      "Iteration 3650, Loss: 1284722940.0422163\n",
      "Iteration 3651, Loss: 1067583153.4232367\n",
      "Iteration 3652, Loss: 1096552681.2019556\n",
      "Iteration 3653, Loss: 1099045579.6405716\n",
      "Iteration 3654, Loss: 1390985992.543666\n",
      "Iteration 3655, Loss: 1070551773.1798319\n",
      "Iteration 3656, Loss: 1207741590.4621117\n",
      "Iteration 3657, Loss: 1262597619.9079063\n",
      "Iteration 3658, Loss: 1244347059.4185743\n",
      "Iteration 3659, Loss: 1323208757.7978346\n",
      "Iteration 3660, Loss: 1381261265.3643537\n",
      "Iteration 3661, Loss: 1234794308.5145915\n",
      "Iteration 3662, Loss: 1259504348.4471836\n",
      "Iteration 3663, Loss: 1073203876.6168131\n",
      "Iteration 3664, Loss: 1059679690.6810268\n",
      "Iteration 3665, Loss: 1388640921.5885348\n",
      "Iteration 3666, Loss: 1053613268.4539812\n",
      "Iteration 3667, Loss: 1057434544.3538003\n",
      "Iteration 3668, Loss: 1090959897.4346704\n",
      "Iteration 3669, Loss: 1108744325.0405114\n",
      "Iteration 3670, Loss: 1152601455.3731115\n",
      "Iteration 3671, Loss: 1147990991.390126\n",
      "Iteration 3672, Loss: 1203411118.054739\n",
      "Iteration 3673, Loss: 1049756412.7385623\n",
      "Iteration 3674, Loss: 1342063414.7448711\n",
      "Iteration 3675, Loss: 1307716464.943678\n",
      "Iteration 3676, Loss: 1323558837.6408992\n",
      "Iteration 3677, Loss: 1332769475.6569543\n",
      "Iteration 3678, Loss: 1274703326.8324816\n",
      "Iteration 3679, Loss: 1219317343.5464942\n",
      "Iteration 3680, Loss: 1205572763.498148\n",
      "Iteration 3681, Loss: 1188689342.7899477\n",
      "Iteration 3682, Loss: 1202750380.0677881\n",
      "Iteration 3683, Loss: 1150776025.4809403\n",
      "Iteration 3684, Loss: 1252317431.6778808\n",
      "Iteration 3685, Loss: 1233139124.9530892\n",
      "Iteration 3686, Loss: 1096258942.5894287\n",
      "Iteration 3687, Loss: 1390491402.050465\n",
      "Iteration 3688, Loss: 1357177835.1036782\n",
      "Iteration 3689, Loss: 1288555990.977725\n",
      "Iteration 3690, Loss: 1233820897.243597\n",
      "Iteration 3691, Loss: 1214783968.3486485\n",
      "Iteration 3692, Loss: 1053078914.2121701\n",
      "Iteration 3693, Loss: 1053693463.6464853\n",
      "Iteration 3694, Loss: 1232961929.9655108\n",
      "Iteration 3695, Loss: 1266403327.2745\n",
      "Iteration 3696, Loss: 1335372315.793184\n",
      "Iteration 3697, Loss: 1137538706.345729\n",
      "Iteration 3698, Loss: 1259823401.0736287\n",
      "Iteration 3699, Loss: 1240078602.4223375\n",
      "Iteration 3700, Loss: 1272151729.903988\n",
      "Iteration 3701, Loss: 1125141187.247351\n",
      "Iteration 3702, Loss: 1180555518.55391\n",
      "Iteration 3703, Loss: 1104980538.2348647\n",
      "Iteration 3704, Loss: 1249644976.9710336\n",
      "Iteration 3705, Loss: 1231299873.4049864\n",
      "Iteration 3706, Loss: 1141736935.810191\n",
      "Iteration 3707, Loss: 1211975300.8985858\n",
      "Iteration 3708, Loss: 1194934911.3514428\n",
      "Iteration 3709, Loss: 1325551088.9540231\n",
      "Iteration 3710, Loss: 1293615819.132354\n",
      "Iteration 3711, Loss: 1270106560.204129\n",
      "Iteration 3712, Loss: 1250934507.6357028\n",
      "Iteration 3713, Loss: 1331026917.7929962\n",
      "Iteration 3714, Loss: 1215482186.0899634\n",
      "Iteration 3715, Loss: 1175250944.3263433\n",
      "Iteration 3716, Loss: 1183352034.899647\n",
      "Iteration 3717, Loss: 1177447756.8772547\n",
      "Iteration 3718, Loss: 1326304645.750193\n",
      "Iteration 3719, Loss: 1276272410.4382453\n",
      "Iteration 3720, Loss: 1243712018.5040224\n",
      "Iteration 3721, Loss: 1238981102.8656309\n",
      "Iteration 3722, Loss: 1121295469.6760287\n",
      "Iteration 3723, Loss: 1177109015.2009902\n",
      "Iteration 3724, Loss: 1058231734.5065691\n",
      "Iteration 3725, Loss: 2945393945.4124923\n",
      "Iteration 3726, Loss: 1393506148.0417354\n",
      "Iteration 3727, Loss: 1074511738.4023278\n",
      "Iteration 3728, Loss: 1073617093.2366743\n",
      "Iteration 3729, Loss: 1080804117.451423\n",
      "Iteration 3730, Loss: 1077881989.9438844\n",
      "Iteration 3731, Loss: 1839528497.569945\n",
      "Iteration 3732, Loss: 1707440923.1439323\n",
      "Iteration 3733, Loss: 1752137782.3632839\n",
      "Iteration 3734, Loss: 1114407117.4744484\n",
      "Iteration 3735, Loss: 1388705558.893593\n",
      "Iteration 3736, Loss: 1407297626.7974896\n",
      "Iteration 3737, Loss: 1193418258.5303884\n",
      "Iteration 3738, Loss: 1168400837.1591644\n",
      "Iteration 3739, Loss: 1168143503.8583653\n",
      "Iteration 3740, Loss: 1188351504.85508\n",
      "Iteration 3741, Loss: 1194930937.0603778\n",
      "Iteration 3742, Loss: 1299509883.3102322\n",
      "Iteration 3743, Loss: 1074522310.481321\n",
      "Iteration 3744, Loss: 1116657079.337419\n",
      "Iteration 3745, Loss: 1226509870.788007\n",
      "Iteration 3746, Loss: 1601710589.522941\n",
      "Iteration 3747, Loss: 1645098775.437391\n",
      "Iteration 3748, Loss: 1081285664.4583972\n",
      "Iteration 3749, Loss: 1291528627.4743588\n",
      "Iteration 3750, Loss: 1126500522.4405615\n",
      "Iteration 3751, Loss: 1259717074.3476083\n",
      "Iteration 3752, Loss: 1337663055.7563884\n",
      "Iteration 3753, Loss: 1347573799.3596702\n",
      "Iteration 3754, Loss: 1315077893.3433874\n",
      "Iteration 3755, Loss: 1335903968.298329\n",
      "Iteration 3756, Loss: 1094938399.0625856\n",
      "Iteration 3757, Loss: 1092514444.4656715\n",
      "Iteration 3758, Loss: 1293184795.524709\n",
      "Iteration 3759, Loss: 1110123848.5464327\n",
      "Iteration 3760, Loss: 1324462592.140511\n",
      "Iteration 3761, Loss: 1163456571.5506477\n",
      "Iteration 3762, Loss: 1198953513.0166626\n",
      "Iteration 3763, Loss: 1187608355.3917518\n",
      "Iteration 3764, Loss: 1181622467.4330285\n",
      "Iteration 3765, Loss: 1210597905.3790796\n",
      "Iteration 3766, Loss: 1240385609.4440117\n",
      "Iteration 3767, Loss: 1265717830.4257526\n",
      "Iteration 3768, Loss: 1339278115.6588264\n",
      "Iteration 3769, Loss: 1358545033.6714158\n",
      "Iteration 3770, Loss: 1362043171.6474438\n",
      "Iteration 3771, Loss: 1364808519.5467715\n",
      "Iteration 3772, Loss: 1289228282.2585182\n",
      "Iteration 3773, Loss: 1282385753.2740166\n",
      "Iteration 3774, Loss: 1229681757.8202841\n",
      "Iteration 3775, Loss: 1262094368.0766528\n",
      "Iteration 3776, Loss: 1170326335.0117574\n",
      "Iteration 3777, Loss: 1062707600.2370226\n",
      "Iteration 3778, Loss: 2636716506.706365\n",
      "Iteration 3779, Loss: 1666419355.0887456\n",
      "Iteration 3780, Loss: 1632183759.567126\n",
      "Iteration 3781, Loss: 1131709466.1279755\n",
      "Iteration 3782, Loss: 1126493722.1525545\n",
      "Iteration 3783, Loss: 1099640515.8606958\n",
      "Iteration 3784, Loss: 1106052553.8199072\n",
      "Iteration 3785, Loss: 1215867634.6484482\n",
      "Iteration 3786, Loss: 1287908076.8779585\n",
      "Iteration 3787, Loss: 1355767175.5216825\n",
      "Iteration 3788, Loss: 1326910273.1416795\n",
      "Iteration 3789, Loss: 1318271492.8145077\n",
      "Iteration 3790, Loss: 1310286977.0358744\n",
      "Iteration 3791, Loss: 1099748155.532143\n",
      "Iteration 3792, Loss: 1152906468.8150826\n",
      "Iteration 3793, Loss: 1189013027.2488322\n",
      "Iteration 3794, Loss: 1161051950.5648687\n",
      "Iteration 3795, Loss: 1140427463.601622\n",
      "Iteration 3796, Loss: 1919771684.821137\n",
      "Iteration 3797, Loss: 1407850957.7143054\n",
      "Iteration 3798, Loss: 1156069776.0351543\n",
      "Iteration 3799, Loss: 1219096796.7988312\n",
      "Iteration 3800, Loss: 1249607252.0266502\n",
      "Iteration 3801, Loss: 1088201441.985118\n",
      "Iteration 3802, Loss: 1089921069.8326356\n",
      "Iteration 3803, Loss: 1313361986.0474849\n",
      "Iteration 3804, Loss: 1275334089.0047433\n",
      "Iteration 3805, Loss: 1296807459.734171\n",
      "Iteration 3806, Loss: 1186640687.232643\n",
      "Iteration 3807, Loss: 1173263489.568084\n",
      "Iteration 3808, Loss: 1282717912.335183\n",
      "Iteration 3809, Loss: 1469374732.4725065\n",
      "Iteration 3810, Loss: 1451047061.8052359\n",
      "Iteration 3811, Loss: 1355126677.2833354\n",
      "Iteration 3812, Loss: 1163615832.0446134\n",
      "Iteration 3813, Loss: 1223083022.1328204\n",
      "Iteration 3814, Loss: 1213331568.429666\n",
      "Iteration 3815, Loss: 1073518934.3713063\n",
      "Iteration 3816, Loss: 1078203984.3203375\n",
      "Iteration 3817, Loss: 1123201037.4152794\n",
      "Iteration 3818, Loss: 1233144156.23067\n",
      "Iteration 3819, Loss: 1072329293.7190609\n",
      "Iteration 3820, Loss: 1325891935.2566981\n",
      "Iteration 3821, Loss: 1134003276.817619\n",
      "Iteration 3822, Loss: 1264574467.2825723\n",
      "Iteration 3823, Loss: 1245039259.2668116\n",
      "Iteration 3824, Loss: 1278691249.9690056\n",
      "Iteration 3825, Loss: 1257081883.3169353\n",
      "Iteration 3826, Loss: 1173982311.041358\n",
      "Iteration 3827, Loss: 1284131356.2344635\n",
      "Iteration 3828, Loss: 1128200198.4890368\n",
      "Iteration 3829, Loss: 1180524572.8076272\n",
      "Iteration 3830, Loss: 1209407300.0048964\n",
      "Iteration 3831, Loss: 1149261684.2793927\n",
      "Iteration 3832, Loss: 1147858662.2568004\n",
      "Iteration 3833, Loss: 1217057691.604592\n",
      "Iteration 3834, Loss: 1129811820.2737656\n",
      "Iteration 3835, Loss: 1727137688.5146484\n",
      "Iteration 3836, Loss: 1112753516.2963297\n",
      "Iteration 3837, Loss: 1229122174.9580557\n",
      "Iteration 3838, Loss: 1122319966.6987596\n",
      "Iteration 3839, Loss: 1373924634.6109974\n",
      "Iteration 3840, Loss: 1124641845.4841244\n",
      "Iteration 3841, Loss: 1127096725.4661398\n",
      "Iteration 3842, Loss: 1184038226.925239\n",
      "Iteration 3843, Loss: 1284063436.7270863\n",
      "Iteration 3844, Loss: 1133057399.16734\n",
      "Iteration 3845, Loss: 1135053521.3379188\n",
      "Iteration 3846, Loss: 1289593231.329599\n",
      "Iteration 3847, Loss: 1121449592.3265798\n",
      "Iteration 3848, Loss: 1200538882.884559\n",
      "Iteration 3849, Loss: 1270510140.0956044\n",
      "Iteration 3850, Loss: 1130120782.3589692\n",
      "Iteration 3851, Loss: 1383458806.9098668\n",
      "Iteration 3852, Loss: 1350064510.6601253\n",
      "Iteration 3853, Loss: 1339189316.61675\n",
      "Iteration 3854, Loss: 1312721081.310863\n",
      "Iteration 3855, Loss: 1229944216.2303312\n",
      "Iteration 3856, Loss: 1257667045.73076\n",
      "Iteration 3857, Loss: 1260481759.8427267\n",
      "Iteration 3858, Loss: 1312377519.279913\n",
      "Iteration 3859, Loss: 1357589583.378209\n",
      "Iteration 3860, Loss: 1361924145.972128\n",
      "Iteration 3861, Loss: 1059609968.2440758\n",
      "Iteration 3862, Loss: 1598966882.5113273\n",
      "Iteration 3863, Loss: 1211504257.4453561\n",
      "Iteration 3864, Loss: 1198683317.2253742\n",
      "Iteration 3865, Loss: 1197469867.7025027\n",
      "Iteration 3866, Loss: 1186010886.024901\n",
      "Iteration 3867, Loss: 1169191810.942679\n",
      "Iteration 3868, Loss: 1162855710.9931207\n",
      "Iteration 3869, Loss: 1154548000.8048673\n",
      "Iteration 3870, Loss: 1329818039.4236684\n",
      "Iteration 3871, Loss: 1376944659.6546025\n",
      "Iteration 3872, Loss: 1347410945.4980774\n",
      "Iteration 3873, Loss: 1110110779.0688365\n",
      "Iteration 3874, Loss: 1160004765.641903\n",
      "Iteration 3875, Loss: 1106762050.8765643\n",
      "Iteration 3876, Loss: 1099431289.682988\n",
      "Iteration 3877, Loss: 1096917345.392244\n",
      "Iteration 3878, Loss: 2082875652.1287735\n",
      "Iteration 3879, Loss: 1065018737.2459064\n",
      "Iteration 3880, Loss: 1067453274.584871\n",
      "Iteration 3881, Loss: 2072229140.171153\n",
      "Iteration 3882, Loss: 1844384954.1206055\n",
      "Iteration 3883, Loss: 2559015913.0140224\n",
      "Iteration 3884, Loss: 1355343527.9171832\n",
      "Iteration 3885, Loss: 1132515843.487316\n",
      "Iteration 3886, Loss: 1136074116.769077\n",
      "Iteration 3887, Loss: 1070037990.364171\n",
      "Iteration 3888, Loss: 1076230491.373094\n",
      "Iteration 3889, Loss: 1896328321.0552309\n",
      "Iteration 3890, Loss: 1677655690.9168034\n",
      "Iteration 3891, Loss: 1082665353.0499563\n",
      "Iteration 3892, Loss: 1090005752.3890278\n",
      "Iteration 3893, Loss: 1087878295.8026392\n",
      "Iteration 3894, Loss: 1756531906.9522645\n",
      "Iteration 3895, Loss: 1118542670.896661\n",
      "Iteration 3896, Loss: 1382048444.2515035\n",
      "Iteration 3897, Loss: 1351791174.2633343\n",
      "Iteration 3898, Loss: 1350147274.6408868\n",
      "Iteration 3899, Loss: 1327775665.290427\n",
      "Iteration 3900, Loss: 1203280466.8937066\n",
      "Iteration 3901, Loss: 1237886063.9167986\n",
      "Iteration 3902, Loss: 1156900466.9765687\n",
      "Iteration 3903, Loss: 1207946862.8712454\n",
      "Iteration 3904, Loss: 1329148082.648681\n",
      "Iteration 3905, Loss: 1231144326.7740204\n",
      "Iteration 3906, Loss: 1221791417.5028348\n",
      "Iteration 3907, Loss: 1283712213.2648907\n",
      "Iteration 3908, Loss: 1357959903.4691708\n",
      "Iteration 3909, Loss: 1329581238.1012745\n",
      "Iteration 3910, Loss: 1322812304.749948\n",
      "Iteration 3911, Loss: 1346244788.3994126\n",
      "Iteration 3912, Loss: 1361736531.9470303\n",
      "Iteration 3913, Loss: 1404472475.0357368\n",
      "Iteration 3914, Loss: 1128401851.041397\n",
      "Iteration 3915, Loss: 1239923115.897242\n",
      "Iteration 3916, Loss: 1065374017.6761256\n",
      "Iteration 3917, Loss: 1278271511.2670124\n",
      "Iteration 3918, Loss: 1177075676.0693016\n",
      "Iteration 3919, Loss: 1206047980.4188247\n",
      "Iteration 3920, Loss: 1152607177.3755674\n",
      "Iteration 3921, Loss: 1203198025.0632088\n",
      "Iteration 3922, Loss: 1682081813.2143786\n",
      "Iteration 3923, Loss: 1615736104.937856\n",
      "Iteration 3924, Loss: 1566241442.8353734\n",
      "Iteration 3925, Loss: 1360226098.9954722\n",
      "Iteration 3926, Loss: 1399920735.8487282\n",
      "Iteration 3927, Loss: 1199910951.2266364\n",
      "Iteration 3928, Loss: 1297958402.034385\n",
      "Iteration 3929, Loss: 1277681811.2290046\n",
      "Iteration 3930, Loss: 1076792943.3236992\n",
      "Iteration 3931, Loss: 1188480275.1288385\n",
      "Iteration 3932, Loss: 1235015484.3915977\n",
      "Iteration 3933, Loss: 1174682826.452879\n",
      "Iteration 3934, Loss: 1113714413.5928407\n",
      "Iteration 3935, Loss: 1117258506.1372685\n",
      "Iteration 3936, Loss: 1108916506.8059678\n",
      "Iteration 3937, Loss: 1543489376.384711\n",
      "Iteration 3938, Loss: 1281358000.91681\n",
      "Iteration 3939, Loss: 1351216089.1524675\n",
      "Iteration 3940, Loss: 1325714441.9807765\n",
      "Iteration 3941, Loss: 1170517759.1665726\n",
      "Iteration 3942, Loss: 1195865932.911286\n",
      "Iteration 3943, Loss: 1712323833.804799\n",
      "Iteration 3944, Loss: 1367852530.0659046\n",
      "Iteration 3945, Loss: 1093795917.4612803\n",
      "Iteration 3946, Loss: 1109301429.055492\n",
      "Iteration 3947, Loss: 1110272344.4937632\n",
      "Iteration 3948, Loss: 1116700944.2210112\n",
      "Iteration 3949, Loss: 1120159772.1552434\n",
      "Iteration 3950, Loss: 1190343676.862523\n",
      "Iteration 3951, Loss: 1181035690.2974515\n",
      "Iteration 3952, Loss: 1284463053.3319438\n",
      "Iteration 3953, Loss: 1266636423.0278647\n",
      "Iteration 3954, Loss: 1169328835.786836\n",
      "Iteration 3955, Loss: 1332352995.2162786\n",
      "Iteration 3956, Loss: 1141930984.7189128\n",
      "Iteration 3957, Loss: 1262921782.4511688\n",
      "Iteration 3958, Loss: 1135386766.5146794\n",
      "Iteration 3959, Loss: 1972010606.9509082\n",
      "Iteration 3960, Loss: 1682909316.5354095\n",
      "Iteration 3961, Loss: 1632175459.9207702\n",
      "Iteration 3962, Loss: 1075967766.40013\n",
      "Iteration 3963, Loss: 1681820858.3031921\n",
      "Iteration 3964, Loss: 1650392847.709455\n",
      "Iteration 3965, Loss: 1142220996.2635875\n",
      "Iteration 3966, Loss: 1141655443.3227878\n",
      "Iteration 3967, Loss: 1216452127.1306858\n",
      "Iteration 3968, Loss: 1282198373.4202662\n",
      "Iteration 3969, Loss: 1337006704.4715686\n",
      "Iteration 3970, Loss: 1086304479.7931983\n",
      "Iteration 3971, Loss: 1461166762.8438532\n",
      "Iteration 3972, Loss: 1422489165.8080688\n",
      "Iteration 3973, Loss: 1171787882.1258197\n",
      "Iteration 3974, Loss: 1332172093.415863\n",
      "Iteration 3975, Loss: 1312665459.403932\n",
      "Iteration 3976, Loss: 1364294307.5850065\n",
      "Iteration 3977, Loss: 1402100127.7796621\n",
      "Iteration 3978, Loss: 1287298632.0069327\n",
      "Iteration 3979, Loss: 1138853805.279555\n",
      "Iteration 3980, Loss: 1273415538.8692234\n",
      "Iteration 3981, Loss: 1184205883.8741007\n",
      "Iteration 3982, Loss: 1245620704.7481651\n",
      "Iteration 3983, Loss: 1100030454.1576807\n",
      "Iteration 3984, Loss: 1280815736.2370353\n",
      "Iteration 3985, Loss: 1335844307.2648342\n",
      "Iteration 3986, Loss: 1352009657.466316\n",
      "Iteration 3987, Loss: 1360988356.154857\n",
      "Iteration 3988, Loss: 1068582460.9038633\n",
      "Iteration 3989, Loss: 1188538589.4521377\n",
      "Iteration 3990, Loss: 1284466595.5727193\n",
      "Iteration 3991, Loss: 1146851189.8531425\n",
      "Iteration 3992, Loss: 1338365065.717356\n",
      "Iteration 3993, Loss: 1138233712.2698169\n",
      "Iteration 3994, Loss: 1185957169.68722\n",
      "Iteration 3995, Loss: 1212010186.6514196\n",
      "Iteration 3996, Loss: 1245381465.3096201\n",
      "Iteration 3997, Loss: 1271343193.827794\n",
      "Iteration 3998, Loss: 1256152178.0589018\n",
      "Iteration 3999, Loss: 1243284625.6557484\n",
      "Iteration 4000, Loss: 1319623592.154138\n",
      "Iteration 4001, Loss: 1282882736.8308089\n",
      "Iteration 4002, Loss: 1122125047.1448681\n",
      "Iteration 4003, Loss: 1378678684.6585383\n",
      "Iteration 4004, Loss: 1352368112.1858325\n",
      "Iteration 4005, Loss: 1396364541.7121248\n",
      "Iteration 4006, Loss: 1229088851.3945093\n",
      "Iteration 4007, Loss: 1320888994.8865094\n",
      "Iteration 4008, Loss: 1344442215.4718022\n",
      "Iteration 4009, Loss: 1356351735.6496096\n",
      "Iteration 4010, Loss: 1365808345.552134\n",
      "Iteration 4011, Loss: 1242289372.2566001\n",
      "Iteration 4012, Loss: 1276370770.4830165\n",
      "Iteration 4013, Loss: 1079551161.8195593\n",
      "Iteration 4014, Loss: 1330962737.6310477\n",
      "Iteration 4015, Loss: 1104646566.183149\n",
      "Iteration 4016, Loss: 1073489138.4314138\n",
      "Iteration 4017, Loss: 1086499457.730658\n",
      "Iteration 4018, Loss: 1214452952.2551506\n",
      "Iteration 4019, Loss: 1228410822.1267698\n",
      "Iteration 4020, Loss: 1286712177.4635901\n",
      "Iteration 4021, Loss: 1073608872.4236445\n",
      "Iteration 4022, Loss: 1171913960.2032998\n",
      "Iteration 4023, Loss: 1322895112.0086136\n",
      "Iteration 4024, Loss: 1231387772.9858165\n",
      "Iteration 4025, Loss: 1215672272.1904945\n",
      "Iteration 4026, Loss: 1258950499.5852842\n",
      "Iteration 4027, Loss: 1240951501.3791668\n",
      "Iteration 4028, Loss: 1065006542.2340316\n",
      "Iteration 4029, Loss: 1253881163.401379\n",
      "Iteration 4030, Loss: 1240584543.4728584\n",
      "Iteration 4031, Loss: 1154196016.7994876\n",
      "Iteration 4032, Loss: 1221959672.8025622\n",
      "Iteration 4033, Loss: 1211690132.8898268\n",
      "Iteration 4034, Loss: 1075865088.1722844\n",
      "Iteration 4035, Loss: 1082732055.5307271\n",
      "Iteration 4036, Loss: 1324343683.6771908\n",
      "Iteration 4037, Loss: 1063690557.1126567\n",
      "Iteration 4038, Loss: 1067334923.0884899\n",
      "Iteration 4039, Loss: 2663416222.2201586\n",
      "Iteration 4040, Loss: 2396772655.7818904\n",
      "Iteration 4041, Loss: 2282695985.4512053\n",
      "Iteration 4042, Loss: 2128382085.6031408\n",
      "Iteration 4043, Loss: 1551598438.7882469\n",
      "Iteration 4044, Loss: 1402627059.361595\n",
      "Iteration 4045, Loss: 1485234988.9628866\n",
      "Iteration 4046, Loss: 1478157534.3590066\n",
      "Iteration 4047, Loss: 1423873801.96868\n",
      "Iteration 4048, Loss: 1125572944.4666412\n",
      "Iteration 4049, Loss: 1177410729.7941039\n",
      "Iteration 4050, Loss: 1183784690.1099265\n",
      "Iteration 4051, Loss: 1231739168.495574\n",
      "Iteration 4052, Loss: 1321410060.9086027\n",
      "Iteration 4053, Loss: 1122051531.9526074\n",
      "Iteration 4054, Loss: 1458487322.8234494\n",
      "Iteration 4055, Loss: 1236177682.1767757\n",
      "Iteration 4056, Loss: 1224390901.2713497\n",
      "Iteration 4057, Loss: 1158684576.04556\n",
      "Iteration 4058, Loss: 1205136722.6511128\n",
      "Iteration 4059, Loss: 1222286899.466657\n",
      "Iteration 4060, Loss: 1064801493.4110167\n",
      "Iteration 4061, Loss: 1247538438.3700194\n",
      "Iteration 4062, Loss: 1319808660.7043562\n",
      "Iteration 4063, Loss: 1260857136.7441065\n",
      "Iteration 4064, Loss: 1150712792.3932514\n",
      "Iteration 4065, Loss: 1264145000.0417025\n",
      "Iteration 4066, Loss: 1178384007.1781702\n",
      "Iteration 4067, Loss: 1226757977.511649\n",
      "Iteration 4068, Loss: 1075656044.342531\n",
      "Iteration 4069, Loss: 1078023554.3735893\n",
      "Iteration 4070, Loss: 1131081563.856818\n",
      "Iteration 4071, Loss: 1331440434.8643475\n",
      "Iteration 4072, Loss: 1062886559.9712095\n",
      "Iteration 4073, Loss: 1062178417.5514714\n",
      "Iteration 4074, Loss: 3358108970.3879733\n",
      "Iteration 4075, Loss: 2071252449.2284913\n",
      "Iteration 4076, Loss: 1807450915.797337\n",
      "Iteration 4077, Loss: 1695784768.3926423\n",
      "Iteration 4078, Loss: 1645307582.8748784\n",
      "Iteration 4079, Loss: 1617627951.8529637\n",
      "Iteration 4080, Loss: 1574490937.632121\n",
      "Iteration 4081, Loss: 1451903690.4643636\n",
      "Iteration 4082, Loss: 1412160225.6984947\n",
      "Iteration 4083, Loss: 1375470016.6253905\n",
      "Iteration 4084, Loss: 1258274356.2085853\n",
      "Iteration 4085, Loss: 1194421748.7650974\n",
      "Iteration 4086, Loss: 1190349459.4411042\n",
      "Iteration 4087, Loss: 1426681004.169242\n",
      "Iteration 4088, Loss: 1398458327.2440388\n",
      "Iteration 4089, Loss: 1174190094.9096568\n",
      "Iteration 4090, Loss: 1302459753.4264789\n",
      "Iteration 4091, Loss: 1287495000.826499\n",
      "Iteration 4092, Loss: 1266963626.010231\n",
      "Iteration 4093, Loss: 1326896638.2013273\n",
      "Iteration 4094, Loss: 1360798257.599477\n",
      "Iteration 4095, Loss: 1297990709.666768\n",
      "Iteration 4096, Loss: 1350606080.3481207\n",
      "Iteration 4097, Loss: 1290795727.1214027\n",
      "Iteration 4098, Loss: 1289744753.768681\n",
      "Iteration 4099, Loss: 1326891526.7853801\n",
      "Iteration 4100, Loss: 1308648987.0530367\n",
      "Iteration 4101, Loss: 1333555207.2150676\n",
      "Iteration 4102, Loss: 1162298849.6402435\n",
      "Iteration 4103, Loss: 1149614373.890946\n",
      "Iteration 4104, Loss: 1137722078.4745362\n",
      "Iteration 4105, Loss: 1316747862.946298\n",
      "Iteration 4106, Loss: 1185980438.52208\n",
      "Iteration 4107, Loss: 1226727678.8845093\n",
      "Iteration 4108, Loss: 1306450014.3329518\n",
      "Iteration 4109, Loss: 1085750057.3441622\n",
      "Iteration 4110, Loss: 1081900903.1270382\n",
      "Iteration 4111, Loss: 1083646004.7661839\n",
      "Iteration 4112, Loss: 1247789420.641793\n",
      "Iteration 4113, Loss: 1190570602.6016374\n",
      "Iteration 4114, Loss: 1305929640.8269594\n",
      "Iteration 4115, Loss: 1330651515.1565628\n",
      "Iteration 4116, Loss: 1090545790.5619795\n",
      "Iteration 4117, Loss: 1600537510.3620546\n",
      "Iteration 4118, Loss: 1214408624.9182482\n",
      "Iteration 4119, Loss: 1294428062.048084\n",
      "Iteration 4120, Loss: 1477224849.9766839\n",
      "Iteration 4121, Loss: 1441769135.813499\n",
      "Iteration 4122, Loss: 1203319946.7063167\n",
      "Iteration 4123, Loss: 1188581211.6724122\n",
      "Iteration 4124, Loss: 1301227218.5797691\n",
      "Iteration 4125, Loss: 1299997986.4502556\n",
      "Iteration 4126, Loss: 1285880519.959382\n",
      "Iteration 4127, Loss: 1348097813.219157\n",
      "Iteration 4128, Loss: 1330282676.6886663\n",
      "Iteration 4129, Loss: 1154162124.6626391\n",
      "Iteration 4130, Loss: 1374967534.4092097\n",
      "Iteration 4131, Loss: 1106575949.4239461\n",
      "Iteration 4132, Loss: 1086381451.4764025\n",
      "Iteration 4133, Loss: 1076713280.8110914\n",
      "Iteration 4134, Loss: 1516258263.9655015\n",
      "Iteration 4135, Loss: 1474440347.4284575\n",
      "Iteration 4136, Loss: 1189909976.413054\n",
      "Iteration 4137, Loss: 1299384498.172987\n",
      "Iteration 4138, Loss: 1260070806.769985\n",
      "Iteration 4139, Loss: 1261843931.3582888\n",
      "Iteration 4140, Loss: 1280287976.5368783\n",
      "Iteration 4141, Loss: 1360577540.472865\n",
      "Iteration 4142, Loss: 1415655169.7357223\n",
      "Iteration 4143, Loss: 1383841737.374195\n",
      "Iteration 4144, Loss: 1359345570.6001976\n",
      "Iteration 4145, Loss: 1337201260.308552\n",
      "Iteration 4146, Loss: 1115882194.9935832\n",
      "Iteration 4147, Loss: 1293272529.4564283\n",
      "Iteration 4148, Loss: 1321240930.0636363\n",
      "Iteration 4149, Loss: 1154437043.5807502\n",
      "Iteration 4150, Loss: 1274123768.2069318\n",
      "Iteration 4151, Loss: 1262593210.5224037\n",
      "Iteration 4152, Loss: 1263130208.7652657\n",
      "Iteration 4153, Loss: 1300087997.7738354\n",
      "Iteration 4154, Loss: 1371397245.8520188\n",
      "Iteration 4155, Loss: 1253545746.8004055\n",
      "Iteration 4156, Loss: 1239288942.3804145\n",
      "Iteration 4157, Loss: 1297189426.7595472\n",
      "Iteration 4158, Loss: 1083558220.1213512\n",
      "Iteration 4159, Loss: 1084064702.772647\n",
      "Iteration 4160, Loss: 1087047514.4195542\n",
      "Iteration 4161, Loss: 1090186811.463237\n",
      "Iteration 4162, Loss: 1294665767.8571837\n",
      "Iteration 4163, Loss: 1350900322.9501245\n",
      "Iteration 4164, Loss: 1073489469.5167719\n",
      "Iteration 4165, Loss: 1280664003.8476672\n",
      "Iteration 4166, Loss: 1497630524.8310504\n",
      "Iteration 4167, Loss: 1194876657.6075437\n",
      "Iteration 4168, Loss: 1187566470.1822784\n",
      "Iteration 4169, Loss: 1084045929.6389852\n",
      "Iteration 4170, Loss: 1084208277.7244914\n",
      "Iteration 4171, Loss: 1652947811.0496483\n",
      "Iteration 4172, Loss: 1701661828.894745\n",
      "Iteration 4173, Loss: 1750501877.352197\n",
      "Iteration 4174, Loss: 1628100572.0388217\n",
      "Iteration 4175, Loss: 1564566487.0943437\n",
      "Iteration 4176, Loss: 1203814334.9076817\n",
      "Iteration 4177, Loss: 1711102374.7627835\n",
      "Iteration 4178, Loss: 1099458317.1882257\n",
      "Iteration 4179, Loss: 1281027577.006486\n",
      "Iteration 4180, Loss: 1268376013.1042995\n",
      "Iteration 4181, Loss: 1095144063.03077\n",
      "Iteration 4182, Loss: 1143138426.9165168\n",
      "Iteration 4183, Loss: 1217584308.5078332\n",
      "Iteration 4184, Loss: 1317058873.597418\n",
      "Iteration 4185, Loss: 1264942986.6929224\n",
      "Iteration 4186, Loss: 1275884486.5021791\n",
      "Iteration 4187, Loss: 1359790846.0991635\n",
      "Iteration 4188, Loss: 1098196991.0905745\n",
      "Iteration 4189, Loss: 1674013213.5124123\n",
      "Iteration 4190, Loss: 1299523755.2025726\n",
      "Iteration 4191, Loss: 1255102768.8151221\n",
      "Iteration 4192, Loss: 1070729182.5289353\n",
      "Iteration 4193, Loss: 1459337788.567967\n",
      "Iteration 4194, Loss: 1503389226.9306295\n",
      "Iteration 4195, Loss: 1272583199.849647\n",
      "Iteration 4196, Loss: 1265092980.414867\n",
      "Iteration 4197, Loss: 1322387974.8632054\n",
      "Iteration 4198, Loss: 1300243445.332743\n",
      "Iteration 4199, Loss: 1331150064.3390932\n",
      "Iteration 4200, Loss: 1391632575.19346\n",
      "Iteration 4201, Loss: 1401160938.8232324\n",
      "Iteration 4202, Loss: 1325501991.9942808\n",
      "Iteration 4203, Loss: 1304280861.0311599\n",
      "Iteration 4204, Loss: 1442688878.8978305\n",
      "Iteration 4205, Loss: 1398725778.8988998\n",
      "Iteration 4206, Loss: 1438800392.0350666\n",
      "Iteration 4207, Loss: 1196001023.6654315\n",
      "Iteration 4208, Loss: 1289094289.4140515\n",
      "Iteration 4209, Loss: 1192292844.5928662\n",
      "Iteration 4210, Loss: 1318067985.006975\n",
      "Iteration 4211, Loss: 1085633454.3460076\n",
      "Iteration 4212, Loss: 1094409504.4469638\n",
      "Iteration 4213, Loss: 1098064225.3403745\n",
      "Iteration 4214, Loss: 1078156531.3504226\n",
      "Iteration 4215, Loss: 1175792738.2846186\n",
      "Iteration 4216, Loss: 1177082220.832616\n",
      "Iteration 4217, Loss: 1287499034.7841516\n",
      "Iteration 4218, Loss: 1078322247.302324\n",
      "Iteration 4219, Loss: 1086034680.0755274\n",
      "Iteration 4220, Loss: 1086063762.3673363\n",
      "Iteration 4221, Loss: 1086088005.30714\n",
      "Iteration 4222, Loss: 1307374383.383688\n",
      "Iteration 4223, Loss: 1083394736.499439\n",
      "Iteration 4224, Loss: 1115362520.6873615\n",
      "Iteration 4225, Loss: 1110768120.3283205\n",
      "Iteration 4226, Loss: 1115900867.6808429\n",
      "Iteration 4227, Loss: 1263762531.4483426\n",
      "Iteration 4228, Loss: 1344018651.3975291\n",
      "Iteration 4229, Loss: 1171982287.4316707\n",
      "Iteration 4230, Loss: 1173142804.9204047\n",
      "Iteration 4231, Loss: 1118929927.67237\n",
      "Iteration 4232, Loss: 1125893511.9089937\n",
      "Iteration 4233, Loss: 1119966809.2241268\n",
      "Iteration 4234, Loss: 1117308437.6405232\n",
      "Iteration 4235, Loss: 1156464708.8321233\n",
      "Iteration 4236, Loss: 1588709447.9306533\n",
      "Iteration 4237, Loss: 1324661609.32977\n",
      "Iteration 4238, Loss: 6733482874.701349\n",
      "Iteration 4239, Loss: 2942777118.8039746\n",
      "Iteration 4240, Loss: 2219677973.015755\n",
      "Iteration 4241, Loss: 1451633185.9815614\n",
      "Iteration 4242, Loss: 2420202972.447166\n",
      "Iteration 4243, Loss: 2226985878.364705\n",
      "Iteration 4244, Loss: 2023013172.7962768\n",
      "Iteration 4245, Loss: 1933434052.485639\n",
      "Iteration 4246, Loss: 1722623666.3951867\n",
      "Iteration 4247, Loss: 1703868085.0026023\n",
      "Iteration 4248, Loss: 1077689050.1548963\n",
      "Iteration 4249, Loss: 1080848621.2102711\n",
      "Iteration 4250, Loss: 1341121726.7465367\n",
      "Iteration 4251, Loss: 1129520329.6490774\n",
      "Iteration 4252, Loss: 1152555637.4548316\n",
      "Iteration 4253, Loss: 1195961687.2068007\n",
      "Iteration 4254, Loss: 1164853511.1835916\n",
      "Iteration 4255, Loss: 1082741357.4858603\n",
      "Iteration 4256, Loss: 1339697827.4745073\n",
      "Iteration 4257, Loss: 1280452638.5234146\n",
      "Iteration 4258, Loss: 1158702603.6027489\n",
      "Iteration 4259, Loss: 1082220974.4155524\n",
      "Iteration 4260, Loss: 1193814071.825871\n",
      "Iteration 4261, Loss: 1191750696.8147402\n",
      "Iteration 4262, Loss: 1303015889.0817156\n",
      "Iteration 4263, Loss: 1356898241.7568634\n",
      "Iteration 4264, Loss: 1369799809.0398507\n",
      "Iteration 4265, Loss: 1416513852.277981\n",
      "Iteration 4266, Loss: 1453706328.5487428\n",
      "Iteration 4267, Loss: 1120415758.4971426\n",
      "Iteration 4268, Loss: 1261907877.8106887\n",
      "Iteration 4269, Loss: 1248605996.7455475\n",
      "Iteration 4270, Loss: 1279440379.2121332\n",
      "Iteration 4271, Loss: 1154273675.4918377\n",
      "Iteration 4272, Loss: 1207307100.7308943\n",
      "Iteration 4273, Loss: 1198584155.5984597\n",
      "Iteration 4274, Loss: 1379124767.591786\n",
      "Iteration 4275, Loss: 1421944009.6403801\n",
      "Iteration 4276, Loss: 1243547921.024497\n",
      "Iteration 4277, Loss: 1148978613.5526733\n",
      "Iteration 4278, Loss: 1261186842.9014635\n",
      "Iteration 4279, Loss: 1141037798.332059\n",
      "Iteration 4280, Loss: 1253892646.3353667\n",
      "Iteration 4281, Loss: 1252966915.5693054\n",
      "Iteration 4282, Loss: 1252098984.8686743\n",
      "Iteration 4283, Loss: 1287448598.2485905\n",
      "Iteration 4284, Loss: 1269497615.4375746\n",
      "Iteration 4285, Loss: 1255125912.1999042\n",
      "Iteration 4286, Loss: 1094830342.7513573\n",
      "Iteration 4287, Loss: 2237251021.00173\n",
      "Iteration 4288, Loss: 1596681740.3912058\n",
      "Iteration 4289, Loss: 1099448314.4564579\n",
      "Iteration 4290, Loss: 1103109796.2179787\n",
      "Iteration 4291, Loss: 1659276962.4059877\n",
      "Iteration 4292, Loss: 1597194044.4136827\n",
      "Iteration 4293, Loss: 1100785909.620662\n",
      "Iteration 4294, Loss: 1136063999.1520422\n",
      "Iteration 4295, Loss: 1228797704.08891\n",
      "Iteration 4296, Loss: 1351418497.0994778\n",
      "Iteration 4297, Loss: 1372935047.815051\n",
      "Iteration 4298, Loss: 1410182133.6177766\n",
      "Iteration 4299, Loss: 1270481562.4950998\n",
      "Iteration 4300, Loss: 1188441840.12648\n",
      "Iteration 4301, Loss: 1179009232.2850666\n",
      "Iteration 4302, Loss: 1480640242.3835747\n",
      "Iteration 4303, Loss: 1448106393.4151027\n",
      "Iteration 4304, Loss: 1257597567.5675652\n",
      "Iteration 4305, Loss: 1208782365.5446293\n",
      "Iteration 4306, Loss: 1169834046.2097793\n",
      "Iteration 4307, Loss: 1295725335.5577939\n",
      "Iteration 4308, Loss: 1260073609.7752154\n",
      "Iteration 4309, Loss: 1320324569.9715106\n",
      "Iteration 4310, Loss: 1108343382.2435663\n",
      "Iteration 4311, Loss: 1163504869.0248215\n",
      "Iteration 4312, Loss: 1278960102.3674228\n",
      "Iteration 4313, Loss: 1268231932.8144274\n",
      "Iteration 4314, Loss: 1255048315.6684878\n",
      "Iteration 4315, Loss: 1161767988.0414815\n",
      "Iteration 4316, Loss: 1165615079.2249272\n",
      "Iteration 4317, Loss: 1169065357.2075226\n",
      "Iteration 4318, Loss: 1345903722.8598418\n",
      "Iteration 4319, Loss: 1322899707.1267436\n",
      "Iteration 4320, Loss: 1356337666.7970302\n",
      "Iteration 4321, Loss: 1293031792.4237216\n",
      "Iteration 4322, Loss: 1087527404.0691948\n",
      "Iteration 4323, Loss: 1338276044.0911484\n",
      "Iteration 4324, Loss: 1089162109.1636543\n",
      "Iteration 4325, Loss: 1298029826.2215662\n",
      "Iteration 4326, Loss: 1283054652.8190753\n",
      "Iteration 4327, Loss: 1275884033.3434515\n",
      "Iteration 4328, Loss: 1271934416.764647\n",
      "Iteration 4329, Loss: 1147722395.8644094\n",
      "Iteration 4330, Loss: 1376324733.9657056\n",
      "Iteration 4331, Loss: 1260834310.9435294\n",
      "Iteration 4332, Loss: 1102299361.2579813\n",
      "Iteration 4333, Loss: 1233886979.59078\n",
      "Iteration 4334, Loss: 1173654608.4961827\n",
      "Iteration 4335, Loss: 1288044575.9532747\n",
      "Iteration 4336, Loss: 1270724291.1298873\n",
      "Iteration 4337, Loss: 1092668649.374085\n",
      "Iteration 4338, Loss: 1822150691.690317\n",
      "Iteration 4339, Loss: 1103387120.2583354\n",
      "Iteration 4340, Loss: 1224304779.6456861\n",
      "Iteration 4341, Loss: 1200199408.2703032\n",
      "Iteration 4342, Loss: 1203406601.0808742\n",
      "Iteration 4343, Loss: 1254367202.2993884\n",
      "Iteration 4344, Loss: 1565643313.466919\n",
      "Iteration 4345, Loss: 1512055376.5483098\n",
      "Iteration 4346, Loss: 1147442831.1118948\n",
      "Iteration 4347, Loss: 1142802970.9109375\n",
      "Iteration 4348, Loss: 1111956847.0954406\n",
      "Iteration 4349, Loss: 1089686444.3273208\n",
      "Iteration 4350, Loss: 1080791339.6443973\n",
      "Iteration 4351, Loss: 1073526751.493926\n",
      "Iteration 4352, Loss: 1326162840.5775669\n",
      "Iteration 4353, Loss: 1303696630.251406\n",
      "Iteration 4354, Loss: 1341658568.45983\n",
      "Iteration 4355, Loss: 1154478229.283846\n",
      "Iteration 4356, Loss: 1155480417.2465577\n",
      "Iteration 4357, Loss: 1881984569.3398306\n",
      "Iteration 4358, Loss: 1815594850.442839\n",
      "Iteration 4359, Loss: 1070239571.4252912\n",
      "Iteration 4360, Loss: 1071196578.0254823\n",
      "Iteration 4361, Loss: 2741221533.4157376\n",
      "Iteration 4362, Loss: 1692841669.8741117\n",
      "Iteration 4363, Loss: 1143956702.9408095\n",
      "Iteration 4364, Loss: 1132124685.5692356\n",
      "Iteration 4365, Loss: 1129461039.847541\n",
      "Iteration 4366, Loss: 1180099045.1369963\n",
      "Iteration 4367, Loss: 1198174880.4244323\n",
      "Iteration 4368, Loss: 1195049761.6063201\n",
      "Iteration 4369, Loss: 1193303046.844305\n",
      "Iteration 4370, Loss: 1224340398.2091556\n",
      "Iteration 4371, Loss: 1219302733.3418922\n",
      "Iteration 4372, Loss: 1265933761.0412188\n",
      "Iteration 4373, Loss: 1254181413.318123\n",
      "Iteration 4374, Loss: 1284756408.345002\n",
      "Iteration 4375, Loss: 1362806586.5869412\n",
      "Iteration 4376, Loss: 1296639340.2905111\n",
      "Iteration 4377, Loss: 1149773650.7106438\n",
      "Iteration 4378, Loss: 1153803028.935572\n",
      "Iteration 4379, Loss: 1362636815.4868994\n",
      "Iteration 4380, Loss: 1334533937.0581799\n",
      "Iteration 4381, Loss: 1092869370.243808\n",
      "Iteration 4382, Loss: 2353249050.346489\n",
      "Iteration 4383, Loss: 2015901738.1266599\n",
      "Iteration 4384, Loss: 1106082739.1760902\n",
      "Iteration 4385, Loss: 2146912456.768778\n",
      "Iteration 4386, Loss: 1112076129.5001462\n",
      "Iteration 4387, Loss: 1132833101.7358868\n",
      "Iteration 4388, Loss: 1321450813.7608783\n",
      "Iteration 4389, Loss: 1140416137.7929108\n",
      "Iteration 4390, Loss: 1318118284.2127724\n",
      "Iteration 4391, Loss: 1302429608.2629447\n",
      "Iteration 4392, Loss: 1093726179.9452245\n",
      "Iteration 4393, Loss: 1101970974.3580418\n",
      "Iteration 4394, Loss: 1153955299.0421884\n",
      "Iteration 4395, Loss: 1148007230.9386518\n",
      "Iteration 4396, Loss: 1263690286.2703972\n",
      "Iteration 4397, Loss: 1300809060.1384158\n",
      "Iteration 4398, Loss: 1299162562.9406755\n",
      "Iteration 4399, Loss: 1285010879.6205034\n",
      "Iteration 4400, Loss: 1274972883.9269917\n",
      "Iteration 4401, Loss: 1177097585.3828857\n",
      "Iteration 4402, Loss: 1821080283.9083273\n",
      "Iteration 4403, Loss: 1118309224.4556453\n",
      "Iteration 4404, Loss: 1127227630.4473586\n",
      "Iteration 4405, Loss: 1129663687.163647\n",
      "Iteration 4406, Loss: 1858766148.3613644\n",
      "Iteration 4407, Loss: 1793038999.6962829\n",
      "Iteration 4408, Loss: 1548158911.96621\n",
      "Iteration 4409, Loss: 1265705200.860456\n",
      "Iteration 4410, Loss: 1098638650.5825834\n",
      "Iteration 4411, Loss: 3682411728.6951365\n",
      "Iteration 4412, Loss: 1114124241.5965483\n",
      "Iteration 4413, Loss: 2822798606.648081\n",
      "Iteration 4414, Loss: 2132302765.3702247\n",
      "Iteration 4415, Loss: 1859857429.9487417\n",
      "Iteration 4416, Loss: 1531185864.7476428\n",
      "Iteration 4417, Loss: 1081448627.9683397\n",
      "Iteration 4418, Loss: 1081821831.3306262\n",
      "Iteration 4419, Loss: 1095571189.6981094\n",
      "Iteration 4420, Loss: 1566714221.024913\n",
      "Iteration 4421, Loss: 1206280362.812231\n",
      "Iteration 4422, Loss: 1086380567.7724009\n",
      "Iteration 4423, Loss: 1077103467.4590595\n",
      "Iteration 4424, Loss: 1079165275.8438692\n",
      "Iteration 4425, Loss: 1087470226.964216\n",
      "Iteration 4426, Loss: 1615209185.1588376\n",
      "Iteration 4427, Loss: 1097485410.4769459\n",
      "Iteration 4428, Loss: 1207294974.6894271\n",
      "Iteration 4429, Loss: 1204715256.2192874\n",
      "Iteration 4430, Loss: 1710399075.486151\n",
      "Iteration 4431, Loss: 1232828357.4517295\n",
      "Iteration 4432, Loss: 1223583982.1191533\n",
      "Iteration 4433, Loss: 1219799541.8795812\n",
      "Iteration 4434, Loss: 1089004741.836033\n",
      "Iteration 4435, Loss: 1101025740.5144022\n",
      "Iteration 4436, Loss: 1100654561.4242861\n",
      "Iteration 4437, Loss: 1340547893.7143075\n",
      "Iteration 4438, Loss: 1402167751.8865976\n",
      "Iteration 4439, Loss: 1364664563.5841699\n",
      "Iteration 4440, Loss: 1337908459.582551\n",
      "Iteration 4441, Loss: 1112974538.004285\n",
      "Iteration 4442, Loss: 1117798040.318323\n",
      "Iteration 4443, Loss: 1109002228.6500216\n",
      "Iteration 4444, Loss: 1106118286.3445668\n",
      "Iteration 4445, Loss: 1105122563.7266688\n",
      "Iteration 4446, Loss: 1328056899.7721677\n",
      "Iteration 4447, Loss: 1156343104.8231087\n",
      "Iteration 4448, Loss: 1907728694.311192\n",
      "Iteration 4449, Loss: 1915424882.1274474\n",
      "Iteration 4450, Loss: 1118273724.201066\n",
      "Iteration 4451, Loss: 1300039237.460048\n",
      "Iteration 4452, Loss: 1212825358.9320905\n",
      "Iteration 4453, Loss: 1346649814.4142041\n",
      "Iteration 4454, Loss: 1158404377.8632429\n",
      "Iteration 4455, Loss: 1082491038.5131896\n",
      "Iteration 4456, Loss: 2542958129.7088933\n",
      "Iteration 4457, Loss: 1788221007.3416202\n",
      "Iteration 4458, Loss: 1378743244.67092\n",
      "Iteration 4459, Loss: 1354338816.3254826\n",
      "Iteration 4460, Loss: 1097615324.2438931\n",
      "Iteration 4461, Loss: 1088598580.3407402\n",
      "Iteration 4462, Loss: 1389638554.2404606\n",
      "Iteration 4463, Loss: 1118000806.5476565\n",
      "Iteration 4464, Loss: 1264518906.8324833\n",
      "Iteration 4465, Loss: 1355571699.0018451\n",
      "Iteration 4466, Loss: 1410691668.9404778\n",
      "Iteration 4467, Loss: 1283753679.0563993\n",
      "Iteration 4468, Loss: 1149308913.828364\n",
      "Iteration 4469, Loss: 1149550836.1302528\n",
      "Iteration 4470, Loss: 1083692549.7490315\n",
      "Iteration 4471, Loss: 1113734090.877224\n",
      "Iteration 4472, Loss: 1081364288.395315\n",
      "Iteration 4473, Loss: 1235992132.6488256\n",
      "Iteration 4474, Loss: 1230105854.3402495\n",
      "Iteration 4475, Loss: 1231412075.2384784\n",
      "Iteration 4476, Loss: 1294708263.4399238\n",
      "Iteration 4477, Loss: 1292603594.263461\n",
      "Iteration 4478, Loss: 1259538405.4661753\n",
      "Iteration 4479, Loss: 1221856824.099569\n",
      "Iteration 4480, Loss: 1171263784.0757957\n",
      "Iteration 4481, Loss: 1163143575.0725837\n",
      "Iteration 4482, Loss: 1144035588.9088085\n",
      "Iteration 4483, Loss: 1191397216.1434927\n",
      "Iteration 4484, Loss: 1176273623.9759846\n",
      "Iteration 4485, Loss: 1499959391.9121974\n",
      "Iteration 4486, Loss: 1376439105.7229252\n",
      "Iteration 4487, Loss: 1102340668.1798642\n",
      "Iteration 4488, Loss: 1111277912.5454967\n",
      "Iteration 4489, Loss: 1120041804.2795503\n",
      "Iteration 4490, Loss: 1125893548.743568\n",
      "Iteration 4491, Loss: 1247712589.9768736\n",
      "Iteration 4492, Loss: 1106596820.874144\n",
      "Iteration 4493, Loss: 1104222890.4564323\n",
      "Iteration 4494, Loss: 1466886638.6012373\n",
      "Iteration 4495, Loss: 1420835903.7014935\n",
      "Iteration 4496, Loss: 1387449312.029727\n",
      "Iteration 4497, Loss: 1118632753.2346935\n",
      "Iteration 4498, Loss: 1281863362.3001473\n",
      "Iteration 4499, Loss: 1087199191.916556\n",
      "Iteration 4500, Loss: 1095108487.7533813\n",
      "Iteration 4501, Loss: 1101434457.770122\n",
      "Iteration 4502, Loss: 2199581928.334582\n",
      "Iteration 4503, Loss: 2003253266.8772638\n",
      "Iteration 4504, Loss: 1951403046.839608\n",
      "Iteration 4505, Loss: 1391188478.6430776\n",
      "Iteration 4506, Loss: 1384876213.863606\n",
      "Iteration 4507, Loss: 1263498420.4151516\n",
      "Iteration 4508, Loss: 1339983645.9589138\n",
      "Iteration 4509, Loss: 1078759514.6854703\n",
      "Iteration 4510, Loss: 1406677511.7428799\n",
      "Iteration 4511, Loss: 1077445396.9885879\n",
      "Iteration 4512, Loss: 1110983427.0783591\n",
      "Iteration 4513, Loss: 1262046410.3597867\n",
      "Iteration 4514, Loss: 1336255950.8746526\n",
      "Iteration 4515, Loss: 1180946551.5975804\n",
      "Iteration 4516, Loss: 1244599040.7466035\n",
      "Iteration 4517, Loss: 1162636861.767629\n",
      "Iteration 4518, Loss: 1208937707.4559057\n",
      "Iteration 4519, Loss: 1311860954.9773784\n",
      "Iteration 4520, Loss: 1366042167.4667313\n",
      "Iteration 4521, Loss: 1378748686.4214656\n",
      "Iteration 4522, Loss: 1351167441.4085488\n",
      "Iteration 4523, Loss: 1177178930.8256967\n",
      "Iteration 4524, Loss: 1342345495.1346724\n",
      "Iteration 4525, Loss: 1236574306.306861\n",
      "Iteration 4526, Loss: 1228120217.9814038\n",
      "Iteration 4527, Loss: 1196652324.5610943\n",
      "Iteration 4528, Loss: 1399174842.3938017\n",
      "Iteration 4529, Loss: 1139256365.5477417\n",
      "Iteration 4530, Loss: 1259828145.9925876\n",
      "Iteration 4531, Loss: 1547833070.6438036\n",
      "Iteration 4532, Loss: 1564003925.7812219\n",
      "Iteration 4533, Loss: 1289795794.5526404\n",
      "Iteration 4534, Loss: 1114946195.7121696\n",
      "Iteration 4535, Loss: 1135677633.3009706\n",
      "Iteration 4536, Loss: 1129276525.2136943\n",
      "Iteration 4537, Loss: 1227797670.1332738\n",
      "Iteration 4538, Loss: 1186622814.8508098\n",
      "Iteration 4539, Loss: 1319439155.6330285\n",
      "Iteration 4540, Loss: 1412147197.7575035\n",
      "Iteration 4541, Loss: 1275121128.3745565\n",
      "Iteration 4542, Loss: 1310401990.4537768\n",
      "Iteration 4543, Loss: 1069041615.2194552\n",
      "Iteration 4544, Loss: 1346375363.7584732\n",
      "Iteration 4545, Loss: 1212429784.2532454\n",
      "Iteration 4546, Loss: 1166138814.6131444\n",
      "Iteration 4547, Loss: 1117077646.4759207\n",
      "Iteration 4548, Loss: 1335601846.1982398\n",
      "Iteration 4549, Loss: 1154663689.4825926\n",
      "Iteration 4550, Loss: 1269641877.781059\n",
      "Iteration 4551, Loss: 1226310782.6107385\n",
      "Iteration 4552, Loss: 1320881383.8687062\n",
      "Iteration 4553, Loss: 1239395445.82681\n",
      "Iteration 4554, Loss: 1320471208.6387527\n",
      "Iteration 4555, Loss: 1297825726.684114\n",
      "Iteration 4556, Loss: 1342972400.0765927\n",
      "Iteration 4557, Loss: 1319790034.7102156\n",
      "Iteration 4558, Loss: 1338156777.190206\n",
      "Iteration 4559, Loss: 1141383795.1808026\n",
      "Iteration 4560, Loss: 1146224166.6692417\n",
      "Iteration 4561, Loss: 1931739522.0095139\n",
      "Iteration 4562, Loss: 1098667787.777055\n",
      "Iteration 4563, Loss: 1102866606.9197788\n",
      "Iteration 4564, Loss: 1099670908.836437\n",
      "Iteration 4565, Loss: 2179044753.602128\n",
      "Iteration 4566, Loss: 1084207020.5033867\n",
      "Iteration 4567, Loss: 1087165699.968311\n",
      "Iteration 4568, Loss: 1475006778.5170205\n",
      "Iteration 4569, Loss: 1118451805.5304055\n",
      "Iteration 4570, Loss: 1111651932.5944831\n",
      "Iteration 4571, Loss: 2817308271.440411\n",
      "Iteration 4572, Loss: 2529701031.893463\n",
      "Iteration 4573, Loss: 1785809105.3925683\n",
      "Iteration 4574, Loss: 1753481760.5638468\n",
      "Iteration 4575, Loss: 1143391879.2583807\n",
      "Iteration 4576, Loss: 1150386798.476916\n",
      "Iteration 4577, Loss: 1140111424.2039096\n",
      "Iteration 4578, Loss: 1308528619.3786528\n",
      "Iteration 4579, Loss: 1205962401.3379095\n",
      "Iteration 4580, Loss: 1209716881.1581273\n",
      "Iteration 4581, Loss: 1153062857.3389857\n",
      "Iteration 4582, Loss: 1159383760.1838422\n",
      "Iteration 4583, Loss: 1162813016.217426\n",
      "Iteration 4584, Loss: 1190667950.3151817\n",
      "Iteration 4585, Loss: 1430929036.917011\n",
      "Iteration 4586, Loss: 1094116746.7488663\n",
      "Iteration 4587, Loss: 1100556798.2491262\n",
      "Iteration 4588, Loss: 1512740386.528844\n",
      "Iteration 4589, Loss: 1497569445.5098104\n",
      "Iteration 4590, Loss: 1199685109.5811615\n",
      "Iteration 4591, Loss: 1160671306.7625926\n",
      "Iteration 4592, Loss: 1090324055.753315\n",
      "Iteration 4593, Loss: 1897820688.1715312\n",
      "Iteration 4594, Loss: 1768119852.6055574\n",
      "Iteration 4595, Loss: 1735820375.1874166\n",
      "Iteration 4596, Loss: 1483026649.3932886\n",
      "Iteration 4597, Loss: 1116943540.6154816\n",
      "Iteration 4598, Loss: 1080549839.6565993\n",
      "Iteration 4599, Loss: 1082344603.61246\n",
      "Iteration 4600, Loss: 1189351706.6131759\n",
      "Iteration 4601, Loss: 1191292038.229427\n",
      "Iteration 4602, Loss: 1231439608.070456\n",
      "Iteration 4603, Loss: 1245763025.0853753\n",
      "Iteration 4604, Loss: 1289027935.6188984\n",
      "Iteration 4605, Loss: 1369548914.7775795\n",
      "Iteration 4606, Loss: 1092158845.4716918\n",
      "Iteration 4607, Loss: 1090801492.097722\n",
      "Iteration 4608, Loss: 1465679335.548063\n",
      "Iteration 4609, Loss: 1376204725.441144\n",
      "Iteration 4610, Loss: 1426293294.3422265\n",
      "Iteration 4611, Loss: 1368758004.0408945\n",
      "Iteration 4612, Loss: 1147937083.368417\n",
      "Iteration 4613, Loss: 1173770968.313794\n",
      "Iteration 4614, Loss: 1172278465.558418\n",
      "Iteration 4615, Loss: 1303009734.00411\n",
      "Iteration 4616, Loss: 1288049877.4933536\n",
      "Iteration 4617, Loss: 1197052896.5173032\n",
      "Iteration 4618, Loss: 1189539733.0897746\n",
      "Iteration 4619, Loss: 1195478069.6565466\n",
      "Iteration 4620, Loss: 1305022043.0867157\n",
      "Iteration 4621, Loss: 1344368882.5656354\n",
      "Iteration 4622, Loss: 1394890121.4047832\n",
      "Iteration 4623, Loss: 1369110306.8709493\n",
      "Iteration 4624, Loss: 1227831621.2056012\n",
      "Iteration 4625, Loss: 1075650668.6213825\n",
      "Iteration 4626, Loss: 1472130314.8758314\n",
      "Iteration 4627, Loss: 1143020870.7012987\n",
      "Iteration 4628, Loss: 1111204348.2706034\n",
      "Iteration 4629, Loss: 1073403623.9095855\n",
      "Iteration 4630, Loss: 1084902611.8994033\n",
      "Iteration 4631, Loss: 1468652695.582088\n",
      "Iteration 4632, Loss: 1299293275.7861993\n",
      "Iteration 4633, Loss: 1324092888.1334789\n",
      "Iteration 4634, Loss: 1244587826.693357\n",
      "Iteration 4635, Loss: 1281820636.9219112\n",
      "Iteration 4636, Loss: 1264653412.199516\n",
      "Iteration 4637, Loss: 1225743899.8831818\n",
      "Iteration 4638, Loss: 1320516275.3621416\n",
      "Iteration 4639, Loss: 1357007359.0286684\n",
      "Iteration 4640, Loss: 1371871114.6971028\n",
      "Iteration 4641, Loss: 1315876466.1858158\n",
      "Iteration 4642, Loss: 1156513594.7060108\n",
      "Iteration 4643, Loss: 1621177304.1615808\n",
      "Iteration 4644, Loss: 1489337043.6949244\n",
      "Iteration 4645, Loss: 1165324559.385479\n",
      "Iteration 4646, Loss: 1214458431.8181164\n",
      "Iteration 4647, Loss: 1217251858.1445634\n",
      "Iteration 4648, Loss: 1077323245.1079617\n",
      "Iteration 4649, Loss: 1261098018.2129984\n",
      "Iteration 4650, Loss: 1297845623.3341656\n",
      "Iteration 4651, Loss: 1186055876.744059\n",
      "Iteration 4652, Loss: 1176883825.4351346\n",
      "Iteration 4653, Loss: 1817235061.8757408\n",
      "Iteration 4654, Loss: 1512303647.0422845\n",
      "Iteration 4655, Loss: 1244564488.3315384\n",
      "Iteration 4656, Loss: 1286675807.0543425\n",
      "Iteration 4657, Loss: 1173227720.3836539\n",
      "Iteration 4658, Loss: 1839160524.4320107\n",
      "Iteration 4659, Loss: 1091077088.1741421\n",
      "Iteration 4660, Loss: 1287358737.5325122\n",
      "Iteration 4661, Loss: 1244375780.632201\n",
      "Iteration 4662, Loss: 1338749817.222329\n",
      "Iteration 4663, Loss: 1095567006.9774234\n",
      "Iteration 4664, Loss: 1107141114.314828\n",
      "Iteration 4665, Loss: 1228646488.0787487\n",
      "Iteration 4666, Loss: 1322446442.9998796\n",
      "Iteration 4667, Loss: 1303057206.7391605\n",
      "Iteration 4668, Loss: 1255101796.5304532\n",
      "Iteration 4669, Loss: 1291380022.3023093\n",
      "Iteration 4670, Loss: 1080624548.0501137\n",
      "Iteration 4671, Loss: 1279005074.1581938\n",
      "Iteration 4672, Loss: 1307082169.8478491\n",
      "Iteration 4673, Loss: 1257056439.432822\n",
      "Iteration 4674, Loss: 1248046486.3285415\n",
      "Iteration 4675, Loss: 1184748842.788004\n",
      "Iteration 4676, Loss: 1295668454.5967953\n",
      "Iteration 4677, Loss: 1280464415.973391\n",
      "Iteration 4678, Loss: 1356148495.6682768\n",
      "Iteration 4679, Loss: 1336296976.3473432\n",
      "Iteration 4680, Loss: 1315697461.049018\n",
      "Iteration 4681, Loss: 1338282543.2328897\n",
      "Iteration 4682, Loss: 1317431154.263063\n",
      "Iteration 4683, Loss: 1152028652.1864417\n",
      "Iteration 4684, Loss: 1214403345.826248\n",
      "Iteration 4685, Loss: 1351257696.523161\n",
      "Iteration 4686, Loss: 1148233239.9894807\n",
      "Iteration 4687, Loss: 1152568574.595939\n",
      "Iteration 4688, Loss: 1193552974.2247274\n",
      "Iteration 4689, Loss: 1193438041.362802\n",
      "Iteration 4690, Loss: 1326177712.6692846\n",
      "Iteration 4691, Loss: 1242814078.5813327\n",
      "Iteration 4692, Loss: 1233326080.9089165\n",
      "Iteration 4693, Loss: 1226746945.3889678\n",
      "Iteration 4694, Loss: 1168998683.8827322\n",
      "Iteration 4695, Loss: 1349102481.6348715\n",
      "Iteration 4696, Loss: 1145070179.6704268\n",
      "Iteration 4697, Loss: 1221110174.2556093\n",
      "Iteration 4698, Loss: 1211871744.1151316\n",
      "Iteration 4699, Loss: 1203429607.1658027\n",
      "Iteration 4700, Loss: 1195709413.4448175\n",
      "Iteration 4701, Loss: 1303568687.2291389\n",
      "Iteration 4702, Loss: 1218094082.8651824\n",
      "Iteration 4703, Loss: 1317134425.1557186\n",
      "Iteration 4704, Loss: 1207189318.711676\n",
      "Iteration 4705, Loss: 1178597382.6796396\n",
      "Iteration 4706, Loss: 1152906901.745988\n",
      "Iteration 4707, Loss: 1230930373.0482328\n",
      "Iteration 4708, Loss: 1244833971.3999522\n",
      "Iteration 4709, Loss: 1306211089.6973615\n",
      "Iteration 4710, Loss: 1361452012.642953\n",
      "Iteration 4711, Loss: 1079662408.8790143\n",
      "Iteration 4712, Loss: 1193897925.6253533\n",
      "Iteration 4713, Loss: 1293759915.16596\n",
      "Iteration 4714, Loss: 1212811828.0444033\n",
      "Iteration 4715, Loss: 1085585553.5069716\n",
      "Iteration 4716, Loss: 1089096778.6542811\n",
      "Iteration 4717, Loss: 1095121201.887623\n",
      "Iteration 4718, Loss: 1099970886.3489094\n",
      "Iteration 4719, Loss: 1155784063.664848\n",
      "Iteration 4720, Loss: 1903173052.1376073\n",
      "Iteration 4721, Loss: 1900387215.0765126\n",
      "Iteration 4722, Loss: 8700426811.514893\n",
      "Iteration 4723, Loss: 6405390881.847824\n",
      "Iteration 4724, Loss: 1084455989.405434\n",
      "Iteration 4725, Loss: 3761034443.001376\n",
      "Iteration 4726, Loss: 1574035217.705823\n",
      "Iteration 4727, Loss: 1118797298.6258101\n",
      "Iteration 4728, Loss: 1176264256.7113013\n",
      "Iteration 4729, Loss: 1170112021.2401745\n",
      "Iteration 4730, Loss: 1153936348.7074625\n",
      "Iteration 4731, Loss: 1703059734.8823671\n",
      "Iteration 4732, Loss: 1362915142.0121844\n",
      "Iteration 4733, Loss: 1361626444.2697406\n",
      "Iteration 4734, Loss: 1346229771.3540587\n",
      "Iteration 4735, Loss: 1126109944.1390133\n",
      "Iteration 4736, Loss: 1185761098.887321\n",
      "Iteration 4737, Loss: 1304713212.2056198\n",
      "Iteration 4738, Loss: 1372782790.9307075\n",
      "Iteration 4739, Loss: 1432072015.404239\n",
      "Iteration 4740, Loss: 1357881366.848194\n",
      "Iteration 4741, Loss: 1139816133.149528\n",
      "Iteration 4742, Loss: 1197287114.5619347\n",
      "Iteration 4743, Loss: 1202752969.998839\n",
      "Iteration 4744, Loss: 1201686714.3683958\n",
      "Iteration 4745, Loss: 1235843835.8930633\n",
      "Iteration 4746, Loss: 1085934284.2076313\n",
      "Iteration 4747, Loss: 1087319318.748228\n",
      "Iteration 4748, Loss: 1420000988.2462444\n",
      "Iteration 4749, Loss: 1348058564.1838448\n",
      "Iteration 4750, Loss: 1228196911.6960402\n",
      "Iteration 4751, Loss: 1141867858.2274323\n",
      "Iteration 4752, Loss: 1140607794.097333\n",
      "Iteration 4753, Loss: 1268476770.842828\n",
      "Iteration 4754, Loss: 1359685891.579143\n",
      "Iteration 4755, Loss: 1343094578.8624635\n",
      "Iteration 4756, Loss: 1250206659.4296956\n",
      "Iteration 4757, Loss: 1244171989.4640853\n",
      "Iteration 4758, Loss: 1344167762.1206033\n",
      "Iteration 4759, Loss: 1121612448.2975116\n",
      "Iteration 4760, Loss: 1272415559.350926\n",
      "Iteration 4761, Loss: 1186914546.0654473\n",
      "Iteration 4762, Loss: 1478447070.3110778\n",
      "Iteration 4763, Loss: 1475726628.5197294\n",
      "Iteration 4764, Loss: 1388705037.5667183\n",
      "Iteration 4765, Loss: 1182362760.9461567\n",
      "Iteration 4766, Loss: 1188337287.6576114\n",
      "Iteration 4767, Loss: 1219772700.983183\n",
      "Iteration 4768, Loss: 1213765663.005258\n",
      "Iteration 4769, Loss: 1204949993.915628\n",
      "Iteration 4770, Loss: 1176182039.181278\n",
      "Iteration 4771, Loss: 1224522749.8728075\n",
      "Iteration 4772, Loss: 1366221755.249536\n",
      "Iteration 4773, Loss: 1418540525.3184247\n",
      "Iteration 4774, Loss: 1413155213.384337\n",
      "Iteration 4775, Loss: 1105120392.2042828\n",
      "Iteration 4776, Loss: 1345796220.7211223\n",
      "Iteration 4777, Loss: 1103618903.962691\n",
      "Iteration 4778, Loss: 1335163376.3286333\n",
      "Iteration 4779, Loss: 1319561845.6421952\n",
      "Iteration 4780, Loss: 1396370076.652432\n",
      "Iteration 4781, Loss: 1371779274.9240909\n",
      "Iteration 4782, Loss: 1101310334.7309487\n",
      "Iteration 4783, Loss: 1357066133.510223\n",
      "Iteration 4784, Loss: 1088855361.4048262\n",
      "Iteration 4785, Loss: 1089530939.4147952\n",
      "Iteration 4786, Loss: 1103019485.2790036\n",
      "Iteration 4787, Loss: 1115207429.2746284\n",
      "Iteration 4788, Loss: 1122877287.9021504\n",
      "Iteration 4789, Loss: 1235888496.663476\n",
      "Iteration 4790, Loss: 1300952794.0229084\n",
      "Iteration 4791, Loss: 1257242622.8187392\n",
      "Iteration 4792, Loss: 1224653154.5221794\n",
      "Iteration 4793, Loss: 1222940137.2674239\n",
      "Iteration 4794, Loss: 1175521924.0772915\n",
      "Iteration 4795, Loss: 1312060307.6663496\n",
      "Iteration 4796, Loss: 1085925775.5960965\n",
      "Iteration 4797, Loss: 1251401620.0921268\n",
      "Iteration 4798, Loss: 1121620318.8121262\n",
      "Iteration 4799, Loss: 1162875042.481316\n",
      "Iteration 4800, Loss: 1219085776.7175112\n",
      "Iteration 4801, Loss: 1222479726.7190058\n",
      "Iteration 4802, Loss: 1214932797.1725714\n",
      "Iteration 4803, Loss: 1297777352.193083\n",
      "Iteration 4804, Loss: 1285045385.672141\n",
      "Iteration 4805, Loss: 1140052121.924657\n",
      "Iteration 4806, Loss: 1145566397.3730066\n",
      "Iteration 4807, Loss: 1302012295.0998592\n",
      "Iteration 4808, Loss: 1137674752.740175\n",
      "Iteration 4809, Loss: 1143144705.103312\n",
      "Iteration 4810, Loss: 1135411686.0133133\n",
      "Iteration 4811, Loss: 1118661860.793909\n",
      "Iteration 4812, Loss: 1113436695.8015532\n",
      "Iteration 4813, Loss: 1228053598.6534162\n",
      "Iteration 4814, Loss: 1647234873.8082054\n",
      "Iteration 4815, Loss: 1334982702.2562337\n",
      "Iteration 4816, Loss: 1187605443.705927\n",
      "Iteration 4817, Loss: 1085799909.1818905\n",
      "Iteration 4818, Loss: 1183696371.2490897\n",
      "Iteration 4819, Loss: 1158676588.5580776\n",
      "Iteration 4820, Loss: 1219686797.7112849\n",
      "Iteration 4821, Loss: 1079928766.2361927\n",
      "Iteration 4822, Loss: 1560693225.0554338\n",
      "Iteration 4823, Loss: 1126375085.6703744\n",
      "Iteration 4824, Loss: 1124850599.2695107\n",
      "Iteration 4825, Loss: 1131408691.513026\n",
      "Iteration 4826, Loss: 1116946412.004836\n",
      "Iteration 4827, Loss: 1123786608.9536772\n",
      "Iteration 4828, Loss: 1120158859.709606\n",
      "Iteration 4829, Loss: 1243561737.8001738\n",
      "Iteration 4830, Loss: 1339940148.071539\n",
      "Iteration 4831, Loss: 1183651600.2755032\n",
      "Iteration 4832, Loss: 1175343692.3523648\n",
      "Iteration 4833, Loss: 1128422892.8227315\n",
      "Iteration 4834, Loss: 1251631399.9370868\n",
      "Iteration 4835, Loss: 1194468215.465274\n",
      "Iteration 4836, Loss: 1133152245.308782\n",
      "Iteration 4837, Loss: 1130379197.7139971\n",
      "Iteration 4838, Loss: 1126319908.27228\n",
      "Iteration 4839, Loss: 1323859730.8139136\n",
      "Iteration 4840, Loss: 1362811928.9021254\n",
      "Iteration 4841, Loss: 1084119530.0471406\n",
      "Iteration 4842, Loss: 1098465519.0814662\n",
      "Iteration 4843, Loss: 1336168086.9896946\n",
      "Iteration 4844, Loss: 1281165922.7800887\n",
      "Iteration 4845, Loss: 1280668578.1832838\n",
      "Iteration 4846, Loss: 1280674026.9130878\n",
      "Iteration 4847, Loss: 1091134551.7410994\n",
      "Iteration 4848, Loss: 1104498752.499884\n",
      "Iteration 4849, Loss: 1216717806.2363317\n",
      "Iteration 4850, Loss: 1202586021.3282075\n",
      "Iteration 4851, Loss: 1353268987.256358\n",
      "Iteration 4852, Loss: 1370146470.562253\n",
      "Iteration 4853, Loss: 1253686551.206518\n",
      "Iteration 4854, Loss: 1111128262.6632004\n",
      "Iteration 4855, Loss: 1163534999.969813\n",
      "Iteration 4856, Loss: 1166055097.7261124\n",
      "Iteration 4857, Loss: 1304347956.5592945\n",
      "Iteration 4858, Loss: 1185572536.796709\n",
      "Iteration 4859, Loss: 1080037293.6041973\n",
      "Iteration 4860, Loss: 1080403381.3416715\n",
      "Iteration 4861, Loss: 1107272658.3224993\n",
      "Iteration 4862, Loss: 1213350064.8036635\n",
      "Iteration 4863, Loss: 1164287666.1323717\n",
      "Iteration 4864, Loss: 1868477745.7986753\n",
      "Iteration 4865, Loss: 1135132456.969176\n",
      "Iteration 4866, Loss: 1142562965.1359057\n",
      "Iteration 4867, Loss: 1116865336.0019474\n",
      "Iteration 4868, Loss: 1115566841.230554\n",
      "Iteration 4869, Loss: 1229819370.8828895\n",
      "Iteration 4870, Loss: 1180521110.9087534\n",
      "Iteration 4871, Loss: 1334999401.208971\n",
      "Iteration 4872, Loss: 1074938649.9318902\n",
      "Iteration 4873, Loss: 1074722903.9204855\n",
      "Iteration 4874, Loss: 1120192558.7273908\n",
      "Iteration 4875, Loss: 1117194057.2996638\n",
      "Iteration 4876, Loss: 1156914038.7267869\n",
      "Iteration 4877, Loss: 1198171646.1540406\n",
      "Iteration 4878, Loss: 1173982889.443091\n",
      "Iteration 4879, Loss: 1296831069.209028\n",
      "Iteration 4880, Loss: 1276692106.7706194\n",
      "Iteration 4881, Loss: 1146225873.7597704\n",
      "Iteration 4882, Loss: 1207457740.895065\n",
      "Iteration 4883, Loss: 1243597331.6340528\n",
      "Iteration 4884, Loss: 1275065366.676472\n",
      "Iteration 4885, Loss: 1168317088.035853\n",
      "Iteration 4886, Loss: 1336951244.2837534\n",
      "Iteration 4887, Loss: 1355054378.5372753\n",
      "Iteration 4888, Loss: 1356082352.7648232\n",
      "Iteration 4889, Loss: 1077300144.3225417\n",
      "Iteration 4890, Loss: 1186779340.6954572\n",
      "Iteration 4891, Loss: 1213980321.924742\n",
      "Iteration 4892, Loss: 1152007998.7951124\n",
      "Iteration 4893, Loss: 1191212130.7621868\n",
      "Iteration 4894, Loss: 1400558053.1067605\n",
      "Iteration 4895, Loss: 1369718275.1962876\n",
      "Iteration 4896, Loss: 1070366549.4115727\n",
      "Iteration 4897, Loss: 1093796603.9663045\n",
      "Iteration 4898, Loss: 1265991492.0140119\n",
      "Iteration 4899, Loss: 1222338852.0407321\n",
      "Iteration 4900, Loss: 1316193878.2610981\n",
      "Iteration 4901, Loss: 1347339779.5114672\n",
      "Iteration 4902, Loss: 1392319366.7189176\n",
      "Iteration 4903, Loss: 1064980855.576818\n",
      "Iteration 4904, Loss: 1065052217.3774959\n",
      "Iteration 4905, Loss: 1350397275.264455\n",
      "Iteration 4906, Loss: 1352770254.1649165\n",
      "Iteration 4907, Loss: 1166049227.586875\n",
      "Iteration 4908, Loss: 1072354979.9160691\n",
      "Iteration 4909, Loss: 1235800176.4617\n",
      "Iteration 4910, Loss: 1076298311.7081995\n",
      "Iteration 4911, Loss: 1644662310.6719608\n",
      "Iteration 4912, Loss: 1565610354.8194475\n",
      "Iteration 4913, Loss: 1523663282.2615395\n",
      "Iteration 4914, Loss: 1461226288.6136444\n",
      "Iteration 4915, Loss: 1466421834.1397357\n",
      "Iteration 4916, Loss: 1469362802.2338426\n",
      "Iteration 4917, Loss: 1085231980.4086287\n",
      "Iteration 4918, Loss: 2406334299.044228\n",
      "Iteration 4919, Loss: 1167532197.6369731\n",
      "Iteration 4920, Loss: 1158751422.979093\n",
      "Iteration 4921, Loss: 1184516893.2710023\n",
      "Iteration 4922, Loss: 1153018065.6068838\n",
      "Iteration 4923, Loss: 1250646663.6304564\n",
      "Iteration 4924, Loss: 1281447590.3565688\n",
      "Iteration 4925, Loss: 1204299145.7744553\n",
      "Iteration 4926, Loss: 1252733945.5053775\n",
      "Iteration 4927, Loss: 1237136652.1911216\n",
      "Iteration 4928, Loss: 1330256764.9804368\n",
      "Iteration 4929, Loss: 1369915585.8238342\n",
      "Iteration 4930, Loss: 1222975612.8173454\n",
      "Iteration 4931, Loss: 1216587023.5560002\n",
      "Iteration 4932, Loss: 1232852517.0875256\n",
      "Iteration 4933, Loss: 1153905941.025353\n",
      "Iteration 4934, Loss: 1157823833.127969\n",
      "Iteration 4935, Loss: 1309290851.9784613\n",
      "Iteration 4936, Loss: 1290805023.5093772\n",
      "Iteration 4937, Loss: 1275176005.5874875\n",
      "Iteration 4938, Loss: 1312046013.2451043\n",
      "Iteration 4939, Loss: 1373254241.5117104\n",
      "Iteration 4940, Loss: 1085581740.4028563\n",
      "Iteration 4941, Loss: 1089193514.2399178\n",
      "Iteration 4942, Loss: 1094960234.7730708\n",
      "Iteration 4943, Loss: 1097625691.920307\n",
      "Iteration 4944, Loss: 1103323645.2564983\n",
      "Iteration 4945, Loss: 1108044399.9316466\n",
      "Iteration 4946, Loss: 1104003838.0645018\n",
      "Iteration 4947, Loss: 1215415804.6752055\n",
      "Iteration 4948, Loss: 1312419171.4848826\n",
      "Iteration 4949, Loss: 1257522839.2390501\n",
      "Iteration 4950, Loss: 1177807945.0635717\n",
      "Iteration 4951, Loss: 1148137532.5450885\n",
      "Iteration 4952, Loss: 1259417164.599487\n",
      "Iteration 4953, Loss: 1246313704.3741233\n",
      "Iteration 4954, Loss: 1230655672.7394319\n",
      "Iteration 4955, Loss: 1290207212.6077738\n",
      "Iteration 4956, Loss: 1364256359.3986204\n",
      "Iteration 4957, Loss: 1219778429.8224719\n",
      "Iteration 4958, Loss: 1163856822.5566454\n",
      "Iteration 4959, Loss: 1146897025.1761847\n",
      "Iteration 4960, Loss: 1234399984.9287984\n",
      "Iteration 4961, Loss: 1276897887.9616733\n",
      "Iteration 4962, Loss: 1355171226.7257082\n",
      "Iteration 4963, Loss: 1239046502.9788444\n",
      "Iteration 4964, Loss: 1588931132.2651103\n",
      "Iteration 4965, Loss: 1460578438.2590446\n",
      "Iteration 4966, Loss: 1353289395.1855438\n",
      "Iteration 4967, Loss: 1405875158.4149275\n",
      "Iteration 4968, Loss: 1359027318.0813098\n",
      "Iteration 4969, Loss: 1086055195.6248364\n",
      "Iteration 4970, Loss: 1223784750.8203268\n",
      "Iteration 4971, Loss: 1167045458.131087\n",
      "Iteration 4972, Loss: 1119312674.25683\n",
      "Iteration 4973, Loss: 1220492373.7666419\n",
      "Iteration 4974, Loss: 1648194302.1582782\n",
      "Iteration 4975, Loss: 1217131898.4562807\n",
      "Iteration 4976, Loss: 1252971890.0461092\n",
      "Iteration 4977, Loss: 1161167791.2044992\n",
      "Iteration 4978, Loss: 1159776626.4101617\n",
      "Iteration 4979, Loss: 1158522952.6481743\n",
      "Iteration 4980, Loss: 1150952669.6681774\n",
      "Iteration 4981, Loss: 1204712261.9040453\n",
      "Iteration 4982, Loss: 1172516698.5271752\n",
      "Iteration 4983, Loss: 1155230254.000032\n",
      "Iteration 4984, Loss: 1157588575.0520923\n",
      "Iteration 4985, Loss: 1242448006.4432352\n",
      "Iteration 4986, Loss: 1242755181.4681828\n",
      "Iteration 4987, Loss: 1588384440.3988905\n",
      "Iteration 4988, Loss: 1548767737.6782405\n",
      "Iteration 4989, Loss: 1178022503.5374343\n",
      "Iteration 4990, Loss: 1077052271.8369355\n",
      "Iteration 4991, Loss: 1434416700.86906\n",
      "Iteration 4992, Loss: 1247452172.096039\n",
      "Iteration 4993, Loss: 1277395278.425694\n",
      "Iteration 4994, Loss: 1083460676.566413\n",
      "Iteration 4995, Loss: 1086915981.0507991\n",
      "Iteration 4996, Loss: 1089750000.6530075\n",
      "Iteration 4997, Loss: 1147646660.7363908\n",
      "Iteration 4998, Loss: 1131198739.3786705\n",
      "Iteration 4999, Loss: 1385222712.8306751\n",
      "Iteration 5000, Loss: 1213701331.0979915\n",
      "Iteration 5001, Loss: 1194001910.6541898\n",
      "Iteration 5002, Loss: 1182944045.463021\n",
      "Iteration 5003, Loss: 1330711654.44578\n",
      "Iteration 5004, Loss: 1155525703.3473804\n",
      "Iteration 5005, Loss: 1208387552.7020667\n",
      "Iteration 5006, Loss: 1196548494.6851861\n",
      "Iteration 5007, Loss: 1185562448.066536\n",
      "Iteration 5008, Loss: 1294355902.5682926\n",
      "Iteration 5009, Loss: 1291328950.3559074\n",
      "Iteration 5010, Loss: 1315824219.766686\n",
      "Iteration 5011, Loss: 1382590944.9396465\n",
      "Iteration 5012, Loss: 1358037830.0828876\n",
      "Iteration 5013, Loss: 1398782800.8035266\n",
      "Iteration 5014, Loss: 1357448348.2247467\n",
      "Iteration 5015, Loss: 1223801831.3860595\n",
      "Iteration 5016, Loss: 1257057762.6026278\n",
      "Iteration 5017, Loss: 1252816463.3485396\n",
      "Iteration 5018, Loss: 1213472600.902386\n",
      "Iteration 5019, Loss: 1184403947.591913\n",
      "Iteration 5020, Loss: 1219338287.6387787\n",
      "Iteration 5021, Loss: 1264445901.6878924\n",
      "Iteration 5022, Loss: 1089735730.0239346\n",
      "Iteration 5023, Loss: 1124392673.1264968\n",
      "Iteration 5024, Loss: 1163558604.445364\n",
      "Iteration 5025, Loss: 1154300561.5339973\n",
      "Iteration 5026, Loss: 1280351706.5039072\n",
      "Iteration 5027, Loss: 1123924488.9164248\n",
      "Iteration 5028, Loss: 1175927321.4141257\n",
      "Iteration 5029, Loss: 1236310751.274413\n",
      "Iteration 5030, Loss: 1587590919.141104\n",
      "Iteration 5031, Loss: 1361786108.0390828\n",
      "Iteration 5032, Loss: 1338520404.5562646\n",
      "Iteration 5033, Loss: 1384637878.874228\n",
      "Iteration 5034, Loss: 1065141995.6878121\n",
      "Iteration 5035, Loss: 1549717396.0619807\n",
      "Iteration 5036, Loss: 1167198934.5906982\n",
      "Iteration 5037, Loss: 1340762867.8929331\n",
      "Iteration 5038, Loss: 1357754813.3457494\n",
      "Iteration 5039, Loss: 1365111367.66574\n",
      "Iteration 5040, Loss: 1070894014.8573654\n",
      "Iteration 5041, Loss: 1066220477.4974842\n",
      "Iteration 5042, Loss: 1059637782.9580742\n",
      "Iteration 5043, Loss: 1550439444.6788664\n",
      "Iteration 5044, Loss: 1508492545.5001247\n",
      "Iteration 5045, Loss: 1080110544.4322996\n",
      "Iteration 5046, Loss: 1206987075.3204892\n",
      "Iteration 5047, Loss: 1237940672.3245015\n",
      "Iteration 5048, Loss: 1266867117.2089243\n",
      "Iteration 5049, Loss: 1074401980.9219418\n",
      "Iteration 5050, Loss: 1316108100.4463582\n",
      "Iteration 5051, Loss: 1291165165.8094437\n",
      "Iteration 5052, Loss: 1326597776.3913214\n",
      "Iteration 5053, Loss: 1369306767.170814\n",
      "Iteration 5054, Loss: 1374332434.923955\n",
      "Iteration 5055, Loss: 1295375447.4060957\n",
      "Iteration 5056, Loss: 1336664372.8895645\n",
      "Iteration 5057, Loss: 1312444334.4464679\n",
      "Iteration 5058, Loss: 1338354342.004802\n",
      "Iteration 5059, Loss: 1131881564.6233475\n",
      "Iteration 5060, Loss: 1129163598.2193863\n",
      "Iteration 5061, Loss: 1170566828.223999\n",
      "Iteration 5062, Loss: 1107514390.7474506\n",
      "Iteration 5063, Loss: 1102870410.3419104\n",
      "Iteration 5064, Loss: 1233831661.5525563\n",
      "Iteration 5065, Loss: 1135467146.4417155\n",
      "Iteration 5066, Loss: 1242550681.1206577\n",
      "Iteration 5067, Loss: 1226725258.4621828\n",
      "Iteration 5068, Loss: 1113334991.993753\n",
      "Iteration 5069, Loss: 1115782860.1133475\n",
      "Iteration 5070, Loss: 1152339677.4121616\n",
      "Iteration 5071, Loss: 1184659496.1046045\n",
      "Iteration 5072, Loss: 1188398132.0122483\n",
      "Iteration 5073, Loss: 1217619174.552546\n",
      "Iteration 5074, Loss: 1328846814.6130984\n",
      "Iteration 5075, Loss: 1318077322.5565703\n",
      "Iteration 5076, Loss: 1335566947.4904616\n",
      "Iteration 5077, Loss: 1280467202.0934582\n",
      "Iteration 5078, Loss: 1190923106.5624146\n",
      "Iteration 5079, Loss: 1365847016.2051337\n",
      "Iteration 5080, Loss: 1071217012.5657727\n",
      "Iteration 5081, Loss: 1318082989.1172297\n",
      "Iteration 5082, Loss: 1335829938.094952\n",
      "Iteration 5083, Loss: 1074996702.9473634\n",
      "Iteration 5084, Loss: 3406055025.7554297\n",
      "Iteration 5085, Loss: 1116201437.7398016\n",
      "Iteration 5086, Loss: 1225711637.9220982\n",
      "Iteration 5087, Loss: 1214561777.731484\n",
      "Iteration 5088, Loss: 1282835298.0805817\n",
      "Iteration 5089, Loss: 1134131663.8475978\n",
      "Iteration 5090, Loss: 1100494884.2306914\n",
      "Iteration 5091, Loss: 1217432284.1623297\n",
      "Iteration 5092, Loss: 1302261876.6970828\n",
      "Iteration 5093, Loss: 1186745744.0906968\n",
      "Iteration 5094, Loss: 1146105518.7301657\n",
      "Iteration 5095, Loss: 1328415972.55196\n",
      "Iteration 5096, Loss: 1131783479.3483822\n",
      "Iteration 5097, Loss: 1717438991.6146681\n",
      "Iteration 5098, Loss: 1405035900.9279702\n",
      "Iteration 5099, Loss: 1370647679.1368735\n",
      "Iteration 5100, Loss: 1063739664.2078729\n",
      "Iteration 5101, Loss: 1109972725.9332235\n",
      "Iteration 5102, Loss: 1130528495.696946\n",
      "Iteration 5103, Loss: 1231278070.9462504\n",
      "Iteration 5104, Loss: 1110158884.3212197\n",
      "Iteration 5105, Loss: 1093074789.8097715\n",
      "Iteration 5106, Loss: 1405780701.3777502\n",
      "Iteration 5107, Loss: 1120266818.140217\n",
      "Iteration 5108, Loss: 1229404628.9926336\n",
      "Iteration 5109, Loss: 1167639511.3265965\n",
      "Iteration 5110, Loss: 1795067408.6993296\n",
      "Iteration 5111, Loss: 1671286972.6936169\n",
      "Iteration 5112, Loss: 1650500514.0660024\n",
      "Iteration 5113, Loss: 1260226726.0863388\n",
      "Iteration 5114, Loss: 8396080254.735261\n",
      "Iteration 5115, Loss: 1886858412.0722814\n",
      "Iteration 5116, Loss: 7942968250.213332\n",
      "Iteration 5117, Loss: 2165954614.804559\n",
      "Iteration 5118, Loss: 2002111445.2501774\n",
      "Iteration 5119, Loss: 1111983541.8018403\n",
      "Iteration 5120, Loss: 1073719829.7250524\n",
      "Iteration 5121, Loss: 1116593123.1300025\n",
      "Iteration 5122, Loss: 1137446203.4162693\n",
      "Iteration 5123, Loss: 1741961915.204146\n",
      "Iteration 5124, Loss: 1124009444.129468\n",
      "Iteration 5125, Loss: 1237770863.8373692\n",
      "Iteration 5126, Loss: 1184693020.9423969\n",
      "Iteration 5127, Loss: 1770012781.3454642\n",
      "Iteration 5128, Loss: 1413022590.7786787\n",
      "Iteration 5129, Loss: 1067222933.1025398\n",
      "Iteration 5130, Loss: 1067362456.8309975\n",
      "Iteration 5131, Loss: 1141230639.186856\n",
      "Iteration 5132, Loss: 1193006750.552777\n",
      "Iteration 5133, Loss: 1166878261.637824\n",
      "Iteration 5134, Loss: 1329671661.4072924\n",
      "Iteration 5135, Loss: 1080010638.3583312\n",
      "Iteration 5136, Loss: 2499661165.524341\n",
      "Iteration 5137, Loss: 1303889266.8487449\n",
      "Iteration 5138, Loss: 1277812916.8095386\n",
      "Iteration 5139, Loss: 1098184941.6299524\n",
      "Iteration 5140, Loss: 1298672161.9520516\n",
      "Iteration 5141, Loss: 1189224801.3505023\n",
      "Iteration 5142, Loss: 1289653675.8274374\n",
      "Iteration 5143, Loss: 1359226872.537279\n",
      "Iteration 5144, Loss: 1299033254.8817565\n",
      "Iteration 5145, Loss: 1325398419.918149\n",
      "Iteration 5146, Loss: 1300526584.9591784\n",
      "Iteration 5147, Loss: 1141850708.3476393\n",
      "Iteration 5148, Loss: 1206864800.239476\n",
      "Iteration 5149, Loss: 1277153079.1580977\n",
      "Iteration 5150, Loss: 1175244931.5100036\n",
      "Iteration 5151, Loss: 1781758465.8600757\n",
      "Iteration 5152, Loss: 1109510823.7045348\n",
      "Iteration 5153, Loss: 1392376422.1541216\n",
      "Iteration 5154, Loss: 1086150905.165256\n",
      "Iteration 5155, Loss: 3617871789.979945\n",
      "Iteration 5156, Loss: 1074553247.939262\n",
      "Iteration 5157, Loss: 1087314775.2876668\n",
      "Iteration 5158, Loss: 1071860472.3925678\n",
      "Iteration 5159, Loss: 1337021995.1139474\n",
      "Iteration 5160, Loss: 1169261828.7753906\n",
      "Iteration 5161, Loss: 1337716483.440679\n",
      "Iteration 5162, Loss: 1330173694.3097978\n",
      "Iteration 5163, Loss: 1103728967.8649294\n",
      "Iteration 5164, Loss: 1157891853.2452557\n",
      "Iteration 5165, Loss: 1336425983.872806\n",
      "Iteration 5166, Loss: 1390199600.907357\n",
      "Iteration 5167, Loss: 1161136663.6534123\n",
      "Iteration 5168, Loss: 1272123802.4970803\n",
      "Iteration 5169, Loss: 1344108055.4057975\n",
      "Iteration 5170, Loss: 1084399192.740684\n",
      "Iteration 5171, Loss: 3573676244.824789\n",
      "Iteration 5172, Loss: 1520753799.3968773\n",
      "Iteration 5173, Loss: 1467278157.0147028\n",
      "Iteration 5174, Loss: 1116172923.7232213\n",
      "Iteration 5175, Loss: 1173158052.523875\n",
      "Iteration 5176, Loss: 1172163545.517347\n",
      "Iteration 5177, Loss: 1290292259.5183578\n",
      "Iteration 5178, Loss: 1272575762.1296031\n",
      "Iteration 5179, Loss: 1307685044.0958931\n",
      "Iteration 5180, Loss: 1374784234.7368643\n",
      "Iteration 5181, Loss: 1248283039.3638108\n",
      "Iteration 5182, Loss: 1080564152.0688727\n",
      "Iteration 5183, Loss: 1299896992.9882557\n",
      "Iteration 5184, Loss: 1329898722.1826057\n",
      "Iteration 5185, Loss: 1234925332.7231896\n",
      "Iteration 5186, Loss: 1080496681.0924592\n",
      "Iteration 5187, Loss: 1483422858.8311944\n",
      "Iteration 5188, Loss: 1201492897.6336472\n",
      "Iteration 5189, Loss: 1175508559.444524\n",
      "Iteration 5190, Loss: 1328719780.2523315\n",
      "Iteration 5191, Loss: 1377923889.6513586\n",
      "Iteration 5192, Loss: 1250987009.4833887\n",
      "Iteration 5193, Loss: 1175993976.4075358\n",
      "Iteration 5194, Loss: 1205117643.6708517\n",
      "Iteration 5195, Loss: 1169585049.9617593\n",
      "Iteration 5196, Loss: 1201677128.0207272\n",
      "Iteration 5197, Loss: 1194454490.110004\n",
      "Iteration 5198, Loss: 1317412673.4802234\n",
      "Iteration 5199, Loss: 1375059551.8046231\n",
      "Iteration 5200, Loss: 1071456540.9019216\n",
      "Iteration 5201, Loss: 2864006586.7362266\n",
      "Iteration 5202, Loss: 2386802357.6800785\n",
      "Iteration 5203, Loss: 2200439121.1640167\n",
      "Iteration 5204, Loss: 1589790301.2146316\n",
      "Iteration 5205, Loss: 1233121180.6213627\n",
      "Iteration 5206, Loss: 1171981511.3545592\n",
      "Iteration 5207, Loss: 1120950524.280428\n",
      "Iteration 5208, Loss: 1068417022.0524161\n",
      "Iteration 5209, Loss: 1096913739.9756181\n",
      "Iteration 5210, Loss: 1093590398.4873784\n",
      "Iteration 5211, Loss: 1267696821.8082395\n",
      "Iteration 5212, Loss: 1171451188.570504\n",
      "Iteration 5213, Loss: 1151860277.949246\n",
      "Iteration 5214, Loss: 1212244102.6855488\n",
      "Iteration 5215, Loss: 1308978382.5490913\n",
      "Iteration 5216, Loss: 1173140021.4870632\n",
      "Iteration 5217, Loss: 1075679823.2452419\n",
      "Iteration 5218, Loss: 1080527071.55402\n",
      "Iteration 5219, Loss: 1124376413.4569528\n",
      "Iteration 5220, Loss: 1383688137.4601536\n",
      "Iteration 5221, Loss: 1088081141.821095\n",
      "Iteration 5222, Loss: 1320574357.1200235\n",
      "Iteration 5223, Loss: 1380291960.114764\n",
      "Iteration 5224, Loss: 1384015786.0916522\n",
      "Iteration 5225, Loss: 1097348495.833229\n",
      "Iteration 5226, Loss: 1072322924.7490135\n",
      "Iteration 5227, Loss: 1171246590.8563638\n",
      "Iteration 5228, Loss: 1111501290.2778046\n",
      "Iteration 5229, Loss: 1108114174.460296\n",
      "Iteration 5230, Loss: 1163055437.447033\n",
      "Iteration 5231, Loss: 1186703965.6042845\n",
      "Iteration 5232, Loss: 1245019877.47083\n",
      "Iteration 5233, Loss: 1165472944.619251\n",
      "Iteration 5234, Loss: 1109466981.4301968\n",
      "Iteration 5235, Loss: 1190866305.2483478\n",
      "Iteration 5236, Loss: 1210876634.7612147\n",
      "Iteration 5237, Loss: 1301189710.6485145\n",
      "Iteration 5238, Loss: 1335801609.8333504\n",
      "Iteration 5239, Loss: 1285732241.9095268\n",
      "Iteration 5240, Loss: 1354766337.3548002\n",
      "Iteration 5241, Loss: 1323970282.836935\n",
      "Iteration 5242, Loss: 1389584269.293316\n",
      "Iteration 5243, Loss: 1157049036.9045596\n",
      "Iteration 5244, Loss: 1070540351.0942955\n",
      "Iteration 5245, Loss: 1111139657.2292495\n",
      "Iteration 5246, Loss: 1114027659.4966414\n",
      "Iteration 5247, Loss: 1247133540.0749388\n",
      "Iteration 5248, Loss: 1165190489.2807913\n",
      "Iteration 5249, Loss: 1154144682.2080848\n",
      "Iteration 5250, Loss: 1146894685.491302\n",
      "Iteration 5251, Loss: 1140329907.24969\n",
      "Iteration 5252, Loss: 1190300074.9976363\n",
      "Iteration 5253, Loss: 1332885183.9249108\n",
      "Iteration 5254, Loss: 1375890594.0379639\n",
      "Iteration 5255, Loss: 1360131846.6764479\n",
      "Iteration 5256, Loss: 1355000744.60315\n",
      "Iteration 5257, Loss: 1149254120.1686633\n",
      "Iteration 5258, Loss: 1145558785.7448065\n",
      "Iteration 5259, Loss: 1197192034.3218787\n",
      "Iteration 5260, Loss: 1116669202.9984944\n",
      "Iteration 5261, Loss: 1068176241.2224733\n",
      "Iteration 5262, Loss: 1072350685.8835254\n",
      "Iteration 5263, Loss: 1318588194.1837506\n",
      "Iteration 5264, Loss: 1164951057.4179046\n",
      "Iteration 5265, Loss: 1200557528.0321944\n",
      "Iteration 5266, Loss: 1679253208.3120677\n",
      "Iteration 5267, Loss: 1291452421.016989\n",
      "Iteration 5268, Loss: 1732612725.3200927\n",
      "Iteration 5269, Loss: 1472049215.9466205\n",
      "Iteration 5270, Loss: 1329356898.2033072\n",
      "Iteration 5271, Loss: 1266881292.9233675\n",
      "Iteration 5272, Loss: 4365903857.786885\n",
      "Iteration 5273, Loss: 2186248245.3184643\n",
      "Iteration 5274, Loss: 2039755678.9342358\n",
      "Iteration 5275, Loss: 1076250492.2894154\n",
      "Iteration 5276, Loss: 1463733392.0208652\n",
      "Iteration 5277, Loss: 1144445141.4343672\n",
      "Iteration 5278, Loss: 1408727484.1123683\n",
      "Iteration 5279, Loss: 1264527349.989349\n",
      "Iteration 5280, Loss: 1254599026.086191\n",
      "Iteration 5281, Loss: 1084148697.0764656\n",
      "Iteration 5282, Loss: 1735200195.4489164\n",
      "Iteration 5283, Loss: 1272181609.833094\n",
      "Iteration 5284, Loss: 1283078628.7515147\n",
      "Iteration 5285, Loss: 1318007055.42907\n",
      "Iteration 5286, Loss: 1095570937.8198\n",
      "Iteration 5287, Loss: 1061834206.7563611\n",
      "Iteration 5288, Loss: 1062254218.9182857\n",
      "Iteration 5289, Loss: 1310754910.4774098\n",
      "Iteration 5290, Loss: 1136850600.598676\n",
      "Iteration 5291, Loss: 1321972349.1488504\n",
      "Iteration 5292, Loss: 1110440282.4568\n",
      "Iteration 5293, Loss: 1146346240.9434078\n",
      "Iteration 5294, Loss: 1336013435.4108675\n",
      "Iteration 5295, Loss: 1341035774.6332798\n",
      "Iteration 5296, Loss: 1053240222.1586376\n",
      "Iteration 5297, Loss: 1055395179.133075\n",
      "Iteration 5298, Loss: 2722112047.799652\n",
      "Iteration 5299, Loss: 2258841118.497431\n",
      "Iteration 5300, Loss: 1647828958.5131133\n",
      "Iteration 5301, Loss: 1531288885.5398698\n",
      "Iteration 5302, Loss: 1226631163.490316\n",
      "Iteration 5303, Loss: 1104769824.5837781\n",
      "Iteration 5304, Loss: 1107139149.4454117\n",
      "Iteration 5305, Loss: 1251753763.5472045\n",
      "Iteration 5306, Loss: 1163520608.4866133\n",
      "Iteration 5307, Loss: 1061736818.391713\n",
      "Iteration 5308, Loss: 1310314461.2824266\n",
      "Iteration 5309, Loss: 1068251980.8619169\n",
      "Iteration 5310, Loss: 1072397777.9833755\n",
      "Iteration 5311, Loss: 1077295667.9712389\n",
      "Iteration 5312, Loss: 1078603205.9099283\n",
      "Iteration 5313, Loss: 1084785383.9404798\n",
      "Iteration 5314, Loss: 1060574220.5509994\n",
      "Iteration 5315, Loss: 1495403570.0926595\n",
      "Iteration 5316, Loss: 1344815098.2744772\n",
      "Iteration 5317, Loss: 1065710213.5574863\n",
      "Iteration 5318, Loss: 1068051301.7961591\n",
      "Iteration 5319, Loss: 1177694970.2565308\n",
      "Iteration 5320, Loss: 1156361159.0186439\n",
      "Iteration 5321, Loss: 1125284678.0873172\n",
      "Iteration 5322, Loss: 1221664124.4857824\n",
      "Iteration 5323, Loss: 1203876178.3375893\n",
      "Iteration 5324, Loss: 1326859713.8890724\n",
      "Iteration 5325, Loss: 1299824640.3704584\n",
      "Iteration 5326, Loss: 1273431381.0932662\n",
      "Iteration 5327, Loss: 1253082505.6327076\n",
      "Iteration 5328, Loss: 1331969948.3362932\n",
      "Iteration 5329, Loss: 1140225043.6040885\n",
      "Iteration 5330, Loss: 1141450796.276939\n",
      "Iteration 5331, Loss: 1899059408.6180768\n",
      "Iteration 5332, Loss: 1243747021.769433\n",
      "Iteration 5333, Loss: 1140848850.4428818\n",
      "Iteration 5334, Loss: 1177224257.21043\n",
      "Iteration 5335, Loss: 1160978347.7411773\n",
      "Iteration 5336, Loss: 1197399007.3916297\n",
      "Iteration 5337, Loss: 1134660305.8099113\n",
      "Iteration 5338, Loss: 1115943256.6438928\n",
      "Iteration 5339, Loss: 1114789874.7321403\n",
      "Iteration 5340, Loss: 1220665141.086024\n",
      "Iteration 5341, Loss: 1248743629.8149123\n",
      "Iteration 5342, Loss: 1058369566.6727561\n",
      "Iteration 5343, Loss: 1250331064.2742355\n",
      "Iteration 5344, Loss: 1081145985.383919\n",
      "Iteration 5345, Loss: 1114339087.5730426\n",
      "Iteration 5346, Loss: 1376740473.9254334\n",
      "Iteration 5347, Loss: 1055452084.6246281\n",
      "Iteration 5348, Loss: 1066100881.1218572\n",
      "Iteration 5349, Loss: 1094692752.170258\n",
      "Iteration 5350, Loss: 1146522309.5129173\n",
      "Iteration 5351, Loss: 1145022980.4387944\n",
      "Iteration 5352, Loss: 1334664467.0164568\n",
      "Iteration 5353, Loss: 1126905128.0437524\n",
      "Iteration 5354, Loss: 1174633639.8383927\n",
      "Iteration 5355, Loss: 1229883424.435575\n",
      "Iteration 5356, Loss: 1224548898.5061326\n",
      "Iteration 5357, Loss: 1305289653.5608482\n",
      "Iteration 5358, Loss: 1110773129.5579238\n",
      "Iteration 5359, Loss: 1164636317.537839\n",
      "Iteration 5360, Loss: 1187575915.0948677\n",
      "Iteration 5361, Loss: 1155451472.5298913\n",
      "Iteration 5362, Loss: 1320739240.4465106\n",
      "Iteration 5363, Loss: 1076775210.6397243\n",
      "Iteration 5364, Loss: 1501300912.0662184\n",
      "Iteration 5365, Loss: 1465884377.0038793\n",
      "Iteration 5366, Loss: 1075915831.0316305\n",
      "Iteration 5367, Loss: 1047845126.16152\n",
      "Iteration 5368, Loss: 1371800113.6603792\n",
      "Iteration 5369, Loss: 1290830957.021353\n",
      "Iteration 5370, Loss: 1224765949.3911982\n",
      "Iteration 5371, Loss: 1111065265.4624965\n",
      "Iteration 5372, Loss: 1145490707.33313\n",
      "Iteration 5373, Loss: 1188480428.8882623\n",
      "Iteration 5374, Loss: 1171480271.193606\n",
      "Iteration 5375, Loss: 1265029163.967995\n",
      "Iteration 5376, Loss: 1120128900.0730107\n",
      "Iteration 5377, Loss: 1187722328.9631104\n",
      "Iteration 5378, Loss: 1057765191.2905979\n",
      "Iteration 5379, Loss: 1106430922.4901984\n",
      "Iteration 5380, Loss: 1100681298.3889525\n",
      "Iteration 5381, Loss: 1085730139.2485123\n",
      "Iteration 5382, Loss: 1431924725.0800204\n",
      "Iteration 5383, Loss: 1414618105.9635484\n",
      "Iteration 5384, Loss: 1372326701.7073486\n",
      "Iteration 5385, Loss: 1073132060.698745\n",
      "Iteration 5386, Loss: 1307119653.6845186\n",
      "Iteration 5387, Loss: 1091123031.278967\n",
      "Iteration 5388, Loss: 1586595626.181712\n",
      "Iteration 5389, Loss: 1203287301.7290425\n",
      "Iteration 5390, Loss: 1188900499.0534666\n",
      "Iteration 5391, Loss: 1357943897.0768428\n",
      "Iteration 5392, Loss: 1232621289.7034025\n",
      "Iteration 5393, Loss: 1050644908.5465053\n",
      "Iteration 5394, Loss: 1052774120.7247026\n",
      "Iteration 5395, Loss: 3009194627.782281\n",
      "Iteration 5396, Loss: 2071348976.2864952\n",
      "Iteration 5397, Loss: 2332326321.310373\n",
      "Iteration 5398, Loss: 2147834505.4874344\n",
      "Iteration 5399, Loss: 1044517525.2524389\n",
      "Iteration 5400, Loss: 1052727910.1597209\n",
      "Iteration 5401, Loss: 2748037567.5342607\n",
      "Iteration 5402, Loss: 1102432479.894937\n",
      "Iteration 5403, Loss: 1527784611.0951161\n",
      "Iteration 5404, Loss: 1566116246.7884514\n",
      "Iteration 5405, Loss: 1171490645.6641693\n",
      "Iteration 5406, Loss: 1888668498.9386945\n",
      "Iteration 5407, Loss: 1654558635.0393274\n",
      "Iteration 5408, Loss: 1066559592.0700161\n",
      "Iteration 5409, Loss: 1280391978.6857648\n",
      "Iteration 5410, Loss: 1322682656.4687657\n",
      "Iteration 5411, Loss: 1291733846.0466192\n",
      "Iteration 5412, Loss: 1262403271.7351038\n",
      "Iteration 5413, Loss: 1058770694.2599634\n",
      "Iteration 5414, Loss: 1097330081.844575\n",
      "Iteration 5415, Loss: 1174867415.2685199\n",
      "Iteration 5416, Loss: 1144333980.0251963\n",
      "Iteration 5417, Loss: 1134759762.1280432\n",
      "Iteration 5418, Loss: 1254077113.3711572\n",
      "Iteration 5419, Loss: 1116026692.8746526\n",
      "Iteration 5420, Loss: 1112225803.189945\n",
      "Iteration 5421, Loss: 1165578083.217951\n",
      "Iteration 5422, Loss: 1174565719.4231713\n",
      "Iteration 5423, Loss: 1190858752.3153415\n",
      "Iteration 5424, Loss: 1179977858.8670816\n",
      "Iteration 5425, Loss: 1155689079.228212\n",
      "Iteration 5426, Loss: 1117179584.447011\n",
      "Iteration 5427, Loss: 1244464448.4739544\n",
      "Iteration 5428, Loss: 1272018497.5711665\n",
      "Iteration 5429, Loss: 1233517365.5902925\n",
      "Iteration 5430, Loss: 1263167119.2266812\n",
      "Iteration 5431, Loss: 1145349409.6423888\n",
      "Iteration 5432, Loss: 1155278650.0260189\n",
      "Iteration 5433, Loss: 1244381633.2167017\n",
      "Iteration 5434, Loss: 1158979838.501189\n",
      "Iteration 5435, Loss: 1190016083.3350394\n",
      "Iteration 5436, Loss: 1216660811.7920132\n",
      "Iteration 5437, Loss: 1266220542.9145057\n",
      "Iteration 5438, Loss: 1160518600.700719\n",
      "Iteration 5439, Loss: 1265352384.064314\n",
      "Iteration 5440, Loss: 1256042760.1602674\n",
      "Iteration 5441, Loss: 1156777634.6147823\n",
      "Iteration 5442, Loss: 1130774870.79025\n",
      "Iteration 5443, Loss: 1130468679.6362104\n",
      "Iteration 5444, Loss: 1274161239.4617136\n",
      "Iteration 5445, Loss: 1249421394.662262\n",
      "Iteration 5446, Loss: 1227217541.698822\n",
      "Iteration 5447, Loss: 1209981731.3327374\n",
      "Iteration 5448, Loss: 1194532446.3286254\n",
      "Iteration 5449, Loss: 1151553222.684678\n",
      "Iteration 5450, Loss: 1139173206.5560045\n",
      "Iteration 5451, Loss: 1184828644.0191574\n",
      "Iteration 5452, Loss: 1277699804.5882537\n",
      "Iteration 5453, Loss: 1291163083.1119664\n",
      "Iteration 5454, Loss: 1172657298.12416\n",
      "Iteration 5455, Loss: 1742638202.8234894\n",
      "Iteration 5456, Loss: 1079166275.714603\n",
      "Iteration 5457, Loss: 1127223505.4220304\n",
      "Iteration 5458, Loss: 1361650876.097647\n",
      "Iteration 5459, Loss: 1287309475.9325821\n",
      "Iteration 5460, Loss: 1299671074.2689328\n",
      "Iteration 5461, Loss: 1289121068.1499186\n",
      "Iteration 5462, Loss: 1054438786.5236957\n",
      "Iteration 5463, Loss: 1491727673.0606534\n",
      "Iteration 5464, Loss: 1213141722.7208712\n",
      "Iteration 5465, Loss: 1119611193.3139758\n",
      "Iteration 5466, Loss: 1200358589.1442797\n",
      "Iteration 5467, Loss: 1048808230.1692022\n",
      "Iteration 5468, Loss: 2866354220.6098304\n",
      "Iteration 5469, Loss: 3298669280.779121\n",
      "Iteration 5470, Loss: 1215311674.2551465\n",
      "Iteration 5471, Loss: 1210429873.0780106\n",
      "Iteration 5472, Loss: 1259441381.855181\n",
      "Iteration 5473, Loss: 1232596940.9211178\n",
      "Iteration 5474, Loss: 1210835992.3619692\n",
      "Iteration 5475, Loss: 1259984523.201508\n",
      "Iteration 5476, Loss: 1303377635.2666962\n",
      "Iteration 5477, Loss: 1270025890.7391417\n",
      "Iteration 5478, Loss: 1461812798.8468795\n",
      "Iteration 5479, Loss: 1471496620.1639054\n",
      "Iteration 5480, Loss: 1356798020.3256516\n",
      "Iteration 5481, Loss: 1312834639.5817566\n",
      "Iteration 5482, Loss: 1058500366.9589775\n",
      "Iteration 5483, Loss: 1053436488.9241772\n",
      "Iteration 5484, Loss: 1043506912.7719803\n",
      "Iteration 5485, Loss: 1060726457.2593791\n",
      "Iteration 5486, Loss: 1189717842.218893\n",
      "Iteration 5487, Loss: 1663903624.9391522\n",
      "Iteration 5488, Loss: 1076617784.9158056\n",
      "Iteration 5489, Loss: 1047805860.1831375\n",
      "Iteration 5490, Loss: 1136555721.6414707\n",
      "Iteration 5491, Loss: 1126338772.4052033\n",
      "Iteration 5492, Loss: 1114921983.5373607\n",
      "Iteration 5493, Loss: 1159724440.3832877\n",
      "Iteration 5494, Loss: 1178695405.131014\n",
      "Iteration 5495, Loss: 1286334783.9289298\n",
      "Iteration 5496, Loss: 1258523786.1956959\n",
      "Iteration 5497, Loss: 1483922272.5529315\n",
      "Iteration 5498, Loss: 1446765738.2891464\n",
      "Iteration 5499, Loss: 1203028650.6088972\n",
      "Iteration 5500, Loss: 1124409396.8086936\n",
      "Iteration 5501, Loss: 1100610068.9654787\n",
      "Iteration 5502, Loss: 1099916659.126577\n",
      "Iteration 5503, Loss: 1091567809.6648564\n",
      "Iteration 5504, Loss: 1381378489.2004025\n",
      "Iteration 5505, Loss: 1397731091.1039639\n",
      "Iteration 5506, Loss: 1175540409.2606587\n",
      "Iteration 5507, Loss: 1122429974.417431\n",
      "Iteration 5508, Loss: 1363921502.1767359\n",
      "Iteration 5509, Loss: 1330174697.6867168\n",
      "Iteration 5510, Loss: 1364473052.8708668\n",
      "Iteration 5511, Loss: 1339401481.3476872\n",
      "Iteration 5512, Loss: 1197892726.0348785\n",
      "Iteration 5513, Loss: 1132197609.0136292\n",
      "Iteration 5514, Loss: 1157352565.5013046\n",
      "Iteration 5515, Loss: 1155017111.4272435\n",
      "Iteration 5516, Loss: 1264076466.7582083\n",
      "Iteration 5517, Loss: 1240366085.6540515\n",
      "Iteration 5518, Loss: 1220159163.0648522\n",
      "Iteration 5519, Loss: 1203329709.088188\n",
      "Iteration 5520, Loss: 1238025396.4401848\n",
      "Iteration 5521, Loss: 1149146189.2908952\n",
      "Iteration 5522, Loss: 1180247620.1075742\n",
      "Iteration 5523, Loss: 1284745663.8955772\n",
      "Iteration 5524, Loss: 1273816985.7121902\n",
      "Iteration 5525, Loss: 1118388488.990675\n",
      "Iteration 5526, Loss: 1068545363.8872073\n",
      "Iteration 5527, Loss: 1070477829.3752434\n",
      "Iteration 5528, Loss: 1189153448.206271\n",
      "Iteration 5529, Loss: 1174885108.216471\n",
      "Iteration 5530, Loss: 1163616920.9494112\n",
      "Iteration 5531, Loss: 1157062271.2012854\n",
      "Iteration 5532, Loss: 1175955448.7620869\n",
      "Iteration 5533, Loss: 1270028826.7389143\n",
      "Iteration 5534, Loss: 1302466260.4703796\n",
      "Iteration 5535, Loss: 1193117050.885674\n",
      "Iteration 5536, Loss: 1178468312.8105524\n",
      "Iteration 5537, Loss: 1293379613.3692772\n",
      "Iteration 5538, Loss: 1099487723.6509645\n",
      "Iteration 5539, Loss: 1235098226.4034333\n",
      "Iteration 5540, Loss: 1296217697.9523382\n",
      "Iteration 5541, Loss: 1046562952.0773762\n",
      "Iteration 5542, Loss: 1221431451.3157845\n",
      "Iteration 5543, Loss: 1235941921.059923\n",
      "Iteration 5544, Loss: 1109679854.965694\n",
      "Iteration 5545, Loss: 1094173340.5692933\n",
      "Iteration 5546, Loss: 1094903854.1043487\n",
      "Iteration 5547, Loss: 1051404825.5879502\n",
      "Iteration 5548, Loss: 1207697533.9938989\n",
      "Iteration 5549, Loss: 1162795229.3794618\n",
      "Iteration 5550, Loss: 1768018809.8308165\n",
      "Iteration 5551, Loss: 1654515417.9136598\n",
      "Iteration 5552, Loss: 1111528302.0117722\n",
      "Iteration 5553, Loss: 1053752154.4917725\n",
      "Iteration 5554, Loss: 1054940870.6242472\n",
      "Iteration 5555, Loss: 1102844800.8019385\n",
      "Iteration 5556, Loss: 1080512277.6255624\n",
      "Iteration 5557, Loss: 1258412777.8622663\n",
      "Iteration 5558, Loss: 1048946015.6891929\n",
      "Iteration 5559, Loss: 1099426197.981035\n",
      "Iteration 5560, Loss: 1089794828.8098538\n",
      "Iteration 5561, Loss: 1387552848.2755733\n",
      "Iteration 5562, Loss: 1139927270.4900196\n",
      "Iteration 5563, Loss: 1053455917.758269\n",
      "Iteration 5564, Loss: 1215188975.0981615\n",
      "Iteration 5565, Loss: 1251336200.288254\n",
      "Iteration 5566, Loss: 1320040109.0319817\n",
      "Iteration 5567, Loss: 1326300646.9936824\n",
      "Iteration 5568, Loss: 1044165598.0218989\n",
      "Iteration 5569, Loss: 1343964180.756713\n",
      "Iteration 5570, Loss: 1367635646.185292\n",
      "Iteration 5571, Loss: 1395128526.9719522\n",
      "Iteration 5572, Loss: 1358653292.654068\n",
      "Iteration 5573, Loss: 1376018504.100655\n",
      "Iteration 5574, Loss: 1185537000.95947\n",
      "Iteration 5575, Loss: 1134814718.6283722\n",
      "Iteration 5576, Loss: 1323792246.1739786\n",
      "Iteration 5577, Loss: 1061949990.6052612\n",
      "Iteration 5578, Loss: 1044749641.5521094\n",
      "Iteration 5579, Loss: 1290505467.4395154\n",
      "Iteration 5580, Loss: 1314508263.5181565\n",
      "Iteration 5581, Loss: 1115264086.3606553\n",
      "Iteration 5582, Loss: 1067238946.1872283\n",
      "Iteration 5583, Loss: 1413018520.084312\n",
      "Iteration 5584, Loss: 1044967855.8090897\n",
      "Iteration 5585, Loss: 1519747411.404632\n",
      "Iteration 5586, Loss: 1384765943.9864721\n",
      "Iteration 5587, Loss: 1057945449.5823286\n",
      "Iteration 5588, Loss: 1336419615.7503264\n",
      "Iteration 5589, Loss: 1296985162.7371995\n",
      "Iteration 5590, Loss: 1268882212.5867536\n",
      "Iteration 5591, Loss: 1244226133.1371188\n",
      "Iteration 5592, Loss: 1130296221.9763372\n",
      "Iteration 5593, Loss: 1652302484.1904614\n",
      "Iteration 5594, Loss: 1410975300.5672424\n",
      "Iteration 5595, Loss: 2175797187.495236\n",
      "Iteration 5596, Loss: 1052393218.0784272\n",
      "Iteration 5597, Loss: 1251715388.999062\n",
      "Iteration 5598, Loss: 1301028127.5401058\n",
      "Iteration 5599, Loss: 1044078405.105953\n",
      "Iteration 5600, Loss: 1070863537.9812529\n",
      "Iteration 5601, Loss: 1299094885.626395\n",
      "Iteration 5602, Loss: 1285901586.2543366\n",
      "Iteration 5603, Loss: 1258700293.6069193\n",
      "Iteration 5604, Loss: 1060516216.3372891\n",
      "Iteration 5605, Loss: 1440347846.223725\n",
      "Iteration 5606, Loss: 1430453371.3550558\n",
      "Iteration 5607, Loss: 1326382454.4029443\n",
      "Iteration 5608, Loss: 1364516845.954723\n",
      "Iteration 5609, Loss: 1387057146.7528477\n",
      "Iteration 5610, Loss: 1422128065.9129891\n",
      "Iteration 5611, Loss: 1163911451.165652\n",
      "Iteration 5612, Loss: 1167673151.6751645\n",
      "Iteration 5613, Loss: 1157313410.7012494\n",
      "Iteration 5614, Loss: 1165736004.3403976\n",
      "Iteration 5615, Loss: 1155753255.3432648\n",
      "Iteration 5616, Loss: 1141175320.6077442\n",
      "Iteration 5617, Loss: 1252966737.0619214\n",
      "Iteration 5618, Loss: 1495404137.6105828\n",
      "Iteration 5619, Loss: 1429636488.2123451\n",
      "Iteration 5620, Loss: 1326989427.0160234\n",
      "Iteration 5621, Loss: 1112647902.4721434\n",
      "Iteration 5622, Loss: 1068080672.190422\n",
      "Iteration 5623, Loss: 1169168721.0677857\n",
      "Iteration 5624, Loss: 1159900225.4184952\n",
      "Iteration 5625, Loss: 1295940136.3851383\n",
      "Iteration 5626, Loss: 1328630532.0756702\n",
      "Iteration 5627, Loss: 1362089387.808443\n",
      "Iteration 5628, Loss: 1217828628.7193341\n",
      "Iteration 5629, Loss: 1137748482.7379289\n",
      "Iteration 5630, Loss: 1306545958.4267695\n",
      "Iteration 5631, Loss: 1339359811.2496283\n",
      "Iteration 5632, Loss: 1319075997.944826\n",
      "Iteration 5633, Loss: 1285921063.642373\n",
      "Iteration 5634, Loss: 1317972511.4776058\n",
      "Iteration 5635, Loss: 1319823569.7751532\n",
      "Iteration 5636, Loss: 1350136174.9173675\n",
      "Iteration 5637, Loss: 1085544210.511777\n",
      "Iteration 5638, Loss: 1187268289.0573738\n",
      "Iteration 5639, Loss: 1164607233.4486985\n",
      "Iteration 5640, Loss: 1255312926.7087908\n",
      "Iteration 5641, Loss: 1058089358.7154051\n",
      "Iteration 5642, Loss: 1165434345.7118742\n",
      "Iteration 5643, Loss: 1153937508.4063177\n",
      "Iteration 5644, Loss: 1142456458.5728827\n",
      "Iteration 5645, Loss: 1130752253.057602\n",
      "Iteration 5646, Loss: 1099740343.475202\n",
      "Iteration 5647, Loss: 1072770938.7326111\n",
      "Iteration 5648, Loss: 1182627612.3362553\n",
      "Iteration 5649, Loss: 1168714194.61423\n",
      "Iteration 5650, Loss: 1047269584.4980332\n",
      "Iteration 5651, Loss: 1185936172.315639\n",
      "Iteration 5652, Loss: 1147880263.9668548\n",
      "Iteration 5653, Loss: 1110568602.5751643\n",
      "Iteration 5654, Loss: 1190876696.7134528\n",
      "Iteration 5655, Loss: 1038936787.67468\n",
      "Iteration 5656, Loss: 1077233359.9048774\n",
      "Iteration 5657, Loss: 2204172350.582485\n",
      "Iteration 5658, Loss: 1805734464.324817\n",
      "Iteration 5659, Loss: 1580207080.0490117\n",
      "Iteration 5660, Loss: 1059887418.8734145\n",
      "Iteration 5661, Loss: 1046058034.9961693\n",
      "Iteration 5662, Loss: 1052886698.74891\n",
      "Iteration 5663, Loss: 1058744701.8407024\n",
      "Iteration 5664, Loss: 1166265206.0583627\n",
      "Iteration 5665, Loss: 1292021135.9662504\n",
      "Iteration 5666, Loss: 1164673954.4956915\n",
      "Iteration 5667, Loss: 1043267875.0502537\n",
      "Iteration 5668, Loss: 1206065015.6712625\n",
      "Iteration 5669, Loss: 1189124244.8516278\n",
      "Iteration 5670, Loss: 1140705954.041797\n",
      "Iteration 5671, Loss: 1049654066.1849855\n",
      "Iteration 5672, Loss: 1351160208.0445595\n",
      "Iteration 5673, Loss: 1128002657.500027\n",
      "Iteration 5674, Loss: 1328218037.4125783\n",
      "Iteration 5675, Loss: 1288639637.9194803\n",
      "Iteration 5676, Loss: 1261008507.8918576\n",
      "Iteration 5677, Loss: 1250919930.1177027\n",
      "Iteration 5678, Loss: 1241664908.8159\n",
      "Iteration 5679, Loss: 1127185989.2197788\n",
      "Iteration 5680, Loss: 1659081229.917769\n",
      "Iteration 5681, Loss: 1187972937.850161\n",
      "Iteration 5682, Loss: 1141847484.9665637\n",
      "Iteration 5683, Loss: 1185422047.9729624\n",
      "Iteration 5684, Loss: 1171197688.574098\n",
      "Iteration 5685, Loss: 1160006520.2885208\n",
      "Iteration 5686, Loss: 1181288802.4258194\n",
      "Iteration 5687, Loss: 1168871761.0348146\n",
      "Iteration 5688, Loss: 1162218391.9704878\n",
      "Iteration 5689, Loss: 1167904491.433787\n",
      "Iteration 5690, Loss: 1196987388.7850056\n",
      "Iteration 5691, Loss: 1140498299.2384436\n",
      "Iteration 5692, Loss: 1200137112.758072\n",
      "Iteration 5693, Loss: 1182031724.1201046\n",
      "Iteration 5694, Loss: 1284496581.8267417\n",
      "Iteration 5695, Loss: 1247331354.7621148\n",
      "Iteration 5696, Loss: 1227585944.942763\n",
      "Iteration 5697, Loss: 1304941663.8827498\n",
      "Iteration 5698, Loss: 1398105356.308773\n",
      "Iteration 5699, Loss: 1063870731.8320119\n",
      "Iteration 5700, Loss: 1113370873.8640332\n",
      "Iteration 5701, Loss: 1418256678.8824534\n",
      "Iteration 5702, Loss: 1310081864.0635796\n",
      "Iteration 5703, Loss: 1059579977.9998695\n",
      "Iteration 5704, Loss: 1062973101.1026796\n",
      "Iteration 5705, Loss: 1066328065.0546929\n",
      "Iteration 5706, Loss: 1096428766.135597\n",
      "Iteration 5707, Loss: 1192384988.6839614\n",
      "Iteration 5708, Loss: 1180189756.8522828\n",
      "Iteration 5709, Loss: 1170434857.1262395\n",
      "Iteration 5710, Loss: 1293842125.3399415\n",
      "Iteration 5711, Loss: 1092618557.864288\n",
      "Iteration 5712, Loss: 1193801103.6367865\n",
      "Iteration 5713, Loss: 1280976829.6752136\n",
      "Iteration 5714, Loss: 1322606805.5290663\n",
      "Iteration 5715, Loss: 1206877781.1050532\n",
      "Iteration 5716, Loss: 1620541467.2712324\n",
      "Iteration 5717, Loss: 1353169215.5869212\n",
      "Iteration 5718, Loss: 2046329700.8201349\n",
      "Iteration 5719, Loss: 2233227568.0140886\n",
      "Iteration 5720, Loss: 1123039977.723186\n",
      "Iteration 5721, Loss: 1040802423.7760599\n",
      "Iteration 5722, Loss: 1042413349.4361352\n",
      "Iteration 5723, Loss: 1041496588.539797\n",
      "Iteration 5724, Loss: 1050942419.8758322\n",
      "Iteration 5725, Loss: 1146285911.21193\n",
      "Iteration 5726, Loss: 1106383602.3279073\n",
      "Iteration 5727, Loss: 1173082389.0807533\n",
      "Iteration 5728, Loss: 1132147441.5510995\n",
      "Iteration 5729, Loss: 1097053262.424902\n",
      "Iteration 5730, Loss: 1268679189.9859967\n",
      "Iteration 5731, Loss: 1305652353.803878\n",
      "Iteration 5732, Loss: 1271531127.8996773\n",
      "Iteration 5733, Loss: 1226651487.7769072\n",
      "Iteration 5734, Loss: 1045174695.9632318\n",
      "Iteration 5735, Loss: 2555750734.391587\n",
      "Iteration 5736, Loss: 1202307793.8150737\n",
      "Iteration 5737, Loss: 1650562057.8890736\n",
      "Iteration 5738, Loss: 1539317181.0527894\n",
      "Iteration 5739, Loss: 1461976418.6245215\n",
      "Iteration 5740, Loss: 1079078872.487295\n",
      "Iteration 5741, Loss: 1716586953.1453483\n",
      "Iteration 5742, Loss: 1089631767.2350857\n",
      "Iteration 5743, Loss: 1083172879.0807598\n",
      "Iteration 5744, Loss: 1126547261.0572917\n",
      "Iteration 5745, Loss: 1177476397.2854648\n",
      "Iteration 5746, Loss: 1363249911.1498973\n",
      "Iteration 5747, Loss: 1317829777.7213326\n",
      "Iteration 5748, Loss: 1062948602.4795635\n",
      "Iteration 5749, Loss: 1059371801.783651\n",
      "Iteration 5750, Loss: 1181566136.175965\n",
      "Iteration 5751, Loss: 1304579969.0394626\n",
      "Iteration 5752, Loss: 1338746980.4020188\n",
      "Iteration 5753, Loss: 1366334835.077184\n",
      "Iteration 5754, Loss: 1315683493.817506\n",
      "Iteration 5755, Loss: 1313972886.8059762\n",
      "Iteration 5756, Loss: 1277118706.3209484\n",
      "Iteration 5757, Loss: 1302686745.6254373\n",
      "Iteration 5758, Loss: 1094266499.957143\n",
      "Iteration 5759, Loss: 1223306241.808881\n",
      "Iteration 5760, Loss: 1173689954.2613313\n",
      "Iteration 5761, Loss: 1135160661.8804526\n",
      "Iteration 5762, Loss: 1110479697.1661298\n",
      "Iteration 5763, Loss: 1089656227.5300395\n",
      "Iteration 5764, Loss: 1086122191.5478272\n",
      "Iteration 5765, Loss: 1166876329.0543463\n",
      "Iteration 5766, Loss: 1144721817.2749794\n",
      "Iteration 5767, Loss: 1524196455.7004619\n",
      "Iteration 5768, Loss: 1450639083.902664\n",
      "Iteration 5769, Loss: 1458842903.3447957\n",
      "Iteration 5770, Loss: 1425884701.5032396\n",
      "Iteration 5771, Loss: 1404610604.46317\n",
      "Iteration 5772, Loss: 1206029675.2333364\n",
      "Iteration 5773, Loss: 1163490430.5566916\n",
      "Iteration 5774, Loss: 1202308805.0065901\n",
      "Iteration 5775, Loss: 1266585557.313617\n",
      "Iteration 5776, Loss: 1277287336.2463515\n",
      "Iteration 5777, Loss: 1232572123.0543556\n",
      "Iteration 5778, Loss: 1080206896.354394\n",
      "Iteration 5779, Loss: 1111958084.823454\n",
      "Iteration 5780, Loss: 1345348533.8860712\n",
      "Iteration 5781, Loss: 1086228671.3377557\n",
      "Iteration 5782, Loss: 1077217113.6892614\n",
      "Iteration 5783, Loss: 2129127651.8921907\n",
      "Iteration 5784, Loss: 1046538268.4965963\n",
      "Iteration 5785, Loss: 1099784607.8077266\n",
      "Iteration 5786, Loss: 1085861653.8088572\n",
      "Iteration 5787, Loss: 1088065608.3558486\n",
      "Iteration 5788, Loss: 1186507535.9725628\n",
      "Iteration 5789, Loss: 1181017769.4357865\n",
      "Iteration 5790, Loss: 1268458005.3241982\n",
      "Iteration 5791, Loss: 1147065149.1055505\n",
      "Iteration 5792, Loss: 1245090200.7465205\n",
      "Iteration 5793, Loss: 1287824353.6035175\n",
      "Iteration 5794, Loss: 1096246949.511991\n",
      "Iteration 5795, Loss: 1094745941.2328157\n",
      "Iteration 5796, Loss: 1354167603.1166747\n",
      "Iteration 5797, Loss: 1268733981.5665295\n",
      "Iteration 5798, Loss: 1242213586.8533618\n",
      "Iteration 5799, Loss: 1176149940.6737032\n",
      "Iteration 5800, Loss: 1162429382.509441\n",
      "Iteration 5801, Loss: 1153395212.2656913\n",
      "Iteration 5802, Loss: 1787939322.897343\n",
      "Iteration 5803, Loss: 1722381286.3132644\n",
      "Iteration 5804, Loss: 1533798606.773207\n",
      "Iteration 5805, Loss: 1300979383.7975602\n",
      "Iteration 5806, Loss: 1056392213.8469169\n",
      "Iteration 5807, Loss: 1066899139.4867011\n",
      "Iteration 5808, Loss: 1426296960.8188827\n",
      "Iteration 5809, Loss: 1164492751.3846133\n",
      "Iteration 5810, Loss: 1264364616.7901866\n",
      "Iteration 5811, Loss: 1113269224.0321777\n",
      "Iteration 5812, Loss: 1274258253.6130369\n",
      "Iteration 5813, Loss: 1177136158.0505567\n",
      "Iteration 5814, Loss: 1160737216.6944914\n",
      "Iteration 5815, Loss: 1185065764.1170754\n",
      "Iteration 5816, Loss: 1222027780.6579459\n",
      "Iteration 5817, Loss: 1297184852.0039444\n",
      "Iteration 5818, Loss: 1269242073.8269298\n",
      "Iteration 5819, Loss: 1330197727.6357856\n",
      "Iteration 5820, Loss: 1117157611.8642972\n",
      "Iteration 5821, Loss: 1192899475.883959\n",
      "Iteration 5822, Loss: 1278685807.4511638\n",
      "Iteration 5823, Loss: 1120599556.351478\n",
      "Iteration 5824, Loss: 1110087098.9135184\n",
      "Iteration 5825, Loss: 1162427480.7201486\n",
      "Iteration 5826, Loss: 1154632277.3967566\n",
      "Iteration 5827, Loss: 1792778422.3442957\n",
      "Iteration 5828, Loss: 1210812274.881742\n",
      "Iteration 5829, Loss: 1137563571.4634182\n",
      "Iteration 5830, Loss: 1156524426.4029002\n",
      "Iteration 5831, Loss: 1262627301.301129\n",
      "Iteration 5832, Loss: 1125383434.6453109\n",
      "Iteration 5833, Loss: 1273378698.9360983\n",
      "Iteration 5834, Loss: 1125512527.7616854\n",
      "Iteration 5835, Loss: 1928081284.6667967\n",
      "Iteration 5836, Loss: 1070840113.3354465\n",
      "Iteration 5837, Loss: 1758591234.1965756\n",
      "Iteration 5838, Loss: 1631779832.6325645\n",
      "Iteration 5839, Loss: 1068491221.8476995\n",
      "Iteration 5840, Loss: 1074837856.8901541\n",
      "Iteration 5841, Loss: 1076811810.846109\n",
      "Iteration 5842, Loss: 1072035555.7338858\n",
      "Iteration 5843, Loss: 2332937902.4627523\n",
      "Iteration 5844, Loss: 2218905213.8528275\n",
      "Iteration 5845, Loss: 1826672812.1596253\n",
      "Iteration 5846, Loss: 1486503115.1104324\n",
      "Iteration 5847, Loss: 1462592842.1910691\n",
      "Iteration 5848, Loss: 1439535926.3698726\n",
      "Iteration 5849, Loss: 1108470611.2249928\n",
      "Iteration 5850, Loss: 1367186308.587379\n",
      "Iteration 5851, Loss: 1398047890.7993615\n",
      "Iteration 5852, Loss: 1051331058.4768283\n",
      "Iteration 5853, Loss: 1299463258.9833841\n",
      "Iteration 5854, Loss: 1276356220.2578852\n",
      "Iteration 5855, Loss: 1143156085.1217222\n",
      "Iteration 5856, Loss: 1207990982.8915381\n",
      "Iteration 5857, Loss: 1238030119.843892\n",
      "Iteration 5858, Loss: 1132428695.9319572\n",
      "Iteration 5859, Loss: 1127464974.1831422\n",
      "Iteration 5860, Loss: 1127694207.0768843\n",
      "Iteration 5861, Loss: 1262970822.875225\n",
      "Iteration 5862, Loss: 1243716946.136852\n",
      "Iteration 5863, Loss: 1251846501.5594676\n",
      "Iteration 5864, Loss: 1162802071.492338\n",
      "Iteration 5865, Loss: 1189450616.4104705\n",
      "Iteration 5866, Loss: 1130800087.265116\n",
      "Iteration 5867, Loss: 1089955674.0983331\n",
      "Iteration 5868, Loss: 1098939228.060361\n",
      "Iteration 5869, Loss: 1289694224.7111323\n",
      "Iteration 5870, Loss: 1347559828.469648\n",
      "Iteration 5871, Loss: 1071714460.6216363\n",
      "Iteration 5872, Loss: 2636619577.705924\n",
      "Iteration 5873, Loss: 1927672710.3588822\n",
      "Iteration 5874, Loss: 1491889365.6396754\n",
      "Iteration 5875, Loss: 1112140978.9617047\n",
      "Iteration 5876, Loss: 3082017169.5720415\n",
      "Iteration 5877, Loss: 2011479223.2401533\n",
      "Iteration 5878, Loss: 1861080980.213169\n",
      "Iteration 5879, Loss: 1238659882.4482348\n",
      "Iteration 5880, Loss: 1161942634.2537286\n",
      "Iteration 5881, Loss: 1265834956.3085656\n",
      "Iteration 5882, Loss: 1258545805.414817\n",
      "Iteration 5883, Loss: 1290842409.986694\n",
      "Iteration 5884, Loss: 1326467740.252171\n",
      "Iteration 5885, Loss: 1221734197.400929\n",
      "Iteration 5886, Loss: 1068622250.313542\n",
      "Iteration 5887, Loss: 1477122068.030722\n",
      "Iteration 5888, Loss: 1466672379.8477683\n",
      "Iteration 5889, Loss: 1170156937.3899682\n",
      "Iteration 5890, Loss: 1161027847.7594562\n",
      "Iteration 5891, Loss: 1190643483.115869\n",
      "Iteration 5892, Loss: 1708398495.1570015\n",
      "Iteration 5893, Loss: 1082316388.4532354\n",
      "Iteration 5894, Loss: 1085807533.3428497\n",
      "Iteration 5895, Loss: 1086063980.9527268\n",
      "Iteration 5896, Loss: 1088351685.64258\n",
      "Iteration 5897, Loss: 2215059650.052366\n",
      "Iteration 5898, Loss: 1061998732.6377819\n",
      "Iteration 5899, Loss: 1129046135.9962435\n",
      "Iteration 5900, Loss: 1130389408.302499\n",
      "Iteration 5901, Loss: 1976113027.785764\n",
      "Iteration 5902, Loss: 1524930742.786292\n",
      "Iteration 5903, Loss: 1497808929.934355\n",
      "Iteration 5904, Loss: 1067564308.2932116\n",
      "Iteration 5905, Loss: 1227114115.9519646\n",
      "Iteration 5906, Loss: 1225395133.1065416\n",
      "Iteration 5907, Loss: 1231220177.0223372\n",
      "Iteration 5908, Loss: 1247268477.9220126\n",
      "Iteration 5909, Loss: 1063053006.6541141\n",
      "Iteration 5910, Loss: 1060517686.8032516\n",
      "Iteration 5911, Loss: 1063734998.8384904\n",
      "Iteration 5912, Loss: 2179864611.4331346\n",
      "Iteration 5913, Loss: 1976336271.140642\n",
      "Iteration 5914, Loss: 1680923077.9876056\n",
      "Iteration 5915, Loss: 1647151948.1618192\n",
      "Iteration 5916, Loss: 1498841905.9847713\n",
      "Iteration 5917, Loss: 1463543572.9025197\n",
      "Iteration 5918, Loss: 1447418490.652289\n",
      "Iteration 5919, Loss: 1457840529.1976724\n",
      "Iteration 5920, Loss: 1447482639.1854117\n",
      "Iteration 5921, Loss: 1355508535.8478577\n",
      "Iteration 5922, Loss: 1333286146.4395168\n",
      "Iteration 5923, Loss: 1153244563.653703\n",
      "Iteration 5924, Loss: 1132775850.935646\n",
      "Iteration 5925, Loss: 1138356655.7171695\n",
      "Iteration 5926, Loss: 1971567865.6094022\n",
      "Iteration 5927, Loss: 1380910715.5303667\n",
      "Iteration 5928, Loss: 1084418161.9607224\n",
      "Iteration 5929, Loss: 1455975502.0728354\n",
      "Iteration 5930, Loss: 1159015630.403775\n",
      "Iteration 5931, Loss: 1181911997.5044236\n",
      "Iteration 5932, Loss: 1181734612.4228947\n",
      "Iteration 5933, Loss: 1773955833.2233717\n",
      "Iteration 5934, Loss: 1716475431.699339\n",
      "Iteration 5935, Loss: 1683444738.7948437\n",
      "Iteration 5936, Loss: 1307062992.0063214\n",
      "Iteration 5937, Loss: 1291951808.3920379\n",
      "Iteration 5938, Loss: 8642121599.388597\n",
      "Iteration 5939, Loss: 2320306898.7262497\n",
      "Iteration 5940, Loss: 1516985031.0209777\n",
      "Iteration 5941, Loss: 2270701702.5201044\n",
      "Iteration 5942, Loss: 2175360505.8408866\n",
      "Iteration 5943, Loss: 1084210413.6452284\n",
      "Iteration 5944, Loss: 1321411050.5656848\n",
      "Iteration 5945, Loss: 1373411872.6416855\n",
      "Iteration 5946, Loss: 1220338749.3029625\n",
      "Iteration 5947, Loss: 1164047478.5175333\n",
      "Iteration 5948, Loss: 1081718926.1980946\n",
      "Iteration 5949, Loss: 1077063187.5813224\n",
      "Iteration 5950, Loss: 1184122121.9563532\n",
      "Iteration 5951, Loss: 1443769892.368963\n",
      "Iteration 5952, Loss: 1441059101.684362\n",
      "Iteration 5953, Loss: 1245015857.7840958\n",
      "Iteration 5954, Loss: 1161216694.683352\n",
      "Iteration 5955, Loss: 1162481780.8426247\n",
      "Iteration 5956, Loss: 1200929227.085964\n",
      "Iteration 5957, Loss: 1192505418.337387\n",
      "Iteration 5958, Loss: 1317783005.586854\n",
      "Iteration 5959, Loss: 1377604365.1111948\n",
      "Iteration 5960, Loss: 1170161503.0366743\n",
      "Iteration 5961, Loss: 1291606669.5350492\n",
      "Iteration 5962, Loss: 1275663378.161906\n",
      "Iteration 5963, Loss: 1084491435.860411\n",
      "Iteration 5964, Loss: 1332967755.169521\n",
      "Iteration 5965, Loss: 1143441204.508116\n",
      "Iteration 5966, Loss: 1187824440.9035985\n",
      "Iteration 5967, Loss: 1123286715.878771\n",
      "Iteration 5968, Loss: 1197787232.8801491\n",
      "Iteration 5969, Loss: 1303854307.5070658\n",
      "Iteration 5970, Loss: 1373844612.0361466\n",
      "Iteration 5971, Loss: 1407236254.78402\n",
      "Iteration 5972, Loss: 1378658887.154087\n",
      "Iteration 5973, Loss: 1307570142.0227065\n",
      "Iteration 5974, Loss: 1288360302.3107104\n",
      "Iteration 5975, Loss: 1125867354.3879075\n",
      "Iteration 5976, Loss: 1120704902.5869236\n",
      "Iteration 5977, Loss: 1234135271.4376001\n",
      "Iteration 5978, Loss: 1256771848.4674597\n",
      "Iteration 5979, Loss: 1151763402.9952796\n",
      "Iteration 5980, Loss: 1114738790.867122\n",
      "Iteration 5981, Loss: 1336930370.4388397\n",
      "Iteration 5982, Loss: 1106758468.9895577\n",
      "Iteration 5983, Loss: 1335076662.1011987\n",
      "Iteration 5984, Loss: 1086245837.0448332\n",
      "Iteration 5985, Loss: 1078909173.5566688\n",
      "Iteration 5986, Loss: 1070398320.6520627\n",
      "Iteration 5987, Loss: 1092989475.3309786\n",
      "Iteration 5988, Loss: 1090102697.069249\n",
      "Iteration 5989, Loss: 1137494812.542621\n",
      "Iteration 5990, Loss: 1189494258.5443094\n",
      "Iteration 5991, Loss: 1168318800.022412\n",
      "Iteration 5992, Loss: 1161250168.6176288\n",
      "Iteration 5993, Loss: 1113330159.8118315\n",
      "Iteration 5994, Loss: 1261000579.1764379\n",
      "Iteration 5995, Loss: 1267987129.9872518\n",
      "Iteration 5996, Loss: 1142231097.8034515\n",
      "Iteration 5997, Loss: 1335520776.294812\n",
      "Iteration 5998, Loss: 1228406660.5842571\n",
      "Iteration 5999, Loss: 1288146575.579578\n",
      "Iteration 6000, Loss: 1072074760.9859353\n",
      "Iteration 6001, Loss: 1270551929.6878228\n",
      "Iteration 6002, Loss: 1257542936.4040346\n",
      "Iteration 6003, Loss: 1324401989.9809191\n",
      "Iteration 6004, Loss: 1221270248.189272\n",
      "Iteration 6005, Loss: 1184982445.4116318\n",
      "Iteration 6006, Loss: 1076008061.8222153\n",
      "Iteration 6007, Loss: 1222777062.127923\n",
      "Iteration 6008, Loss: 1287113416.367113\n",
      "Iteration 6009, Loss: 1141366169.7016516\n",
      "Iteration 6010, Loss: 1184238225.4206636\n",
      "Iteration 6011, Loss: 1069151443.90945\n",
      "Iteration 6012, Loss: 1233774067.5101955\n",
      "Iteration 6013, Loss: 1342685970.2614784\n",
      "Iteration 6014, Loss: 1224909724.6795464\n",
      "Iteration 6015, Loss: 1341807955.6385257\n",
      "Iteration 6016, Loss: 1385869970.7831125\n",
      "Iteration 6017, Loss: 1355555774.5383506\n",
      "Iteration 6018, Loss: 1329364352.680209\n",
      "Iteration 6019, Loss: 1303766536.6876538\n",
      "Iteration 6020, Loss: 1144033628.2228503\n",
      "Iteration 6021, Loss: 1298211102.844879\n",
      "Iteration 6022, Loss: 1367403578.5692945\n",
      "Iteration 6023, Loss: 1358002890.4872167\n",
      "Iteration 6024, Loss: 1332373991.704697\n",
      "Iteration 6025, Loss: 1226678335.5969956\n",
      "Iteration 6026, Loss: 1153531004.7226293\n",
      "Iteration 6027, Loss: 1353814007.5384831\n",
      "Iteration 6028, Loss: 1351834061.4873214\n",
      "Iteration 6029, Loss: 1067311408.3931247\n",
      "Iteration 6030, Loss: 1083767066.3513284\n",
      "Iteration 6031, Loss: 1091223849.4291818\n",
      "Iteration 6032, Loss: 1139066713.8015418\n",
      "Iteration 6033, Loss: 1232963811.3544903\n",
      "Iteration 6034, Loss: 1282479076.5296202\n",
      "Iteration 6035, Loss: 1265480634.5118363\n",
      "Iteration 6036, Loss: 1513944314.1695304\n",
      "Iteration 6037, Loss: 1475229630.4124048\n",
      "Iteration 6038, Loss: 1433594191.441352\n",
      "Iteration 6039, Loss: 1386053011.168975\n",
      "Iteration 6040, Loss: 1208429328.0214343\n",
      "Iteration 6041, Loss: 1225147271.8746886\n",
      "Iteration 6042, Loss: 1171646046.803945\n",
      "Iteration 6043, Loss: 1200167861.7265053\n",
      "Iteration 6044, Loss: 1219613566.3402536\n",
      "Iteration 6045, Loss: 1209890999.1888168\n",
      "Iteration 6046, Loss: 1203284014.975273\n",
      "Iteration 6047, Loss: 1248975463.3495038\n",
      "Iteration 6048, Loss: 1554900705.3909254\n",
      "Iteration 6049, Loss: 1485697311.639208\n",
      "Iteration 6050, Loss: 1234541661.8622596\n",
      "Iteration 6051, Loss: 1239447175.9093256\n",
      "Iteration 6052, Loss: 1153654677.1799183\n",
      "Iteration 6053, Loss: 1111640157.0661278\n",
      "Iteration 6054, Loss: 2132887721.700863\n",
      "Iteration 6055, Loss: 1180730460.5770714\n",
      "Iteration 6056, Loss: 1198605638.3153472\n",
      "Iteration 6057, Loss: 1313500882.4043627\n",
      "Iteration 6058, Loss: 1295132030.3995042\n",
      "Iteration 6059, Loss: 1277746976.4784834\n",
      "Iteration 6060, Loss: 1080821973.8115742\n",
      "Iteration 6061, Loss: 1134474733.4402945\n",
      "Iteration 6062, Loss: 1745383022.303664\n",
      "Iteration 6063, Loss: 1512952155.2026467\n",
      "Iteration 6064, Loss: 1130761198.635173\n",
      "Iteration 6065, Loss: 1065099341.5540898\n",
      "Iteration 6066, Loss: 1120305658.2225535\n",
      "Iteration 6067, Loss: 1379949704.9346728\n",
      "Iteration 6068, Loss: 1345297424.7235913\n",
      "Iteration 6069, Loss: 1392758955.852448\n",
      "Iteration 6070, Loss: 1059865021.5275576\n",
      "Iteration 6071, Loss: 1144548868.6777263\n",
      "Iteration 6072, Loss: 1102395697.1549375\n",
      "Iteration 6073, Loss: 1095844770.0651572\n",
      "Iteration 6074, Loss: 1212089941.23966\n",
      "Iteration 6075, Loss: 1159184760.1959052\n",
      "Iteration 6076, Loss: 1158526803.536817\n",
      "Iteration 6077, Loss: 1329326217.7816336\n",
      "Iteration 6078, Loss: 1339786427.815611\n",
      "Iteration 6079, Loss: 1145738894.0848126\n",
      "Iteration 6080, Loss: 1182472784.8222482\n",
      "Iteration 6081, Loss: 1156475738.513368\n",
      "Iteration 6082, Loss: 1265220807.977268\n",
      "Iteration 6083, Loss: 1247633315.6416311\n",
      "Iteration 6084, Loss: 1164805390.3674116\n",
      "Iteration 6085, Loss: 1193760051.5561056\n",
      "Iteration 6086, Loss: 1701113484.9289107\n",
      "Iteration 6087, Loss: 1410720493.3685517\n",
      "Iteration 6088, Loss: 1067735826.1559379\n",
      "Iteration 6089, Loss: 1055305022.0705118\n",
      "Iteration 6090, Loss: 3450835560.771066\n",
      "Iteration 6091, Loss: 1206836333.521038\n",
      "Iteration 6092, Loss: 1090561793.281614\n",
      "Iteration 6093, Loss: 1381821349.4937074\n",
      "Iteration 6094, Loss: 1379839972.7904692\n",
      "Iteration 6095, Loss: 1061438547.1718949\n",
      "Iteration 6096, Loss: 1334959978.8921113\n",
      "Iteration 6097, Loss: 1324221634.9947886\n",
      "Iteration 6098, Loss: 1297816680.7444859\n",
      "Iteration 6099, Loss: 1272905951.4567943\n",
      "Iteration 6100, Loss: 1322186046.1977825\n",
      "Iteration 6101, Loss: 1055513855.9242935\n",
      "Iteration 6102, Loss: 2542529783.001284\n",
      "Iteration 6103, Loss: 1767912271.9668899\n",
      "Iteration 6104, Loss: 1361577240.332723\n",
      "Iteration 6105, Loss: 1209744178.412621\n",
      "Iteration 6106, Loss: 1059618294.8059434\n",
      "Iteration 6107, Loss: 1061030111.6622939\n",
      "Iteration 6108, Loss: 1147478698.4196177\n",
      "Iteration 6109, Loss: 1215951749.050892\n",
      "Iteration 6110, Loss: 1205361523.0922923\n",
      "Iteration 6111, Loss: 1147378459.7289724\n",
      "Iteration 6112, Loss: 1146116618.093856\n",
      "Iteration 6113, Loss: 1145014194.3868632\n",
      "Iteration 6114, Loss: 1193880647.9752133\n",
      "Iteration 6115, Loss: 1330636194.0553434\n",
      "Iteration 6116, Loss: 1339910306.606074\n",
      "Iteration 6117, Loss: 1244134345.925931\n",
      "Iteration 6118, Loss: 1240074718.9264328\n",
      "Iteration 6119, Loss: 1235351815.7317896\n",
      "Iteration 6120, Loss: 1057838663.6320236\n",
      "Iteration 6121, Loss: 1057019549.2639374\n",
      "Iteration 6122, Loss: 1058122557.0012774\n",
      "Iteration 6123, Loss: 1229129321.042144\n",
      "Iteration 6124, Loss: 1144897942.3771064\n",
      "Iteration 6125, Loss: 1064970436.0248353\n",
      "Iteration 6126, Loss: 1063531360.579002\n",
      "Iteration 6127, Loss: 1065269270.1782547\n",
      "Iteration 6128, Loss: 1067544824.9143506\n",
      "Iteration 6129, Loss: 1073495105.3821402\n",
      "Iteration 6130, Loss: 1334129659.6646879\n",
      "Iteration 6131, Loss: 1323684851.4118955\n",
      "Iteration 6132, Loss: 1160456828.9976375\n",
      "Iteration 6133, Loss: 1263118293.665769\n",
      "Iteration 6134, Loss: 1214421514.389625\n",
      "Iteration 6135, Loss: 1150224055.8219702\n",
      "Iteration 6136, Loss: 1270259121.8797045\n",
      "Iteration 6137, Loss: 1250236522.0552797\n",
      "Iteration 6138, Loss: 1313430555.279901\n",
      "Iteration 6139, Loss: 1362463186.2466087\n",
      "Iteration 6140, Loss: 1239529131.913257\n",
      "Iteration 6141, Loss: 1264361455.757092\n",
      "Iteration 6142, Loss: 1215423991.566551\n",
      "Iteration 6143, Loss: 1221636687.6310034\n",
      "Iteration 6144, Loss: 1056699504.376906\n",
      "Iteration 6145, Loss: 1057611487.6329528\n",
      "Iteration 6146, Loss: 1334037220.4963963\n",
      "Iteration 6147, Loss: 1073243451.3103393\n",
      "Iteration 6148, Loss: 2433056038.5690246\n",
      "Iteration 6149, Loss: 1934196096.6284943\n",
      "Iteration 6150, Loss: 1058319165.6064224\n",
      "Iteration 6151, Loss: 1057527020.873619\n",
      "Iteration 6152, Loss: 1058526366.0699775\n",
      "Iteration 6153, Loss: 1058423150.9021984\n",
      "Iteration 6154, Loss: 1828390983.9163964\n",
      "Iteration 6155, Loss: 1064432140.5831643\n",
      "Iteration 6156, Loss: 1061813938.7235198\n",
      "Iteration 6157, Loss: 1485486553.1656532\n",
      "Iteration 6158, Loss: 1391732581.0606096\n",
      "Iteration 6159, Loss: 1118530549.2677867\n",
      "Iteration 6160, Loss: 1094458099.4915407\n",
      "Iteration 6161, Loss: 1433726111.2139142\n",
      "Iteration 6162, Loss: 1423656930.0764518\n",
      "Iteration 6163, Loss: 1150523190.3757787\n",
      "Iteration 6164, Loss: 1271460491.802139\n",
      "Iteration 6165, Loss: 1169529750.6576512\n",
      "Iteration 6166, Loss: 1273908412.0579915\n",
      "Iteration 6167, Loss: 1344719308.9695122\n",
      "Iteration 6168, Loss: 1126958217.2637613\n",
      "Iteration 6169, Loss: 1124752341.240134\n",
      "Iteration 6170, Loss: 1199456604.548972\n",
      "Iteration 6171, Loss: 1255771929.2993267\n",
      "Iteration 6172, Loss: 1238781222.6797283\n",
      "Iteration 6173, Loss: 1123150107.9776866\n",
      "Iteration 6174, Loss: 1231604310.4625857\n",
      "Iteration 6175, Loss: 1269127172.589528\n",
      "Iteration 6176, Loss: 1069709784.9812227\n",
      "Iteration 6177, Loss: 1213056871.591854\n",
      "Iteration 6178, Loss: 1244220680.0407732\n",
      "Iteration 6179, Loss: 1321532257.5267706\n",
      "Iteration 6180, Loss: 1221304752.684249\n",
      "Iteration 6181, Loss: 1261650553.8797278\n",
      "Iteration 6182, Loss: 1213182904.3126354\n",
      "Iteration 6183, Loss: 1132564645.2927084\n",
      "Iteration 6184, Loss: 1202319569.6289053\n",
      "Iteration 6185, Loss: 1287219216.970801\n",
      "Iteration 6186, Loss: 1311955190.4094033\n",
      "Iteration 6187, Loss: 1287033787.86089\n",
      "Iteration 6188, Loss: 1232352062.8016605\n",
      "Iteration 6189, Loss: 1069244207.8771901\n",
      "Iteration 6190, Loss: 1459112248.2609353\n",
      "Iteration 6191, Loss: 1404208958.4455996\n",
      "Iteration 6192, Loss: 1345120156.634845\n",
      "Iteration 6193, Loss: 1060810125.2610737\n",
      "Iteration 6194, Loss: 1254532754.3178527\n",
      "Iteration 6195, Loss: 1079541763.854321\n",
      "Iteration 6196, Loss: 1077950568.1455653\n",
      "Iteration 6197, Loss: 1316537996.156134\n",
      "Iteration 6198, Loss: 1246858505.2746344\n",
      "Iteration 6199, Loss: 1124323154.4513695\n",
      "Iteration 6200, Loss: 1996407851.5467956\n",
      "Iteration 6201, Loss: 1078432150.1016464\n",
      "Iteration 6202, Loss: 1124953984.8469865\n",
      "Iteration 6203, Loss: 1064817939.1327842\n",
      "Iteration 6204, Loss: 1065197131.6643606\n",
      "Iteration 6205, Loss: 1074564617.962299\n",
      "Iteration 6206, Loss: 1272483605.99138\n",
      "Iteration 6207, Loss: 1333781772.054765\n",
      "Iteration 6208, Loss: 1073120475.7410055\n",
      "Iteration 6209, Loss: 1077130355.715991\n",
      "Iteration 6210, Loss: 1091470942.524923\n",
      "Iteration 6211, Loss: 1126090476.2945833\n",
      "Iteration 6212, Loss: 1253705137.937833\n",
      "Iteration 6213, Loss: 1247347584.2251785\n",
      "Iteration 6214, Loss: 1229688574.3154666\n",
      "Iteration 6215, Loss: 1187945147.054556\n",
      "Iteration 6216, Loss: 1173291995.366501\n",
      "Iteration 6217, Loss: 1217351309.872668\n",
      "Iteration 6218, Loss: 1254305219.2486427\n",
      "Iteration 6219, Loss: 1137589032.4441488\n",
      "Iteration 6220, Loss: 1168080669.2876332\n",
      "Iteration 6221, Loss: 1182446344.4235756\n",
      "Iteration 6222, Loss: 1106623310.4916964\n",
      "Iteration 6223, Loss: 1221042891.0885847\n",
      "Iteration 6224, Loss: 1246679552.0799448\n",
      "Iteration 6225, Loss: 1254227954.8611677\n",
      "Iteration 6226, Loss: 1162474121.0911682\n",
      "Iteration 6227, Loss: 1198579036.0081868\n",
      "Iteration 6228, Loss: 1063488926.3655814\n",
      "Iteration 6229, Loss: 1063255447.1013024\n",
      "Iteration 6230, Loss: 1310692627.6303096\n",
      "Iteration 6231, Loss: 1321281622.5969644\n",
      "Iteration 6232, Loss: 1242094181.5318778\n",
      "Iteration 6233, Loss: 1236956892.148286\n",
      "Iteration 6234, Loss: 1230372296.6065228\n",
      "Iteration 6235, Loss: 1226045179.9801128\n",
      "Iteration 6236, Loss: 1053621794.0328233\n",
      "Iteration 6237, Loss: 3505576045.879113\n",
      "Iteration 6238, Loss: 2110030307.6255376\n",
      "Iteration 6239, Loss: 1607143747.761742\n",
      "Iteration 6240, Loss: 1135720931.9627326\n",
      "Iteration 6241, Loss: 1351164825.0841796\n",
      "Iteration 6242, Loss: 1326928421.8877656\n",
      "Iteration 6243, Loss: 1281996067.163651\n",
      "Iteration 6244, Loss: 1116304510.9155111\n",
      "Iteration 6245, Loss: 1867167921.9883707\n",
      "Iteration 6246, Loss: 1729671802.31818\n",
      "Iteration 6247, Loss: 1087165304.9766858\n",
      "Iteration 6248, Loss: 1095482784.1566424\n",
      "Iteration 6249, Loss: 1092648734.1452048\n",
      "Iteration 6250, Loss: 1326384311.5082116\n",
      "Iteration 6251, Loss: 1229176088.7984133\n",
      "Iteration 6252, Loss: 1216484061.5154452\n",
      "Iteration 6253, Loss: 1310037116.3240533\n",
      "Iteration 6254, Loss: 1077477432.1084776\n",
      "Iteration 6255, Loss: 2522300801.369885\n",
      "Iteration 6256, Loss: 2129788860.1669357\n",
      "Iteration 6257, Loss: 2543214052.8322067\n",
      "Iteration 6258, Loss: 1068198040.884636\n",
      "Iteration 6259, Loss: 1084741588.2298663\n",
      "Iteration 6260, Loss: 1099956398.7720368\n",
      "Iteration 6261, Loss: 1118806971.2763765\n",
      "Iteration 6262, Loss: 1174750577.1251063\n",
      "Iteration 6263, Loss: 1060900347.441838\n",
      "Iteration 6264, Loss: 1506269341.3967593\n",
      "Iteration 6265, Loss: 1302131431.0066109\n",
      "Iteration 6266, Loss: 1143227863.1647992\n",
      "Iteration 6267, Loss: 1168362482.310225\n",
      "Iteration 6268, Loss: 1269098021.2771502\n",
      "Iteration 6269, Loss: 1171213287.4974248\n",
      "Iteration 6270, Loss: 1065924005.2986151\n",
      "Iteration 6271, Loss: 1061446165.1190138\n",
      "Iteration 6272, Loss: 1062122827.0059276\n",
      "Iteration 6273, Loss: 1097644571.765544\n",
      "Iteration 6274, Loss: 2029554863.2676046\n",
      "Iteration 6275, Loss: 1852777845.126363\n",
      "Iteration 6276, Loss: 1382782884.0851114\n",
      "Iteration 6277, Loss: 1075434078.166396\n",
      "Iteration 6278, Loss: 1464714587.31128\n",
      "Iteration 6279, Loss: 1428299566.505249\n",
      "Iteration 6280, Loss: 1378776421.9143248\n",
      "Iteration 6281, Loss: 1347663698.978074\n",
      "Iteration 6282, Loss: 1061546877.0453898\n",
      "Iteration 6283, Loss: 1555179947.274308\n",
      "Iteration 6284, Loss: 1511584340.711167\n",
      "Iteration 6285, Loss: 1181831275.9029393\n",
      "Iteration 6286, Loss: 1141487165.9289663\n",
      "Iteration 6287, Loss: 1105743198.504395\n",
      "Iteration 6288, Loss: 1105853992.4394617\n",
      "Iteration 6289, Loss: 1400597916.5875912\n",
      "Iteration 6290, Loss: 1102751808.2588189\n",
      "Iteration 6291, Loss: 2050449443.4703305\n",
      "Iteration 6292, Loss: 1876716394.477248\n",
      "Iteration 6293, Loss: 1666820169.8877473\n",
      "Iteration 6294, Loss: 1576687756.7456236\n",
      "Iteration 6295, Loss: 1194605753.6997316\n",
      "Iteration 6296, Loss: 1325884500.0472908\n",
      "Iteration 6297, Loss: 1302974051.6263309\n",
      "Iteration 6298, Loss: 1076140428.1131837\n",
      "Iteration 6299, Loss: 3079217444.3508244\n",
      "Iteration 6300, Loss: 2699010150.3693566\n",
      "Iteration 6301, Loss: 3008461589.8647356\n",
      "Iteration 6302, Loss: 1074270734.3167107\n",
      "Iteration 6303, Loss: 1311494782.893968\n",
      "Iteration 6304, Loss: 1254307915.16107\n",
      "Iteration 6305, Loss: 1185192351.6902926\n",
      "Iteration 6306, Loss: 1207281096.822756\n",
      "Iteration 6307, Loss: 1134232309.86575\n",
      "Iteration 6308, Loss: 1237981244.0751338\n",
      "Iteration 6309, Loss: 1224919118.9250357\n",
      "Iteration 6310, Loss: 1210072305.9857948\n",
      "Iteration 6311, Loss: 1321899372.4179368\n",
      "Iteration 6312, Loss: 1234494186.408894\n",
      "Iteration 6313, Loss: 1157436570.8418043\n",
      "Iteration 6314, Loss: 1208374262.7148693\n",
      "Iteration 6315, Loss: 1208526708.4106805\n",
      "Iteration 6316, Loss: 1305937692.4000363\n",
      "Iteration 6317, Loss: 1150895355.5038836\n",
      "Iteration 6318, Loss: 1179398895.4528525\n",
      "Iteration 6319, Loss: 1182438377.5639727\n",
      "Iteration 6320, Loss: 1169338357.0816426\n",
      "Iteration 6321, Loss: 1278186195.5867472\n",
      "Iteration 6322, Loss: 1197403632.7926552\n",
      "Iteration 6323, Loss: 1178536803.2014496\n",
      "Iteration 6324, Loss: 1207619457.9955816\n",
      "Iteration 6325, Loss: 1160234241.031939\n",
      "Iteration 6326, Loss: 1162269428.0115478\n",
      "Iteration 6327, Loss: 1209435351.9312022\n",
      "Iteration 6328, Loss: 1276868467.9516673\n",
      "Iteration 6329, Loss: 1068214796.6927438\n",
      "Iteration 6330, Loss: 1547874054.8620524\n",
      "Iteration 6331, Loss: 1328655106.3626041\n",
      "Iteration 6332, Loss: 1255016906.900092\n",
      "Iteration 6333, Loss: 1269583720.5062187\n",
      "Iteration 6334, Loss: 1294141436.887461\n",
      "Iteration 6335, Loss: 1330977389.128826\n",
      "Iteration 6336, Loss: 1118569079.4482844\n",
      "Iteration 6337, Loss: 1099442634.7178555\n",
      "Iteration 6338, Loss: 1102399592.0862024\n",
      "Iteration 6339, Loss: 1215835744.5077808\n",
      "Iteration 6340, Loss: 1205847453.212128\n",
      "Iteration 6341, Loss: 1200011848.095473\n",
      "Iteration 6342, Loss: 1186902888.9513583\n",
      "Iteration 6343, Loss: 1211068745.9830065\n",
      "Iteration 6344, Loss: 1200065074.4531312\n",
      "Iteration 6345, Loss: 1301433531.0257897\n",
      "Iteration 6346, Loss: 1338579299.3555624\n",
      "Iteration 6347, Loss: 1377064494.5125847\n",
      "Iteration 6348, Loss: 1092990070.5359733\n",
      "Iteration 6349, Loss: 1062496408.4984\n",
      "Iteration 6350, Loss: 1339295739.8546147\n",
      "Iteration 6351, Loss: 1163056377.3796816\n",
      "Iteration 6352, Loss: 1199089802.6080883\n",
      "Iteration 6353, Loss: 1153407212.480748\n",
      "Iteration 6354, Loss: 1152216258.3482943\n",
      "Iteration 6355, Loss: 1262702416.7474751\n",
      "Iteration 6356, Loss: 1136367945.646644\n",
      "Iteration 6357, Loss: 1119085796.492438\n",
      "Iteration 6358, Loss: 1235253574.5548596\n",
      "Iteration 6359, Loss: 1577520492.1721687\n",
      "Iteration 6360, Loss: 1354722390.8983078\n",
      "Iteration 6361, Loss: 1344127717.8430438\n",
      "Iteration 6362, Loss: 1217578614.9595037\n",
      "Iteration 6363, Loss: 1131687277.1008456\n",
      "Iteration 6364, Loss: 1134052381.9560053\n",
      "Iteration 6365, Loss: 1954843099.7790918\n",
      "Iteration 6366, Loss: 1089709775.3312588\n",
      "Iteration 6367, Loss: 1078602148.5169396\n",
      "Iteration 6368, Loss: 1064486271.5998739\n",
      "Iteration 6369, Loss: 3011608399.0385594\n",
      "Iteration 6370, Loss: 1147497813.1931334\n",
      "Iteration 6371, Loss: 1144010097.3489203\n",
      "Iteration 6372, Loss: 1784854970.5635538\n",
      "Iteration 6373, Loss: 1671533791.193775\n",
      "Iteration 6374, Loss: 1463089555.1557744\n",
      "Iteration 6375, Loss: 1312549677.753555\n",
      "Iteration 6376, Loss: 1210593078.5285728\n",
      "Iteration 6377, Loss: 1883009295.5338647\n",
      "Iteration 6378, Loss: 1674053696.0480525\n",
      "Iteration 6379, Loss: 1511506990.2822871\n",
      "Iteration 6380, Loss: 1060349192.6179975\n",
      "Iteration 6381, Loss: 1093663768.4215105\n",
      "Iteration 6382, Loss: 1261606040.3857043\n",
      "Iteration 6383, Loss: 1291099047.6078649\n",
      "Iteration 6384, Loss: 1440517068.1073792\n",
      "Iteration 6385, Loss: 1375929361.6396668\n",
      "Iteration 6386, Loss: 1149170316.0292723\n",
      "Iteration 6387, Loss: 1206356457.1285472\n",
      "Iteration 6388, Loss: 1196157588.2974718\n",
      "Iteration 6389, Loss: 1141552840.5424078\n",
      "Iteration 6390, Loss: 1134437936.7831528\n",
      "Iteration 6391, Loss: 1246150603.7945595\n",
      "Iteration 6392, Loss: 1258462778.2980752\n",
      "Iteration 6393, Loss: 1180374803.8967853\n",
      "Iteration 6394, Loss: 1280150731.614483\n",
      "Iteration 6395, Loss: 1305942233.4662166\n",
      "Iteration 6396, Loss: 1185214437.8695877\n",
      "Iteration 6397, Loss: 1281812679.4913785\n",
      "Iteration 6398, Loss: 1123898112.724921\n",
      "Iteration 6399, Loss: 1990179339.106915\n",
      "Iteration 6400, Loss: 1821500752.1296804\n",
      "Iteration 6401, Loss: 1492445876.4504223\n",
      "Iteration 6402, Loss: 1467497426.214885\n",
      "Iteration 6403, Loss: 1112609755.1547248\n",
      "Iteration 6404, Loss: 2077669971.7713275\n",
      "Iteration 6405, Loss: 1546101122.1045165\n",
      "Iteration 6406, Loss: 1486064611.2877653\n",
      "Iteration 6407, Loss: 1079283338.8097835\n",
      "Iteration 6408, Loss: 1147839679.5193503\n",
      "Iteration 6409, Loss: 1198584200.190756\n",
      "Iteration 6410, Loss: 1296827827.3317924\n",
      "Iteration 6411, Loss: 1169730471.0099401\n",
      "Iteration 6412, Loss: 1138554196.3722079\n",
      "Iteration 6413, Loss: 1136392725.7081487\n",
      "Iteration 6414, Loss: 1186975774.5315\n",
      "Iteration 6415, Loss: 1172592801.3283634\n",
      "Iteration 6416, Loss: 1111814646.5679312\n",
      "Iteration 6417, Loss: 1132619679.3828528\n",
      "Iteration 6418, Loss: 1293213990.7424014\n",
      "Iteration 6419, Loss: 1238657774.4256024\n",
      "Iteration 6420, Loss: 1060844674.8889257\n",
      "Iteration 6421, Loss: 1060196111.5522035\n",
      "Iteration 6422, Loss: 2833720758.8674564\n",
      "Iteration 6423, Loss: 1395763182.3813818\n",
      "Iteration 6424, Loss: 1369753202.1115892\n",
      "Iteration 6425, Loss: 1209741735.477476\n",
      "Iteration 6426, Loss: 1657897750.3828444\n",
      "Iteration 6427, Loss: 1085536102.7519085\n",
      "Iteration 6428, Loss: 1092891787.2577014\n",
      "Iteration 6429, Loss: 1098749149.6931827\n",
      "Iteration 6430, Loss: 1312539899.885375\n",
      "Iteration 6431, Loss: 1289812734.4346578\n",
      "Iteration 6432, Loss: 1272151721.9886332\n",
      "Iteration 6433, Loss: 1307844914.8072917\n",
      "Iteration 6434, Loss: 1324900855.3947487\n",
      "Iteration 6435, Loss: 1077947404.166595\n",
      "Iteration 6436, Loss: 1062154534.8007352\n",
      "Iteration 6437, Loss: 1072875057.2738606\n",
      "Iteration 6438, Loss: 1114328672.9161792\n",
      "Iteration 6439, Loss: 1093601035.7916522\n",
      "Iteration 6440, Loss: 1111109105.4941819\n",
      "Iteration 6441, Loss: 1092995662.6726332\n",
      "Iteration 6442, Loss: 1209220314.7818894\n",
      "Iteration 6443, Loss: 1197948708.5107284\n",
      "Iteration 6444, Loss: 1189942632.8247952\n",
      "Iteration 6445, Loss: 1222213297.4933484\n",
      "Iteration 6446, Loss: 1210662184.9399633\n",
      "Iteration 6447, Loss: 1118542185.233605\n",
      "Iteration 6448, Loss: 1140551510.272607\n",
      "Iteration 6449, Loss: 1250964175.2512856\n",
      "Iteration 6450, Loss: 1327883934.008883\n",
      "Iteration 6451, Loss: 1339687847.054816\n",
      "Iteration 6452, Loss: 1351407949.4322789\n",
      "Iteration 6453, Loss: 1354917948.4016166\n",
      "Iteration 6454, Loss: 1154490003.6830397\n",
      "Iteration 6455, Loss: 1183777167.242302\n",
      "Iteration 6456, Loss: 1227020924.656931\n",
      "Iteration 6457, Loss: 1261078949.0569863\n",
      "Iteration 6458, Loss: 1241870441.045751\n",
      "Iteration 6459, Loss: 1205256072.0335677\n",
      "Iteration 6460, Loss: 1176917868.6471148\n",
      "Iteration 6461, Loss: 1272984433.641368\n",
      "Iteration 6462, Loss: 1266215312.5474267\n",
      "Iteration 6463, Loss: 1248971541.1967878\n",
      "Iteration 6464, Loss: 1189055760.4150672\n",
      "Iteration 6465, Loss: 1181121865.48057\n",
      "Iteration 6466, Loss: 1209684039.5996788\n",
      "Iteration 6467, Loss: 1198897156.2786336\n",
      "Iteration 6468, Loss: 1266227688.2355447\n",
      "Iteration 6469, Loss: 1336661208.3934665\n",
      "Iteration 6470, Loss: 1311280389.56435\n",
      "Iteration 6471, Loss: 1332908337.5055308\n",
      "Iteration 6472, Loss: 1054050589.5430026\n",
      "Iteration 6473, Loss: 1067852677.8540438\n",
      "Iteration 6474, Loss: 1056982919.1306018\n",
      "Iteration 6475, Loss: 1058668028.0651459\n",
      "Iteration 6476, Loss: 1060130522.3502297\n",
      "Iteration 6477, Loss: 1480668940.5165162\n",
      "Iteration 6478, Loss: 1165004193.7139177\n",
      "Iteration 6479, Loss: 1172859329.592831\n",
      "Iteration 6480, Loss: 1272412178.2645974\n",
      "Iteration 6481, Loss: 1250689420.403922\n",
      "Iteration 6482, Loss: 1232778351.8855364\n",
      "Iteration 6483, Loss: 1264571076.2922752\n",
      "Iteration 6484, Loss: 1244065986.0779305\n",
      "Iteration 6485, Loss: 1198096972.0393982\n",
      "Iteration 6486, Loss: 1284442484.497001\n",
      "Iteration 6487, Loss: 1191049226.4862165\n",
      "Iteration 6488, Loss: 1285897514.6346262\n",
      "Iteration 6489, Loss: 1175290474.2338495\n",
      "Iteration 6490, Loss: 1105363411.7074795\n",
      "Iteration 6491, Loss: 1099434222.6922448\n",
      "Iteration 6492, Loss: 1104174919.6613111\n",
      "Iteration 6493, Loss: 1097787174.8478456\n",
      "Iteration 6494, Loss: 1097529998.2752573\n",
      "Iteration 6495, Loss: 1254968392.7017455\n",
      "Iteration 6496, Loss: 1276069119.419011\n",
      "Iteration 6497, Loss: 1064121584.255213\n",
      "Iteration 6498, Loss: 1065720841.3615654\n",
      "Iteration 6499, Loss: 2590150824.6710773\n",
      "Iteration 6500, Loss: 1897210543.2794495\n",
      "Iteration 6501, Loss: 1102872135.5592248\n",
      "Iteration 6502, Loss: 1385709429.8302352\n",
      "Iteration 6503, Loss: 1070697874.2131633\n",
      "Iteration 6504, Loss: 1646346051.5611377\n",
      "Iteration 6505, Loss: 1608689207.2487023\n",
      "Iteration 6506, Loss: 1549274288.9624329\n",
      "Iteration 6507, Loss: 1500387259.8555813\n",
      "Iteration 6508, Loss: 1340809663.5602353\n",
      "Iteration 6509, Loss: 1050876538.2463027\n",
      "Iteration 6510, Loss: 1051372816.9795785\n",
      "Iteration 6511, Loss: 3607069237.934632\n",
      "Iteration 6512, Loss: 4909206618.6078615\n",
      "Iteration 6513, Loss: 4446302524.991011\n",
      "Iteration 6514, Loss: 3616381093.4272304\n",
      "Iteration 6515, Loss: 3315066755.030208\n",
      "Iteration 6516, Loss: 31767783890.889595\n",
      "Iteration 6517, Loss: 19116598213.099823\n",
      "Iteration 6518, Loss: 78131282675.88492\n",
      "Iteration 6519, Loss: 22044607403.050987\n",
      "Iteration 6520, Loss: 21981165276.6548\n",
      "Iteration 6521, Loss: 1597078217.8456247\n",
      "Iteration 6522, Loss: 1372752973.9159117\n",
      "Iteration 6523, Loss: 1319423959.684938\n",
      "Iteration 6524, Loss: 1169777984.837327\n",
      "Iteration 6525, Loss: 1193587558.309613\n",
      "Iteration 6526, Loss: 1272285069.1061714\n",
      "Iteration 6527, Loss: 1122889415.1568885\n",
      "Iteration 6528, Loss: 1092717059.0885868\n",
      "Iteration 6529, Loss: 1077823679.543928\n",
      "Iteration 6530, Loss: 1072182388.2003262\n",
      "Iteration 6531, Loss: 1116775898.0286531\n",
      "Iteration 6532, Loss: 1320634137.7605405\n",
      "Iteration 6533, Loss: 1086256952.9149234\n",
      "Iteration 6534, Loss: 1129175815.504166\n",
      "Iteration 6535, Loss: 1178936903.9050434\n",
      "Iteration 6536, Loss: 1041732708.2323493\n",
      "Iteration 6537, Loss: 2584858747.511912\n",
      "Iteration 6538, Loss: 1327510128.9820087\n",
      "Iteration 6539, Loss: 1035408153.4533254\n",
      "Iteration 6540, Loss: 1068777750.8506756\n",
      "Iteration 6541, Loss: 1069575531.2727308\n",
      "Iteration 6542, Loss: 1064948156.6239647\n",
      "Iteration 6543, Loss: 1106623755.576324\n",
      "Iteration 6544, Loss: 1085831935.747971\n",
      "Iteration 6545, Loss: 1358343419.977223\n",
      "Iteration 6546, Loss: 1327407801.157969\n",
      "Iteration 6547, Loss: 1349389967.5669692\n",
      "Iteration 6548, Loss: 1308297182.0655935\n",
      "Iteration 6549, Loss: 1311028353.5593703\n",
      "Iteration 6550, Loss: 1105153237.6923578\n",
      "Iteration 6551, Loss: 1173729932.9794888\n",
      "Iteration 6552, Loss: 1272796824.572236\n",
      "Iteration 6553, Loss: 1309218596.3578346\n",
      "Iteration 6554, Loss: 1197835057.5859482\n",
      "Iteration 6555, Loss: 1274884374.5289145\n",
      "Iteration 6556, Loss: 1110310816.6638372\n",
      "Iteration 6557, Loss: 1105644524.8937905\n",
      "Iteration 6558, Loss: 1987890300.1099906\n",
      "Iteration 6559, Loss: 1810382629.431155\n",
      "Iteration 6560, Loss: 1334019456.2109656\n",
      "Iteration 6561, Loss: 1035198455.0870609\n",
      "Iteration 6562, Loss: 1040770726.9816726\n",
      "Iteration 6563, Loss: 1048064053.2163063\n",
      "Iteration 6564, Loss: 2481888998.1119947\n",
      "Iteration 6565, Loss: 1038493840.6192129\n",
      "Iteration 6566, Loss: 1473915606.5413094\n",
      "Iteration 6567, Loss: 1323862476.2517822\n",
      "Iteration 6568, Loss: 1306620684.8769984\n",
      "Iteration 6569, Loss: 1347145471.1711733\n",
      "Iteration 6570, Loss: 1182197708.019112\n",
      "Iteration 6571, Loss: 1120027950.7176628\n",
      "Iteration 6572, Loss: 1118266063.5002873\n",
      "Iteration 6573, Loss: 1222088564.6558611\n",
      "Iteration 6574, Loss: 1123272015.6957555\n",
      "Iteration 6575, Loss: 1236926847.7846801\n",
      "Iteration 6576, Loss: 1279387396.1529088\n",
      "Iteration 6577, Loss: 1265797140.73455\n",
      "Iteration 6578, Loss: 1048063021.019679\n",
      "Iteration 6579, Loss: 1292056129.1831753\n",
      "Iteration 6580, Loss: 1162681104.2544224\n",
      "Iteration 6581, Loss: 1151984940.414346\n",
      "Iteration 6582, Loss: 1143747423.0085618\n",
      "Iteration 6583, Loss: 1107850713.24193\n",
      "Iteration 6584, Loss: 1214097750.9220397\n",
      "Iteration 6585, Loss: 1206241995.8220387\n",
      "Iteration 6586, Loss: 1267576240.1748266\n",
      "Iteration 6587, Loss: 1049960814.1677943\n",
      "Iteration 6588, Loss: 1041967769.8206725\n",
      "Iteration 6589, Loss: 1471486035.030051\n",
      "Iteration 6590, Loss: 1267602586.2131445\n",
      "Iteration 6591, Loss: 1094505732.3561463\n",
      "Iteration 6592, Loss: 2082096490.375582\n",
      "Iteration 6593, Loss: 1935284324.7637827\n",
      "Iteration 6594, Loss: 1067437518.7526133\n",
      "Iteration 6595, Loss: 1068969364.2714579\n",
      "Iteration 6596, Loss: 5056098749.54314\n",
      "Iteration 6597, Loss: 3898698428.3194947\n",
      "Iteration 6598, Loss: 1893400603.8741462\n",
      "Iteration 6599, Loss: 1366169753.6183712\n",
      "Iteration 6600, Loss: 1155846905.7289927\n",
      "Iteration 6601, Loss: 1059382804.6417816\n",
      "Iteration 6602, Loss: 1156662963.2342408\n",
      "Iteration 6603, Loss: 1186518645.5306394\n",
      "Iteration 6604, Loss: 1172027562.2694771\n",
      "Iteration 6605, Loss: 1163275049.972575\n",
      "Iteration 6606, Loss: 1148008118.7877085\n",
      "Iteration 6607, Loss: 1137451183.530612\n",
      "Iteration 6608, Loss: 1328398517.492214\n",
      "Iteration 6609, Loss: 1362778495.3653815\n",
      "Iteration 6610, Loss: 1345753411.5564644\n",
      "Iteration 6611, Loss: 1325318873.8814728\n",
      "Iteration 6612, Loss: 1361493031.7044342\n",
      "Iteration 6613, Loss: 1317768787.0460756\n",
      "Iteration 6614, Loss: 1362130041.1947727\n",
      "Iteration 6615, Loss: 1325456172.248136\n",
      "Iteration 6616, Loss: 1205335449.3431547\n",
      "Iteration 6617, Loss: 1166724210.8205988\n",
      "Iteration 6618, Loss: 1196806670.090718\n",
      "Iteration 6619, Loss: 1317966647.034718\n",
      "Iteration 6620, Loss: 1286037769.501116\n",
      "Iteration 6621, Loss: 1259645258.577126\n",
      "Iteration 6622, Loss: 1237988132.900597\n",
      "Iteration 6623, Loss: 1310843529.4736416\n",
      "Iteration 6624, Loss: 1060988213.9170614\n",
      "Iteration 6625, Loss: 1071896770.1096247\n",
      "Iteration 6626, Loss: 1121443102.7954936\n",
      "Iteration 6627, Loss: 1111600062.6794655\n",
      "Iteration 6628, Loss: 1112888999.67727\n",
      "Iteration 6629, Loss: 1302331871.4369726\n",
      "Iteration 6630, Loss: 1195130329.6978655\n",
      "Iteration 6631, Loss: 1143969326.369271\n",
      "Iteration 6632, Loss: 1181132536.4995847\n",
      "Iteration 6633, Loss: 1168313251.360528\n",
      "Iteration 6634, Loss: 1212308855.6113858\n",
      "Iteration 6635, Loss: 1271684863.3525684\n",
      "Iteration 6636, Loss: 1250552420.3059967\n",
      "Iteration 6637, Loss: 1050807189.8892059\n",
      "Iteration 6638, Loss: 3149361379.58419\n",
      "Iteration 6639, Loss: 2036184310.8613002\n",
      "Iteration 6640, Loss: 1069946329.7097354\n",
      "Iteration 6641, Loss: 1170936787.676134\n",
      "Iteration 6642, Loss: 1759735653.2814512\n",
      "Iteration 6643, Loss: 1281534093.085014\n",
      "Iteration 6644, Loss: 1200536550.2081249\n",
      "Iteration 6645, Loss: 1049056994.3437467\n",
      "Iteration 6646, Loss: 1049566521.542367\n",
      "Iteration 6647, Loss: 1377484043.328776\n",
      "Iteration 6648, Loss: 1293080683.5892673\n",
      "Iteration 6649, Loss: 1312270117.7503066\n",
      "Iteration 6650, Loss: 1063478701.2337787\n",
      "Iteration 6651, Loss: 1171475516.1809645\n",
      "Iteration 6652, Loss: 1202093458.5900786\n",
      "Iteration 6653, Loss: 1321227834.5064385\n",
      "Iteration 6654, Loss: 1131884207.4090583\n",
      "Iteration 6655, Loss: 1126997411.5459046\n",
      "Iteration 6656, Loss: 1340359618.1520016\n",
      "Iteration 6657, Loss: 1051223157.4307122\n",
      "Iteration 6658, Loss: 2829959291.7265453\n",
      "Iteration 6659, Loss: 2687945107.9032764\n",
      "Iteration 6660, Loss: 1814030816.0628927\n",
      "Iteration 6661, Loss: 1685628407.6690788\n",
      "Iteration 6662, Loss: 1205244810.5702095\n",
      "Iteration 6663, Loss: 1056049734.4099076\n",
      "Iteration 6664, Loss: 1056536699.981355\n",
      "Iteration 6665, Loss: 1427604723.2071674\n",
      "Iteration 6666, Loss: 1409980449.2181208\n",
      "Iteration 6667, Loss: 1378859778.020477\n",
      "Iteration 6668, Loss: 1302683580.2255702\n",
      "Iteration 6669, Loss: 1277114780.024111\n",
      "Iteration 6670, Loss: 1167703166.1857657\n",
      "Iteration 6671, Loss: 1269195546.3140075\n",
      "Iteration 6672, Loss: 1303497335.592161\n",
      "Iteration 6673, Loss: 1265700061.4453511\n",
      "Iteration 6674, Loss: 1259694350.6651678\n",
      "Iteration 6675, Loss: 1119733091.0821106\n",
      "Iteration 6676, Loss: 1168775593.2463572\n",
      "Iteration 6677, Loss: 1310646766.9175687\n",
      "Iteration 6678, Loss: 1329275203.1883385\n",
      "Iteration 6679, Loss: 1193723921.240734\n",
      "Iteration 6680, Loss: 1292198569.6009986\n",
      "Iteration 6681, Loss: 1327746945.7035363\n",
      "Iteration 6682, Loss: 1214888054.0873885\n",
      "Iteration 6683, Loss: 1626355293.797596\n",
      "Iteration 6684, Loss: 1272506465.6955254\n",
      "Iteration 6685, Loss: 1055622964.5719362\n",
      "Iteration 6686, Loss: 1391967866.2627707\n",
      "Iteration 6687, Loss: 1358111245.1792696\n",
      "Iteration 6688, Loss: 1059077889.5142324\n",
      "Iteration 6689, Loss: 1257939218.2041793\n",
      "Iteration 6690, Loss: 1163585369.6512907\n",
      "Iteration 6691, Loss: 1150781530.4249108\n",
      "Iteration 6692, Loss: 1266238151.5429316\n",
      "Iteration 6693, Loss: 1258922902.2081838\n",
      "Iteration 6694, Loss: 1237372043.1177864\n",
      "Iteration 6695, Loss: 1259232816.7234292\n",
      "Iteration 6696, Loss: 1164786587.6542726\n",
      "Iteration 6697, Loss: 1062011226.7269245\n",
      "Iteration 6698, Loss: 2032679486.4834082\n",
      "Iteration 6699, Loss: 1439335819.0866039\n",
      "Iteration 6700, Loss: 1146473121.9387515\n",
      "Iteration 6701, Loss: 1596436609.1915107\n",
      "Iteration 6702, Loss: 1226522073.7369018\n",
      "Iteration 6703, Loss: 1054126221.2724843\n",
      "Iteration 6704, Loss: 1243434694.8191118\n",
      "Iteration 6705, Loss: 1226284424.4405208\n",
      "Iteration 6706, Loss: 1309611291.557023\n",
      "Iteration 6707, Loss: 1184515161.1457796\n",
      "Iteration 6708, Loss: 1186823147.8665063\n",
      "Iteration 6709, Loss: 1299725970.616081\n",
      "Iteration 6710, Loss: 1228401671.1042616\n",
      "Iteration 6711, Loss: 1133474215.6645472\n",
      "Iteration 6712, Loss: 1212504260.4144373\n",
      "Iteration 6713, Loss: 1110135864.3659537\n",
      "Iteration 6714, Loss: 1179598926.2795448\n",
      "Iteration 6715, Loss: 1324472918.4467711\n",
      "Iteration 6716, Loss: 1296763096.3977427\n",
      "Iteration 6717, Loss: 1131795239.778822\n",
      "Iteration 6718, Loss: 1320961381.4330585\n",
      "Iteration 6719, Loss: 1272806485.9692805\n",
      "Iteration 6720, Loss: 1320662887.6547399\n",
      "Iteration 6721, Loss: 1336001065.7829032\n",
      "Iteration 6722, Loss: 1310733569.4484544\n",
      "Iteration 6723, Loss: 1285307516.3707833\n",
      "Iteration 6724, Loss: 1122864953.7025216\n",
      "Iteration 6725, Loss: 1227310586.6653864\n",
      "Iteration 6726, Loss: 1166827126.014117\n",
      "Iteration 6727, Loss: 1132008987.4106731\n",
      "Iteration 6728, Loss: 1170782168.6359816\n",
      "Iteration 6729, Loss: 1145476276.5083456\n",
      "Iteration 6730, Loss: 1338948796.6741805\n",
      "Iteration 6731, Loss: 1212708917.720539\n",
      "Iteration 6732, Loss: 1218485063.8755045\n",
      "Iteration 6733, Loss: 1298887417.525114\n",
      "Iteration 6734, Loss: 1424685433.8070092\n",
      "Iteration 6735, Loss: 1379241224.9741359\n",
      "Iteration 6736, Loss: 1301610802.7249424\n",
      "Iteration 6737, Loss: 1421322066.6593263\n",
      "Iteration 6738, Loss: 1321365796.2198894\n",
      "Iteration 6739, Loss: 1373698780.0353472\n",
      "Iteration 6740, Loss: 1121207352.80741\n",
      "Iteration 6741, Loss: 1122197416.4231067\n",
      "Iteration 6742, Loss: 1231794146.68296\n",
      "Iteration 6743, Loss: 1267702563.009779\n",
      "Iteration 6744, Loss: 1335599138.009498\n",
      "Iteration 6745, Loss: 1342798387.9162965\n",
      "Iteration 6746, Loss: 1070457559.178311\n",
      "Iteration 6747, Loss: 1180741485.9884727\n",
      "Iteration 6748, Loss: 1167313147.3006096\n",
      "Iteration 6749, Loss: 1235106733.5714579\n",
      "Iteration 6750, Loss: 1193881833.9509099\n",
      "Iteration 6751, Loss: 1236491990.318841\n",
      "Iteration 6752, Loss: 1165577618.2062426\n",
      "Iteration 6753, Loss: 1278007628.8366299\n",
      "Iteration 6754, Loss: 1253108509.8098962\n",
      "Iteration 6755, Loss: 1257823435.4771383\n",
      "Iteration 6756, Loss: 1333079148.1973884\n",
      "Iteration 6757, Loss: 1091396241.7309813\n",
      "Iteration 6758, Loss: 1192795799.8415875\n",
      "Iteration 6759, Loss: 1223693591.790115\n",
      "Iteration 6760, Loss: 1169700070.4140475\n",
      "Iteration 6761, Loss: 1130151367.7348135\n",
      "Iteration 6762, Loss: 1075876497.0549765\n",
      "Iteration 6763, Loss: 1269700814.2189662\n",
      "Iteration 6764, Loss: 1251082442.5653489\n",
      "Iteration 6765, Loss: 1124270691.0903845\n",
      "Iteration 6766, Loss: 1208912724.8025303\n",
      "Iteration 6767, Loss: 1197117725.7779248\n",
      "Iteration 6768, Loss: 1133134646.4311016\n",
      "Iteration 6769, Loss: 1189829203.0809324\n",
      "Iteration 6770, Loss: 1162543606.1346605\n",
      "Iteration 6771, Loss: 1153436900.987576\n",
      "Iteration 6772, Loss: 1549852160.3190656\n",
      "Iteration 6773, Loss: 1505571036.3472428\n",
      "Iteration 6774, Loss: 1077379341.4505684\n",
      "Iteration 6775, Loss: 1080201641.8515563\n",
      "Iteration 6776, Loss: 1523713026.368106\n",
      "Iteration 6777, Loss: 1267197485.4144151\n",
      "Iteration 6778, Loss: 1249163595.4541898\n",
      "Iteration 6779, Loss: 1263742040.5884476\n",
      "Iteration 6780, Loss: 1246371018.4364593\n",
      "Iteration 6781, Loss: 1130156007.6059966\n",
      "Iteration 6782, Loss: 1282343263.119293\n",
      "Iteration 6783, Loss: 1301370024.898794\n",
      "Iteration 6784, Loss: 1363311395.77177\n",
      "Iteration 6785, Loss: 1337314942.1590414\n",
      "Iteration 6786, Loss: 1374923071.0430446\n",
      "Iteration 6787, Loss: 1240014586.0296175\n",
      "Iteration 6788, Loss: 1147278590.780676\n",
      "Iteration 6789, Loss: 1217714535.6848767\n",
      "Iteration 6790, Loss: 1205854819.4907382\n",
      "Iteration 6791, Loss: 1319185506.292175\n",
      "Iteration 6792, Loss: 1058406860.5638366\n",
      "Iteration 6793, Loss: 1096342325.6534867\n",
      "Iteration 6794, Loss: 1148677339.7895925\n",
      "Iteration 6795, Loss: 1205984431.533643\n",
      "Iteration 6796, Loss: 1651735024.9926932\n",
      "Iteration 6797, Loss: 1555958465.1624901\n",
      "Iteration 6798, Loss: 1134220882.4363074\n",
      "Iteration 6799, Loss: 1682214523.4205396\n",
      "Iteration 6800, Loss: 1435717636.982669\n",
      "Iteration 6801, Loss: 1115897110.8716435\n",
      "Iteration 6802, Loss: 1106234456.920232\n",
      "Iteration 6803, Loss: 1375958083.13828\n",
      "Iteration 6804, Loss: 1102378706.398628\n",
      "Iteration 6805, Loss: 1065919946.5405811\n",
      "Iteration 6806, Loss: 2078776289.2977462\n",
      "Iteration 6807, Loss: 1111464481.9116726\n",
      "Iteration 6808, Loss: 1298981328.932638\n",
      "Iteration 6809, Loss: 1122880374.5891645\n",
      "Iteration 6810, Loss: 1202299708.4388268\n",
      "Iteration 6811, Loss: 1296247709.3162334\n",
      "Iteration 6812, Loss: 1276406826.573266\n",
      "Iteration 6813, Loss: 1133448097.4885454\n",
      "Iteration 6814, Loss: 1117497368.078121\n",
      "Iteration 6815, Loss: 1096570892.4971385\n",
      "Iteration 6816, Loss: 1103029828.1942086\n",
      "Iteration 6817, Loss: 1105526109.559696\n",
      "Iteration 6818, Loss: 1108642603.8403437\n",
      "Iteration 6819, Loss: 1232288052.230557\n",
      "Iteration 6820, Loss: 1170574680.5124419\n",
      "Iteration 6821, Loss: 1171437924.1105227\n",
      "Iteration 6822, Loss: 1283425147.2662945\n",
      "Iteration 6823, Loss: 1348288463.0537431\n",
      "Iteration 6824, Loss: 1278937974.9346173\n",
      "Iteration 6825, Loss: 1171492433.0987127\n",
      "Iteration 6826, Loss: 1199652701.6526976\n",
      "Iteration 6827, Loss: 1294754140.2648168\n",
      "Iteration 6828, Loss: 1339013424.5404656\n",
      "Iteration 6829, Loss: 1073478892.2548747\n",
      "Iteration 6830, Loss: 1116864217.05096\n",
      "Iteration 6831, Loss: 1118918912.6721342\n",
      "Iteration 6832, Loss: 1114273604.475107\n",
      "Iteration 6833, Loss: 1164027693.308282\n",
      "Iteration 6834, Loss: 1160555412.3718677\n",
      "Iteration 6835, Loss: 1267217177.7848878\n",
      "Iteration 6836, Loss: 1156390763.3587015\n",
      "Iteration 6837, Loss: 1273041141.721369\n",
      "Iteration 6838, Loss: 1146102392.425182\n",
      "Iteration 6839, Loss: 1204091058.2370613\n",
      "Iteration 6840, Loss: 1330605509.478933\n",
      "Iteration 6841, Loss: 1093214192.1525936\n",
      "Iteration 6842, Loss: 1209743716.178387\n",
      "Iteration 6843, Loss: 1265012085.2384572\n",
      "Iteration 6844, Loss: 1246222705.4917247\n",
      "Iteration 6845, Loss: 1089174740.598142\n",
      "Iteration 6846, Loss: 1084191636.919528\n",
      "Iteration 6847, Loss: 1085239921.9932134\n",
      "Iteration 6848, Loss: 1703661513.3046365\n",
      "Iteration 6849, Loss: 1625301501.8213644\n",
      "Iteration 6850, Loss: 1057578773.668292\n",
      "Iteration 6851, Loss: 2957287103.2120976\n",
      "Iteration 6852, Loss: 1351380282.7678149\n",
      "Iteration 6853, Loss: 2106389899.8590262\n",
      "Iteration 6854, Loss: 1939787535.541791\n",
      "Iteration 6855, Loss: 1113974044.1624274\n",
      "Iteration 6856, Loss: 1067525650.9157906\n",
      "Iteration 6857, Loss: 1211532194.9890158\n",
      "Iteration 6858, Loss: 1119244880.9122407\n",
      "Iteration 6859, Loss: 1156597043.6310222\n",
      "Iteration 6860, Loss: 1843439316.966137\n",
      "Iteration 6861, Loss: 1636988749.5270119\n",
      "Iteration 6862, Loss: 1135064945.2365077\n",
      "Iteration 6863, Loss: 1069718335.6786836\n",
      "Iteration 6864, Loss: 1329714298.2342834\n",
      "Iteration 6865, Loss: 1346776029.3529353\n",
      "Iteration 6866, Loss: 1355295541.0635846\n",
      "Iteration 6867, Loss: 1109764412.5669897\n",
      "Iteration 6868, Loss: 1524900859.5932984\n",
      "Iteration 6869, Loss: 1567900137.280794\n",
      "Iteration 6870, Loss: 1355587586.7998004\n",
      "Iteration 6871, Loss: 1339524664.5951552\n",
      "Iteration 6872, Loss: 1220924863.0838485\n",
      "Iteration 6873, Loss: 1219283912.7718883\n",
      "Iteration 6874, Loss: 1179730995.193572\n",
      "Iteration 6875, Loss: 1287905260.7500198\n",
      "Iteration 6876, Loss: 1307097941.8876145\n",
      "Iteration 6877, Loss: 1234212053.2376347\n",
      "Iteration 6878, Loss: 1221098158.7476137\n",
      "Iteration 6879, Loss: 1154717248.4459076\n",
      "Iteration 6880, Loss: 1172688778.0568228\n",
      "Iteration 6881, Loss: 1184379722.5553122\n",
      "Iteration 6882, Loss: 1242826428.8128452\n",
      "Iteration 6883, Loss: 1229504849.9055011\n",
      "Iteration 6884, Loss: 1213005032.6135752\n",
      "Iteration 6885, Loss: 1158675661.7886693\n",
      "Iteration 6886, Loss: 1151298078.6999996\n",
      "Iteration 6887, Loss: 1106623372.172508\n",
      "Iteration 6888, Loss: 1156633150.3621805\n",
      "Iteration 6889, Loss: 1283651735.654478\n",
      "Iteration 6890, Loss: 1068669352.908459\n",
      "Iteration 6891, Loss: 1321248215.4539647\n",
      "Iteration 6892, Loss: 1299692158.4109066\n",
      "Iteration 6893, Loss: 1293157251.5869026\n",
      "Iteration 6894, Loss: 1139988513.5252204\n",
      "Iteration 6895, Loss: 1227440475.062883\n",
      "Iteration 6896, Loss: 1153287913.6827064\n",
      "Iteration 6897, Loss: 1288344057.377719\n",
      "Iteration 6898, Loss: 1268536453.4388025\n",
      "Iteration 6899, Loss: 1300488852.9223325\n",
      "Iteration 6900, Loss: 1108970215.537308\n",
      "Iteration 6901, Loss: 1129316111.013794\n",
      "Iteration 6902, Loss: 1979938828.4296968\n",
      "Iteration 6903, Loss: 1112875291.883579\n",
      "Iteration 6904, Loss: 1133889134.0853746\n",
      "Iteration 6905, Loss: 1373619857.9035177\n",
      "Iteration 6906, Loss: 1205087141.1988053\n",
      "Iteration 6907, Loss: 1128826911.0115938\n",
      "Iteration 6908, Loss: 1357810679.432661\n",
      "Iteration 6909, Loss: 1328723197.3029702\n",
      "Iteration 6910, Loss: 1303638463.5257876\n",
      "Iteration 6911, Loss: 1341792047.9315612\n",
      "Iteration 6912, Loss: 1073960721.1323402\n",
      "Iteration 6913, Loss: 1910428949.6942077\n",
      "Iteration 6914, Loss: 1694933661.2545955\n",
      "Iteration 6915, Loss: 1078459425.705816\n",
      "Iteration 6916, Loss: 1282827180.229248\n",
      "Iteration 6917, Loss: 1330659962.0065665\n",
      "Iteration 6918, Loss: 1375220558.9108229\n",
      "Iteration 6919, Loss: 1363664373.2050645\n",
      "Iteration 6920, Loss: 1291295807.6721537\n",
      "Iteration 6921, Loss: 1341443102.0927994\n",
      "Iteration 6922, Loss: 1391136746.3491433\n",
      "Iteration 6923, Loss: 1390972677.3056183\n",
      "Iteration 6924, Loss: 1069588361.3154669\n",
      "Iteration 6925, Loss: 1065088584.8144633\n",
      "Iteration 6926, Loss: 1126153275.8767138\n",
      "Iteration 6927, Loss: 1359067448.263631\n",
      "Iteration 6928, Loss: 1326864365.5115628\n",
      "Iteration 6929, Loss: 1137564976.651452\n",
      "Iteration 6930, Loss: 1139743958.6336472\n",
      "Iteration 6931, Loss: 1344856400.77418\n",
      "Iteration 6932, Loss: 1203638693.468368\n",
      "Iteration 6933, Loss: 1298073017.8574739\n",
      "Iteration 6934, Loss: 1278195440.6373985\n",
      "Iteration 6935, Loss: 1258623642.1833477\n",
      "Iteration 6936, Loss: 1080586905.3287654\n",
      "Iteration 6937, Loss: 1226749418.0603154\n",
      "Iteration 6938, Loss: 1313978891.0836554\n",
      "Iteration 6939, Loss: 1140222671.18127\n",
      "Iteration 6940, Loss: 1920315313.2761176\n",
      "Iteration 6941, Loss: 1770140710.868199\n",
      "Iteration 6942, Loss: 1090320139.366983\n",
      "Iteration 6943, Loss: 1202785023.3649883\n",
      "Iteration 6944, Loss: 1195501632.6299577\n",
      "Iteration 6945, Loss: 1168206165.9750776\n",
      "Iteration 6946, Loss: 1071084775.3083853\n",
      "Iteration 6947, Loss: 2542254541.651535\n",
      "Iteration 6948, Loss: 1324916470.0598478\n",
      "Iteration 6949, Loss: 1095841258.3961651\n",
      "Iteration 6950, Loss: 1321780943.8665519\n",
      "Iteration 6951, Loss: 1248540559.2810113\n",
      "Iteration 6952, Loss: 1282061563.8787363\n",
      "Iteration 6953, Loss: 1137541594.4479692\n",
      "Iteration 6954, Loss: 1288008000.4348724\n",
      "Iteration 6955, Loss: 1070674849.9435992\n",
      "Iteration 6956, Loss: 1352520742.4853446\n",
      "Iteration 6957, Loss: 1062285890.7188338\n",
      "Iteration 6958, Loss: 1063959942.5136582\n",
      "Iteration 6959, Loss: 2043891973.8779607\n",
      "Iteration 6960, Loss: 1077523259.291562\n",
      "Iteration 6961, Loss: 1075741698.2842119\n",
      "Iteration 6962, Loss: 1119470073.4240832\n",
      "Iteration 6963, Loss: 1155957138.885659\n",
      "Iteration 6964, Loss: 1152804759.0858397\n",
      "Iteration 6965, Loss: 1166540881.2979095\n",
      "Iteration 6966, Loss: 1313790382.8373504\n",
      "Iteration 6967, Loss: 1288617460.7141914\n",
      "Iteration 6968, Loss: 1348265106.5138507\n",
      "Iteration 6969, Loss: 1065593062.6490343\n",
      "Iteration 6970, Loss: 1067332264.3726263\n",
      "Iteration 6971, Loss: 1066924603.4960005\n",
      "Iteration 6972, Loss: 1211158098.2968283\n",
      "Iteration 6973, Loss: 1290289242.06964\n",
      "Iteration 6974, Loss: 1143410176.4947016\n",
      "Iteration 6975, Loss: 1143264171.9789119\n",
      "Iteration 6976, Loss: 1233177614.9624064\n",
      "Iteration 6977, Loss: 1265285827.4971578\n",
      "Iteration 6978, Loss: 1066221908.0394033\n",
      "Iteration 6979, Loss: 1077139566.6422994\n",
      "Iteration 6980, Loss: 1220934736.9784539\n",
      "Iteration 6981, Loss: 1202902841.9623232\n",
      "Iteration 6982, Loss: 1186714283.1593895\n",
      "Iteration 6983, Loss: 1174931247.9598024\n",
      "Iteration 6984, Loss: 1207030106.8981962\n",
      "Iteration 6985, Loss: 1234983037.5993598\n",
      "Iteration 6986, Loss: 1192035720.7281094\n",
      "Iteration 6987, Loss: 1159059704.4661784\n",
      "Iteration 6988, Loss: 1201755805.5454698\n",
      "Iteration 6989, Loss: 1326503186.323177\n",
      "Iteration 6990, Loss: 1340258113.459825\n",
      "Iteration 6991, Loss: 1239040786.8143334\n",
      "Iteration 6992, Loss: 1259524869.191343\n",
      "Iteration 6993, Loss: 1240418415.2676213\n",
      "Iteration 6994, Loss: 1259274517.4413207\n",
      "Iteration 6995, Loss: 1292613148.6385572\n",
      "Iteration 6996, Loss: 1337039336.412821\n",
      "Iteration 6997, Loss: 1071850473.852523\n",
      "Iteration 6998, Loss: 1458204006.4333794\n",
      "Iteration 6999, Loss: 1116428429.7861595\n",
      "Iteration 7000, Loss: 1286057476.0054102\n",
      "Iteration 7001, Loss: 1261991045.0845373\n",
      "Iteration 7002, Loss: 1309067563.0403018\n",
      "Iteration 7003, Loss: 1219021492.596199\n",
      "Iteration 7004, Loss: 1598881629.7028048\n",
      "Iteration 7005, Loss: 1060771530.5223657\n",
      "Iteration 7006, Loss: 1100030439.9954364\n",
      "Iteration 7007, Loss: 1147085303.6329138\n",
      "Iteration 7008, Loss: 1178438010.8928928\n",
      "Iteration 7009, Loss: 1721719259.217051\n",
      "Iteration 7010, Loss: 1586896666.4794626\n",
      "Iteration 7011, Loss: 1108579963.8333216\n",
      "Iteration 7012, Loss: 1056017190.2339168\n",
      "Iteration 7013, Loss: 1726199019.7887132\n",
      "Iteration 7014, Loss: 1445588688.755472\n",
      "Iteration 7015, Loss: 1082514217.0947404\n",
      "Iteration 7016, Loss: 3787818842.6544952\n",
      "Iteration 7017, Loss: 2438133562.2398334\n",
      "Iteration 7018, Loss: 1053009492.9575111\n",
      "Iteration 7019, Loss: 1251007083.3467288\n",
      "Iteration 7020, Loss: 1071617690.6110326\n",
      "Iteration 7021, Loss: 1068949712.065334\n",
      "Iteration 7022, Loss: 2361249185.573781\n",
      "Iteration 7023, Loss: 1067438496.5002811\n",
      "Iteration 7024, Loss: 1047692367.5934137\n",
      "Iteration 7025, Loss: 1046701136.3345733\n",
      "Iteration 7026, Loss: 1046229699.5149802\n",
      "Iteration 7027, Loss: 1231728711.598585\n",
      "Iteration 7028, Loss: 1248806575.3131833\n",
      "Iteration 7029, Loss: 1167778757.5639164\n",
      "Iteration 7030, Loss: 1197498646.605327\n",
      "Iteration 7031, Loss: 1203258730.8429494\n",
      "Iteration 7032, Loss: 1166523722.5094118\n",
      "Iteration 7033, Loss: 1757775508.5984256\n",
      "Iteration 7034, Loss: 1647086888.1776478\n",
      "Iteration 7035, Loss: 1084227445.9954705\n",
      "Iteration 7036, Loss: 1117485426.8466828\n",
      "Iteration 7037, Loss: 1280136953.7319956\n",
      "Iteration 7038, Loss: 1313242486.9311883\n",
      "Iteration 7039, Loss: 1284914620.921621\n",
      "Iteration 7040, Loss: 1303861151.5996828\n",
      "Iteration 7041, Loss: 1275944444.8385649\n",
      "Iteration 7042, Loss: 1251304712.4611316\n",
      "Iteration 7043, Loss: 1200761182.4324832\n",
      "Iteration 7044, Loss: 1188422296.3820496\n",
      "Iteration 7045, Loss: 1139061526.3554897\n",
      "Iteration 7046, Loss: 1207755068.090438\n",
      "Iteration 7047, Loss: 1243839936.0493436\n",
      "Iteration 7048, Loss: 1162322324.4853425\n",
      "Iteration 7049, Loss: 1472886885.7253757\n",
      "Iteration 7050, Loss: 1066848458.5534829\n",
      "Iteration 7051, Loss: 1071638383.0164417\n",
      "Iteration 7052, Loss: 1068827337.7400824\n",
      "Iteration 7053, Loss: 1287292898.782439\n",
      "Iteration 7054, Loss: 1190341460.3218186\n",
      "Iteration 7055, Loss: 1174694399.2351556\n",
      "Iteration 7056, Loss: 1128633831.5318143\n",
      "Iteration 7057, Loss: 1189680138.5075796\n",
      "Iteration 7058, Loss: 1287859762.2649472\n",
      "Iteration 7059, Loss: 1342939724.1214929\n",
      "Iteration 7060, Loss: 1340225696.4689376\n",
      "Iteration 7061, Loss: 1304188763.9816887\n",
      "Iteration 7062, Loss: 1262209306.502889\n",
      "Iteration 7063, Loss: 1067700206.2020227\n",
      "Iteration 7064, Loss: 1201745515.0882862\n",
      "Iteration 7065, Loss: 1187893243.6122642\n",
      "Iteration 7066, Loss: 1176979680.1263855\n",
      "Iteration 7067, Loss: 1135421378.6799011\n",
      "Iteration 7068, Loss: 1159527453.320027\n",
      "Iteration 7069, Loss: 1155477498.3606474\n",
      "Iteration 7070, Loss: 1176269400.7795968\n",
      "Iteration 7071, Loss: 1102904265.1000514\n",
      "Iteration 7072, Loss: 1383359450.3749986\n",
      "Iteration 7073, Loss: 1339072664.786649\n",
      "Iteration 7074, Loss: 1375180162.9099853\n",
      "Iteration 7075, Loss: 1335753100.4736638\n",
      "Iteration 7076, Loss: 1152210813.1547723\n",
      "Iteration 7077, Loss: 1187376566.9014127\n",
      "Iteration 7078, Loss: 1174938603.6156871\n",
      "Iteration 7079, Loss: 1161086127.1503646\n",
      "Iteration 7080, Loss: 1137213322.9214075\n",
      "Iteration 7081, Loss: 1640263761.241854\n",
      "Iteration 7082, Loss: 1291975790.124735\n",
      "Iteration 7083, Loss: 1280996675.1090765\n",
      "Iteration 7084, Loss: 6533216633.813231\n",
      "Iteration 7085, Loss: 2915320520.7837114\n",
      "Iteration 7086, Loss: 4828551902.153478\n",
      "Iteration 7087, Loss: 3915414767.4434533\n",
      "Iteration 7088, Loss: 3701855273.353161\n",
      "Iteration 7089, Loss: 1111574896.5275962\n",
      "Iteration 7090, Loss: 1054073768.435946\n",
      "Iteration 7091, Loss: 2127732203.5914068\n",
      "Iteration 7092, Loss: 1733224473.8722987\n",
      "Iteration 7093, Loss: 1121272765.850847\n",
      "Iteration 7094, Loss: 1170233530.933073\n",
      "Iteration 7095, Loss: 1196702880.103187\n",
      "Iteration 7096, Loss: 1185987133.6100838\n",
      "Iteration 7097, Loss: 1171997708.402961\n",
      "Iteration 7098, Loss: 1173408915.4087787\n",
      "Iteration 7099, Loss: 1272664728.4977236\n",
      "Iteration 7100, Loss: 1136553369.830637\n",
      "Iteration 7101, Loss: 1097371570.669521\n",
      "Iteration 7102, Loss: 1086545391.7260206\n",
      "Iteration 7103, Loss: 1316454622.8285213\n",
      "Iteration 7104, Loss: 1289204299.1820946\n",
      "Iteration 7105, Loss: 1121497876.1275625\n",
      "Iteration 7106, Loss: 2004100009.1817825\n",
      "Iteration 7107, Loss: 1085989278.7714965\n",
      "Iteration 7108, Loss: 1123648825.2490506\n",
      "Iteration 7109, Loss: 1160479177.3787322\n",
      "Iteration 7110, Loss: 1189519508.382644\n",
      "Iteration 7111, Loss: 1376357100.6743164\n",
      "Iteration 7112, Loss: 1100613522.1717975\n",
      "Iteration 7113, Loss: 2156580070.6179442\n",
      "Iteration 7114, Loss: 1102663598.9986737\n",
      "Iteration 7115, Loss: 1156393455.09298\n",
      "Iteration 7116, Loss: 1128960567.9679563\n",
      "Iteration 7117, Loss: 1171035523.7332456\n",
      "Iteration 7118, Loss: 1229404255.6143687\n",
      "Iteration 7119, Loss: 1215952146.3896484\n",
      "Iteration 7120, Loss: 1281319508.8191504\n",
      "Iteration 7121, Loss: 1308728016.3210843\n",
      "Iteration 7122, Loss: 1098428529.5376103\n",
      "Iteration 7123, Loss: 1260955618.2933717\n",
      "Iteration 7124, Loss: 1243417752.8266196\n",
      "Iteration 7125, Loss: 1228977664.9320464\n",
      "Iteration 7126, Loss: 1266604626.3502998\n",
      "Iteration 7127, Loss: 1336198477.3352797\n",
      "Iteration 7128, Loss: 1304933003.9686637\n",
      "Iteration 7129, Loss: 1232005178.020318\n",
      "Iteration 7130, Loss: 1146271631.325372\n",
      "Iteration 7131, Loss: 1146975645.4276268\n",
      "Iteration 7132, Loss: 1285958477.9765341\n",
      "Iteration 7133, Loss: 1232273300.9071379\n",
      "Iteration 7134, Loss: 1138107876.3109121\n",
      "Iteration 7135, Loss: 1174884491.2778332\n",
      "Iteration 7136, Loss: 1171524998.030337\n",
      "Iteration 7137, Loss: 1199521935.2789292\n",
      "Iteration 7138, Loss: 1294901169.8950424\n",
      "Iteration 7139, Loss: 1236309783.8106482\n",
      "Iteration 7140, Loss: 1091915205.261578\n",
      "Iteration 7141, Loss: 1085635151.1933239\n",
      "Iteration 7142, Loss: 1255380617.059804\n",
      "Iteration 7143, Loss: 1158251273.9781384\n",
      "Iteration 7144, Loss: 1155090356.2923424\n",
      "Iteration 7145, Loss: 1127270454.5919516\n",
      "Iteration 7146, Loss: 1177244128.7395751\n",
      "Iteration 7147, Loss: 1151513684.1433284\n",
      "Iteration 7148, Loss: 1198951051.1505058\n",
      "Iteration 7149, Loss: 1292925716.611012\n",
      "Iteration 7150, Loss: 1269120495.816344\n",
      "Iteration 7151, Loss: 1250380092.1259882\n",
      "Iteration 7152, Loss: 1172195381.9613903\n",
      "Iteration 7153, Loss: 1272850280.5201716\n",
      "Iteration 7154, Loss: 1253428328.7460144\n",
      "Iteration 7155, Loss: 1207949970.2787092\n",
      "Iteration 7156, Loss: 1299996340.2604327\n",
      "Iteration 7157, Loss: 1265716362.4627395\n",
      "Iteration 7158, Loss: 1169893249.0171084\n",
      "Iteration 7159, Loss: 1180183286.347316\n",
      "Iteration 7160, Loss: 1208896760.1026804\n",
      "Iteration 7161, Loss: 1151539698.3838778\n",
      "Iteration 7162, Loss: 1066979565.1543373\n",
      "Iteration 7163, Loss: 1985220221.2869759\n",
      "Iteration 7164, Loss: 1259773856.293392\n",
      "Iteration 7165, Loss: 1293994218.4369795\n",
      "Iteration 7166, Loss: 1071480130.4703548\n",
      "Iteration 7167, Loss: 1079438358.9640048\n",
      "Iteration 7168, Loss: 1126272987.1316688\n",
      "Iteration 7169, Loss: 1113568113.2257605\n",
      "Iteration 7170, Loss: 1229192560.5510821\n",
      "Iteration 7171, Loss: 1137487051.247429\n",
      "Iteration 7172, Loss: 1194595802.365395\n",
      "Iteration 7173, Loss: 1270227785.5942514\n",
      "Iteration 7174, Loss: 1186721269.2383409\n",
      "Iteration 7175, Loss: 1283360529.412555\n",
      "Iteration 7176, Loss: 1300586468.3367677\n",
      "Iteration 7177, Loss: 1134823729.8947923\n",
      "Iteration 7178, Loss: 1287532689.0199702\n",
      "Iteration 7179, Loss: 1165164371.0826483\n",
      "Iteration 7180, Loss: 1797642332.3473804\n",
      "Iteration 7181, Loss: 1084477949.9821022\n",
      "Iteration 7182, Loss: 1498575315.5300503\n",
      "Iteration 7183, Loss: 1358302733.294999\n",
      "Iteration 7184, Loss: 1295000597.6458983\n",
      "Iteration 7185, Loss: 1237656231.552949\n",
      "Iteration 7186, Loss: 1222938848.8912594\n",
      "Iteration 7187, Loss: 1179270699.8084795\n",
      "Iteration 7188, Loss: 1210028902.0948982\n",
      "Iteration 7189, Loss: 1059501642.429101\n",
      "Iteration 7190, Loss: 1350520468.9162645\n",
      "Iteration 7191, Loss: 1074361457.0638766\n",
      "Iteration 7192, Loss: 1347010778.7638237\n",
      "Iteration 7193, Loss: 1354800080.1001759\n",
      "Iteration 7194, Loss: 1354048343.813375\n",
      "Iteration 7195, Loss: 1064539294.9735752\n",
      "Iteration 7196, Loss: 1123697121.2018805\n",
      "Iteration 7197, Loss: 1774553025.655249\n",
      "Iteration 7198, Loss: 1730838664.325836\n",
      "Iteration 7199, Loss: 1634226316.0368636\n",
      "Iteration 7200, Loss: 1486333677.100032\n",
      "Iteration 7201, Loss: 1170465339.9185307\n",
      "Iteration 7202, Loss: 1140227759.821124\n",
      "Iteration 7203, Loss: 1301394021.6316395\n",
      "Iteration 7204, Loss: 1333476667.400183\n",
      "Iteration 7205, Loss: 1198102594.4698348\n",
      "Iteration 7206, Loss: 1164212313.0714316\n",
      "Iteration 7207, Loss: 1251309456.1869695\n",
      "Iteration 7208, Loss: 1070984371.2500511\n",
      "Iteration 7209, Loss: 1310601080.5754037\n",
      "Iteration 7210, Loss: 1286785260.6059496\n",
      "Iteration 7211, Loss: 1169571347.1405363\n",
      "Iteration 7212, Loss: 1331268529.3674893\n",
      "Iteration 7213, Loss: 1348322059.234887\n",
      "Iteration 7214, Loss: 1389037814.6877499\n",
      "Iteration 7215, Loss: 1099928038.2369378\n",
      "Iteration 7216, Loss: 1066143780.9067011\n",
      "Iteration 7217, Loss: 1077167745.3941398\n",
      "Iteration 7218, Loss: 1076104602.4880133\n",
      "Iteration 7219, Loss: 1128530903.0399723\n",
      "Iteration 7220, Loss: 1237047246.4481728\n",
      "Iteration 7221, Loss: 1167683506.9534855\n",
      "Iteration 7222, Loss: 1135120194.6486187\n",
      "Iteration 7223, Loss: 1135322623.9389563\n",
      "Iteration 7224, Loss: 1129329261.56633\n",
      "Iteration 7225, Loss: 1733504217.028413\n",
      "Iteration 7226, Loss: 1343739938.6043608\n",
      "Iteration 7227, Loss: 1391766163.2988327\n",
      "Iteration 7228, Loss: 1431546092.1064005\n",
      "Iteration 7229, Loss: 1415285053.5073023\n",
      "Iteration 7230, Loss: 1116217065.2439365\n",
      "Iteration 7231, Loss: 1095976502.281612\n",
      "Iteration 7232, Loss: 1308551950.805489\n",
      "Iteration 7233, Loss: 1287930415.9724958\n",
      "Iteration 7234, Loss: 1128075014.0929084\n",
      "Iteration 7235, Loss: 1260869215.5883853\n",
      "Iteration 7236, Loss: 1171687732.638726\n",
      "Iteration 7237, Loss: 1180578902.9830565\n",
      "Iteration 7238, Loss: 1751041367.2661676\n",
      "Iteration 7239, Loss: 1691492539.7570255\n",
      "Iteration 7240, Loss: 1394703210.4747982\n",
      "Iteration 7241, Loss: 1350172061.574196\n",
      "Iteration 7242, Loss: 1355923922.9078407\n",
      "Iteration 7243, Loss: 1248975862.017441\n",
      "Iteration 7244, Loss: 1179706173.7590675\n",
      "Iteration 7245, Loss: 1283879850.7531521\n",
      "Iteration 7246, Loss: 1279136499.6811223\n",
      "Iteration 7247, Loss: 1343625565.7880664\n",
      "Iteration 7248, Loss: 1229513612.3140812\n",
      "Iteration 7249, Loss: 1272815744.2815714\n",
      "Iteration 7250, Loss: 1077790363.680188\n",
      "Iteration 7251, Loss: 1083979211.6594834\n",
      "Iteration 7252, Loss: 1069402104.1404918\n",
      "Iteration 7253, Loss: 1213402906.632344\n",
      "Iteration 7254, Loss: 1338009704.5505974\n",
      "Iteration 7255, Loss: 1165529929.2277498\n",
      "Iteration 7256, Loss: 1145549353.2865973\n",
      "Iteration 7257, Loss: 1249404181.6614041\n",
      "Iteration 7258, Loss: 1179393068.2204537\n",
      "Iteration 7259, Loss: 1180478369.6086183\n",
      "Iteration 7260, Loss: 1179448300.2207518\n",
      "Iteration 7261, Loss: 1173538057.4785798\n",
      "Iteration 7262, Loss: 1277474835.4665246\n",
      "Iteration 7263, Loss: 1147355521.8042312\n",
      "Iteration 7264, Loss: 1071395115.7391278\n",
      "Iteration 7265, Loss: 1304886088.625008\n",
      "Iteration 7266, Loss: 1256656935.1558642\n",
      "Iteration 7267, Loss: 1253429344.2251632\n",
      "Iteration 7268, Loss: 1211191880.5960193\n",
      "Iteration 7269, Loss: 1074756187.1580095\n",
      "Iteration 7270, Loss: 1127018854.363392\n",
      "Iteration 7271, Loss: 2012448847.4351416\n",
      "Iteration 7272, Loss: 1829126210.6621659\n",
      "Iteration 7273, Loss: 1484004349.7151544\n",
      "Iteration 7274, Loss: 1278991726.86542\n",
      "Iteration 7275, Loss: 1066251500.2197126\n",
      "Iteration 7276, Loss: 2263004109.508843\n",
      "Iteration 7277, Loss: 1670148315.7067566\n",
      "Iteration 7278, Loss: 1576867829.3632762\n",
      "Iteration 7279, Loss: 1085071287.3930552\n",
      "Iteration 7280, Loss: 2362396622.221801\n",
      "Iteration 7281, Loss: 1844393364.1764822\n",
      "Iteration 7282, Loss: 1739697612.1640396\n",
      "Iteration 7283, Loss: 1711086427.2791324\n",
      "Iteration 7284, Loss: 1572254687.5705101\n",
      "Iteration 7285, Loss: 1579684574.5427988\n",
      "Iteration 7286, Loss: 1309935232.8497398\n",
      "Iteration 7287, Loss: 1209246348.4486477\n",
      "Iteration 7288, Loss: 1120568294.1321168\n",
      "Iteration 7289, Loss: 1114572269.280234\n",
      "Iteration 7290, Loss: 1193586226.8656363\n",
      "Iteration 7291, Loss: 1189935101.823681\n",
      "Iteration 7292, Loss: 1190750749.5396898\n",
      "Iteration 7293, Loss: 1291363649.6823478\n",
      "Iteration 7294, Loss: 1327916368.6915367\n",
      "Iteration 7295, Loss: 1078244215.0214224\n",
      "Iteration 7296, Loss: 1065610827.7254627\n",
      "Iteration 7297, Loss: 3926579020.393574\n",
      "Iteration 7298, Loss: 1723148092.888367\n",
      "Iteration 7299, Loss: 1680321026.1305172\n",
      "Iteration 7300, Loss: 1283970927.2397766\n",
      "Iteration 7301, Loss: 1343574504.7128198\n",
      "Iteration 7302, Loss: 1381391250.0314827\n",
      "Iteration 7303, Loss: 1063289866.4305416\n",
      "Iteration 7304, Loss: 1107374796.939505\n",
      "Iteration 7305, Loss: 1507747121.840298\n",
      "Iteration 7306, Loss: 1392160405.2503166\n",
      "Iteration 7307, Loss: 1065778491.6504761\n",
      "Iteration 7308, Loss: 1431785534.1149228\n",
      "Iteration 7309, Loss: 1179350542.8573756\n",
      "Iteration 7310, Loss: 1283357257.1714385\n",
      "Iteration 7311, Loss: 1176936331.420982\n",
      "Iteration 7312, Loss: 1164723979.9666972\n",
      "Iteration 7313, Loss: 1276716906.434447\n",
      "Iteration 7314, Loss: 1252857956.3305128\n",
      "Iteration 7315, Loss: 1125110003.846047\n",
      "Iteration 7316, Loss: 1127134371.612176\n",
      "Iteration 7317, Loss: 1079132036.6567867\n",
      "Iteration 7318, Loss: 1426898127.3592136\n",
      "Iteration 7319, Loss: 1181306678.8780553\n",
      "Iteration 7320, Loss: 1213898739.3989716\n",
      "Iteration 7321, Loss: 1149396566.06186\n",
      "Iteration 7322, Loss: 1067894878.6310986\n",
      "Iteration 7323, Loss: 1223672531.333127\n",
      "Iteration 7324, Loss: 1601020708.953036\n",
      "Iteration 7325, Loss: 1137047203.021405\n",
      "Iteration 7326, Loss: 1243483996.5551877\n",
      "Iteration 7327, Loss: 1227306209.8158298\n",
      "Iteration 7328, Loss: 1067786197.7285784\n",
      "Iteration 7329, Loss: 1118636083.6417336\n",
      "Iteration 7330, Loss: 1232974846.4651358\n",
      "Iteration 7331, Loss: 1142234170.8652666\n",
      "Iteration 7332, Loss: 1199105400.877248\n",
      "Iteration 7333, Loss: 1332128419.83159\n",
      "Iteration 7334, Loss: 1302276202.0764658\n",
      "Iteration 7335, Loss: 1202172968.7947192\n",
      "Iteration 7336, Loss: 1294810888.176635\n",
      "Iteration 7337, Loss: 1058318996.9472249\n",
      "Iteration 7338, Loss: 1318821325.2243207\n",
      "Iteration 7339, Loss: 1336359420.0034246\n",
      "Iteration 7340, Loss: 1069375548.9030266\n",
      "Iteration 7341, Loss: 1067179657.8711312\n",
      "Iteration 7342, Loss: 1066921981.658681\n",
      "Iteration 7343, Loss: 1072941888.6511185\n",
      "Iteration 7344, Loss: 1218620057.5906487\n",
      "Iteration 7345, Loss: 1147995752.7563066\n",
      "Iteration 7346, Loss: 1181473544.1995666\n",
      "Iteration 7347, Loss: 1153791557.4204378\n",
      "Iteration 7348, Loss: 1123461292.3301554\n",
      "Iteration 7349, Loss: 1165812610.3324084\n",
      "Iteration 7350, Loss: 1061499593.7354558\n",
      "Iteration 7351, Loss: 1323859643.0544405\n",
      "Iteration 7352, Loss: 1070967315.6938484\n",
      "Iteration 7353, Loss: 1265264553.707823\n",
      "Iteration 7354, Loss: 1336587386.7149217\n",
      "Iteration 7355, Loss: 1088281631.0943952\n",
      "Iteration 7356, Loss: 1138662971.8104165\n",
      "Iteration 7357, Loss: 1247659977.0872197\n",
      "Iteration 7358, Loss: 1121178620.2512581\n",
      "Iteration 7359, Loss: 1115077749.8754063\n",
      "Iteration 7360, Loss: 1359664811.5062459\n",
      "Iteration 7361, Loss: 1059537599.3852938\n",
      "Iteration 7362, Loss: 1052375528.0376865\n",
      "Iteration 7363, Loss: 1136629993.5440257\n",
      "Iteration 7364, Loss: 1315535236.276216\n",
      "Iteration 7365, Loss: 1384489690.924893\n",
      "Iteration 7366, Loss: 1088952243.6527994\n",
      "Iteration 7367, Loss: 1387680358.4431734\n",
      "Iteration 7368, Loss: 1186486259.2486873\n",
      "Iteration 7369, Loss: 1183596571.7938316\n",
      "Iteration 7370, Loss: 1156393850.0836282\n",
      "Iteration 7371, Loss: 1155312357.3491943\n",
      "Iteration 7372, Loss: 1811457681.9169137\n",
      "Iteration 7373, Loss: 1740520719.6545935\n",
      "Iteration 7374, Loss: 1453520388.0780706\n",
      "Iteration 7375, Loss: 1416527096.2362285\n",
      "Iteration 7376, Loss: 1099200776.7918403\n",
      "Iteration 7377, Loss: 1079362649.5503235\n",
      "Iteration 7378, Loss: 1703428821.6444933\n",
      "Iteration 7379, Loss: 1193886112.1484845\n",
      "Iteration 7380, Loss: 1136985973.6476107\n",
      "Iteration 7381, Loss: 1329733991.824218\n",
      "Iteration 7382, Loss: 1335606547.606348\n",
      "Iteration 7383, Loss: 1126124511.7563593\n",
      "Iteration 7384, Loss: 1317572526.119661\n",
      "Iteration 7385, Loss: 1385477730.4316185\n",
      "Iteration 7386, Loss: 1072813818.4588978\n",
      "Iteration 7387, Loss: 1317642663.9281094\n",
      "Iteration 7388, Loss: 1206524720.71283\n",
      "Iteration 7389, Loss: 1195619703.6548874\n",
      "Iteration 7390, Loss: 1287107595.5672069\n",
      "Iteration 7391, Loss: 1442812734.939815\n",
      "Iteration 7392, Loss: 1069208986.7534493\n",
      "Iteration 7393, Loss: 1073567568.1722156\n",
      "Iteration 7394, Loss: 1285957658.6256783\n",
      "Iteration 7395, Loss: 1320356964.3300962\n",
      "Iteration 7396, Loss: 1090456243.0703323\n",
      "Iteration 7397, Loss: 1137316177.7472923\n",
      "Iteration 7398, Loss: 1339922354.864195\n",
      "Iteration 7399, Loss: 1278961156.950331\n",
      "Iteration 7400, Loss: 1270575167.4209707\n",
      "Iteration 7401, Loss: 1247917282.8035429\n",
      "Iteration 7402, Loss: 1120015346.855796\n",
      "Iteration 7403, Loss: 1121808352.2331886\n",
      "Iteration 7404, Loss: 1222979191.9037776\n",
      "Iteration 7405, Loss: 1257848909.8801527\n",
      "Iteration 7406, Loss: 1236297142.0631802\n",
      "Iteration 7407, Loss: 1085587688.198342\n",
      "Iteration 7408, Loss: 1199660888.6694956\n",
      "Iteration 7409, Loss: 1147089565.477514\n",
      "Iteration 7410, Loss: 1182602648.1040375\n",
      "Iteration 7411, Loss: 1212634123.9529748\n",
      "Iteration 7412, Loss: 1238468850.957614\n",
      "Iteration 7413, Loss: 1269704608.2042782\n",
      "Iteration 7414, Loss: 1247916174.6197658\n",
      "Iteration 7415, Loss: 1226996883.884981\n",
      "Iteration 7416, Loss: 1278366167.1728458\n",
      "Iteration 7417, Loss: 1293347975.3595018\n",
      "Iteration 7418, Loss: 1157417507.5893338\n",
      "Iteration 7419, Loss: 1124421233.279075\n",
      "Iteration 7420, Loss: 1123572663.301204\n",
      "Iteration 7421, Loss: 1346663302.1498601\n",
      "Iteration 7422, Loss: 1314983804.2185016\n",
      "Iteration 7423, Loss: 1267777324.9093995\n",
      "Iteration 7424, Loss: 1246714641.016029\n",
      "Iteration 7425, Loss: 1155858564.8372526\n",
      "Iteration 7426, Loss: 1185519360.9102952\n",
      "Iteration 7427, Loss: 1201332881.8748395\n",
      "Iteration 7428, Loss: 1210065313.8227603\n",
      "Iteration 7429, Loss: 1147838746.6384885\n",
      "Iteration 7430, Loss: 1139249457.6245105\n",
      "Iteration 7431, Loss: 1186250093.4604187\n",
      "Iteration 7432, Loss: 1216188235.962284\n",
      "Iteration 7433, Loss: 1203292034.267903\n",
      "Iteration 7434, Loss: 1062312113.5211107\n",
      "Iteration 7435, Loss: 1220438594.1643424\n",
      "Iteration 7436, Loss: 1166654540.0687883\n",
      "Iteration 7437, Loss: 1057078687.1871057\n",
      "Iteration 7438, Loss: 1056654430.7669607\n",
      "Iteration 7439, Loss: 2157930656.6365376\n",
      "Iteration 7440, Loss: 1556557599.8174717\n",
      "Iteration 7441, Loss: 1166157072.8588328\n",
      "Iteration 7442, Loss: 1061718079.8569331\n",
      "Iteration 7443, Loss: 1156961003.315361\n",
      "Iteration 7444, Loss: 1272703414.7848916\n",
      "Iteration 7445, Loss: 1246943477.13672\n",
      "Iteration 7446, Loss: 1326249229.8024538\n",
      "Iteration 7447, Loss: 1049931466.1983382\n",
      "Iteration 7448, Loss: 1049347914.9455963\n",
      "Iteration 7449, Loss: 1058308383.4270085\n",
      "Iteration 7450, Loss: 1096512116.1345549\n",
      "Iteration 7451, Loss: 1089357965.515901\n",
      "Iteration 7452, Loss: 1093398340.0510166\n",
      "Iteration 7453, Loss: 1230412044.25943\n",
      "Iteration 7454, Loss: 1255149604.454908\n",
      "Iteration 7455, Loss: 1233923566.307033\n",
      "Iteration 7456, Loss: 1254667871.8535476\n",
      "Iteration 7457, Loss: 1241823591.196785\n",
      "Iteration 7458, Loss: 1151226597.032582\n",
      "Iteration 7459, Loss: 1141099040.7360094\n",
      "Iteration 7460, Loss: 1059755084.55768\n",
      "Iteration 7461, Loss: 1099767369.666356\n",
      "Iteration 7462, Loss: 1100305725.7860377\n",
      "Iteration 7463, Loss: 1100781777.4440198\n",
      "Iteration 7464, Loss: 1308313747.4077697\n",
      "Iteration 7465, Loss: 1356775957.081235\n",
      "Iteration 7466, Loss: 1046855890.5157864\n",
      "Iteration 7467, Loss: 1049702053.7711252\n",
      "Iteration 7468, Loss: 1139386984.51523\n",
      "Iteration 7469, Loss: 1129261850.9461994\n",
      "Iteration 7470, Loss: 1051007073.0458094\n",
      "Iteration 7471, Loss: 1367825720.7321055\n",
      "Iteration 7472, Loss: 1107918436.820948\n",
      "Iteration 7473, Loss: 1141106880.2560158\n",
      "Iteration 7474, Loss: 1309223840.4052455\n",
      "Iteration 7475, Loss: 1100317572.423695\n",
      "Iteration 7476, Loss: 2067840651.791373\n",
      "Iteration 7477, Loss: 1061776769.7018753\n",
      "Iteration 7478, Loss: 1064813346.0137383\n",
      "Iteration 7479, Loss: 1067701333.5547967\n",
      "Iteration 7480, Loss: 1187936247.1711595\n",
      "Iteration 7481, Loss: 1215811548.298426\n",
      "Iteration 7482, Loss: 1589355316.9916384\n",
      "Iteration 7483, Loss: 1528118264.7074065\n",
      "Iteration 7484, Loss: 1355343696.3612542\n",
      "Iteration 7485, Loss: 1322356231.8853307\n",
      "Iteration 7486, Loss: 1120877145.460844\n",
      "Iteration 7487, Loss: 1954660290.2273898\n",
      "Iteration 7488, Loss: 1638137368.7514439\n",
      "Iteration 7489, Loss: 1145070848.9847515\n",
      "Iteration 7490, Loss: 1598434813.9478292\n",
      "Iteration 7491, Loss: 1188709942.4163823\n",
      "Iteration 7492, Loss: 1177111938.8297634\n",
      "Iteration 7493, Loss: 7500389211.205942\n",
      "Iteration 7494, Loss: 2062133120.6968882\n",
      "Iteration 7495, Loss: 1063550008.4979498\n",
      "Iteration 7496, Loss: 1442223417.4385548\n",
      "Iteration 7497, Loss: 1369968525.0378036\n",
      "Iteration 7498, Loss: 1230198481.1826086\n",
      "Iteration 7499, Loss: 1186984405.3407023\n",
      "Iteration 7500, Loss: 1177786970.8963575\n",
      "Iteration 7501, Loss: 1296833399.8191202\n",
      "Iteration 7502, Loss: 1195884122.7366388\n",
      "Iteration 7503, Loss: 1138321751.683935\n",
      "Iteration 7504, Loss: 1259991127.1435983\n",
      "Iteration 7505, Loss: 1209520300.1299272\n",
      "Iteration 7506, Loss: 1063167486.3475832\n",
      "Iteration 7507, Loss: 1067000631.5970675\n",
      "Iteration 7508, Loss: 1616933126.537669\n",
      "Iteration 7509, Loss: 1557918509.935624\n",
      "Iteration 7510, Loss: 1506884648.967169\n",
      "Iteration 7511, Loss: 1482480302.181173\n",
      "Iteration 7512, Loss: 1078147617.4624162\n",
      "Iteration 7513, Loss: 1110715771.4396415\n",
      "Iteration 7514, Loss: 1245607299.1408787\n",
      "Iteration 7515, Loss: 1277009815.7349029\n",
      "Iteration 7516, Loss: 1126855086.9794598\n",
      "Iteration 7517, Loss: 1356719725.3336444\n",
      "Iteration 7518, Loss: 1327007879.8054035\n",
      "Iteration 7519, Loss: 1275954058.6546898\n",
      "Iteration 7520, Loss: 1293531105.168792\n",
      "Iteration 7521, Loss: 1354661191.3814847\n",
      "Iteration 7522, Loss: 1390796903.5451329\n",
      "Iteration 7523, Loss: 1388607304.9591255\n",
      "Iteration 7524, Loss: 1074507680.8829954\n",
      "Iteration 7525, Loss: 1312081534.4977303\n",
      "Iteration 7526, Loss: 1053621349.8783516\n",
      "Iteration 7527, Loss: 1304245581.067671\n",
      "Iteration 7528, Loss: 1094558957.5729578\n",
      "Iteration 7529, Loss: 1088639861.3871229\n",
      "Iteration 7530, Loss: 1298496312.9682896\n",
      "Iteration 7531, Loss: 1052714692.2527192\n",
      "Iteration 7532, Loss: 1228214216.073783\n",
      "Iteration 7533, Loss: 1264913578.2865932\n",
      "Iteration 7534, Loss: 1311591502.5942\n",
      "Iteration 7535, Loss: 1063500504.2359641\n",
      "Iteration 7536, Loss: 1104512307.0027847\n",
      "Iteration 7537, Loss: 1194887659.0422509\n",
      "Iteration 7538, Loss: 1222511303.9943228\n",
      "Iteration 7539, Loss: 1203474905.7722507\n",
      "Iteration 7540, Loss: 1136941616.843332\n",
      "Iteration 7541, Loss: 1092312044.350423\n",
      "Iteration 7542, Loss: 2040611799.7740016\n",
      "Iteration 7543, Loss: 1094988164.7827592\n",
      "Iteration 7544, Loss: 1132480530.4262187\n",
      "Iteration 7545, Loss: 1205343532.258494\n",
      "Iteration 7546, Loss: 1188016167.1332889\n",
      "Iteration 7547, Loss: 1172505232.6661701\n",
      "Iteration 7548, Loss: 1160977585.657739\n",
      "Iteration 7549, Loss: 1262750730.0665233\n",
      "Iteration 7550, Loss: 1240900651.496524\n",
      "Iteration 7551, Loss: 1167398288.2118316\n",
      "Iteration 7552, Loss: 1267516191.477755\n",
      "Iteration 7553, Loss: 1334778955.8839147\n",
      "Iteration 7554, Loss: 1371562713.9577236\n",
      "Iteration 7555, Loss: 1289291789.2956204\n",
      "Iteration 7556, Loss: 1058590021.8842773\n",
      "Iteration 7557, Loss: 1049714221.8457161\n",
      "Iteration 7558, Loss: 1049732915.5017009\n",
      "Iteration 7559, Loss: 1050016226.1765809\n",
      "Iteration 7560, Loss: 1059301126.6931648\n",
      "Iteration 7561, Loss: 1304561600.5044007\n",
      "Iteration 7562, Loss: 1128670870.4934237\n",
      "Iteration 7563, Loss: 1183509831.0820105\n",
      "Iteration 7564, Loss: 1319503230.1781921\n",
      "Iteration 7565, Loss: 1330098352.3608813\n",
      "Iteration 7566, Loss: 1329062800.7542486\n",
      "Iteration 7567, Loss: 1371924787.0367658\n",
      "Iteration 7568, Loss: 1291703631.8363254\n",
      "Iteration 7569, Loss: 1225497157.087562\n",
      "Iteration 7570, Loss: 1304045394.9260423\n",
      "Iteration 7571, Loss: 1277358074.484233\n",
      "Iteration 7572, Loss: 1103410816.6972787\n",
      "Iteration 7573, Loss: 1348926108.8057373\n",
      "Iteration 7574, Loss: 1320992015.2268875\n",
      "Iteration 7575, Loss: 1252972385.610517\n",
      "Iteration 7576, Loss: 1204007013.6907718\n",
      "Iteration 7577, Loss: 1192637577.1032207\n",
      "Iteration 7578, Loss: 1143825699.9945006\n",
      "Iteration 7579, Loss: 1874143198.4971905\n",
      "Iteration 7580, Loss: 1724862514.8090289\n",
      "Iteration 7581, Loss: 1401460319.4693763\n",
      "Iteration 7582, Loss: 1250347181.3502426\n",
      "Iteration 7583, Loss: 1325010835.2188814\n",
      "Iteration 7584, Loss: 1220050302.400166\n",
      "Iteration 7585, Loss: 1216906748.2105832\n",
      "Iteration 7586, Loss: 1298960267.5637994\n",
      "Iteration 7587, Loss: 1274654458.8495677\n",
      "Iteration 7588, Loss: 1321461362.2974865\n",
      "Iteration 7589, Loss: 1291686496.0182197\n",
      "Iteration 7590, Loss: 1197384273.1924326\n",
      "Iteration 7591, Loss: 1681301616.5788085\n",
      "Iteration 7592, Loss: 1477966787.5398118\n",
      "Iteration 7593, Loss: 1075632596.4063902\n",
      "Iteration 7594, Loss: 3517164124.5396676\n",
      "Iteration 7595, Loss: 1834124373.4775004\n",
      "Iteration 7596, Loss: 1056397239.2687482\n",
      "Iteration 7597, Loss: 1056527059.0822337\n",
      "Iteration 7598, Loss: 1059738684.13124\n",
      "Iteration 7599, Loss: 1798333652.3672583\n",
      "Iteration 7600, Loss: 1227862578.6957407\n",
      "Iteration 7601, Loss: 1266247101.9872277\n",
      "Iteration 7602, Loss: 1500255063.080962\n",
      "Iteration 7603, Loss: 1301498559.0749702\n",
      "Iteration 7604, Loss: 1205675930.0331554\n",
      "Iteration 7605, Loss: 1296377218.8339798\n",
      "Iteration 7606, Loss: 1274212262.4380016\n",
      "Iteration 7607, Loss: 1178282935.4619188\n",
      "Iteration 7608, Loss: 1204388821.2537167\n",
      "Iteration 7609, Loss: 1193340881.3082318\n",
      "Iteration 7610, Loss: 1117642136.002174\n",
      "Iteration 7611, Loss: 1249386416.6693246\n",
      "Iteration 7612, Loss: 1202131389.7364407\n",
      "Iteration 7613, Loss: 1276744596.7921877\n",
      "Iteration 7614, Loss: 1180140452.5082486\n",
      "Iteration 7615, Loss: 1145191419.7857509\n",
      "Iteration 7616, Loss: 1354984475.1530802\n",
      "Iteration 7617, Loss: 1078567280.8417194\n",
      "Iteration 7618, Loss: 1091105019.6818266\n",
      "Iteration 7619, Loss: 1219369851.613148\n",
      "Iteration 7620, Loss: 1276491783.848576\n",
      "Iteration 7621, Loss: 1252499743.5926726\n",
      "Iteration 7622, Loss: 1182116697.5652103\n",
      "Iteration 7623, Loss: 1317343591.9233153\n",
      "Iteration 7624, Loss: 1376259864.222266\n",
      "Iteration 7625, Loss: 1086365733.4196384\n",
      "Iteration 7626, Loss: 1135466386.8394897\n",
      "Iteration 7627, Loss: 1244047924.859115\n",
      "Iteration 7628, Loss: 1176420021.6255744\n",
      "Iteration 7629, Loss: 1141366588.1447048\n",
      "Iteration 7630, Loss: 1166349793.3330104\n",
      "Iteration 7631, Loss: 1160564129.6002133\n",
      "Iteration 7632, Loss: 1158074302.7700622\n",
      "Iteration 7633, Loss: 1329421497.2012095\n",
      "Iteration 7634, Loss: 1381269511.317415\n",
      "Iteration 7635, Loss: 1153901228.6686864\n",
      "Iteration 7636, Loss: 1564540301.9241712\n",
      "Iteration 7637, Loss: 1418065164.6777577\n",
      "Iteration 7638, Loss: 1414102047.3460102\n",
      "Iteration 7639, Loss: 1077148772.2309663\n",
      "Iteration 7640, Loss: 1281728885.7166743\n",
      "Iteration 7641, Loss: 1260774299.0357065\n",
      "Iteration 7642, Loss: 1338916278.9078655\n",
      "Iteration 7643, Loss: 1113529340.2076437\n",
      "Iteration 7644, Loss: 1116545284.9262822\n",
      "Iteration 7645, Loss: 1163695445.4971552\n",
      "Iteration 7646, Loss: 1143071725.7867932\n",
      "Iteration 7647, Loss: 1134349695.9604833\n",
      "Iteration 7648, Loss: 1118691072.772831\n",
      "Iteration 7649, Loss: 1165613109.5121071\n",
      "Iteration 7650, Loss: 1185405511.5791276\n",
      "Iteration 7651, Loss: 1735339381.1600568\n",
      "Iteration 7652, Loss: 1093085732.619147\n",
      "Iteration 7653, Loss: 1097883017.5612044\n",
      "Iteration 7654, Loss: 1325023148.8923924\n",
      "Iteration 7655, Loss: 1061031269.3773241\n",
      "Iteration 7656, Loss: 1063621049.9200425\n",
      "Iteration 7657, Loss: 1063554292.5585274\n",
      "Iteration 7658, Loss: 1494690250.351203\n",
      "Iteration 7659, Loss: 1299903697.5839198\n",
      "Iteration 7660, Loss: 1255355751.0347285\n",
      "Iteration 7661, Loss: 1339242486.0027041\n",
      "Iteration 7662, Loss: 1313168800.2575772\n",
      "Iteration 7663, Loss: 1350316199.8575897\n",
      "Iteration 7664, Loss: 1109929999.3270211\n",
      "Iteration 7665, Loss: 1115365489.1522608\n",
      "Iteration 7666, Loss: 1120259613.4179394\n",
      "Iteration 7667, Loss: 1109749924.9719481\n",
      "Iteration 7668, Loss: 1189593104.4985518\n",
      "Iteration 7669, Loss: 1150765885.2134829\n",
      "Iteration 7670, Loss: 1146869163.1509893\n",
      "Iteration 7671, Loss: 1258211206.8989432\n",
      "Iteration 7672, Loss: 1253843948.7448463\n",
      "Iteration 7673, Loss: 1278022575.6906357\n",
      "Iteration 7674, Loss: 1258160961.388159\n",
      "Iteration 7675, Loss: 1310573712.0671968\n",
      "Iteration 7676, Loss: 1142221283.9858916\n",
      "Iteration 7677, Loss: 1138948741.2348397\n",
      "Iteration 7678, Loss: 1130264568.1840875\n",
      "Iteration 7679, Loss: 1099595411.793907\n",
      "Iteration 7680, Loss: 2049344493.8681319\n",
      "Iteration 7681, Loss: 1762101905.503246\n",
      "Iteration 7682, Loss: 1807081174.2736347\n",
      "Iteration 7683, Loss: 1705781463.8194666\n",
      "Iteration 7684, Loss: 1457870139.5463858\n",
      "Iteration 7685, Loss: 1284181128.4545681\n",
      "Iteration 7686, Loss: 1068928053.6401188\n",
      "Iteration 7687, Loss: 1068939453.7854997\n",
      "Iteration 7688, Loss: 1068667762.4676049\n",
      "Iteration 7689, Loss: 1164701607.7711182\n",
      "Iteration 7690, Loss: 1283353361.2655017\n",
      "Iteration 7691, Loss: 1259651433.583488\n",
      "Iteration 7692, Loss: 1261432723.2430913\n",
      "Iteration 7693, Loss: 1172083334.9420466\n",
      "Iteration 7694, Loss: 1139244329.005268\n",
      "Iteration 7695, Loss: 1267489121.2194448\n",
      "Iteration 7696, Loss: 1299002526.616223\n",
      "Iteration 7697, Loss: 1237410294.09864\n",
      "Iteration 7698, Loss: 1236047167.358594\n",
      "Iteration 7699, Loss: 1269038258.4309623\n",
      "Iteration 7700, Loss: 1258087244.1955738\n",
      "Iteration 7701, Loss: 1241565739.8957465\n",
      "Iteration 7702, Loss: 1091209922.0784192\n",
      "Iteration 7703, Loss: 1092393946.831538\n",
      "Iteration 7704, Loss: 1088119046.4928493\n",
      "Iteration 7705, Loss: 1089415705.2322664\n",
      "Iteration 7706, Loss: 1688375604.5335035\n",
      "Iteration 7707, Loss: 1600203797.205437\n",
      "Iteration 7708, Loss: 1595323666.8571382\n",
      "Iteration 7709, Loss: 1192430259.6148012\n",
      "Iteration 7710, Loss: 1059896616.0947319\n",
      "Iteration 7711, Loss: 1061435349.7187742\n",
      "Iteration 7712, Loss: 1067960933.9719043\n",
      "Iteration 7713, Loss: 1681721547.6784983\n",
      "Iteration 7714, Loss: 1358999784.6633365\n",
      "Iteration 7715, Loss: 1334027735.1557648\n",
      "Iteration 7716, Loss: 1114727284.1516764\n",
      "Iteration 7717, Loss: 1226993147.1297982\n",
      "Iteration 7718, Loss: 1151827440.207808\n",
      "Iteration 7719, Loss: 1072369323.8528415\n",
      "Iteration 7720, Loss: 1101900165.2384493\n",
      "Iteration 7721, Loss: 1121079412.7108083\n",
      "Iteration 7722, Loss: 1122696914.1625986\n",
      "Iteration 7723, Loss: 1786376676.4042897\n",
      "Iteration 7724, Loss: 1435886848.0781016\n",
      "Iteration 7725, Loss: 1073309809.4689715\n",
      "Iteration 7726, Loss: 1115132419.258258\n",
      "Iteration 7727, Loss: 1117938277.9828084\n",
      "Iteration 7728, Loss: 1380440858.1857374\n",
      "Iteration 7729, Loss: 1345023239.099291\n",
      "Iteration 7730, Loss: 1071199632.5833199\n",
      "Iteration 7731, Loss: 2576753562.709957\n",
      "Iteration 7732, Loss: 1233988194.372078\n",
      "Iteration 7733, Loss: 4044737638.3129396\n",
      "Iteration 7734, Loss: 4046779465.2412224\n",
      "Iteration 7735, Loss: 3808008107.11347\n",
      "Iteration 7736, Loss: 3233859128.618325\n",
      "Iteration 7737, Loss: 1090932898.1627274\n",
      "Iteration 7738, Loss: 2589481278.603908\n",
      "Iteration 7739, Loss: 4139620606.231942\n",
      "Iteration 7740, Loss: 1107962292.0572805\n",
      "Iteration 7741, Loss: 1064706444.6685826\n",
      "Iteration 7742, Loss: 4031356507.81502\n",
      "Iteration 7743, Loss: 3493738135.963419\n",
      "Iteration 7744, Loss: 3065498775.0372324\n",
      "Iteration 7745, Loss: 1791803405.5802217\n",
      "Iteration 7746, Loss: 1607758236.3120525\n",
      "Iteration 7747, Loss: 1531439992.8885367\n",
      "Iteration 7748, Loss: 1235182110.8692572\n",
      "Iteration 7749, Loss: 1224256536.9872203\n",
      "Iteration 7750, Loss: 1218258698.7190077\n",
      "Iteration 7751, Loss: 1317219600.6311884\n",
      "Iteration 7752, Loss: 1297941829.2196934\n",
      "Iteration 7753, Loss: 1330251349.4128714\n",
      "Iteration 7754, Loss: 1348242424.4563606\n",
      "Iteration 7755, Loss: 1091683062.7688363\n",
      "Iteration 7756, Loss: 1075968944.6919005\n",
      "Iteration 7757, Loss: 3250328242.3279285\n",
      "Iteration 7758, Loss: 3279465603.0423594\n",
      "Iteration 7759, Loss: 1097394935.8440907\n",
      "Iteration 7760, Loss: 1071248173.8163\n",
      "Iteration 7761, Loss: 1071054350.3529255\n",
      "Iteration 7762, Loss: 1077798084.9489026\n",
      "Iteration 7763, Loss: 1077836974.258016\n",
      "Iteration 7764, Loss: 1311280654.0728655\n",
      "Iteration 7765, Loss: 1110651196.6936884\n",
      "Iteration 7766, Loss: 1230663513.9767764\n",
      "Iteration 7767, Loss: 1068026885.2818648\n",
      "Iteration 7768, Loss: 1080512797.548438\n",
      "Iteration 7769, Loss: 1078836466.180065\n",
      "Iteration 7770, Loss: 1480641901.002941\n",
      "Iteration 7771, Loss: 1380303882.2123682\n",
      "Iteration 7772, Loss: 1353514566.1987264\n",
      "Iteration 7773, Loss: 1158458432.859901\n",
      "Iteration 7774, Loss: 1226882302.113593\n",
      "Iteration 7775, Loss: 1132241894.2243216\n",
      "Iteration 7776, Loss: 1095541481.8366988\n",
      "Iteration 7777, Loss: 1722936015.4405746\n",
      "Iteration 7778, Loss: 1623410868.3710659\n",
      "Iteration 7779, Loss: 1114885898.9835558\n",
      "Iteration 7780, Loss: 1120171316.7956367\n",
      "Iteration 7781, Loss: 1117844308.6042376\n",
      "Iteration 7782, Loss: 1157349768.3851717\n",
      "Iteration 7783, Loss: 1116251696.1347208\n",
      "Iteration 7784, Loss: 1244580044.8755395\n",
      "Iteration 7785, Loss: 1160940908.288284\n",
      "Iteration 7786, Loss: 1298934368.5625393\n",
      "Iteration 7787, Loss: 1282588310.819178\n",
      "Iteration 7788, Loss: 1488608576.4533703\n",
      "Iteration 7789, Loss: 1533476638.4321506\n",
      "Iteration 7790, Loss: 1072431038.7848116\n",
      "Iteration 7791, Loss: 1095663578.2944565\n",
      "Iteration 7792, Loss: 1144543558.1955564\n",
      "Iteration 7793, Loss: 1204727029.717179\n",
      "Iteration 7794, Loss: 1686411915.8892515\n",
      "Iteration 7795, Loss: 1459445446.073639\n",
      "Iteration 7796, Loss: 1254741254.711405\n",
      "Iteration 7797, Loss: 1137792965.4911835\n",
      "Iteration 7798, Loss: 1183533367.4852529\n",
      "Iteration 7799, Loss: 1327643892.0227404\n",
      "Iteration 7800, Loss: 1236542266.0323417\n",
      "Iteration 7801, Loss: 1268149855.8275747\n",
      "Iteration 7802, Loss: 1168812218.0565608\n",
      "Iteration 7803, Loss: 1194813239.8928258\n",
      "Iteration 7804, Loss: 1196400446.8116808\n",
      "Iteration 7805, Loss: 1194063893.158968\n",
      "Iteration 7806, Loss: 1159585184.8562691\n",
      "Iteration 7807, Loss: 1160237193.738212\n",
      "Iteration 7808, Loss: 1339926421.944986\n",
      "Iteration 7809, Loss: 1140329422.55378\n",
      "Iteration 7810, Loss: 1378071235.37703\n",
      "Iteration 7811, Loss: 1348074535.8620481\n",
      "Iteration 7812, Loss: 1171985508.1111712\n",
      "Iteration 7813, Loss: 1814656007.0783768\n",
      "Iteration 7814, Loss: 1069649680.9431081\n",
      "Iteration 7815, Loss: 1350685996.969785\n",
      "Iteration 7816, Loss: 1390738203.870344\n",
      "Iteration 7817, Loss: 1066324383.2943208\n",
      "Iteration 7818, Loss: 1143060899.7522264\n",
      "Iteration 7819, Loss: 1352167379.266686\n",
      "Iteration 7820, Loss: 1358637808.2112079\n",
      "Iteration 7821, Loss: 1238551331.706654\n",
      "Iteration 7822, Loss: 1325963749.4219096\n",
      "Iteration 7823, Loss: 1233046620.0065148\n",
      "Iteration 7824, Loss: 1598156947.733154\n",
      "Iteration 7825, Loss: 1065907429.5641258\n",
      "Iteration 7826, Loss: 1077691293.072076\n",
      "Iteration 7827, Loss: 1076114325.671407\n",
      "Iteration 7828, Loss: 1074747055.9632294\n",
      "Iteration 7829, Loss: 1087436787.2817369\n",
      "Iteration 7830, Loss: 1226649095.709403\n",
      "Iteration 7831, Loss: 1211755635.0828524\n",
      "Iteration 7832, Loss: 1308592168.3967893\n",
      "Iteration 7833, Loss: 1150894288.6189685\n",
      "Iteration 7834, Loss: 1611565165.821967\n",
      "Iteration 7835, Loss: 1145981453.7929857\n",
      "Iteration 7836, Loss: 1221811281.8999126\n",
      "Iteration 7837, Loss: 1315527321.7307754\n",
      "Iteration 7838, Loss: 1065563294.0722517\n",
      "Iteration 7839, Loss: 1068982368.0290014\n",
      "Iteration 7840, Loss: 1211702585.7472985\n",
      "Iteration 7841, Loss: 1127649361.0461388\n",
      "Iteration 7842, Loss: 1121022383.2640471\n",
      "Iteration 7843, Loss: 1210311581.4064043\n",
      "Iteration 7844, Loss: 1162342508.57968\n",
      "Iteration 7845, Loss: 1269179453.7424188\n",
      "Iteration 7846, Loss: 1253602372.2356486\n",
      "Iteration 7847, Loss: 1184867972.6582325\n",
      "Iteration 7848, Loss: 1178870714.584077\n",
      "Iteration 7849, Loss: 1772328637.745274\n",
      "Iteration 7850, Loss: 1436084607.8575945\n",
      "Iteration 7851, Loss: 1190929378.9271612\n",
      "Iteration 7852, Loss: 1166701811.8926978\n",
      "Iteration 7853, Loss: 1075326832.7413473\n",
      "Iteration 7854, Loss: 1086479563.0762522\n",
      "Iteration 7855, Loss: 1085877606.6694071\n",
      "Iteration 7856, Loss: 1133501350.7783146\n",
      "Iteration 7857, Loss: 1294836882.2886598\n",
      "Iteration 7858, Loss: 1346669266.0595636\n",
      "Iteration 7859, Loss: 1139722869.237517\n",
      "Iteration 7860, Loss: 1274164262.7228782\n",
      "Iteration 7861, Loss: 1331527894.818214\n",
      "Iteration 7862, Loss: 1352076737.5877798\n",
      "Iteration 7863, Loss: 1115912970.9289792\n",
      "Iteration 7864, Loss: 2097204546.761621\n",
      "Iteration 7865, Loss: 1825600901.556187\n",
      "Iteration 7866, Loss: 1799733177.9064438\n",
      "Iteration 7867, Loss: 1068977950.7999388\n",
      "Iteration 7868, Loss: 1080444318.1257372\n",
      "Iteration 7869, Loss: 1147874078.7960598\n",
      "Iteration 7870, Loss: 1150164741.687874\n",
      "Iteration 7871, Loss: 1257822558.9752915\n",
      "Iteration 7872, Loss: 1147497307.8383784\n",
      "Iteration 7873, Loss: 1895873685.7042\n",
      "Iteration 7874, Loss: 1574694399.53162\n",
      "Iteration 7875, Loss: 2627911288.641126\n",
      "Iteration 7876, Loss: 1147666084.0537689\n",
      "Iteration 7877, Loss: 1141434457.7461667\n",
      "Iteration 7878, Loss: 1255176133.1834002\n",
      "Iteration 7879, Loss: 1147753162.8888578\n",
      "Iteration 7880, Loss: 1341274877.4011981\n",
      "Iteration 7881, Loss: 1351135867.40238\n",
      "Iteration 7882, Loss: 1389777279.8424041\n",
      "Iteration 7883, Loss: 1362182080.8221607\n",
      "Iteration 7884, Loss: 1401636256.3022516\n",
      "Iteration 7885, Loss: 1196355673.634496\n",
      "Iteration 7886, Loss: 1226645314.891325\n",
      "Iteration 7887, Loss: 1150121643.2674637\n",
      "Iteration 7888, Loss: 1256091943.0638552\n",
      "Iteration 7889, Loss: 1144000244.9196959\n",
      "Iteration 7890, Loss: 1143292556.6273324\n",
      "Iteration 7891, Loss: 1267941217.4495363\n",
      "Iteration 7892, Loss: 1262606996.0374641\n",
      "Iteration 7893, Loss: 1336504439.021999\n",
      "Iteration 7894, Loss: 1384960297.8642929\n",
      "Iteration 7895, Loss: 1064547948.3853297\n",
      "Iteration 7896, Loss: 1059310027.7858746\n",
      "Iteration 7897, Loss: 1064853409.3005956\n",
      "Iteration 7898, Loss: 1064836938.805117\n",
      "Iteration 7899, Loss: 1114924914.112003\n",
      "Iteration 7900, Loss: 1222625610.5399508\n",
      "Iteration 7901, Loss: 1299680317.040481\n",
      "Iteration 7902, Loss: 1246035822.420508\n",
      "Iteration 7903, Loss: 1140006963.2262323\n",
      "Iteration 7904, Loss: 1139970975.4014897\n",
      "Iteration 7905, Loss: 1168078530.0518453\n",
      "Iteration 7906, Loss: 1176825736.449046\n",
      "Iteration 7907, Loss: 1220360905.0475533\n",
      "Iteration 7908, Loss: 1144847113.0976756\n",
      "Iteration 7909, Loss: 1061991001.1857882\n",
      "Iteration 7910, Loss: 1068117771.305561\n",
      "Iteration 7911, Loss: 1066964750.2476505\n",
      "Iteration 7912, Loss: 1072925911.8866037\n",
      "Iteration 7913, Loss: 2408134282.30855\n",
      "Iteration 7914, Loss: 2307479785.3160386\n",
      "Iteration 7915, Loss: 3087741966.52057\n",
      "Iteration 7916, Loss: 2569572813.427553\n",
      "Iteration 7917, Loss: 1530249629.4464386\n",
      "Iteration 7918, Loss: 1568987163.844633\n",
      "Iteration 7919, Loss: 1115261361.9624834\n",
      "Iteration 7920, Loss: 1098686152.7519715\n",
      "Iteration 7921, Loss: 1132495137.071224\n",
      "Iteration 7922, Loss: 1272834569.9858997\n",
      "Iteration 7923, Loss: 1250625747.6180494\n",
      "Iteration 7924, Loss: 1509972982.058987\n",
      "Iteration 7925, Loss: 1164766936.9405525\n",
      "Iteration 7926, Loss: 1263185611.1930285\n",
      "Iteration 7927, Loss: 1242343075.5985122\n",
      "Iteration 7928, Loss: 1117341446.4588938\n",
      "Iteration 7929, Loss: 1227393651.4378092\n",
      "Iteration 7930, Loss: 1260090383.314392\n",
      "Iteration 7931, Loss: 1064801610.0071787\n",
      "Iteration 7932, Loss: 1576239090.0192556\n",
      "Iteration 7933, Loss: 1275180899.9609835\n",
      "Iteration 7934, Loss: 1459788302.750876\n",
      "Iteration 7935, Loss: 1129253890.083207\n",
      "Iteration 7936, Loss: 1312683314.2942197\n",
      "Iteration 7937, Loss: 1300198890.8326604\n",
      "Iteration 7938, Loss: 1333957385.2232091\n",
      "Iteration 7939, Loss: 1296048692.877338\n",
      "Iteration 7940, Loss: 1172699778.7440107\n",
      "Iteration 7941, Loss: 1098167753.3889096\n",
      "Iteration 7942, Loss: 1363114931.0719151\n",
      "Iteration 7943, Loss: 1333044386.970201\n",
      "Iteration 7944, Loss: 1354165508.0966928\n",
      "Iteration 7945, Loss: 1329771260.1232576\n",
      "Iteration 7946, Loss: 1083320721.311327\n",
      "Iteration 7947, Loss: 1446960622.6527781\n",
      "Iteration 7948, Loss: 1266272475.5865068\n",
      "Iteration 7949, Loss: 1312377000.0731006\n",
      "Iteration 7950, Loss: 1285154462.7727292\n",
      "Iteration 7951, Loss: 1057075606.6839226\n",
      "Iteration 7952, Loss: 1051344411.0244168\n",
      "Iteration 7953, Loss: 1052440500.365656\n",
      "Iteration 7954, Loss: 1053801465.3016081\n",
      "Iteration 7955, Loss: 1064337890.7299194\n",
      "Iteration 7956, Loss: 1068252373.5390208\n",
      "Iteration 7957, Loss: 1071708783.055561\n",
      "Iteration 7958, Loss: 1121735664.8709948\n",
      "Iteration 7959, Loss: 1351412810.2516334\n",
      "Iteration 7960, Loss: 1325350451.7716484\n",
      "Iteration 7961, Loss: 1335328041.7137995\n",
      "Iteration 7962, Loss: 1205156423.9616091\n",
      "Iteration 7963, Loss: 1286579394.1233256\n",
      "Iteration 7964, Loss: 1060259334.9355807\n",
      "Iteration 7965, Loss: 1052679446.2974224\n",
      "Iteration 7966, Loss: 1323331348.110566\n",
      "Iteration 7967, Loss: 1072064153.4484146\n",
      "Iteration 7968, Loss: 1122920766.5274482\n",
      "Iteration 7969, Loss: 1226707247.15504\n",
      "Iteration 7970, Loss: 1227261955.3239596\n",
      "Iteration 7971, Loss: 1157661537.4917033\n",
      "Iteration 7972, Loss: 1152577336.0688088\n",
      "Iteration 7973, Loss: 1263149180.1215389\n",
      "Iteration 7974, Loss: 1128563966.4657247\n",
      "Iteration 7975, Loss: 1106284206.4101465\n",
      "Iteration 7976, Loss: 1284420986.0568228\n",
      "Iteration 7977, Loss: 1113297408.5084443\n",
      "Iteration 7978, Loss: 1147217251.7925236\n",
      "Iteration 7979, Loss: 1113185849.6730056\n",
      "Iteration 7980, Loss: 1224328812.6389747\n",
      "Iteration 7981, Loss: 1255967200.1863835\n",
      "Iteration 7982, Loss: 1239801508.5773726\n",
      "Iteration 7983, Loss: 1287238441.3277543\n",
      "Iteration 7984, Loss: 1258508580.3019063\n",
      "Iteration 7985, Loss: 1243829376.7178502\n",
      "Iteration 7986, Loss: 1223766350.1293883\n",
      "Iteration 7987, Loss: 1293017205.534596\n",
      "Iteration 7988, Loss: 1333703185.5814114\n",
      "Iteration 7989, Loss: 1079587962.7354891\n",
      "Iteration 7990, Loss: 1074332819.5695677\n",
      "Iteration 7991, Loss: 1071428826.5107834\n",
      "Iteration 7992, Loss: 1102709975.3695743\n",
      "Iteration 7993, Loss: 1090880884.8931887\n",
      "Iteration 7994, Loss: 2153771894.637551\n",
      "Iteration 7995, Loss: 1058554864.8224672\n",
      "Iteration 7996, Loss: 2524823369.50262\n",
      "Iteration 7997, Loss: 2268990201.234957\n",
      "Iteration 7998, Loss: 2941786701.146683\n",
      "Iteration 7999, Loss: 1456856433.244942\n",
      "Iteration 8000, Loss: 1871613152.6810803\n",
      "Iteration 8001, Loss: 1371332241.848587\n",
      "Iteration 8002, Loss: 1139760433.008434\n",
      "Iteration 8003, Loss: 1215049205.2550616\n",
      "Iteration 8004, Loss: 1135135238.1420279\n",
      "Iteration 8005, Loss: 1883277794.0555618\n",
      "Iteration 8006, Loss: 1482077942.2558842\n",
      "Iteration 8007, Loss: 1421243943.3475835\n",
      "Iteration 8008, Loss: 1400424785.2498932\n",
      "Iteration 8009, Loss: 1062732868.0636972\n",
      "Iteration 8010, Loss: 1307964684.1009626\n",
      "Iteration 8011, Loss: 1047072596.3134788\n",
      "Iteration 8012, Loss: 3263445522.0431113\n",
      "Iteration 8013, Loss: 2802572127.2964945\n",
      "Iteration 8014, Loss: 1112967703.536333\n",
      "Iteration 8015, Loss: 1051517401.9809532\n",
      "Iteration 8016, Loss: 1048194723.5320903\n",
      "Iteration 8017, Loss: 1056713343.6334757\n",
      "Iteration 8018, Loss: 1054877007.7911185\n",
      "Iteration 8019, Loss: 1079469314.3208551\n",
      "Iteration 8020, Loss: 1186171309.761589\n",
      "Iteration 8021, Loss: 1170943587.2068238\n",
      "Iteration 8022, Loss: 1267321907.7712984\n",
      "Iteration 8023, Loss: 1246026648.677513\n",
      "Iteration 8024, Loss: 1293304850.1959658\n",
      "Iteration 8025, Loss: 1282440162.3867586\n",
      "Iteration 8026, Loss: 1341575074.3246675\n",
      "Iteration 8027, Loss: 1201177889.108046\n",
      "Iteration 8028, Loss: 1287111581.3282008\n",
      "Iteration 8029, Loss: 1297769572.0501258\n",
      "Iteration 8030, Loss: 1254532262.306907\n",
      "Iteration 8031, Loss: 1058371148.5285693\n",
      "Iteration 8032, Loss: 1200199851.940529\n",
      "Iteration 8033, Loss: 1644360906.8875253\n",
      "Iteration 8034, Loss: 1554648653.306389\n",
      "Iteration 8035, Loss: 1125926456.816374\n",
      "Iteration 8036, Loss: 1696656544.6823025\n",
      "Iteration 8037, Loss: 1239587418.8207881\n",
      "Iteration 8038, Loss: 1047119471.9080292\n",
      "Iteration 8039, Loss: 1346684083.4852524\n",
      "Iteration 8040, Loss: 1350943523.5668716\n",
      "Iteration 8041, Loss: 1145754369.2222214\n",
      "Iteration 8042, Loss: 1314304368.040556\n",
      "Iteration 8043, Loss: 1200965755.9767516\n",
      "Iteration 8044, Loss: 1113943743.041673\n",
      "Iteration 8045, Loss: 1084529898.0099535\n",
      "Iteration 8046, Loss: 2217476298.41817\n",
      "Iteration 8047, Loss: 1128473879.7725027\n",
      "Iteration 8048, Loss: 1067224175.3659625\n",
      "Iteration 8049, Loss: 1116905806.2416387\n",
      "Iteration 8050, Loss: 1115325006.4969609\n",
      "Iteration 8051, Loss: 1115859542.3794968\n",
      "Iteration 8052, Loss: 1083577661.7750463\n",
      "Iteration 8053, Loss: 1052121860.4900699\n",
      "Iteration 8054, Loss: 1055116914.2208694\n",
      "Iteration 8055, Loss: 1456087130.3071651\n",
      "Iteration 8056, Loss: 1414964127.1913674\n",
      "Iteration 8057, Loss: 1138327065.0135865\n",
      "Iteration 8058, Loss: 1194756401.4484825\n",
      "Iteration 8059, Loss: 1134012797.9682946\n",
      "Iteration 8060, Loss: 1200348128.4621499\n",
      "Iteration 8061, Loss: 1257680250.2593536\n",
      "Iteration 8062, Loss: 1122388997.4254105\n",
      "Iteration 8063, Loss: 1231638393.721051\n",
      "Iteration 8064, Loss: 1308263760.9993336\n",
      "Iteration 8065, Loss: 1126856776.2191577\n",
      "Iteration 8066, Loss: 1235011566.2462986\n",
      "Iteration 8067, Loss: 1161382840.9236846\n",
      "Iteration 8068, Loss: 1120006319.2225442\n",
      "Iteration 8069, Loss: 1166364990.9453335\n",
      "Iteration 8070, Loss: 1269074677.0020287\n",
      "Iteration 8071, Loss: 1242366938.0560286\n",
      "Iteration 8072, Loss: 1222656089.3201604\n",
      "Iteration 8073, Loss: 1244973122.7366652\n",
      "Iteration 8074, Loss: 1223163124.0587866\n",
      "Iteration 8075, Loss: 1127798498.9948409\n",
      "Iteration 8076, Loss: 1065609063.351517\n",
      "Iteration 8077, Loss: 1068641667.3434262\n",
      "Iteration 8078, Loss: 1069277867.0698642\n",
      "Iteration 8079, Loss: 1066047164.0003015\n",
      "Iteration 8080, Loss: 1444050069.730865\n",
      "Iteration 8081, Loss: 1153678147.4200113\n",
      "Iteration 8082, Loss: 1179353705.651811\n",
      "Iteration 8083, Loss: 1176725828.4301777\n",
      "Iteration 8084, Loss: 1132833317.1232302\n",
      "Iteration 8085, Loss: 1244834084.7412853\n",
      "Iteration 8086, Loss: 1529488250.9873035\n",
      "Iteration 8087, Loss: 1465985607.7887406\n",
      "Iteration 8088, Loss: 1442785225.261813\n",
      "Iteration 8089, Loss: 1138711335.0579288\n",
      "Iteration 8090, Loss: 1135759655.7701504\n",
      "Iteration 8091, Loss: 1170197587.5122228\n",
      "Iteration 8092, Loss: 1100656681.897552\n",
      "Iteration 8093, Loss: 1524473645.246964\n",
      "Iteration 8094, Loss: 1486195772.4214723\n",
      "Iteration 8095, Loss: 1092811821.377725\n",
      "Iteration 8096, Loss: 1592610095.6976643\n",
      "Iteration 8097, Loss: 1193924492.4950714\n",
      "Iteration 8098, Loss: 1058006483.509577\n",
      "Iteration 8099, Loss: 1207135348.9280562\n",
      "Iteration 8100, Loss: 1107958643.771401\n",
      "Iteration 8101, Loss: 1096802462.4513206\n",
      "Iteration 8102, Loss: 1202556688.2630293\n",
      "Iteration 8103, Loss: 1168321817.789779\n",
      "Iteration 8104, Loss: 1154083351.2850478\n",
      "Iteration 8105, Loss: 1148476668.4671848\n",
      "Iteration 8106, Loss: 1157863861.852785\n",
      "Iteration 8107, Loss: 1266452577.104329\n",
      "Iteration 8108, Loss: 1060309381.629351\n",
      "Iteration 8109, Loss: 1070256452.325845\n",
      "Iteration 8110, Loss: 1279990054.0816166\n",
      "Iteration 8111, Loss: 1168823163.7786844\n",
      "Iteration 8112, Loss: 1267152500.1763952\n",
      "Iteration 8113, Loss: 1241458631.814916\n",
      "Iteration 8114, Loss: 1289964812.3825183\n",
      "Iteration 8115, Loss: 1341552304.9644954\n",
      "Iteration 8116, Loss: 1202849740.1037452\n",
      "Iteration 8117, Loss: 1289062178.2141738\n",
      "Iteration 8118, Loss: 1134733408.7141883\n",
      "Iteration 8119, Loss: 1241334712.4308963\n",
      "Iteration 8120, Loss: 1314758035.438666\n",
      "Iteration 8121, Loss: 1046579506.0265747\n",
      "Iteration 8122, Loss: 1125022189.9202962\n",
      "Iteration 8123, Loss: 1063378647.352702\n",
      "Iteration 8124, Loss: 1282219033.8719897\n",
      "Iteration 8125, Loss: 1133302463.1196055\n",
      "Iteration 8126, Loss: 1121849289.1228178\n",
      "Iteration 8127, Loss: 1101053463.0137992\n",
      "Iteration 8128, Loss: 1211976433.5798244\n",
      "Iteration 8129, Loss: 1607542660.1412725\n",
      "Iteration 8130, Loss: 1119691292.4737449\n",
      "Iteration 8131, Loss: 1350509754.8692567\n",
      "Iteration 8132, Loss: 1372902619.0363226\n",
      "Iteration 8133, Loss: 1077876823.7473767\n",
      "Iteration 8134, Loss: 1463629295.9450288\n",
      "Iteration 8135, Loss: 1501133445.9513316\n",
      "Iteration 8136, Loss: 1207674514.6399362\n",
      "Iteration 8137, Loss: 1192040548.895723\n",
      "Iteration 8138, Loss: 1150938431.2845945\n",
      "Iteration 8139, Loss: 1817578317.4844413\n",
      "Iteration 8140, Loss: 1593135627.675362\n",
      "Iteration 8141, Loss: 1632110271.3159988\n",
      "Iteration 8142, Loss: 1510436012.9874291\n",
      "Iteration 8143, Loss: 1386507038.6851075\n",
      "Iteration 8144, Loss: 1106992904.1219685\n",
      "Iteration 8145, Loss: 1100957043.7609057\n",
      "Iteration 8146, Loss: 1308965314.0519202\n",
      "Iteration 8147, Loss: 1322024162.5274262\n",
      "Iteration 8148, Loss: 1208679161.6816707\n",
      "Iteration 8149, Loss: 1139033541.3389394\n",
      "Iteration 8150, Loss: 1158332885.574791\n",
      "Iteration 8151, Loss: 1144774110.8286679\n",
      "Iteration 8152, Loss: 1139991418.409501\n",
      "Iteration 8153, Loss: 1194302693.7589986\n",
      "Iteration 8154, Loss: 1102880923.5250037\n",
      "Iteration 8155, Loss: 1100364311.897876\n",
      "Iteration 8156, Loss: 1103062593.8575318\n",
      "Iteration 8157, Loss: 1234416415.206261\n",
      "Iteration 8158, Loss: 1207644376.4035628\n",
      "Iteration 8159, Loss: 1268963655.4094558\n",
      "Iteration 8160, Loss: 1328330437.1159894\n",
      "Iteration 8161, Loss: 1330976484.26139\n",
      "Iteration 8162, Loss: 1371177637.1615086\n",
      "Iteration 8163, Loss: 1138926373.2900784\n",
      "Iteration 8164, Loss: 1132477794.4005163\n",
      "Iteration 8165, Loss: 1161831484.6134565\n",
      "Iteration 8166, Loss: 1184066003.8556433\n",
      "Iteration 8167, Loss: 1277216571.971039\n",
      "Iteration 8168, Loss: 1052778773.2774438\n",
      "Iteration 8169, Loss: 1250889638.9281874\n",
      "Iteration 8170, Loss: 1199663925.9084733\n",
      "Iteration 8171, Loss: 1251306031.1156187\n",
      "Iteration 8172, Loss: 1508785605.0826075\n",
      "Iteration 8173, Loss: 1547007260.9064324\n",
      "Iteration 8174, Loss: 1090269061.6291208\n",
      "Iteration 8175, Loss: 1091745905.1477969\n",
      "Iteration 8176, Loss: 1228172964.881357\n",
      "Iteration 8177, Loss: 1092952708.5312088\n",
      "Iteration 8178, Loss: 1305673656.4752347\n",
      "Iteration 8179, Loss: 1044179107.7725468\n",
      "Iteration 8180, Loss: 1054819525.399201\n",
      "Iteration 8181, Loss: 1052418607.5465916\n",
      "Iteration 8182, Loss: 1309395571.518949\n",
      "Iteration 8183, Loss: 1279244058.1268003\n",
      "Iteration 8184, Loss: 1050806429.9223454\n",
      "Iteration 8185, Loss: 1152689181.990993\n",
      "Iteration 8186, Loss: 1143014513.3857539\n",
      "Iteration 8187, Loss: 1167429445.7837718\n",
      "Iteration 8188, Loss: 1117821680.0025034\n",
      "Iteration 8189, Loss: 1117283914.9264152\n",
      "Iteration 8190, Loss: 1335248285.2237504\n",
      "Iteration 8191, Loss: 1056207744.9825044\n",
      "Iteration 8192, Loss: 1162625520.0960474\n",
      "Iteration 8193, Loss: 1153678002.1353261\n",
      "Iteration 8194, Loss: 1306773912.8198009\n",
      "Iteration 8195, Loss: 1277019530.8022268\n",
      "Iteration 8196, Loss: 1114745369.4090264\n",
      "Iteration 8197, Loss: 1150416044.6243453\n",
      "Iteration 8198, Loss: 1244731699.6735873\n",
      "Iteration 8199, Loss: 1041938751.7303392\n",
      "Iteration 8200, Loss: 1050666472.0280492\n",
      "Iteration 8201, Loss: 1059341374.3713951\n",
      "Iteration 8202, Loss: 1088203076.0096512\n",
      "Iteration 8203, Loss: 1244103032.0906243\n",
      "Iteration 8204, Loss: 1309514832.7912655\n",
      "Iteration 8205, Loss: 1139807650.2743998\n",
      "Iteration 8206, Loss: 1105946966.9192789\n",
      "Iteration 8207, Loss: 1231455661.848497\n",
      "Iteration 8208, Loss: 1110070164.3423004\n",
      "Iteration 8209, Loss: 1131027555.1399875\n",
      "Iteration 8210, Loss: 1172179746.7068067\n",
      "Iteration 8211, Loss: 1133519702.2759004\n",
      "Iteration 8212, Loss: 1127674580.6941767\n",
      "Iteration 8213, Loss: 1260765754.5833216\n",
      "Iteration 8214, Loss: 1277924723.0245903\n",
      "Iteration 8215, Loss: 1431447171.750855\n",
      "Iteration 8216, Loss: 1051845931.4848763\n",
      "Iteration 8217, Loss: 1329433396.5711324\n",
      "Iteration 8218, Loss: 1312186822.5154095\n",
      "Iteration 8219, Loss: 1344383197.3945155\n",
      "Iteration 8220, Loss: 1370620335.7797794\n",
      "Iteration 8221, Loss: 1176126586.9600224\n",
      "Iteration 8222, Loss: 1163093199.3242245\n",
      "Iteration 8223, Loss: 1148856254.20126\n",
      "Iteration 8224, Loss: 1045255188.325172\n",
      "Iteration 8225, Loss: 1053424935.5376992\n",
      "Iteration 8226, Loss: 1093750260.4206815\n",
      "Iteration 8227, Loss: 1143322313.5635793\n",
      "Iteration 8228, Loss: 1300190277.727913\n",
      "Iteration 8229, Loss: 1107828386.5866406\n",
      "Iteration 8230, Loss: 1087173048.5387807\n",
      "Iteration 8231, Loss: 1086065238.1502445\n",
      "Iteration 8232, Loss: 1117191968.7176323\n",
      "Iteration 8233, Loss: 1109592729.9715147\n",
      "Iteration 8234, Loss: 1072293070.8429358\n",
      "Iteration 8235, Loss: 1381362945.3271759\n",
      "Iteration 8236, Loss: 1222761663.770261\n",
      "Iteration 8237, Loss: 1293962581.2575247\n",
      "Iteration 8238, Loss: 1109745328.1894953\n",
      "Iteration 8239, Loss: 1073110900.9230148\n",
      "Iteration 8240, Loss: 1068384296.5867984\n",
      "Iteration 8241, Loss: 1190240045.509768\n",
      "Iteration 8242, Loss: 1136662872.2850673\n",
      "Iteration 8243, Loss: 1167789019.9117875\n",
      "Iteration 8244, Loss: 1257238544.9435244\n",
      "Iteration 8245, Loss: 1105681622.663531\n",
      "Iteration 8246, Loss: 1136712217.0613267\n",
      "Iteration 8247, Loss: 1165873432.5138316\n",
      "Iteration 8248, Loss: 1147819652.231056\n",
      "Iteration 8249, Loss: 1138456750.399677\n",
      "Iteration 8250, Loss: 1103955068.175184\n",
      "Iteration 8251, Loss: 1228957225.1717548\n",
      "Iteration 8252, Loss: 1219318179.1522155\n",
      "Iteration 8253, Loss: 1090347545.8449821\n",
      "Iteration 8254, Loss: 1089000106.9758186\n",
      "Iteration 8255, Loss: 1138068849.0670993\n",
      "Iteration 8256, Loss: 1126098634.9555738\n",
      "Iteration 8257, Loss: 1890162454.1939104\n",
      "Iteration 8258, Loss: 1746135051.6645608\n",
      "Iteration 8259, Loss: 1551593380.5501819\n",
      "Iteration 8260, Loss: 1516071564.3584497\n",
      "Iteration 8261, Loss: 1468421005.6199234\n",
      "Iteration 8262, Loss: 1148751268.5504224\n",
      "Iteration 8263, Loss: 1170108246.951714\n",
      "Iteration 8264, Loss: 1260521404.7085373\n",
      "Iteration 8265, Loss: 1234589484.5072181\n",
      "Iteration 8266, Loss: 1076579160.400805\n",
      "Iteration 8267, Loss: 1267672897.021477\n",
      "Iteration 8268, Loss: 1325142364.2207122\n",
      "Iteration 8269, Loss: 1328418040.0730147\n",
      "Iteration 8270, Loss: 1250628314.3700933\n",
      "Iteration 8271, Loss: 1225352169.2698357\n",
      "Iteration 8272, Loss: 1254774880.7941706\n",
      "Iteration 8273, Loss: 1296135090.5445817\n",
      "Iteration 8274, Loss: 1050790791.5468218\n",
      "Iteration 8275, Loss: 1059398744.1199672\n",
      "Iteration 8276, Loss: 1841054418.0967612\n",
      "Iteration 8277, Loss: 1063584978.3320228\n",
      "Iteration 8278, Loss: 1163336626.656851\n",
      "Iteration 8279, Loss: 1132290681.7648888\n",
      "Iteration 8280, Loss: 1126651234.5621998\n",
      "Iteration 8281, Loss: 1157639170.6751208\n",
      "Iteration 8282, Loss: 1144840813.5068834\n",
      "Iteration 8283, Loss: 1196660018.3056636\n",
      "Iteration 8284, Loss: 1105630357.873836\n",
      "Iteration 8285, Loss: 1160567842.5468647\n",
      "Iteration 8286, Loss: 1155557080.908912\n",
      "Iteration 8287, Loss: 1168624031.681822\n",
      "Iteration 8288, Loss: 1157380355.5404763\n",
      "Iteration 8289, Loss: 1149016719.8562906\n",
      "Iteration 8290, Loss: 1508899380.4041162\n",
      "Iteration 8291, Loss: 1157776921.1855748\n",
      "Iteration 8292, Loss: 1308208010.9694223\n",
      "Iteration 8293, Loss: 1207360202.9282541\n",
      "Iteration 8294, Loss: 1255466570.2721784\n",
      "Iteration 8295, Loss: 1229875271.7908642\n",
      "Iteration 8296, Loss: 1242375227.845461\n",
      "Iteration 8297, Loss: 1142533962.9890518\n",
      "Iteration 8298, Loss: 1239295420.0361488\n",
      "Iteration 8299, Loss: 1119652378.0893517\n",
      "Iteration 8300, Loss: 1163359898.4138124\n",
      "Iteration 8301, Loss: 1141827472.1581073\n",
      "Iteration 8302, Loss: 1314713156.1138725\n",
      "Iteration 8303, Loss: 1203675397.5071368\n",
      "Iteration 8304, Loss: 1186295406.7556584\n",
      "Iteration 8305, Loss: 1278250519.6209798\n",
      "Iteration 8306, Loss: 1317367426.0166483\n",
      "Iteration 8307, Loss: 1094272540.7556057\n",
      "Iteration 8308, Loss: 1203834496.907054\n",
      "Iteration 8309, Loss: 1186281299.297319\n",
      "Iteration 8310, Loss: 1040213696.2557774\n",
      "Iteration 8311, Loss: 2389447360.5196004\n",
      "Iteration 8312, Loss: 2152656894.948291\n",
      "Iteration 8313, Loss: 2590369962.506921\n",
      "Iteration 8314, Loss: 2396777362.390621\n",
      "Iteration 8315, Loss: 2974629000.0034337\n",
      "Iteration 8316, Loss: 2807499212.7251377\n",
      "Iteration 8317, Loss: 2434122774.9235635\n",
      "Iteration 8318, Loss: 2199268900.2049108\n",
      "Iteration 8319, Loss: 1147479211.9738045\n",
      "Iteration 8320, Loss: 1259560737.9210265\n",
      "Iteration 8321, Loss: 1249651522.4750152\n",
      "Iteration 8322, Loss: 1227024639.3277888\n",
      "Iteration 8323, Loss: 1080181326.643426\n",
      "Iteration 8324, Loss: 1083773878.7457895\n",
      "Iteration 8325, Loss: 1220593969.8905563\n",
      "Iteration 8326, Loss: 1240479040.3069065\n",
      "Iteration 8327, Loss: 1144443810.9897604\n",
      "Iteration 8328, Loss: 1254192235.9513507\n",
      "Iteration 8329, Loss: 1059402512.612328\n",
      "Iteration 8330, Loss: 1167109200.9776974\n",
      "Iteration 8331, Loss: 1155344786.6912289\n",
      "Iteration 8332, Loss: 1206762780.8732076\n",
      "Iteration 8333, Loss: 1049532168.1565952\n",
      "Iteration 8334, Loss: 2721685585.3622694\n",
      "Iteration 8335, Loss: 2041299871.8898413\n",
      "Iteration 8336, Loss: 1767502545.3731554\n",
      "Iteration 8337, Loss: 1668960831.4109206\n",
      "Iteration 8338, Loss: 2795955373.344287\n",
      "Iteration 8339, Loss: 2815538353.198491\n",
      "Iteration 8340, Loss: 1874750430.0346105\n",
      "Iteration 8341, Loss: 1344963099.1490085\n",
      "Iteration 8342, Loss: 1273988714.7462695\n",
      "Iteration 8343, Loss: 1246471488.9329965\n",
      "Iteration 8344, Loss: 1067105127.9068648\n",
      "Iteration 8345, Loss: 1177026875.4901688\n",
      "Iteration 8346, Loss: 1176927103.8525608\n",
      "Iteration 8347, Loss: 1214722319.8932328\n",
      "Iteration 8348, Loss: 1147585560.6697857\n",
      "Iteration 8349, Loss: 1110440654.4159806\n",
      "Iteration 8350, Loss: 1143285930.8836622\n",
      "Iteration 8351, Loss: 1275194005.751325\n",
      "Iteration 8352, Loss: 1308334653.454342\n",
      "Iteration 8353, Loss: 1294830558.562253\n",
      "Iteration 8354, Loss: 1301816487.9697835\n",
      "Iteration 8355, Loss: 1085644648.5694919\n",
      "Iteration 8356, Loss: 1189179611.8808346\n",
      "Iteration 8357, Loss: 1175732524.9151323\n",
      "Iteration 8358, Loss: 1202603523.9169621\n",
      "Iteration 8359, Loss: 1201417806.9326022\n",
      "Iteration 8360, Loss: 1625932031.5045125\n",
      "Iteration 8361, Loss: 1501971571.4141643\n",
      "Iteration 8362, Loss: 1202959168.2956986\n",
      "Iteration 8363, Loss: 1202403162.0641944\n",
      "Iteration 8364, Loss: 1186914984.520661\n",
      "Iteration 8365, Loss: 1275188422.8201869\n",
      "Iteration 8366, Loss: 1235251683.2392094\n",
      "Iteration 8367, Loss: 1226996589.668835\n",
      "Iteration 8368, Loss: 1153694966.7406983\n",
      "Iteration 8369, Loss: 1495389200.6916797\n",
      "Iteration 8370, Loss: 1160971414.6277707\n",
      "Iteration 8371, Loss: 1219626001.350386\n",
      "Iteration 8372, Loss: 1297051742.604995\n",
      "Iteration 8373, Loss: 1303404078.1839166\n",
      "Iteration 8374, Loss: 1085148039.4764473\n",
      "Iteration 8375, Loss: 1088207988.644825\n",
      "Iteration 8376, Loss: 1568555900.1987472\n",
      "Iteration 8377, Loss: 1501119213.802913\n",
      "Iteration 8378, Loss: 1245551412.9870017\n",
      "Iteration 8379, Loss: 1224747338.4183795\n",
      "Iteration 8380, Loss: 1300679070.9318168\n",
      "Iteration 8381, Loss: 1314635470.965365\n",
      "Iteration 8382, Loss: 1318927941.740003\n",
      "Iteration 8383, Loss: 1288061390.601256\n",
      "Iteration 8384, Loss: 1343633511.8622649\n",
      "Iteration 8385, Loss: 1044763256.9561995\n",
      "Iteration 8386, Loss: 1059318768.4733145\n",
      "Iteration 8387, Loss: 1298126151.55543\n",
      "Iteration 8388, Loss: 1170326545.0689359\n",
      "Iteration 8389, Loss: 1169202389.507347\n",
      "Iteration 8390, Loss: 1158889386.8474958\n",
      "Iteration 8391, Loss: 1253586769.0904307\n",
      "Iteration 8392, Loss: 1229559630.2960274\n",
      "Iteration 8393, Loss: 1157860266.1982486\n",
      "Iteration 8394, Loss: 1146008240.8029883\n",
      "Iteration 8395, Loss: 1144252778.480739\n",
      "Iteration 8396, Loss: 1175574087.7516336\n",
      "Iteration 8397, Loss: 1189096723.492246\n",
      "Iteration 8398, Loss: 1313544444.1473982\n",
      "Iteration 8399, Loss: 1176774640.8539047\n",
      "Iteration 8400, Loss: 1268942189.266577\n",
      "Iteration 8401, Loss: 1238539241.0940652\n",
      "Iteration 8402, Loss: 1244265861.1801367\n",
      "Iteration 8403, Loss: 1245132196.8618975\n",
      "Iteration 8404, Loss: 1290492281.9399977\n",
      "Iteration 8405, Loss: 1265746565.4439688\n",
      "Iteration 8406, Loss: 1137234350.686492\n",
      "Iteration 8407, Loss: 1168693644.9063697\n",
      "Iteration 8408, Loss: 1184778593.3827355\n",
      "Iteration 8409, Loss: 1140399578.6244454\n",
      "Iteration 8410, Loss: 1853164627.6303692\n",
      "Iteration 8411, Loss: 1101155874.9317546\n",
      "Iteration 8412, Loss: 1369958388.966743\n",
      "Iteration 8413, Loss: 1335796913.2733762\n",
      "Iteration 8414, Loss: 1305048334.1703146\n",
      "Iteration 8415, Loss: 1125422866.7631128\n",
      "Iteration 8416, Loss: 1054252097.6765511\n",
      "Iteration 8417, Loss: 1311506992.6058767\n",
      "Iteration 8418, Loss: 1080798415.932674\n",
      "Iteration 8419, Loss: 1078061155.6514053\n",
      "Iteration 8420, Loss: 1181661260.0221627\n",
      "Iteration 8421, Loss: 1209118233.9538124\n",
      "Iteration 8422, Loss: 1137944865.6288211\n",
      "Iteration 8423, Loss: 1162053141.6256692\n",
      "Iteration 8424, Loss: 1264694990.5025215\n",
      "Iteration 8425, Loss: 1286209216.863101\n",
      "Iteration 8426, Loss: 1313291232.7824514\n",
      "Iteration 8427, Loss: 1043596948.9316245\n",
      "Iteration 8428, Loss: 2395493351.3240905\n",
      "Iteration 8429, Loss: 2056195212.6550565\n",
      "Iteration 8430, Loss: 1918257101.6743164\n",
      "Iteration 8431, Loss: 1392316180.7467883\n",
      "Iteration 8432, Loss: 1303523068.0981653\n",
      "Iteration 8433, Loss: 1221086225.727279\n",
      "Iteration 8434, Loss: 1146748884.6875117\n",
      "Iteration 8435, Loss: 1251990590.1356974\n",
      "Iteration 8436, Loss: 1074624529.304582\n",
      "Iteration 8437, Loss: 1186684464.8894546\n",
      "Iteration 8438, Loss: 1154016423.2653644\n",
      "Iteration 8439, Loss: 1255191968.9293857\n",
      "Iteration 8440, Loss: 1067671785.8311224\n",
      "Iteration 8441, Loss: 1210403008.873037\n",
      "Iteration 8442, Loss: 1234735922.8594253\n",
      "Iteration 8443, Loss: 1130247147.6232553\n",
      "Iteration 8444, Loss: 1107041078.2715812\n",
      "Iteration 8445, Loss: 1149334142.686471\n",
      "Iteration 8446, Loss: 1174344748.686936\n",
      "Iteration 8447, Loss: 1125230765.8740633\n",
      "Iteration 8448, Loss: 1123038935.4435227\n",
      "Iteration 8449, Loss: 1269784789.1632853\n",
      "Iteration 8450, Loss: 1260861611.3392725\n",
      "Iteration 8451, Loss: 1239298288.0144258\n",
      "Iteration 8452, Loss: 1268692363.9080372\n",
      "Iteration 8453, Loss: 1472349675.6388388\n",
      "Iteration 8454, Loss: 1443035467.2787035\n",
      "Iteration 8455, Loss: 1334026739.2230618\n",
      "Iteration 8456, Loss: 1048325795.3009857\n",
      "Iteration 8457, Loss: 1047027903.4008399\n",
      "Iteration 8458, Loss: 1049336786.2360407\n",
      "Iteration 8459, Loss: 1093352722.9614832\n",
      "Iteration 8460, Loss: 1385710555.5469713\n",
      "Iteration 8461, Loss: 1378883098.3176885\n",
      "Iteration 8462, Loss: 1109054797.9100559\n",
      "Iteration 8463, Loss: 1205368367.5501938\n",
      "Iteration 8464, Loss: 1189787094.408635\n",
      "Iteration 8465, Loss: 1285951152.3200266\n",
      "Iteration 8466, Loss: 1304532950.9871416\n",
      "Iteration 8467, Loss: 1089588548.212948\n",
      "Iteration 8468, Loss: 1085202035.891832\n",
      "Iteration 8469, Loss: 1428126131.8497279\n",
      "Iteration 8470, Loss: 1377665557.1497748\n",
      "Iteration 8471, Loss: 1348297217.8782315\n",
      "Iteration 8472, Loss: 1317435853.2363114\n",
      "Iteration 8473, Loss: 1130904913.5278747\n",
      "Iteration 8474, Loss: 1668189585.4947524\n",
      "Iteration 8475, Loss: 1561055487.1265924\n",
      "Iteration 8476, Loss: 1124225212.5382407\n",
      "Iteration 8477, Loss: 1723221251.210661\n",
      "Iteration 8478, Loss: 1661379652.6155858\n",
      "Iteration 8479, Loss: 1198212332.8349688\n",
      "Iteration 8480, Loss: 1182648532.4164202\n",
      "Iteration 8481, Loss: 1176419076.8088093\n",
      "Iteration 8482, Loss: 1057497540.9297035\n",
      "Iteration 8483, Loss: 1197364486.8733766\n",
      "Iteration 8484, Loss: 1188834899.9685438\n",
      "Iteration 8485, Loss: 1204845304.1086144\n",
      "Iteration 8486, Loss: 1235083688.6099246\n",
      "Iteration 8487, Loss: 1138048376.0041986\n",
      "Iteration 8488, Loss: 1215174414.0557714\n",
      "Iteration 8489, Loss: 1106054711.5746295\n",
      "Iteration 8490, Loss: 1107668179.2274432\n",
      "Iteration 8491, Loss: 1152884605.2287066\n",
      "Iteration 8492, Loss: 1320766316.43169\n",
      "Iteration 8493, Loss: 1309715498.787359\n",
      "Iteration 8494, Loss: 1222851471.5406084\n",
      "Iteration 8495, Loss: 1182702096.4527726\n",
      "Iteration 8496, Loss: 1238037835.4172256\n",
      "Iteration 8497, Loss: 1270025949.8526103\n",
      "Iteration 8498, Loss: 1245087523.9101331\n",
      "Iteration 8499, Loss: 1252699330.182678\n",
      "Iteration 8500, Loss: 1272383501.7587621\n",
      "Iteration 8501, Loss: 1328239627.328879\n",
      "Iteration 8502, Loss: 1373327555.663336\n",
      "Iteration 8503, Loss: 1082028204.9870398\n",
      "Iteration 8504, Loss: 1066025001.466787\n",
      "Iteration 8505, Loss: 1086579947.0282195\n",
      "Iteration 8506, Loss: 1082557517.3868792\n",
      "Iteration 8507, Loss: 1222955770.9229608\n",
      "Iteration 8508, Loss: 1217246268.625388\n",
      "Iteration 8509, Loss: 1247402215.4175284\n",
      "Iteration 8510, Loss: 1154233438.098645\n",
      "Iteration 8511, Loss: 1129466087.2554457\n",
      "Iteration 8512, Loss: 1126764185.5516658\n",
      "Iteration 8513, Loss: 1191812832.6176813\n",
      "Iteration 8514, Loss: 1154592474.1197326\n",
      "Iteration 8515, Loss: 1186001972.0988345\n",
      "Iteration 8516, Loss: 1224642806.5481257\n",
      "Iteration 8517, Loss: 1218566805.9359386\n",
      "Iteration 8518, Loss: 1093147444.443237\n",
      "Iteration 8519, Loss: 1205545262.1923645\n",
      "Iteration 8520, Loss: 1200518710.7842062\n",
      "Iteration 8521, Loss: 1187632086.3304567\n",
      "Iteration 8522, Loss: 1197670979.0181346\n",
      "Iteration 8523, Loss: 1185347630.6211371\n",
      "Iteration 8524, Loss: 1174428730.4345546\n",
      "Iteration 8525, Loss: 1165205165.3356786\n",
      "Iteration 8526, Loss: 1163014107.6653376\n",
      "Iteration 8527, Loss: 1315017451.9622357\n",
      "Iteration 8528, Loss: 1352448837.561943\n",
      "Iteration 8529, Loss: 1060521056.3745476\n",
      "Iteration 8530, Loss: 1051421413.7363169\n",
      "Iteration 8531, Loss: 1074042217.5535674\n",
      "Iteration 8532, Loss: 1069292618.060054\n",
      "Iteration 8533, Loss: 1074946987.8919325\n",
      "Iteration 8534, Loss: 1071931648.3800256\n",
      "Iteration 8535, Loss: 1302103987.7557514\n",
      "Iteration 8536, Loss: 1346808450.7004025\n",
      "Iteration 8537, Loss: 1336593395.6007564\n",
      "Iteration 8538, Loss: 1119061905.554131\n",
      "Iteration 8539, Loss: 1185861223.2821422\n",
      "Iteration 8540, Loss: 1359064133.1380298\n",
      "Iteration 8541, Loss: 1113527858.5987754\n",
      "Iteration 8542, Loss: 1349032090.5172417\n",
      "Iteration 8543, Loss: 1070479378.5796634\n",
      "Iteration 8544, Loss: 1056177803.6538725\n",
      "Iteration 8545, Loss: 1066322650.286296\n",
      "Iteration 8546, Loss: 2370583160.9508185\n",
      "Iteration 8547, Loss: 1200757884.02353\n",
      "Iteration 8548, Loss: 1042260351.9923067\n",
      "Iteration 8549, Loss: 1049510710.868203\n",
      "Iteration 8550, Loss: 1084735849.07696\n",
      "Iteration 8551, Loss: 1241800105.3543305\n",
      "Iteration 8552, Loss: 1294097830.6775343\n",
      "Iteration 8553, Loss: 1053251103.8610224\n",
      "Iteration 8554, Loss: 1051682844.0605043\n",
      "Iteration 8555, Loss: 1045541850.3073092\n",
      "Iteration 8556, Loss: 2688481619.480818\n",
      "Iteration 8557, Loss: 2347636900.0219536\n",
      "Iteration 8558, Loss: 1530630933.9571157\n",
      "Iteration 8559, Loss: 1500890602.93957\n",
      "Iteration 8560, Loss: 1347758511.6937335\n",
      "Iteration 8561, Loss: 1047533771.1709789\n",
      "Iteration 8562, Loss: 2112595136.8972247\n",
      "Iteration 8563, Loss: 1053344397.008049\n",
      "Iteration 8564, Loss: 1110441801.5490675\n",
      "Iteration 8565, Loss: 1151444619.5555882\n",
      "Iteration 8566, Loss: 1317762687.876493\n",
      "Iteration 8567, Loss: 1363043636.0670285\n",
      "Iteration 8568, Loss: 1280748426.4636548\n",
      "Iteration 8569, Loss: 1335387002.6341107\n",
      "Iteration 8570, Loss: 1321395597.3789945\n",
      "Iteration 8571, Loss: 1183434994.1273975\n",
      "Iteration 8572, Loss: 1373710342.1846936\n",
      "Iteration 8573, Loss: 1141374261.9996285\n",
      "Iteration 8574, Loss: 1194778300.1428952\n",
      "Iteration 8575, Loss: 1247768940.849395\n",
      "Iteration 8576, Loss: 1116423823.7279222\n",
      "Iteration 8577, Loss: 1074822920.1441853\n",
      "Iteration 8578, Loss: 1177953961.887541\n",
      "Iteration 8579, Loss: 1205625718.329712\n",
      "Iteration 8580, Loss: 1191927456.7431874\n",
      "Iteration 8581, Loss: 1175235606.0935035\n",
      "Iteration 8582, Loss: 1263693730.1223786\n",
      "Iteration 8583, Loss: 1239937315.6800315\n",
      "Iteration 8584, Loss: 1220977352.977008\n",
      "Iteration 8585, Loss: 1178936060.201029\n",
      "Iteration 8586, Loss: 1263988975.1986842\n",
      "Iteration 8587, Loss: 1330914152.6052258\n",
      "Iteration 8588, Loss: 1317737852.6577485\n",
      "Iteration 8589, Loss: 1214054280.0895061\n",
      "Iteration 8590, Loss: 1609289614.805309\n",
      "Iteration 8591, Loss: 1200506566.7325823\n",
      "Iteration 8592, Loss: 1188591229.3374028\n",
      "Iteration 8593, Loss: 1153247729.0285184\n",
      "Iteration 8594, Loss: 1821355397.737554\n",
      "Iteration 8595, Loss: 1092850023.697593\n",
      "Iteration 8596, Loss: 1111160777.1411285\n",
      "Iteration 8597, Loss: 1113091954.0698068\n",
      "Iteration 8598, Loss: 1283519431.4355226\n",
      "Iteration 8599, Loss: 1249462287.4091573\n",
      "Iteration 8600, Loss: 1060375780.772065\n",
      "Iteration 8601, Loss: 1059575192.1675739\n",
      "Iteration 8602, Loss: 1057561364.4961278\n",
      "Iteration 8603, Loss: 1170189899.4599502\n",
      "Iteration 8604, Loss: 1160844190.9268088\n",
      "Iteration 8605, Loss: 1179904205.999952\n",
      "Iteration 8606, Loss: 1052117865.3701633\n",
      "Iteration 8607, Loss: 1053384688.7332877\n",
      "Iteration 8608, Loss: 1057671566.9990873\n",
      "Iteration 8609, Loss: 1063323252.9660021\n",
      "Iteration 8610, Loss: 1061960488.7300919\n",
      "Iteration 8611, Loss: 1072909164.4542533\n",
      "Iteration 8612, Loss: 1069040985.3960665\n",
      "Iteration 8613, Loss: 1315961692.8138773\n",
      "Iteration 8614, Loss: 1285142728.3348541\n",
      "Iteration 8615, Loss: 1114042933.7711356\n",
      "Iteration 8616, Loss: 1099516432.4385488\n",
      "Iteration 8617, Loss: 1194824687.8196144\n",
      "Iteration 8618, Loss: 1203281746.7750099\n",
      "Iteration 8619, Loss: 1639598890.738126\n",
      "Iteration 8620, Loss: 1055720314.8786094\n",
      "Iteration 8621, Loss: 1218874716.9753048\n",
      "Iteration 8622, Loss: 1216322908.7952838\n",
      "Iteration 8623, Loss: 1153163674.2832623\n",
      "Iteration 8624, Loss: 1147832347.2805429\n",
      "Iteration 8625, Loss: 1173059481.8177657\n",
      "Iteration 8626, Loss: 1269000051.121192\n",
      "Iteration 8627, Loss: 1119377075.8567073\n",
      "Iteration 8628, Loss: 1246191420.4940636\n",
      "Iteration 8629, Loss: 1196782360.7411518\n",
      "Iteration 8630, Loss: 1171912423.0827127\n",
      "Iteration 8631, Loss: 1051095877.1304623\n",
      "Iteration 8632, Loss: 1053802647.5069045\n",
      "Iteration 8633, Loss: 1055451589.4636208\n",
      "Iteration 8634, Loss: 1060857965.1631514\n",
      "Iteration 8635, Loss: 2566305884.463588\n",
      "Iteration 8636, Loss: 2302471744.1752048\n",
      "Iteration 8637, Loss: 2003299099.3393676\n",
      "Iteration 8638, Loss: 1877521677.3496604\n",
      "Iteration 8639, Loss: 1626767442.6381292\n",
      "Iteration 8640, Loss: 1076072645.5200088\n",
      "Iteration 8641, Loss: 1179570936.0048182\n",
      "Iteration 8642, Loss: 1168172768.8596184\n",
      "Iteration 8643, Loss: 1131616068.606531\n",
      "Iteration 8644, Loss: 1242176449.331774\n",
      "Iteration 8645, Loss: 1225095761.3405755\n",
      "Iteration 8646, Loss: 1589571863.2007298\n",
      "Iteration 8647, Loss: 1064816394.9036016\n",
      "Iteration 8648, Loss: 1115084503.2650394\n",
      "Iteration 8649, Loss: 1195961885.9369965\n",
      "Iteration 8650, Loss: 1674825020.0667017\n",
      "Iteration 8651, Loss: 1230546363.064019\n",
      "Iteration 8652, Loss: 1068626829.6376446\n",
      "Iteration 8653, Loss: 1055490298.4011515\n",
      "Iteration 8654, Loss: 1051724639.8707205\n",
      "Iteration 8655, Loss: 1059256717.3225408\n",
      "Iteration 8656, Loss: 1086602800.218153\n",
      "Iteration 8657, Loss: 1199313033.9595556\n",
      "Iteration 8658, Loss: 1046067214.4610729\n",
      "Iteration 8659, Loss: 1440345897.737345\n",
      "Iteration 8660, Loss: 1455979586.0186505\n",
      "Iteration 8661, Loss: 1327095127.6727927\n",
      "Iteration 8662, Loss: 1326203143.541239\n",
      "Iteration 8663, Loss: 1206271144.0794451\n",
      "Iteration 8664, Loss: 1138671554.003115\n",
      "Iteration 8665, Loss: 1195137618.0730722\n",
      "Iteration 8666, Loss: 1178305626.3064191\n",
      "Iteration 8667, Loss: 1174000989.1627185\n",
      "Iteration 8668, Loss: 1273813733.8272552\n",
      "Iteration 8669, Loss: 1249907799.2913864\n",
      "Iteration 8670, Loss: 1074875003.5559294\n",
      "Iteration 8671, Loss: 1186837645.6691122\n",
      "Iteration 8672, Loss: 1183427712.6597507\n",
      "Iteration 8673, Loss: 1275726500.1723351\n",
      "Iteration 8674, Loss: 1288711629.6567698\n",
      "Iteration 8675, Loss: 1265339598.0623677\n",
      "Iteration 8676, Loss: 1239364898.6426778\n",
      "Iteration 8677, Loss: 1259141651.935002\n",
      "Iteration 8678, Loss: 1322803060.868281\n",
      "Iteration 8679, Loss: 1209137038.1301675\n",
      "Iteration 8680, Loss: 1139658051.3568063\n",
      "Iteration 8681, Loss: 1331071521.745283\n",
      "Iteration 8682, Loss: 1327604001.0013535\n",
      "Iteration 8683, Loss: 1067970161.3589101\n",
      "Iteration 8684, Loss: 1068573553.1771748\n",
      "Iteration 8685, Loss: 1206324989.5990407\n",
      "Iteration 8686, Loss: 1273778969.6461859\n",
      "Iteration 8687, Loss: 1459781073.221336\n",
      "Iteration 8688, Loss: 1433228848.273038\n",
      "Iteration 8689, Loss: 1332353333.7252147\n",
      "Iteration 8690, Loss: 1318631454.44573\n",
      "Iteration 8691, Loss: 1182490983.911022\n",
      "Iteration 8692, Loss: 1274429291.6312346\n",
      "Iteration 8693, Loss: 1307363123.6317701\n",
      "Iteration 8694, Loss: 1104858105.627696\n",
      "Iteration 8695, Loss: 1079301445.0604324\n",
      "Iteration 8696, Loss: 1127528292.9694738\n",
      "Iteration 8697, Loss: 1310201524.5046155\n",
      "Iteration 8698, Loss: 1323001499.78438\n",
      "Iteration 8699, Loss: 1324171905.1139092\n",
      "Iteration 8700, Loss: 1287848168.0490477\n",
      "Iteration 8701, Loss: 1151862602.9836078\n",
      "Iteration 8702, Loss: 1054233864.1935511\n",
      "Iteration 8703, Loss: 1053733674.278058\n",
      "Iteration 8704, Loss: 1090009415.075775\n",
      "Iteration 8705, Loss: 1088309287.7916367\n",
      "Iteration 8706, Loss: 1133864041.9653044\n",
      "Iteration 8707, Loss: 1107167046.5405407\n",
      "Iteration 8708, Loss: 1147852331.9911861\n",
      "Iteration 8709, Loss: 1139384408.4656756\n",
      "Iteration 8710, Loss: 1325790505.769964\n",
      "Iteration 8711, Loss: 1119798513.3863602\n",
      "Iteration 8712, Loss: 1156259263.1648118\n",
      "Iteration 8713, Loss: 1115397031.1170495\n",
      "Iteration 8714, Loss: 1113558252.4429076\n",
      "Iteration 8715, Loss: 1185083214.4100769\n",
      "Iteration 8716, Loss: 1098906030.076267\n",
      "Iteration 8717, Loss: 1144864307.311639\n",
      "Iteration 8718, Loss: 1246821877.409355\n",
      "Iteration 8719, Loss: 1155856151.325407\n",
      "Iteration 8720, Loss: 1254405339.1288614\n",
      "Iteration 8721, Loss: 1317893523.952723\n",
      "Iteration 8722, Loss: 1145105489.9419923\n",
      "Iteration 8723, Loss: 1121022603.376962\n",
      "Iteration 8724, Loss: 1118236781.8889382\n",
      "Iteration 8725, Loss: 1182718476.1345637\n",
      "Iteration 8726, Loss: 1234189933.5817263\n",
      "Iteration 8727, Loss: 1043634240.8188925\n",
      "Iteration 8728, Loss: 1049930811.1282808\n",
      "Iteration 8729, Loss: 1151588629.3079407\n",
      "Iteration 8730, Loss: 1164593343.4355328\n",
      "Iteration 8731, Loss: 1116708922.5585196\n",
      "Iteration 8732, Loss: 1168986507.5504906\n",
      "Iteration 8733, Loss: 1293145365.3430452\n",
      "Iteration 8734, Loss: 1215652476.186034\n",
      "Iteration 8735, Loss: 1255236424.5159707\n",
      "Iteration 8736, Loss: 1052748446.5150443\n",
      "Iteration 8737, Loss: 1055678628.3885754\n",
      "Iteration 8738, Loss: 1288783256.4291449\n",
      "Iteration 8739, Loss: 1327131275.6807668\n",
      "Iteration 8740, Loss: 1110709292.2019744\n",
      "Iteration 8741, Loss: 1100891234.3653753\n",
      "Iteration 8742, Loss: 1303205905.987962\n",
      "Iteration 8743, Loss: 1289683267.7520835\n",
      "Iteration 8744, Loss: 1098527197.7779918\n",
      "Iteration 8745, Loss: 1367402603.3905325\n",
      "Iteration 8746, Loss: 1083825070.2797976\n",
      "Iteration 8747, Loss: 1081908971.4625552\n",
      "Iteration 8748, Loss: 1047232701.6649592\n",
      "Iteration 8749, Loss: 1136084711.0541873\n",
      "Iteration 8750, Loss: 1588898616.5602171\n",
      "Iteration 8751, Loss: 1179883836.6613555\n",
      "Iteration 8752, Loss: 1177072133.6457536\n",
      "Iteration 8753, Loss: 1641746170.8625772\n",
      "Iteration 8754, Loss: 1414729133.8332775\n",
      "Iteration 8755, Loss: 1394031849.1690907\n",
      "Iteration 8756, Loss: 1136888453.3029103\n",
      "Iteration 8757, Loss: 1322295338.004699\n",
      "Iteration 8758, Loss: 1142181905.3641367\n",
      "Iteration 8759, Loss: 1128990194.250408\n",
      "Iteration 8760, Loss: 1236902443.4247415\n",
      "Iteration 8761, Loss: 1053361301.9094055\n",
      "Iteration 8762, Loss: 1051048011.713964\n",
      "Iteration 8763, Loss: 1087010421.5870123\n",
      "Iteration 8764, Loss: 1131980838.4086335\n",
      "Iteration 8765, Loss: 1248022553.8287857\n",
      "Iteration 8766, Loss: 1236870893.1845732\n",
      "Iteration 8767, Loss: 1111803334.675096\n",
      "Iteration 8768, Loss: 1193811792.9423218\n",
      "Iteration 8769, Loss: 1217501905.6337607\n",
      "Iteration 8770, Loss: 1121966581.6210113\n",
      "Iteration 8771, Loss: 1110807801.4964044\n",
      "Iteration 8772, Loss: 1213030912.876477\n",
      "Iteration 8773, Loss: 1283026777.8786\n",
      "Iteration 8774, Loss: 1336342065.1924703\n",
      "Iteration 8775, Loss: 1125819673.9077983\n",
      "Iteration 8776, Loss: 1204513305.6985416\n",
      "Iteration 8777, Loss: 1119610112.7980237\n",
      "Iteration 8778, Loss: 1141676900.3537595\n",
      "Iteration 8779, Loss: 1135949520.1132982\n",
      "Iteration 8780, Loss: 1125374629.9738073\n",
      "Iteration 8781, Loss: 1175693866.8841064\n",
      "Iteration 8782, Loss: 1174739318.4033804\n",
      "Iteration 8783, Loss: 1161406413.6316063\n",
      "Iteration 8784, Loss: 1089618945.0472\n",
      "Iteration 8785, Loss: 1248210024.9198298\n",
      "Iteration 8786, Loss: 1123790634.035645\n",
      "Iteration 8787, Loss: 1248696412.3385797\n",
      "Iteration 8788, Loss: 1195037081.914857\n",
      "Iteration 8789, Loss: 1223150568.1836011\n",
      "Iteration 8790, Loss: 1251491361.0190644\n",
      "Iteration 8791, Loss: 1316200951.3743463\n",
      "Iteration 8792, Loss: 1055352101.7998309\n",
      "Iteration 8793, Loss: 1087173827.2591765\n",
      "Iteration 8794, Loss: 1185009196.7558177\n",
      "Iteration 8795, Loss: 1138954159.0317898\n",
      "Iteration 8796, Loss: 1046161467.7312292\n",
      "Iteration 8797, Loss: 1044210747.1215296\n",
      "Iteration 8798, Loss: 2902904887.8017163\n",
      "Iteration 8799, Loss: 1114212816.1049826\n",
      "Iteration 8800, Loss: 1221907933.17221\n",
      "Iteration 8801, Loss: 1203084719.5363724\n",
      "Iteration 8802, Loss: 1284675734.4318843\n",
      "Iteration 8803, Loss: 1316815701.006781\n",
      "Iteration 8804, Loss: 1039716331.1345253\n",
      "Iteration 8805, Loss: 1039670545.990465\n",
      "Iteration 8806, Loss: 1124717225.5755603\n",
      "Iteration 8807, Loss: 1098294768.834383\n",
      "Iteration 8808, Loss: 1089320618.6993945\n",
      "Iteration 8809, Loss: 1370861449.3660958\n",
      "Iteration 8810, Loss: 1052455528.2918106\n",
      "Iteration 8811, Loss: 1377971472.3520527\n",
      "Iteration 8812, Loss: 1133172684.4060078\n",
      "Iteration 8813, Loss: 1314615242.4351182\n",
      "Iteration 8814, Loss: 1354817399.7553477\n",
      "Iteration 8815, Loss: 1274784964.4692338\n",
      "Iteration 8816, Loss: 1444484658.5788805\n",
      "Iteration 8817, Loss: 1207813374.4832764\n",
      "Iteration 8818, Loss: 1255345731.5179698\n",
      "Iteration 8819, Loss: 1485408001.3241847\n",
      "Iteration 8820, Loss: 1061450312.5838184\n",
      "Iteration 8821, Loss: 1066629787.7296495\n",
      "Iteration 8822, Loss: 1293557441.820463\n",
      "Iteration 8823, Loss: 1305495355.129231\n",
      "Iteration 8824, Loss: 1316584777.5717812\n",
      "Iteration 8825, Loss: 1201902128.5969157\n",
      "Iteration 8826, Loss: 1160310676.4331932\n",
      "Iteration 8827, Loss: 1151763543.9469612\n",
      "Iteration 8828, Loss: 1203455470.1751895\n",
      "Iteration 8829, Loss: 1283433392.358775\n",
      "Iteration 8830, Loss: 1253520880.5205402\n",
      "Iteration 8831, Loss: 1060559195.3170236\n",
      "Iteration 8832, Loss: 1185390374.6232643\n",
      "Iteration 8833, Loss: 1119514244.2120917\n",
      "Iteration 8834, Loss: 1118278502.1241825\n",
      "Iteration 8835, Loss: 1259454223.954232\n",
      "Iteration 8836, Loss: 1290277425.9219444\n",
      "Iteration 8837, Loss: 1223110572.1769168\n",
      "Iteration 8838, Loss: 1089625668.964352\n",
      "Iteration 8839, Loss: 1081639588.9051619\n",
      "Iteration 8840, Loss: 1191761447.779467\n",
      "Iteration 8841, Loss: 1158765146.7335224\n",
      "Iteration 8842, Loss: 1087181988.087469\n",
      "Iteration 8843, Loss: 1069445368.0395885\n",
      "Iteration 8844, Loss: 2245170709.918829\n",
      "Iteration 8845, Loss: 2098802751.363509\n",
      "Iteration 8846, Loss: 1040077063.0718634\n",
      "Iteration 8847, Loss: 1086164641.654975\n",
      "Iteration 8848, Loss: 1536331023.5442274\n",
      "Iteration 8849, Loss: 1524215107.7945566\n",
      "Iteration 8850, Loss: 1081156830.3749573\n",
      "Iteration 8851, Loss: 1073605100.2147882\n",
      "Iteration 8852, Loss: 1071493847.2738054\n",
      "Iteration 8853, Loss: 1072393535.8747447\n",
      "Iteration 8854, Loss: 1208844894.6791217\n",
      "Iteration 8855, Loss: 1104112618.6027064\n",
      "Iteration 8856, Loss: 1295478390.690935\n",
      "Iteration 8857, Loss: 1048385986.1167746\n",
      "Iteration 8858, Loss: 1074101304.9577854\n",
      "Iteration 8859, Loss: 1041242528.5609199\n",
      "Iteration 8860, Loss: 1767237918.5561378\n",
      "Iteration 8861, Loss: 1199085147.8317575\n",
      "Iteration 8862, Loss: 1181850797.6668053\n",
      "Iteration 8863, Loss: 1681665447.6944745\n",
      "Iteration 8864, Loss: 1060289001.6206418\n",
      "Iteration 8865, Loss: 1413253634.3485177\n",
      "Iteration 8866, Loss: 1128519126.8739338\n",
      "Iteration 8867, Loss: 1158862687.6195529\n",
      "Iteration 8868, Loss: 1046938263.1504283\n",
      "Iteration 8869, Loss: 1709508942.9038403\n",
      "Iteration 8870, Loss: 1661780605.854512\n",
      "Iteration 8871, Loss: 1599897366.651854\n",
      "Iteration 8872, Loss: 1544456220.4294677\n",
      "Iteration 8873, Loss: 1305518651.7265527\n",
      "Iteration 8874, Loss: 1193145827.742836\n",
      "Iteration 8875, Loss: 1228870689.4881332\n",
      "Iteration 8876, Loss: 1130754870.1759307\n",
      "Iteration 8877, Loss: 1051294259.0724571\n",
      "Iteration 8878, Loss: 2087016145.7227235\n",
      "Iteration 8879, Loss: 1987783664.4276881\n",
      "Iteration 8880, Loss: 1822636020.8203979\n",
      "Iteration 8881, Loss: 1635369181.8506622\n",
      "Iteration 8882, Loss: 1579453199.632986\n",
      "Iteration 8883, Loss: 1278375274.3148174\n",
      "Iteration 8884, Loss: 1311934629.706883\n",
      "Iteration 8885, Loss: 1283912609.1729908\n",
      "Iteration 8886, Loss: 1304771645.49008\n",
      "Iteration 8887, Loss: 1313803038.6687171\n",
      "Iteration 8888, Loss: 1062772182.9549391\n",
      "Iteration 8889, Loss: 1950967762.6867564\n",
      "Iteration 8890, Loss: 1867980708.7680027\n",
      "Iteration 8891, Loss: 1465859801.144184\n",
      "Iteration 8892, Loss: 1428822342.0630987\n",
      "Iteration 8893, Loss: 1236665629.1620407\n",
      "Iteration 8894, Loss: 1054719118.3594205\n",
      "Iteration 8895, Loss: 1245118800.3786654\n",
      "Iteration 8896, Loss: 1228142352.1672432\n",
      "Iteration 8897, Loss: 1166259225.5977159\n",
      "Iteration 8898, Loss: 1179830771.7402887\n",
      "Iteration 8899, Loss: 1208223073.3847938\n",
      "Iteration 8900, Loss: 1152701219.5963395\n",
      "Iteration 8901, Loss: 1062343981.2389776\n",
      "Iteration 8902, Loss: 1067903544.7746212\n",
      "Iteration 8903, Loss: 1097459796.9615586\n",
      "Iteration 8904, Loss: 1145235971.3842633\n",
      "Iteration 8905, Loss: 1319885003.9234478\n",
      "Iteration 8906, Loss: 1124506693.4653327\n",
      "Iteration 8907, Loss: 1252441987.890211\n",
      "Iteration 8908, Loss: 1174083161.9065993\n",
      "Iteration 8909, Loss: 1190203258.6039155\n",
      "Iteration 8910, Loss: 1230933459.7927997\n",
      "Iteration 8911, Loss: 1264355915.7665849\n",
      "Iteration 8912, Loss: 1242565097.2639627\n",
      "Iteration 8913, Loss: 1231512355.8783686\n",
      "Iteration 8914, Loss: 1261186481.9444468\n",
      "Iteration 8915, Loss: 1496909055.108319\n",
      "Iteration 8916, Loss: 1134177341.457863\n",
      "Iteration 8917, Loss: 1102074864.1490057\n",
      "Iteration 8918, Loss: 4140501461.933228\n",
      "Iteration 8919, Loss: 4130972150.204235\n",
      "Iteration 8920, Loss: 3681430956.0070653\n",
      "Iteration 8921, Loss: 3687509178.7840466\n",
      "Iteration 8922, Loss: 1240401474.8645546\n",
      "Iteration 8923, Loss: 1072052065.5020101\n",
      "Iteration 8924, Loss: 1062870387.882395\n",
      "Iteration 8925, Loss: 1054794697.4381069\n",
      "Iteration 8926, Loss: 1224158914.5465014\n",
      "Iteration 8927, Loss: 1255411693.5975015\n",
      "Iteration 8928, Loss: 1509675375.3831074\n",
      "Iteration 8929, Loss: 1341718215.551854\n",
      "Iteration 8930, Loss: 1066326860.050666\n",
      "Iteration 8931, Loss: 1073089934.4898925\n",
      "Iteration 8932, Loss: 1213241298.1094036\n",
      "Iteration 8933, Loss: 1145352400.140775\n",
      "Iteration 8934, Loss: 1191805600.5576942\n",
      "Iteration 8935, Loss: 1176399934.728971\n",
      "Iteration 8936, Loss: 1305995589.813615\n",
      "Iteration 8937, Loss: 1280406334.6383262\n",
      "Iteration 8938, Loss: 1061669811.4285334\n",
      "Iteration 8939, Loss: 1211702927.5769792\n",
      "Iteration 8940, Loss: 1238563980.9694781\n",
      "Iteration 8941, Loss: 1194169784.01484\n",
      "Iteration 8942, Loss: 1063164062.0585552\n",
      "Iteration 8943, Loss: 1165605140.3361204\n",
      "Iteration 8944, Loss: 1233456681.4267182\n",
      "Iteration 8945, Loss: 1214723855.9805722\n",
      "Iteration 8946, Loss: 1203367991.1001186\n",
      "Iteration 8947, Loss: 1190393229.0271616\n",
      "Iteration 8948, Loss: 1185352361.0955691\n",
      "Iteration 8949, Loss: 1326646538.2111084\n",
      "Iteration 8950, Loss: 1132045426.51276\n",
      "Iteration 8951, Loss: 1062889248.4428854\n",
      "Iteration 8952, Loss: 1205766151.681245\n",
      "Iteration 8953, Loss: 1295631881.077441\n",
      "Iteration 8954, Loss: 1274063268.2858064\n",
      "Iteration 8955, Loss: 1332324155.03092\n",
      "Iteration 8956, Loss: 1089589148.2428865\n",
      "Iteration 8957, Loss: 2222403286.477483\n",
      "Iteration 8958, Loss: 1371612246.448397\n",
      "Iteration 8959, Loss: 1099722190.792893\n",
      "Iteration 8960, Loss: 1135427530.3476663\n",
      "Iteration 8961, Loss: 1322260778.8894901\n",
      "Iteration 8962, Loss: 1337414674.79998\n",
      "Iteration 8963, Loss: 1312420466.426857\n",
      "Iteration 8964, Loss: 1330812756.2336357\n",
      "Iteration 8965, Loss: 1320175487.7738085\n",
      "Iteration 8966, Loss: 1292524520.1610944\n",
      "Iteration 8967, Loss: 1355334944.3432384\n",
      "Iteration 8968, Loss: 1206945232.3799632\n",
      "Iteration 8969, Loss: 1154144283.083685\n",
      "Iteration 8970, Loss: 1133654485.5139225\n",
      "Iteration 8971, Loss: 1223526844.0099027\n",
      "Iteration 8972, Loss: 1309470851.1908207\n",
      "Iteration 8973, Loss: 1359168501.3784335\n",
      "Iteration 8974, Loss: 1056497308.2786512\n",
      "Iteration 8975, Loss: 1052855921.1675248\n",
      "Iteration 8976, Loss: 1063401383.1406026\n",
      "Iteration 8977, Loss: 1206662930.0221524\n",
      "Iteration 8978, Loss: 1261588397.2645571\n",
      "Iteration 8979, Loss: 1246850258.5074437\n",
      "Iteration 8980, Loss: 1228979248.8649828\n",
      "Iteration 8981, Loss: 1252976050.5581305\n",
      "Iteration 8982, Loss: 1251972164.7087026\n",
      "Iteration 8983, Loss: 1252166747.3366764\n",
      "Iteration 8984, Loss: 1233291075.1383817\n",
      "Iteration 8985, Loss: 1140647017.527966\n",
      "Iteration 8986, Loss: 1135523938.318076\n",
      "Iteration 8987, Loss: 1650159950.9115794\n",
      "Iteration 8988, Loss: 1053282977.8470317\n",
      "Iteration 8989, Loss: 1130533672.965219\n",
      "Iteration 8990, Loss: 1240416794.1194465\n",
      "Iteration 8991, Loss: 1080666842.7116354\n",
      "Iteration 8992, Loss: 1082980072.3072803\n",
      "Iteration 8993, Loss: 1099367204.9792671\n",
      "Iteration 8994, Loss: 1144832137.8775117\n",
      "Iteration 8995, Loss: 1138150455.9901295\n",
      "Iteration 8996, Loss: 1118568677.5861442\n",
      "Iteration 8997, Loss: 1358153157.449703\n",
      "Iteration 8998, Loss: 1320886534.1567953\n",
      "Iteration 8999, Loss: 1185596614.0940697\n",
      "Iteration 9000, Loss: 1176369279.7074876\n",
      "Iteration 9001, Loss: 1129135472.4499106\n",
      "Iteration 9002, Loss: 1069761406.5920966\n",
      "Iteration 9003, Loss: 1067580679.4186289\n",
      "Iteration 9004, Loss: 1065875136.0591166\n",
      "Iteration 9005, Loss: 1070321346.1160747\n",
      "Iteration 9006, Loss: 1073482916.8897152\n",
      "Iteration 9007, Loss: 1074381577.6561856\n",
      "Iteration 9008, Loss: 1080544518.2785225\n",
      "Iteration 9009, Loss: 1301281193.3231485\n",
      "Iteration 9010, Loss: 1275462332.7038817\n",
      "Iteration 9011, Loss: 1335904205.2358067\n",
      "Iteration 9012, Loss: 1277128293.1334789\n",
      "Iteration 9013, Loss: 1256407231.398239\n",
      "Iteration 9014, Loss: 1249785162.2014978\n",
      "Iteration 9015, Loss: 1235599726.2957344\n",
      "Iteration 9016, Loss: 1117319460.4755926\n",
      "Iteration 9017, Loss: 1430154250.5145853\n",
      "Iteration 9018, Loss: 1382819506.9354467\n",
      "Iteration 9019, Loss: 1194241753.8600626\n",
      "Iteration 9020, Loss: 1267058641.0977693\n",
      "Iteration 9021, Loss: 1260305610.1551006\n",
      "Iteration 9022, Loss: 1242586012.1039991\n",
      "Iteration 9023, Loss: 1171003285.528447\n",
      "Iteration 9024, Loss: 1215596837.9616659\n",
      "Iteration 9025, Loss: 1053668925.7567223\n",
      "Iteration 9026, Loss: 3388677771.480172\n",
      "Iteration 9027, Loss: 2028055691.7764328\n",
      "Iteration 9028, Loss: 2252218184.026323\n",
      "Iteration 9029, Loss: 1152633588.6750903\n",
      "Iteration 9030, Loss: 1588525618.1690187\n",
      "Iteration 9031, Loss: 1502830194.2526007\n",
      "Iteration 9032, Loss: 1102231017.9027097\n",
      "Iteration 9033, Loss: 1950052655.0433211\n",
      "Iteration 9034, Loss: 1078626994.0975282\n",
      "Iteration 9035, Loss: 1519550333.391794\n",
      "Iteration 9036, Loss: 1126291623.9651155\n",
      "Iteration 9037, Loss: 1397173014.696359\n",
      "Iteration 9038, Loss: 1337257271.6361125\n",
      "Iteration 9039, Loss: 1053246551.3109773\n",
      "Iteration 9040, Loss: 1053068534.481917\n",
      "Iteration 9041, Loss: 3110610322.6503797\n",
      "Iteration 9042, Loss: 2617837179.791725\n",
      "Iteration 9043, Loss: 1097254966.687564\n",
      "Iteration 9044, Loss: 1060595359.109763\n",
      "Iteration 9045, Loss: 1060807789.2216349\n",
      "Iteration 9046, Loss: 1069625362.0448287\n",
      "Iteration 9047, Loss: 1582004991.22027\n",
      "Iteration 9048, Loss: 1073164690.9372454\n",
      "Iteration 9049, Loss: 1289137116.6136637\n",
      "Iteration 9050, Loss: 1177238407.344523\n",
      "Iteration 9051, Loss: 1274704645.5030553\n",
      "Iteration 9052, Loss: 1240610329.641547\n",
      "Iteration 9053, Loss: 1196558642.8890417\n",
      "Iteration 9054, Loss: 1326938712.4371111\n",
      "Iteration 9055, Loss: 1363179396.04922\n",
      "Iteration 9056, Loss: 1332772754.2977905\n",
      "Iteration 9057, Loss: 1068231704.1556377\n",
      "Iteration 9058, Loss: 1177786733.7485483\n",
      "Iteration 9059, Loss: 1105949934.4041228\n",
      "Iteration 9060, Loss: 1918705531.5234532\n",
      "Iteration 9061, Loss: 1058145745.9508028\n",
      "Iteration 9062, Loss: 1063302107.6587548\n",
      "Iteration 9063, Loss: 1474317787.84889\n",
      "Iteration 9064, Loss: 1163942020.2749345\n",
      "Iteration 9065, Loss: 1310864836.0256937\n",
      "Iteration 9066, Loss: 1331460808.2437935\n",
      "Iteration 9067, Loss: 1338458793.0894232\n",
      "Iteration 9068, Loss: 1352901324.9778254\n",
      "Iteration 9069, Loss: 1277747903.5721745\n",
      "Iteration 9070, Loss: 1254985633.0073745\n",
      "Iteration 9071, Loss: 1177318933.9960957\n",
      "Iteration 9072, Loss: 1137685117.4773772\n",
      "Iteration 9073, Loss: 1192173512.2451928\n",
      "Iteration 9074, Loss: 1183779964.7743957\n",
      "Iteration 9075, Loss: 1182621961.8087409\n",
      "Iteration 9076, Loss: 1176788285.962514\n",
      "Iteration 9077, Loss: 1183007568.9018912\n",
      "Iteration 9078, Loss: 1239232900.1093159\n",
      "Iteration 9079, Loss: 1290955269.399552\n",
      "Iteration 9080, Loss: 1142096939.8464994\n",
      "Iteration 9081, Loss: 1067546887.9097703\n",
      "Iteration 9082, Loss: 1211731962.435743\n",
      "Iteration 9083, Loss: 1262149716.8147104\n",
      "Iteration 9084, Loss: 1127556733.679988\n",
      "Iteration 9085, Loss: 1150777246.376752\n",
      "Iteration 9086, Loss: 1129242598.7318153\n",
      "Iteration 9087, Loss: 1109846453.8913908\n",
      "Iteration 9088, Loss: 1087451088.5012107\n",
      "Iteration 9089, Loss: 1313104520.838619\n",
      "Iteration 9090, Loss: 1328926628.1590867\n",
      "Iteration 9091, Loss: 1122931320.344807\n",
      "Iteration 9092, Loss: 1978153957.6005893\n",
      "Iteration 9093, Loss: 1877611312.640294\n",
      "Iteration 9094, Loss: 1771530283.667104\n",
      "Iteration 9095, Loss: 1580409520.7330973\n",
      "Iteration 9096, Loss: 1083877848.5711532\n",
      "Iteration 9097, Loss: 1198467762.0454483\n",
      "Iteration 9098, Loss: 1228312725.2708595\n",
      "Iteration 9099, Loss: 1263259979.3306081\n",
      "Iteration 9100, Loss: 1334159710.303623\n",
      "Iteration 9101, Loss: 1123341529.6297522\n",
      "Iteration 9102, Loss: 1109390251.7139869\n",
      "Iteration 9103, Loss: 1111370723.3490422\n",
      "Iteration 9104, Loss: 1245640816.051172\n",
      "Iteration 9105, Loss: 1267313191.0723653\n",
      "Iteration 9106, Loss: 1245790845.4485738\n",
      "Iteration 9107, Loss: 1254592440.7150683\n",
      "Iteration 9108, Loss: 1283904373.8804145\n",
      "Iteration 9109, Loss: 1450068160.100921\n",
      "Iteration 9110, Loss: 1086356403.9820595\n",
      "Iteration 9111, Loss: 1065338599.648966\n",
      "Iteration 9112, Loss: 1150532505.1826255\n",
      "Iteration 9113, Loss: 1147358844.3473978\n",
      "Iteration 9114, Loss: 1143103472.9423294\n",
      "Iteration 9115, Loss: 1093892194.624118\n",
      "Iteration 9116, Loss: 1085901568.3484704\n",
      "Iteration 9117, Loss: 1088075050.084542\n",
      "Iteration 9118, Loss: 1092782887.2136362\n",
      "Iteration 9119, Loss: 1080422231.9850767\n",
      "Iteration 9120, Loss: 1125698650.2090724\n",
      "Iteration 9121, Loss: 1950238108.7823918\n",
      "Iteration 9122, Loss: 1886523757.8797088\n",
      "Iteration 9123, Loss: 1926718638.8148966\n",
      "Iteration 9124, Loss: 1966337272.38441\n",
      "Iteration 9125, Loss: 1507271293.7451818\n",
      "Iteration 9126, Loss: 1133847071.132171\n",
      "Iteration 9127, Loss: 1126325326.6921291\n",
      "Iteration 9128, Loss: 1091346297.3798773\n",
      "Iteration 9129, Loss: 1087831952.7329829\n",
      "Iteration 9130, Loss: 1410282118.1979606\n",
      "Iteration 9131, Loss: 1363117872.5292976\n",
      "Iteration 9132, Loss: 1314868833.3905373\n",
      "Iteration 9133, Loss: 1065870804.4095366\n",
      "Iteration 9134, Loss: 1067376613.4653531\n",
      "Iteration 9135, Loss: 1072074097.5537473\n",
      "Iteration 9136, Loss: 1075974997.329965\n",
      "Iteration 9137, Loss: 1188625723.1776407\n",
      "Iteration 9138, Loss: 1106398245.6591027\n",
      "Iteration 9139, Loss: 1151492729.8441951\n",
      "Iteration 9140, Loss: 1119151114.0363772\n",
      "Iteration 9141, Loss: 1173938246.5923052\n",
      "Iteration 9142, Loss: 1162648882.9735997\n",
      "Iteration 9143, Loss: 1315998895.841749\n",
      "Iteration 9144, Loss: 1133238838.2490485\n",
      "Iteration 9145, Loss: 1189546035.9709694\n",
      "Iteration 9146, Loss: 1283962726.2094152\n",
      "Iteration 9147, Loss: 1118418382.3936632\n",
      "Iteration 9148, Loss: 1120423088.079348\n",
      "Iteration 9149, Loss: 1122165344.8754022\n",
      "Iteration 9150, Loss: 1316684025.337246\n",
      "Iteration 9151, Loss: 1107979582.510339\n",
      "Iteration 9152, Loss: 1102820950.866831\n",
      "Iteration 9153, Loss: 1084752158.7921495\n",
      "Iteration 9154, Loss: 1134270628.8339\n",
      "Iteration 9155, Loss: 1242771457.7323918\n",
      "Iteration 9156, Loss: 1164181549.8113482\n",
      "Iteration 9157, Loss: 1174093166.7364838\n",
      "Iteration 9158, Loss: 1187143276.476531\n",
      "Iteration 9159, Loss: 1280343914.2700837\n",
      "Iteration 9160, Loss: 1256863957.647613\n",
      "Iteration 9161, Loss: 1175071188.9988296\n",
      "Iteration 9162, Loss: 1163595597.7011445\n",
      "Iteration 9163, Loss: 1784984018.110291\n",
      "Iteration 9164, Loss: 1352307826.1569843\n",
      "Iteration 9165, Loss: 1389117643.8861642\n",
      "Iteration 9166, Loss: 1067580314.9990125\n",
      "Iteration 9167, Loss: 1278859135.0055342\n",
      "Iteration 9168, Loss: 1255388932.4371827\n",
      "Iteration 9169, Loss: 1236800466.1980608\n",
      "Iteration 9170, Loss: 1094827263.155247\n",
      "Iteration 9171, Loss: 1388158645.1937065\n",
      "Iteration 9172, Loss: 1357150269.3470967\n",
      "Iteration 9173, Loss: 1363026418.3188398\n",
      "Iteration 9174, Loss: 1349092100.4244087\n",
      "Iteration 9175, Loss: 1275101184.7507226\n",
      "Iteration 9176, Loss: 1250490156.3403962\n",
      "Iteration 9177, Loss: 1272048655.6299953\n",
      "Iteration 9178, Loss: 1111150400.920351\n",
      "Iteration 9179, Loss: 1225114164.096038\n",
      "Iteration 9180, Loss: 1277360649.5636146\n",
      "Iteration 9181, Loss: 1126320109.9108267\n",
      "Iteration 9182, Loss: 1060755491.7587758\n",
      "Iteration 9183, Loss: 1064407007.0772742\n",
      "Iteration 9184, Loss: 1104034325.2414212\n",
      "Iteration 9185, Loss: 1314983809.8046575\n",
      "Iteration 9186, Loss: 1323479515.0982032\n",
      "Iteration 9187, Loss: 1375193493.9527202\n",
      "Iteration 9188, Loss: 1231487540.8722196\n",
      "Iteration 9189, Loss: 1168097050.263018\n",
      "Iteration 9190, Loss: 1211750544.3367832\n",
      "Iteration 9191, Loss: 1171098335.859169\n",
      "Iteration 9192, Loss: 1169729627.4281611\n",
      "Iteration 9193, Loss: 1201294569.1682265\n",
      "Iteration 9194, Loss: 1188555107.8822327\n",
      "Iteration 9195, Loss: 1157342324.261963\n",
      "Iteration 9196, Loss: 1320620559.1434627\n",
      "Iteration 9197, Loss: 1134027126.2701178\n",
      "Iteration 9198, Loss: 1112428315.8656926\n",
      "Iteration 9199, Loss: 1100826211.0679057\n",
      "Iteration 9200, Loss: 1135989537.2900853\n",
      "Iteration 9201, Loss: 1244548332.9458709\n",
      "Iteration 9202, Loss: 1170454269.1905742\n",
      "Iteration 9203, Loss: 1172634216.1196196\n",
      "Iteration 9204, Loss: 1755208168.7493815\n",
      "Iteration 9205, Loss: 1279228254.4472928\n",
      "Iteration 9206, Loss: 1239830336.7736406\n",
      "Iteration 9207, Loss: 1206039148.3415904\n",
      "Iteration 9208, Loss: 1086217481.2754598\n",
      "Iteration 9209, Loss: 1492001815.9341881\n",
      "Iteration 9210, Loss: 1164338092.452657\n",
      "Iteration 9211, Loss: 1228094065.1228182\n",
      "Iteration 9212, Loss: 1089434803.4592776\n",
      "Iteration 9213, Loss: 1056129918.472278\n",
      "Iteration 9214, Loss: 1066418275.3412234\n",
      "Iteration 9215, Loss: 1109382978.6823604\n",
      "Iteration 9216, Loss: 1106693582.3852906\n",
      "Iteration 9217, Loss: 1334109105.4691763\n",
      "Iteration 9218, Loss: 1068431622.9293342\n",
      "Iteration 9219, Loss: 1048240062.8411454\n",
      "Iteration 9220, Loss: 1058290599.7508516\n",
      "Iteration 9221, Loss: 1068107347.5126969\n",
      "Iteration 9222, Loss: 1169147740.1636589\n",
      "Iteration 9223, Loss: 1263028619.7596242\n",
      "Iteration 9224, Loss: 1059725637.7222685\n",
      "Iteration 9225, Loss: 1272193129.2670176\n",
      "Iteration 9226, Loss: 1247743418.5032282\n",
      "Iteration 9227, Loss: 1226746056.852062\n",
      "Iteration 9228, Loss: 1247682658.5388262\n",
      "Iteration 9229, Loss: 1227827842.05016\n",
      "Iteration 9230, Loss: 1565777007.3419793\n",
      "Iteration 9231, Loss: 1507837044.881305\n",
      "Iteration 9232, Loss: 1183952067.5294964\n",
      "Iteration 9233, Loss: 1972709347.3433435\n",
      "Iteration 9234, Loss: 1166400942.8573716\n",
      "Iteration 9235, Loss: 1185497591.7032566\n",
      "Iteration 9236, Loss: 1213486864.1324081\n",
      "Iteration 9237, Loss: 1316821433.6816297\n",
      "Iteration 9238, Loss: 1290065907.0163922\n",
      "Iteration 9239, Loss: 1102838001.7691462\n",
      "Iteration 9240, Loss: 1169916762.9565969\n",
      "Iteration 9241, Loss: 1158304449.6281137\n",
      "Iteration 9242, Loss: 1144310444.6640518\n",
      "Iteration 9243, Loss: 1243428374.743016\n",
      "Iteration 9244, Loss: 1318927406.302946\n",
      "Iteration 9245, Loss: 1121962137.636179\n",
      "Iteration 9246, Loss: 1052225358.0633394\n",
      "Iteration 9247, Loss: 1143848528.4726868\n",
      "Iteration 9248, Loss: 1168416461.485904\n",
      "Iteration 9249, Loss: 1198653630.6809871\n",
      "Iteration 9250, Loss: 1159738573.1946323\n",
      "Iteration 9251, Loss: 1171533490.6608944\n",
      "Iteration 9252, Loss: 1199308940.0860245\n",
      "Iteration 9253, Loss: 1300973295.4558947\n",
      "Iteration 9254, Loss: 1232542571.3708928\n",
      "Iteration 9255, Loss: 1107967361.3980508\n",
      "Iteration 9256, Loss: 1079518729.8245263\n",
      "Iteration 9257, Loss: 1196442741.3165834\n",
      "Iteration 9258, Loss: 1200725344.7641027\n",
      "Iteration 9259, Loss: 1053961632.0455749\n",
      "Iteration 9260, Loss: 1057074909.2168738\n",
      "Iteration 9261, Loss: 1060216212.8894534\n",
      "Iteration 9262, Loss: 1062019160.2628218\n",
      "Iteration 9263, Loss: 1192636710.6963217\n",
      "Iteration 9264, Loss: 1660458810.7898424\n",
      "Iteration 9265, Loss: 1049743619.8470666\n",
      "Iteration 9266, Loss: 1073056313.5746694\n",
      "Iteration 9267, Loss: 1110150659.1373506\n",
      "Iteration 9268, Loss: 1219165897.6834197\n",
      "Iteration 9269, Loss: 1201133431.0372803\n",
      "Iteration 9270, Loss: 1202662786.6380007\n",
      "Iteration 9271, Loss: 1133614574.603451\n",
      "Iteration 9272, Loss: 1177469760.9874005\n",
      "Iteration 9273, Loss: 1095759536.0883126\n",
      "Iteration 9274, Loss: 1189255695.7174497\n",
      "Iteration 9275, Loss: 1135528891.5482411\n",
      "Iteration 9276, Loss: 1878210490.489361\n",
      "Iteration 9277, Loss: 1226279267.6809359\n",
      "Iteration 9278, Loss: 1249642343.388535\n",
      "Iteration 9279, Loss: 1111451207.1667478\n",
      "Iteration 9280, Loss: 1359752159.4565554\n",
      "Iteration 9281, Loss: 1310172786.2905018\n",
      "Iteration 9282, Loss: 1213741900.7176938\n",
      "Iteration 9283, Loss: 1252073405.2055168\n",
      "Iteration 9284, Loss: 1320570141.5453026\n",
      "Iteration 9285, Loss: 1264783513.265364\n",
      "Iteration 9286, Loss: 1142085627.5634103\n",
      "Iteration 9287, Loss: 1311794003.1852167\n",
      "Iteration 9288, Loss: 1044354329.0850717\n",
      "Iteration 9289, Loss: 1081476708.6497824\n",
      "Iteration 9290, Loss: 1302533830.9665763\n",
      "Iteration 9291, Loss: 1269349437.0704322\n",
      "Iteration 9292, Loss: 1115886871.8470762\n",
      "Iteration 9293, Loss: 1108419866.91579\n",
      "Iteration 9294, Loss: 1160852025.8065186\n",
      "Iteration 9295, Loss: 1158210597.6508577\n",
      "Iteration 9296, Loss: 1052619313.3590156\n",
      "Iteration 9297, Loss: 1192755286.647859\n",
      "Iteration 9298, Loss: 1099563050.438333\n",
      "Iteration 9299, Loss: 1100327716.4449189\n",
      "Iteration 9300, Loss: 1100065379.2960873\n",
      "Iteration 9301, Loss: 1169691286.8724058\n",
      "Iteration 9302, Loss: 1185871685.5262349\n",
      "Iteration 9303, Loss: 1211928420.0534706\n",
      "Iteration 9304, Loss: 1128704318.4015164\n",
      "Iteration 9305, Loss: 1127887387.0403273\n",
      "Iteration 9306, Loss: 1335660086.761757\n",
      "Iteration 9307, Loss: 1362256885.424092\n",
      "Iteration 9308, Loss: 1359520840.2204301\n",
      "Iteration 9309, Loss: 1326000986.658886\n",
      "Iteration 9310, Loss: 1079652476.9523888\n",
      "Iteration 9311, Loss: 1102106938.993937\n",
      "Iteration 9312, Loss: 1144044492.7507825\n",
      "Iteration 9313, Loss: 1203247453.9061182\n",
      "Iteration 9314, Loss: 1232098745.1324694\n",
      "Iteration 9315, Loss: 1129214515.638041\n",
      "Iteration 9316, Loss: 1204798634.5327177\n",
      "Iteration 9317, Loss: 1160967445.260413\n",
      "Iteration 9318, Loss: 1260564709.3346763\n",
      "Iteration 9319, Loss: 1102215475.4422448\n",
      "Iteration 9320, Loss: 2052727115.5785017\n",
      "Iteration 9321, Loss: 1062347939.1278347\n",
      "Iteration 9322, Loss: 1858885953.789988\n",
      "Iteration 9323, Loss: 1649937523.1013978\n",
      "Iteration 9324, Loss: 1571860092.499818\n",
      "Iteration 9325, Loss: 1517925750.0374897\n",
      "Iteration 9326, Loss: 1477588851.7670085\n",
      "Iteration 9327, Loss: 1486458871.9925082\n",
      "Iteration 9328, Loss: 1150955644.3627536\n",
      "Iteration 9329, Loss: 1093072522.7100043\n",
      "Iteration 9330, Loss: 1560929855.925583\n",
      "Iteration 9331, Loss: 1067394144.626974\n",
      "Iteration 9332, Loss: 1070532873.7410015\n",
      "Iteration 9333, Loss: 1172325902.2126193\n",
      "Iteration 9334, Loss: 1268672367.6676054\n",
      "Iteration 9335, Loss: 1051667104.1177292\n",
      "Iteration 9336, Loss: 4039519395.4105096\n",
      "Iteration 9337, Loss: 9128381745.086866\n",
      "Iteration 9338, Loss: 6657347146.849141\n",
      "Iteration 9339, Loss: 19825064381.715363\n",
      "Iteration 9340, Loss: 1088359347.6532342\n",
      "Iteration 9341, Loss: 1123433528.820964\n",
      "Iteration 9342, Loss: 1108114914.7805986\n",
      "Iteration 9343, Loss: 1222515625.4281151\n",
      "Iteration 9344, Loss: 1109706300.54813\n",
      "Iteration 9345, Loss: 1139308410.5905623\n",
      "Iteration 9346, Loss: 1125452258.5478082\n",
      "Iteration 9347, Loss: 1089599949.6565351\n",
      "Iteration 9348, Loss: 1136698859.820099\n",
      "Iteration 9349, Loss: 1286638930.313527\n",
      "Iteration 9350, Loss: 1197377930.4308248\n",
      "Iteration 9351, Loss: 1175102637.0778751\n",
      "Iteration 9352, Loss: 1128126560.2772985\n",
      "Iteration 9353, Loss: 1123161715.2326784\n",
      "Iteration 9354, Loss: 1604463346.8727455\n",
      "Iteration 9355, Loss: 1437951379.4689755\n",
      "Iteration 9356, Loss: 1057145372.0539293\n",
      "Iteration 9357, Loss: 1046405520.20334\n",
      "Iteration 9358, Loss: 1024833220.3525964\n",
      "Iteration 9359, Loss: 1099755440.623811\n",
      "Iteration 9360, Loss: 1126993485.417559\n",
      "Iteration 9361, Loss: 1148709578.7644374\n",
      "Iteration 9362, Loss: 1143065937.977614\n",
      "Iteration 9363, Loss: 1092126059.4577885\n",
      "Iteration 9364, Loss: 1336553851.5749724\n",
      "Iteration 9365, Loss: 1250666751.2875545\n",
      "Iteration 9366, Loss: 1034051046.1276613\n",
      "Iteration 9367, Loss: 1057082000.3057547\n",
      "Iteration 9368, Loss: 1053883272.0162843\n",
      "Iteration 9369, Loss: 1047309004.4478409\n",
      "Iteration 9370, Loss: 1142139528.1456997\n",
      "Iteration 9371, Loss: 1149267998.4162698\n",
      "Iteration 9372, Loss: 1172396323.5330014\n",
      "Iteration 9373, Loss: 1666542284.996887\n",
      "Iteration 9374, Loss: 1596061199.183795\n",
      "Iteration 9375, Loss: 1023005855.6445478\n",
      "Iteration 9376, Loss: 1328650523.6701112\n",
      "Iteration 9377, Loss: 1305998960.3531082\n",
      "Iteration 9378, Loss: 1335379495.8931205\n",
      "Iteration 9379, Loss: 1320675431.0546787\n",
      "Iteration 9380, Loss: 1074014511.590462\n",
      "Iteration 9381, Loss: 2131265259.1823483\n",
      "Iteration 9382, Loss: 1033989585.1342751\n",
      "Iteration 9383, Loss: 2575412069.1336393\n",
      "Iteration 9384, Loss: 1196586275.4457443\n",
      "Iteration 9385, Loss: 1131142269.1658335\n",
      "Iteration 9386, Loss: 1037833635.3473625\n",
      "Iteration 9387, Loss: 3462980115.4352956\n",
      "Iteration 9388, Loss: 1078031622.6945167\n",
      "Iteration 9389, Loss: 1076899409.7678592\n",
      "Iteration 9390, Loss: 1072096557.9653686\n",
      "Iteration 9391, Loss: 1113096710.4469233\n",
      "Iteration 9392, Loss: 1099426918.52934\n",
      "Iteration 9393, Loss: 1077033174.787609\n",
      "Iteration 9394, Loss: 1559485219.0755236\n",
      "Iteration 9395, Loss: 1379060266.2421238\n",
      "Iteration 9396, Loss: 1120694412.714894\n",
      "Iteration 9397, Loss: 1087874784.8977883\n",
      "Iteration 9398, Loss: 1072647470.1974468\n",
      "Iteration 9399, Loss: 1061785228.6629897\n",
      "Iteration 9400, Loss: 1055483718.5996659\n",
      "Iteration 9401, Loss: 1050231131.2981906\n",
      "Iteration 9402, Loss: 1047150074.1197506\n",
      "Iteration 9403, Loss: 1612779531.3358493\n",
      "Iteration 9404, Loss: 1527073666.7470083\n",
      "Iteration 9405, Loss: 1123427348.5583348\n",
      "Iteration 9406, Loss: 1319753619.6853979\n",
      "Iteration 9407, Loss: 1191103219.7534199\n",
      "Iteration 9408, Loss: 1638761230.303641\n",
      "Iteration 9409, Loss: 1180028875.682805\n",
      "Iteration 9410, Loss: 1268162941.9489057\n",
      "Iteration 9411, Loss: 1229232173.5502417\n",
      "Iteration 9412, Loss: 1066904653.7459611\n",
      "Iteration 9413, Loss: 1067524125.3166076\n",
      "Iteration 9414, Loss: 1112972939.8582268\n",
      "Iteration 9415, Loss: 1088353632.742943\n",
      "Iteration 9416, Loss: 2091747808.0430007\n",
      "Iteration 9417, Loss: 1833458777.5140805\n",
      "Iteration 9418, Loss: 1302346323.7981627\n",
      "Iteration 9419, Loss: 1999340038.7178779\n",
      "Iteration 9420, Loss: 1918167502.6150217\n",
      "Iteration 9421, Loss: 1729386559.1508274\n",
      "Iteration 9422, Loss: 1242502748.3843446\n",
      "Iteration 9423, Loss: 1162606811.6236384\n",
      "Iteration 9424, Loss: 1162234175.336303\n",
      "Iteration 9425, Loss: 1161878920.314445\n",
      "Iteration 9426, Loss: 1133649780.639378\n",
      "Iteration 9427, Loss: 1027233074.8213086\n",
      "Iteration 9428, Loss: 1036177157.8902034\n",
      "Iteration 9429, Loss: 1033562093.0881666\n",
      "Iteration 9430, Loss: 1076750956.9690545\n",
      "Iteration 9431, Loss: 1075704212.9683442\n",
      "Iteration 9432, Loss: 1071144666.9183896\n",
      "Iteration 9433, Loss: 1363405570.3847773\n",
      "Iteration 9434, Loss: 1203381406.45239\n",
      "Iteration 9435, Loss: 1026763992.4077126\n",
      "Iteration 9436, Loss: 1270545018.3988101\n",
      "Iteration 9437, Loss: 1429122600.709344\n",
      "Iteration 9438, Loss: 1384033410.7114186\n",
      "Iteration 9439, Loss: 1343955785.7918918\n",
      "Iteration 9440, Loss: 1041672826.4699588\n",
      "Iteration 9441, Loss: 1279624024.9393294\n",
      "Iteration 9442, Loss: 1093599910.904407\n",
      "Iteration 9443, Loss: 1216644090.799136\n",
      "Iteration 9444, Loss: 1296636615.8992884\n",
      "Iteration 9445, Loss: 1220746740.3151693\n",
      "Iteration 9446, Loss: 1195498481.723132\n",
      "Iteration 9447, Loss: 1028123532.9429065\n",
      "Iteration 9448, Loss: 1036954891.0081308\n",
      "Iteration 9449, Loss: 1172764061.306603\n",
      "Iteration 9450, Loss: 1253749187.0201473\n",
      "Iteration 9451, Loss: 1120615306.223418\n",
      "Iteration 9452, Loss: 1213668884.4582727\n",
      "Iteration 9453, Loss: 1189539030.1859844\n",
      "Iteration 9454, Loss: 1094322580.0241463\n",
      "Iteration 9455, Loss: 1056180243.0002314\n",
      "Iteration 9456, Loss: 1058214447.8563583\n",
      "Iteration 9457, Loss: 1050535501.1256973\n",
      "Iteration 9458, Loss: 1155343386.7836242\n",
      "Iteration 9459, Loss: 1150029899.0131142\n",
      "Iteration 9460, Loss: 1131908656.1241508\n",
      "Iteration 9461, Loss: 1268625458.9650493\n",
      "Iteration 9462, Loss: 1132195376.3293138\n",
      "Iteration 9463, Loss: 1121763026.303583\n",
      "Iteration 9464, Loss: 1148256174.4507635\n",
      "Iteration 9465, Loss: 1157080934.8094459\n",
      "Iteration 9466, Loss: 1273086914.4649236\n",
      "Iteration 9467, Loss: 1206215743.8784382\n",
      "Iteration 9468, Loss: 1243942926.9735296\n",
      "Iteration 9469, Loss: 1182956674.821741\n",
      "Iteration 9470, Loss: 1260320062.470323\n",
      "Iteration 9471, Loss: 1288883375.4260454\n",
      "Iteration 9472, Loss: 1197783069.451575\n",
      "Iteration 9473, Loss: 1117047666.8462923\n",
      "Iteration 9474, Loss: 1108900940.3198152\n",
      "Iteration 9475, Loss: 1086773622.234521\n",
      "Iteration 9476, Loss: 1134036829.7007875\n",
      "Iteration 9477, Loss: 1158941499.428275\n",
      "Iteration 9478, Loss: 1151823526.4007568\n",
      "Iteration 9479, Loss: 1138065945.2956707\n",
      "Iteration 9480, Loss: 1230204029.2026515\n",
      "Iteration 9481, Loss: 1063514812.8075879\n",
      "Iteration 9482, Loss: 1169314180.9023979\n",
      "Iteration 9483, Loss: 1128957850.7112737\n",
      "Iteration 9484, Loss: 1176066978.0296476\n",
      "Iteration 9485, Loss: 1663517685.795225\n",
      "Iteration 9486, Loss: 1296601410.815291\n",
      "Iteration 9487, Loss: 1104236488.8563056\n",
      "Iteration 9488, Loss: 1248311415.1526303\n",
      "Iteration 9489, Loss: 1215179063.2525144\n",
      "Iteration 9490, Loss: 1133664387.498232\n",
      "Iteration 9491, Loss: 1290349315.5016732\n",
      "Iteration 9492, Loss: 1270569901.8721242\n",
      "Iteration 9493, Loss: 1238310745.8928206\n",
      "Iteration 9494, Loss: 1251996806.1151836\n",
      "Iteration 9495, Loss: 1150002153.0767436\n",
      "Iteration 9496, Loss: 1100588492.1539707\n",
      "Iteration 9497, Loss: 1217237764.1383612\n",
      "Iteration 9498, Loss: 1291811466.5549932\n",
      "Iteration 9499, Loss: 1034259958.4765334\n",
      "Iteration 9500, Loss: 1360059179.109272\n",
      "Iteration 9501, Loss: 1389890063.9369879\n",
      "Iteration 9502, Loss: 1194047420.2473614\n",
      "Iteration 9503, Loss: 1264230231.987006\n",
      "Iteration 9504, Loss: 1098645251.5476687\n",
      "Iteration 9505, Loss: 1137204816.257793\n",
      "Iteration 9506, Loss: 1121682730.6963406\n",
      "Iteration 9507, Loss: 1146296896.203599\n",
      "Iteration 9508, Loss: 1093221341.5440688\n",
      "Iteration 9509, Loss: 1139635916.789961\n",
      "Iteration 9510, Loss: 1237953073.2567127\n",
      "Iteration 9511, Loss: 1214992409.0037363\n",
      "Iteration 9512, Loss: 1097547450.0786788\n",
      "Iteration 9513, Loss: 1092136182.0349011\n",
      "Iteration 9514, Loss: 1123918920.5216627\n",
      "Iteration 9515, Loss: 1147887688.3309417\n",
      "Iteration 9516, Loss: 1170749209.8370023\n",
      "Iteration 9517, Loss: 1021020552.8857076\n",
      "Iteration 9518, Loss: 3276226104.58089\n",
      "Iteration 9519, Loss: 2430852091.334591\n",
      "Iteration 9520, Loss: 2318793817.426556\n",
      "Iteration 9521, Loss: 1999650826.6151285\n",
      "Iteration 9522, Loss: 1551717545.4679286\n",
      "Iteration 9523, Loss: 1099679693.8250833\n",
      "Iteration 9524, Loss: 1726126253.1971638\n",
      "Iteration 9525, Loss: 1647814250.2969398\n",
      "Iteration 9526, Loss: 1183936753.3984125\n",
      "Iteration 9527, Loss: 1064788394.1039335\n",
      "Iteration 9528, Loss: 1047484066.0488925\n",
      "Iteration 9529, Loss: 1041740419.6279091\n",
      "Iteration 9530, Loss: 1014726858.7180281\n",
      "Iteration 9531, Loss: 1014696499.0458747\n",
      "Iteration 9532, Loss: 1200193915.6928232\n",
      "Iteration 9533, Loss: 1118421937.9749858\n",
      "Iteration 9534, Loss: 1027434772.4568701\n",
      "Iteration 9535, Loss: 1124970744.5514972\n",
      "Iteration 9536, Loss: 1285836435.9458156\n",
      "Iteration 9537, Loss: 1308114904.1729596\n",
      "Iteration 9538, Loss: 1195388764.6131217\n",
      "Iteration 9539, Loss: 1181556513.5187051\n",
      "Iteration 9540, Loss: 1105592204.7981615\n",
      "Iteration 9541, Loss: 1099720405.1979961\n",
      "Iteration 9542, Loss: 1143863221.5005\n",
      "Iteration 9543, Loss: 1225132074.1051667\n",
      "Iteration 9544, Loss: 1191388880.957606\n",
      "Iteration 9545, Loss: 1105584516.064368\n",
      "Iteration 9546, Loss: 1089732258.3668087\n",
      "Iteration 9547, Loss: 1181746456.3558502\n",
      "Iteration 9548, Loss: 1014168234.6433655\n",
      "Iteration 9549, Loss: 1292322619.7983031\n",
      "Iteration 9550, Loss: 1054862643.1801524\n",
      "Iteration 9551, Loss: 1051816947.0992919\n",
      "Iteration 9552, Loss: 1361836365.7069926\n",
      "Iteration 9553, Loss: 1358811236.082485\n",
      "Iteration 9554, Loss: 1036609704.6450626\n",
      "Iteration 9555, Loss: 1128228884.0823483\n",
      "Iteration 9556, Loss: 1020004051.1733174\n",
      "Iteration 9557, Loss: 1269215219.1441448\n",
      "Iteration 9558, Loss: 1235914326.2471929\n",
      "Iteration 9559, Loss: 1261194105.561453\n",
      "Iteration 9560, Loss: 1018924810.1766578\n",
      "Iteration 9561, Loss: 2374982292.7062616\n",
      "Iteration 9562, Loss: 2076950800.5013309\n",
      "Iteration 9563, Loss: 1318814422.9134283\n",
      "Iteration 9564, Loss: 1236828840.6714356\n",
      "Iteration 9565, Loss: 1198719735.8523192\n",
      "Iteration 9566, Loss: 1070070142.6596211\n",
      "Iteration 9567, Loss: 1143961161.2905817\n",
      "Iteration 9568, Loss: 1106577754.9380803\n",
      "Iteration 9569, Loss: 1241473989.071055\n",
      "Iteration 9570, Loss: 1210522212.4129457\n",
      "Iteration 9571, Loss: 1027169483.1142627\n",
      "Iteration 9572, Loss: 2596605299.3382726\n",
      "Iteration 9573, Loss: 2150852475.2853994\n",
      "Iteration 9574, Loss: 2173542095.5623546\n",
      "Iteration 9575, Loss: 1060835553.5172566\n",
      "Iteration 9576, Loss: 1157197415.4870038\n",
      "Iteration 9577, Loss: 1186475178.8049982\n",
      "Iteration 9578, Loss: 1019605134.8608546\n",
      "Iteration 9579, Loss: 3454868015.940164\n",
      "Iteration 9580, Loss: 2586915653.3008547\n",
      "Iteration 9581, Loss: 2740998714.7571893\n",
      "Iteration 9582, Loss: 1240454971.0839486\n",
      "Iteration 9583, Loss: 1048007225.2221209\n",
      "Iteration 9584, Loss: 1042011969.6460701\n",
      "Iteration 9585, Loss: 1042710776.3608574\n",
      "Iteration 9586, Loss: 1043085285.5690669\n",
      "Iteration 9587, Loss: 1055255345.1294798\n",
      "Iteration 9588, Loss: 1052789649.3137703\n",
      "Iteration 9589, Loss: 1044715387.0079535\n",
      "Iteration 9590, Loss: 1786055462.070174\n",
      "Iteration 9591, Loss: 1314087475.4785478\n",
      "Iteration 9592, Loss: 1292760473.6006823\n",
      "Iteration 9593, Loss: 1289929206.5954394\n",
      "Iteration 9594, Loss: 1095311974.0649724\n",
      "Iteration 9595, Loss: 1045517235.6771443\n",
      "Iteration 9596, Loss: 1040832213.7656631\n",
      "Iteration 9597, Loss: 1112753259.6533682\n",
      "Iteration 9598, Loss: 1158474544.1888263\n",
      "Iteration 9599, Loss: 1161106686.636681\n",
      "Iteration 9600, Loss: 1069954548.9213071\n",
      "Iteration 9601, Loss: 1379638123.9172292\n",
      "Iteration 9602, Loss: 1335903857.2725563\n",
      "Iteration 9603, Loss: 1044629727.5683863\n",
      "Iteration 9604, Loss: 1047923422.0880758\n",
      "Iteration 9605, Loss: 1021764445.327185\n",
      "Iteration 9606, Loss: 1035967336.7362102\n",
      "Iteration 9607, Loss: 1170669073.7605867\n",
      "Iteration 9608, Loss: 1144313755.9462588\n",
      "Iteration 9609, Loss: 1127300558.4688363\n",
      "Iteration 9610, Loss: 1170569564.6041048\n",
      "Iteration 9611, Loss: 1284722647.224175\n",
      "Iteration 9612, Loss: 1095088029.558816\n",
      "Iteration 9613, Loss: 1133310378.6354477\n",
      "Iteration 9614, Loss: 1120977453.6582952\n",
      "Iteration 9615, Loss: 1108135981.4648056\n",
      "Iteration 9616, Loss: 1076307497.472602\n",
      "Iteration 9617, Loss: 1179315604.0164397\n",
      "Iteration 9618, Loss: 1252522324.3608341\n",
      "Iteration 9619, Loss: 1454764074.9790661\n",
      "Iteration 9620, Loss: 1403470619.3769546\n",
      "Iteration 9621, Loss: 1077930740.9733238\n",
      "Iteration 9622, Loss: 1073917362.3288593\n",
      "Iteration 9623, Loss: 1216141694.9184127\n",
      "Iteration 9624, Loss: 1202934850.4417632\n",
      "Iteration 9625, Loss: 1115396573.2592177\n",
      "Iteration 9626, Loss: 1309960631.0918121\n",
      "Iteration 9627, Loss: 1155724983.1804166\n",
      "Iteration 9628, Loss: 1235834203.716674\n",
      "Iteration 9629, Loss: 1497604448.9401505\n",
      "Iteration 9630, Loss: 1060154890.3163013\n",
      "Iteration 9631, Loss: 1060939971.6164042\n",
      "Iteration 9632, Loss: 1277431831.9727433\n",
      "Iteration 9633, Loss: 1295191472.6274385\n",
      "Iteration 9634, Loss: 1035025245.7447463\n",
      "Iteration 9635, Loss: 1034549826.1196175\n",
      "Iteration 9636, Loss: 1449426330.7182937\n",
      "Iteration 9637, Loss: 1039840346.7028987\n",
      "Iteration 9638, Loss: 1616174350.9351485\n",
      "Iteration 9639, Loss: 1530278060.4659438\n",
      "Iteration 9640, Loss: 1473172591.9421272\n",
      "Iteration 9641, Loss: 1408686414.4287562\n",
      "Iteration 9642, Loss: 1379696098.6184905\n",
      "Iteration 9643, Loss: 1278182102.1193266\n",
      "Iteration 9644, Loss: 1100200894.2858381\n",
      "Iteration 9645, Loss: 1092386605.9482026\n",
      "Iteration 9646, Loss: 2031927862.6021338\n",
      "Iteration 9647, Loss: 1142573708.4047434\n",
      "Iteration 9648, Loss: 1231753553.2785602\n",
      "Iteration 9649, Loss: 1175029659.8914824\n",
      "Iteration 9650, Loss: 1200332058.9231699\n",
      "Iteration 9651, Loss: 1179574151.7585678\n",
      "Iteration 9652, Loss: 1161243955.5784662\n",
      "Iteration 9653, Loss: 1146964554.276978\n",
      "Iteration 9654, Loss: 1035237711.3630819\n",
      "Iteration 9655, Loss: 2158737525.8059187\n",
      "Iteration 9656, Loss: 1928026627.5241249\n",
      "Iteration 9657, Loss: 1476142672.9332402\n",
      "Iteration 9658, Loss: 1317287977.5163069\n",
      "Iteration 9659, Loss: 1049299552.836776\n",
      "Iteration 9660, Loss: 1031103376.0149004\n",
      "Iteration 9661, Loss: 1033862377.0609679\n",
      "Iteration 9662, Loss: 1166451133.4547138\n",
      "Iteration 9663, Loss: 1250519651.3467407\n",
      "Iteration 9664, Loss: 1301531036.3839142\n",
      "Iteration 9665, Loss: 1070417885.2626823\n",
      "Iteration 9666, Loss: 1116198682.5746999\n",
      "Iteration 9667, Loss: 1164625409.9548209\n",
      "Iteration 9668, Loss: 1247696436.9917462\n",
      "Iteration 9669, Loss: 1283779576.0119724\n",
      "Iteration 9670, Loss: 1088615925.7338586\n",
      "Iteration 9671, Loss: 1211870165.1002047\n",
      "Iteration 9672, Loss: 1252497302.49806\n",
      "Iteration 9673, Loss: 1216872954.652241\n",
      "Iteration 9674, Loss: 1088481736.62039\n",
      "Iteration 9675, Loss: 1084775120.5137565\n",
      "Iteration 9676, Loss: 1076770893.5965223\n",
      "Iteration 9677, Loss: 1093670813.7014594\n",
      "Iteration 9678, Loss: 1073665801.4343522\n",
      "Iteration 9679, Loss: 1071833562.5642599\n",
      "Iteration 9680, Loss: 1374413324.995213\n",
      "Iteration 9681, Loss: 1321586824.3515387\n",
      "Iteration 9682, Loss: 1164822647.274578\n",
      "Iteration 9683, Loss: 1139295652.1061878\n",
      "Iteration 9684, Loss: 1152578640.82307\n",
      "Iteration 9685, Loss: 1450304470.1204548\n",
      "Iteration 9686, Loss: 1402930815.382032\n",
      "Iteration 9687, Loss: 1287448487.2468324\n",
      "Iteration 9688, Loss: 1042520278.5901213\n",
      "Iteration 9689, Loss: 2019569900.5686617\n",
      "Iteration 9690, Loss: 1034386220.5089986\n",
      "Iteration 9691, Loss: 3176818866.7587748\n",
      "Iteration 9692, Loss: 1418082825.1456127\n",
      "Iteration 9693, Loss: 1393929035.7837286\n",
      "Iteration 9694, Loss: 1199939625.3667185\n",
      "Iteration 9695, Loss: 1183111258.2972698\n",
      "Iteration 9696, Loss: 1167876478.752193\n",
      "Iteration 9697, Loss: 1180665889.943227\n",
      "Iteration 9698, Loss: 1279249345.687036\n",
      "Iteration 9699, Loss: 1266446878.8918974\n",
      "Iteration 9700, Loss: 1050377511.6896805\n",
      "Iteration 9701, Loss: 1460106334.3911917\n",
      "Iteration 9702, Loss: 1091528540.5793068\n",
      "Iteration 9703, Loss: 1299240094.272867\n",
      "Iteration 9704, Loss: 1083397818.8573892\n",
      "Iteration 9705, Loss: 1127351673.6750245\n",
      "Iteration 9706, Loss: 1157451902.8296983\n",
      "Iteration 9707, Loss: 1164342245.6001399\n",
      "Iteration 9708, Loss: 1113147839.940184\n",
      "Iteration 9709, Loss: 1094945661.4880576\n",
      "Iteration 9710, Loss: 1233028913.9244797\n",
      "Iteration 9711, Loss: 1209773251.937361\n",
      "Iteration 9712, Loss: 1280001132.1491916\n",
      "Iteration 9713, Loss: 1432607756.8555977\n",
      "Iteration 9714, Loss: 1046750601.4510703\n",
      "Iteration 9715, Loss: 1037303642.681728\n",
      "Iteration 9716, Loss: 1053735738.9461895\n",
      "Iteration 9717, Loss: 1419434843.9273555\n",
      "Iteration 9718, Loss: 1369490693.190889\n",
      "Iteration 9719, Loss: 1048483282.4792895\n",
      "Iteration 9720, Loss: 1083752489.7882998\n",
      "Iteration 9721, Loss: 1114442546.2043023\n",
      "Iteration 9722, Loss: 1937113889.096477\n",
      "Iteration 9723, Loss: 1379607405.9830334\n",
      "Iteration 9724, Loss: 1035081301.4869556\n",
      "Iteration 9725, Loss: 1031717198.1935174\n",
      "Iteration 9726, Loss: 2796420217.7894044\n",
      "Iteration 9727, Loss: 2292562539.1620307\n",
      "Iteration 9728, Loss: 1066536990.7431643\n",
      "Iteration 9729, Loss: 1173537047.3701882\n",
      "Iteration 9730, Loss: 1086928531.6995893\n",
      "Iteration 9731, Loss: 1195110896.1991525\n",
      "Iteration 9732, Loss: 1258180602.5645907\n",
      "Iteration 9733, Loss: 1162789289.6346817\n",
      "Iteration 9734, Loss: 1282048588.9246876\n",
      "Iteration 9735, Loss: 1098575884.9649055\n",
      "Iteration 9736, Loss: 1083098477.835896\n",
      "Iteration 9737, Loss: 1212038569.4562764\n",
      "Iteration 9738, Loss: 1190051813.5398102\n",
      "Iteration 9739, Loss: 1271673837.1474228\n",
      "Iteration 9740, Loss: 1044816214.8032092\n",
      "Iteration 9741, Loss: 1298857162.8095362\n",
      "Iteration 9742, Loss: 1163391585.004972\n",
      "Iteration 9743, Loss: 1044419406.0560986\n",
      "Iteration 9744, Loss: 2784584355.1611814\n",
      "Iteration 9745, Loss: 1041548960.5728916\n",
      "Iteration 9746, Loss: 1043257853.017922\n",
      "Iteration 9747, Loss: 1042363226.7425556\n",
      "Iteration 9748, Loss: 1128457429.0683343\n",
      "Iteration 9749, Loss: 1170076867.0734959\n",
      "Iteration 9750, Loss: 1137658970.7600324\n",
      "Iteration 9751, Loss: 1176189171.4233642\n",
      "Iteration 9752, Loss: 1158164406.6896806\n",
      "Iteration 9753, Loss: 1041717984.6050161\n",
      "Iteration 9754, Loss: 2937138783.368663\n",
      "Iteration 9755, Loss: 1072653764.0877368\n",
      "Iteration 9756, Loss: 1273134617.8538961\n",
      "Iteration 9757, Loss: 1222857441.8499439\n",
      "Iteration 9758, Loss: 1294336916.3447325\n",
      "Iteration 9759, Loss: 1184034783.0237412\n",
      "Iteration 9760, Loss: 1169236134.2707984\n",
      "Iteration 9761, Loss: 1174740566.88601\n",
      "Iteration 9762, Loss: 1286344909.80912\n",
      "Iteration 9763, Loss: 1315709327.402805\n",
      "Iteration 9764, Loss: 1141268673.1459603\n",
      "Iteration 9765, Loss: 1132190452.668566\n",
      "Iteration 9766, Loss: 1155475216.3576214\n",
      "Iteration 9767, Loss: 1256919436.0041733\n",
      "Iteration 9768, Loss: 1276675605.4165614\n",
      "Iteration 9769, Loss: 1313478905.0437922\n",
      "Iteration 9770, Loss: 1115846423.1800237\n",
      "Iteration 9771, Loss: 1059422077.7903848\n",
      "Iteration 9772, Loss: 1157866083.4252603\n",
      "Iteration 9773, Loss: 1114913710.8372202\n",
      "Iteration 9774, Loss: 1214815281.6075613\n",
      "Iteration 9775, Loss: 1289239619.0170393\n",
      "Iteration 9776, Loss: 1083115807.7875872\n",
      "Iteration 9777, Loss: 1184048907.030163\n",
      "Iteration 9778, Loss: 1137095599.4015057\n",
      "Iteration 9779, Loss: 1305720168.7090592\n",
      "Iteration 9780, Loss: 1108443125.7164307\n",
      "Iteration 9781, Loss: 1210140959.1733592\n",
      "Iteration 9782, Loss: 1277855582.7359838\n",
      "Iteration 9783, Loss: 1247435506.6748233\n",
      "Iteration 9784, Loss: 1224770653.72318\n",
      "Iteration 9785, Loss: 1213411644.5204196\n",
      "Iteration 9786, Loss: 1232073056.7958083\n",
      "Iteration 9787, Loss: 1141950341.7172205\n",
      "Iteration 9788, Loss: 1264583858.0029526\n",
      "Iteration 9789, Loss: 1311662467.3266692\n",
      "Iteration 9790, Loss: 1275950532.5992856\n",
      "Iteration 9791, Loss: 1263312449.6078098\n",
      "Iteration 9792, Loss: 1237128166.7934287\n",
      "Iteration 9793, Loss: 1311717630.7886312\n",
      "Iteration 9794, Loss: 1374623644.868108\n",
      "Iteration 9795, Loss: 1038704151.3827116\n",
      "Iteration 9796, Loss: 1077941681.9762347\n",
      "Iteration 9797, Loss: 1090436698.009124\n",
      "Iteration 9798, Loss: 1139875389.1593182\n",
      "Iteration 9799, Loss: 1128067244.7965956\n",
      "Iteration 9800, Loss: 1156482802.3982441\n",
      "Iteration 9801, Loss: 1217285138.8283184\n",
      "Iteration 9802, Loss: 1125262210.079749\n",
      "Iteration 9803, Loss: 1123248544.602153\n",
      "Iteration 9804, Loss: 1658320138.4283016\n",
      "Iteration 9805, Loss: 1575046522.1590815\n",
      "Iteration 9806, Loss: 1167593420.4717035\n",
      "Iteration 9807, Loss: 1175292828.5288808\n",
      "Iteration 9808, Loss: 1307708213.221891\n",
      "Iteration 9809, Loss: 1038542088.2164936\n",
      "Iteration 9810, Loss: 2888919528.91053\n",
      "Iteration 9811, Loss: 2361977505.461735\n",
      "Iteration 9812, Loss: 1061572028.3004938\n",
      "Iteration 9813, Loss: 1040496049.1156578\n",
      "Iteration 9814, Loss: 1079691972.850029\n",
      "Iteration 9815, Loss: 1391412846.452191\n",
      "Iteration 9816, Loss: 1131719009.0006967\n",
      "Iteration 9817, Loss: 1183520009.4729452\n",
      "Iteration 9818, Loss: 1210549121.8007226\n",
      "Iteration 9819, Loss: 1203120748.9080887\n",
      "Iteration 9820, Loss: 1047656676.0712731\n",
      "Iteration 9821, Loss: 1156550967.6347096\n",
      "Iteration 9822, Loss: 1161641069.0916224\n",
      "Iteration 9823, Loss: 1188365144.0358682\n",
      "Iteration 9824, Loss: 1271653576.8258405\n",
      "Iteration 9825, Loss: 1233881159.589552\n",
      "Iteration 9826, Loss: 1309900538.9550836\n",
      "Iteration 9827, Loss: 1115973906.2155533\n",
      "Iteration 9828, Loss: 1948954412.9934573\n",
      "Iteration 9829, Loss: 1861438054.064557\n",
      "Iteration 9830, Loss: 1167816213.8029962\n",
      "Iteration 9831, Loss: 1148243141.1452644\n",
      "Iteration 9832, Loss: 1121685876.7261589\n",
      "Iteration 9833, Loss: 1112780912.446554\n",
      "Iteration 9834, Loss: 1144923971.9775565\n",
      "Iteration 9835, Loss: 1196887665.8748665\n",
      "Iteration 9836, Loss: 1106001961.6799853\n",
      "Iteration 9837, Loss: 1160994883.9207418\n",
      "Iteration 9838, Loss: 1166430937.3883643\n",
      "Iteration 9839, Loss: 1140862296.9610937\n",
      "Iteration 9840, Loss: 1307897900.5011954\n",
      "Iteration 9841, Loss: 1119563450.3721335\n",
      "Iteration 9842, Loss: 1365723185.9328737\n",
      "Iteration 9843, Loss: 1330735219.1873431\n",
      "Iteration 9844, Loss: 1199190348.5673532\n",
      "Iteration 9845, Loss: 1180285685.632366\n",
      "Iteration 9846, Loss: 1697148039.7736423\n",
      "Iteration 9847, Loss: 1293801257.5499887\n",
      "Iteration 9848, Loss: 1045385087.995635\n",
      "Iteration 9849, Loss: 1058991618.5444946\n",
      "Iteration 9850, Loss: 1087792931.6351643\n",
      "Iteration 9851, Loss: 1541541943.9745111\n",
      "Iteration 9852, Loss: 1082631564.2076025\n",
      "Iteration 9853, Loss: 1067965007.9527988\n",
      "Iteration 9854, Loss: 1069162490.8737055\n",
      "Iteration 9855, Loss: 1178332218.8614366\n",
      "Iteration 9856, Loss: 1306837269.1208413\n",
      "Iteration 9857, Loss: 1341954362.3748505\n",
      "Iteration 9858, Loss: 1182425181.5956273\n",
      "Iteration 9859, Loss: 1268895816.5582197\n",
      "Iteration 9860, Loss: 1110756595.637403\n",
      "Iteration 9861, Loss: 1091142854.407344\n",
      "Iteration 9862, Loss: 1109724611.135128\n",
      "Iteration 9863, Loss: 1153311833.2122777\n",
      "Iteration 9864, Loss: 1228594998.3216202\n",
      "Iteration 9865, Loss: 1151433355.5934913\n",
      "Iteration 9866, Loss: 1170938258.1123178\n",
      "Iteration 9867, Loss: 1258889089.7296069\n",
      "Iteration 9868, Loss: 1232793264.3065128\n",
      "Iteration 9869, Loss: 1123351752.1686392\n",
      "Iteration 9870, Loss: 1121698515.647151\n",
      "Iteration 9871, Loss: 1301259092.3154032\n",
      "Iteration 9872, Loss: 1096223250.112737\n",
      "Iteration 9873, Loss: 1484063319.043552\n",
      "Iteration 9874, Loss: 1437417042.9412887\n",
      "Iteration 9875, Loss: 1312263794.104565\n",
      "Iteration 9876, Loss: 1284266253.0579531\n",
      "Iteration 9877, Loss: 1220356265.8273358\n",
      "Iteration 9878, Loss: 1041124685.4030963\n",
      "Iteration 9879, Loss: 1042827557.935872\n",
      "Iteration 9880, Loss: 1042472087.5926138\n",
      "Iteration 9881, Loss: 1134635237.4118748\n",
      "Iteration 9882, Loss: 1216357812.7467253\n",
      "Iteration 9883, Loss: 1085465685.032529\n",
      "Iteration 9884, Loss: 1086148153.886331\n",
      "Iteration 9885, Loss: 1387626534.9826012\n",
      "Iteration 9886, Loss: 1265181139.0618086\n",
      "Iteration 9887, Loss: 1324637390.6716378\n",
      "Iteration 9888, Loss: 1295345576.8727958\n",
      "Iteration 9889, Loss: 1317065644.6330369\n",
      "Iteration 9890, Loss: 1111381372.9627843\n",
      "Iteration 9891, Loss: 1215237393.642215\n",
      "Iteration 9892, Loss: 1040099368.53749\n",
      "Iteration 9893, Loss: 1046396630.5026416\n",
      "Iteration 9894, Loss: 1047204340.3824166\n",
      "Iteration 9895, Loss: 1185694694.2115314\n",
      "Iteration 9896, Loss: 1093704019.3850741\n",
      "Iteration 9897, Loss: 2090492233.1724544\n",
      "Iteration 9898, Loss: 1889370644.253635\n",
      "Iteration 9899, Loss: 1799455886.7046\n",
      "Iteration 9900, Loss: 1589298602.8951838\n",
      "Iteration 9901, Loss: 1518768212.9127045\n",
      "Iteration 9902, Loss: 1144181498.4825914\n",
      "Iteration 9903, Loss: 1131318460.5661843\n",
      "Iteration 9904, Loss: 1232087908.3362513\n",
      "Iteration 9905, Loss: 1305829064.1960819\n",
      "Iteration 9906, Loss: 1081729552.947024\n",
      "Iteration 9907, Loss: 1287453504.563206\n",
      "Iteration 9908, Loss: 1223699024.7454548\n",
      "Iteration 9909, Loss: 1202821144.9943397\n",
      "Iteration 9910, Loss: 1135122000.4656274\n",
      "Iteration 9911, Loss: 1235049716.3028052\n",
      "Iteration 9912, Loss: 1220501722.7782917\n",
      "Iteration 9913, Loss: 1175861476.4642165\n",
      "Iteration 9914, Loss: 1159877895.9497056\n",
      "Iteration 9915, Loss: 1777612760.971136\n",
      "Iteration 9916, Loss: 1576780747.886785\n",
      "Iteration 9917, Loss: 1338391892.902511\n",
      "Iteration 9918, Loss: 1375818614.172135\n",
      "Iteration 9919, Loss: 1342021714.3673875\n",
      "Iteration 9920, Loss: 1375937346.265935\n",
      "Iteration 9921, Loss: 1069945244.7331026\n",
      "Iteration 9922, Loss: 1071475109.8665266\n",
      "Iteration 9923, Loss: 1076716547.504899\n",
      "Iteration 9924, Loss: 1403870229.468678\n",
      "Iteration 9925, Loss: 1089877387.809439\n",
      "Iteration 9926, Loss: 1072229359.98154\n",
      "Iteration 9927, Loss: 1070195746.5067216\n",
      "Iteration 9928, Loss: 1057327647.2973526\n",
      "Iteration 9929, Loss: 1047544316.624661\n",
      "Iteration 9930, Loss: 2208875743.4110084\n",
      "Iteration 9931, Loss: 1627864473.4818766\n",
      "Iteration 9932, Loss: 1263444398.594415\n",
      "Iteration 9933, Loss: 1244581447.1374137\n",
      "Iteration 9934, Loss: 1719132658.0970783\n",
      "Iteration 9935, Loss: 1625384731.9102294\n",
      "Iteration 9936, Loss: 1085651143.8466976\n",
      "Iteration 9937, Loss: 1087003594.3556268\n",
      "Iteration 9938, Loss: 1303786992.9895513\n",
      "Iteration 9939, Loss: 1173293240.9707143\n",
      "Iteration 9940, Loss: 1164316793.7311082\n",
      "Iteration 9941, Loss: 1296147341.7921553\n",
      "Iteration 9942, Loss: 1231230607.8878415\n",
      "Iteration 9943, Loss: 1304465438.6013849\n",
      "Iteration 9944, Loss: 1044622754.5462966\n",
      "Iteration 9945, Loss: 1525771717.38758\n",
      "Iteration 9946, Loss: 1563356138.3910048\n",
      "Iteration 9947, Loss: 1126962730.4279692\n",
      "Iteration 9948, Loss: 1159967797.7113733\n",
      "Iteration 9949, Loss: 1254165504.6876988\n",
      "Iteration 9950, Loss: 1244673766.5899131\n",
      "Iteration 9951, Loss: 1222880696.6462731\n",
      "Iteration 9952, Loss: 1252633863.3165476\n",
      "Iteration 9953, Loss: 1236481072.1238236\n",
      "Iteration 9954, Loss: 1214255142.558638\n",
      "Iteration 9955, Loss: 1207983913.079276\n",
      "Iteration 9956, Loss: 1099360092.4903674\n",
      "Iteration 9957, Loss: 1902277076.52187\n",
      "Iteration 9958, Loss: 1762700890.1861548\n",
      "Iteration 9959, Loss: 1208301759.5423648\n",
      "Iteration 9960, Loss: 1141744027.4865685\n",
      "Iteration 9961, Loss: 1129990615.8064325\n",
      "Iteration 9962, Loss: 1153373839.9334364\n",
      "Iteration 9963, Loss: 1207245427.6811142\n",
      "Iteration 9964, Loss: 1241553661.3163867\n",
      "Iteration 9965, Loss: 1298616258.4248714\n",
      "Iteration 9966, Loss: 1267336599.0334926\n",
      "Iteration 9967, Loss: 1243274310.2099888\n",
      "Iteration 9968, Loss: 1193940185.1964228\n",
      "Iteration 9969, Loss: 1664512789.2027266\n",
      "Iteration 9970, Loss: 1305860277.97925\n",
      "Iteration 9971, Loss: 1320851620.9605148\n",
      "Iteration 9972, Loss: 1358491957.111623\n",
      "Iteration 9973, Loss: 1113919284.7326272\n",
      "Iteration 9974, Loss: 1105254407.9868615\n",
      "Iteration 9975, Loss: 1158143013.9884987\n",
      "Iteration 9976, Loss: 1201139496.9067264\n",
      "Iteration 9977, Loss: 1112736354.982964\n",
      "Iteration 9978, Loss: 1429970406.418741\n",
      "Iteration 9979, Loss: 1060047361.0364655\n",
      "Iteration 9980, Loss: 1087864816.8885617\n",
      "Iteration 9981, Loss: 1089623197.1226497\n",
      "Iteration 9982, Loss: 1227049031.992222\n",
      "Iteration 9983, Loss: 1132778689.4229414\n",
      "Iteration 9984, Loss: 1185602782.1090925\n",
      "Iteration 9985, Loss: 1224106672.4048867\n",
      "Iteration 9986, Loss: 1162566312.8871195\n",
      "Iteration 9987, Loss: 1150597711.520956\n",
      "Iteration 9988, Loss: 1310808494.391929\n",
      "Iteration 9989, Loss: 1213195009.1187527\n",
      "Iteration 9990, Loss: 1242939706.4939272\n",
      "Iteration 9991, Loss: 1245268838.5417814\n",
      "Iteration 9992, Loss: 1237285419.7436392\n",
      "Iteration 9993, Loss: 1217685557.220541\n",
      "Iteration 9994, Loss: 1247604594.0353768\n",
      "Iteration 9995, Loss: 1067258479.8077629\n",
      "Iteration 9996, Loss: 1072582849.5421635\n",
      "Iteration 9997, Loss: 1213062807.1635792\n",
      "Iteration 9998, Loss: 1258032700.6688914\n",
      "Iteration 9999, Loss: 1203138771.2909436\n",
      "Iteration 10000, Loss: 1226954896.134909\n",
      "Predicted values  [ 86999.45 119798.63  37315.24]\n",
      "Real values       [[ 93087.56416713]\n",
      " [125216.02455331]\n",
      " [ 38575.94680508]]\n",
      "Trained W         4572.964565690326\n",
      "Trained b         [30953.76135329]\n",
      "0.9389321212589531\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 50  # Number of data points\n",
    "X = np.random.uniform(1, 20, num_samples).reshape(-1, 1)  # Random years of experience between 1 and 20\n",
    "true_W = 5000  # True weight (salary increase per year)\n",
    "true_b = 30000  # True bias (base salary)\n",
    "noise = np.random.normal(0, 5000, num_samples).reshape(-1, 1)  # Add Gaussian noise (stddev = 5000)\n",
    "Y = true_W * X + true_b + noise  # Linear relationship with noise\n",
    "\n",
    "# Convert to DataFrame and save to CSV (optional)\n",
    "df = pd.DataFrame(np.hstack((X, Y)), columns=[\"YearsExperience\", \"Salary\"])\n",
    "df.to_csv(\"synthetic_salary_data.csv\", index=False)  # Save dataset to CSV\n",
    "\n",
    "# Splitting dataset into train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3, random_state=0)\n",
    "\n",
    "# Model training     \n",
    "model = MyLinReg(max_iter=10000) \n",
    "model.fit( X_train, Y_train ) \n",
    "    \n",
    "# Prediction on test set \n",
    "Y_pred = model.predict( X_test ) \n",
    "print( \"Predicted values \", np.round( Y_pred[:3], 2 ) )     \n",
    "print( \"Real values      \", Y_test[:3] )    \n",
    "print( \"Trained W        \",  model.W[0] )   \n",
    "print( \"Trained b        \", model.b) \n",
    "\n",
    "score = model.score(X_test,Y_test)\n",
    "print(score)\n",
    "\n",
    "# Visualization on test set     \n",
    "plt.scatter( X_test, Y_test, color = 'blue' )    \n",
    "plt.plot( X_test, Y_pred, color = 'orange' )   \n",
    "plt.title( 'Salary vs Experience' ) \n",
    "plt.xlabel( 'Years of Experience' )   \n",
    "plt.ylabel( 'Salary' ) \n",
    "plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Qd: The Journaling of Your Regressor \n",
    "\n",
    "For the journal, write a full explanation of how you implemented the linear regressor, including a code walk-through (or mini-review of the most interesting parts).\n",
    "\n",
    "### Qe: Mathematical Foundation for Training a Linear Regressor\n",
    "\n",
    "You must also include the theoretical mathematical foundation for the linear regressor using the following equations and graphs (free to include in your journal without cite/reference), and relate them directly to your code:\n",
    "\n",
    "* Design matrix of size $(n, d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "    \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands v01, remember: no newlines in defs}\n",
    "    \\rem{MACRO eq: equation <#1:lhs> <#2:rhs>}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\rem{MACRO arr: array <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\rem{MACRO ac: array column vector <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\rem{MACRO st: subscript text <#1:content>}\n",
    "    \\def\\st#1{_{\\textrm{#1}}}\n",
    "    \\rem{MACRO norm: norm caligari L <#1:content>}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\rem{MACRO obs: ??}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\rem{MACRO diff: math differetial operator <#1:content>}\n",
    "    \\def\\diff#1{\\mathrm{d}#1} \n",
    "    \\rem{MACRO half: shorthand for 1/2}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\rem{MACRO pfrac: partial fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\rem{MACRO dfrac: differetial operator fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "    \\rem{MACRO pown: power and parantesis (train/test..) <#1:content>}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\rem{MACROS powi, pown: shorthands for power (i) and (n)}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\rem{MACROS powtest, powertrain: power (test) and (train)}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\rem{MACRO boldmatrix: bold matix/vector notation} \n",
    "    \\def\\boldmatrix#1{\\mathbf{#1}} \n",
    "    \\rem{MACROS X,Z,x,y,w: bold X,Z,x etc.} \n",
    "    \\def\\bX{\\boldmatrix{X}}\n",
    "    \\def\\bZ{\\boldmatrix{Z}}\n",
    "    \\def\\bx{\\boldmatrix{x}}\n",
    "    \\def\\by{\\boldmatrix{y}}\n",
    "    \\def\\bw{\\boldmatrix{w}}\n",
    "    \\def\\bz{\\boldmatrix{z}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\rem{MACROS stpred, sttrue: shorthand for subscript 'pred' and 'true'}\n",
    "    \\def\\stpred{\\st{pred}~}\n",
    "    \\def\\sttrue{\\st{true}~}\n",
    "    \\rem{MACROS ypred, ytrue:   shorthand for scalar y 'pred' and 'true'}\n",
    "    \\def\\ytrue{y\\sttrue}\n",
    "    \\def\\ypred{y\\stpred} \n",
    "    \\rem{MACROS bypred, bytrue: shorthand for vecor y 'pred' and 'true'} \n",
    "    \\def\\bypred{\\boldmatrix{y}\\stpred}\n",
    "    \\def\\bytrue{\\boldmatrix{y}\\sttrue} \n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2} \\\\\n",
    "            \\vdots      &             &        & \\vdots      \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn   \\\\\n",
    "        } \n",
    "$$\n",
    "\n",
    "* Target ground-truth column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\bytrue =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1}\\sttrue \\\\\n",
    "     y\\pown{2}\\sttrue \\\\\n",
    "     \\vdots           \\\\\n",
    "     y\\pown{n}\\sttrue \\\\\n",
    "  } \n",
    "$$\n",
    "\n",
    "* Bias factor, and by convention in the following (prepend one)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} & \\mapsto \\bx\\powni\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Weight column vector of size $d+1$ (i.e. with bias or intercept element $w_0$ prepended)\n",
    "\n",
    "$$\n",
    "\\bw =\n",
    "    \\ac{c}{\n",
    "         w_0    \\\\\n",
    "         w_1    \\\\\n",
    "         w_2    \\\\\n",
    "         \\vdots \\\\\n",
    "         w_d    \\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "* Linear regression model hypothesis function for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  ~~~~~~~~~~~~~~~\n",
    "  h(\\bx\\powni;\\bw) &= \\ypred\\powni \\\\\n",
    "                   &= \\bw^\\top \\bx\\powni ~~~~ (\\bx\\powni~\\textrm{with bias element})\\\\ \n",
    "                   &= w_0  \\cdot 1+ w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni & \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Individual losses based on the $\\norm{2}^2$ (last part assuming one dimensional output)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || \\ypred \\powni         - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || h(\\bx\\powni;\\bw)      - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - \\ytrue\\powni~ \\right)^2 ~~~~~ \\textrm{(only for 1D output)}\n",
    "}\n",
    "$$\n",
    "\n",
    "* MSE loss function\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\textrm{MSE}(\\bX,\\bytrue;\\bw)  &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni\\sttrue \\right)^2\\\\\n",
    "                                   &= \\frac{1}{n} ||\\bX \\bw - \\bytrue||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "\n",
    "* Loss function, proportional to (R)MSE\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "   J &= \\frac{1}{2} ||\\bX \\bw - \\bytrue||_2^2\\\\\n",
    "     &  \\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "* Training: computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total loss\n",
    "\n",
    "$$\n",
    "  \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "* Visualization of $\\textrm{argmin}_\\bw$ means to the argument of $\\bw$ that minimizes the $J$ function. The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always forms a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">\n",
    "\n",
    "#### Training I: The Closed-form Solution\n",
    "\n",
    "* Finding the optimal weight in a _one-step_ analytic expression \n",
    "\n",
    "$$\n",
    "  \\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1}~ \\bX^\\top \\bytrue\n",
    "$$\n",
    "\n",
    "\n",
    "#### Training II: Numerical Optimization \n",
    "\n",
    "* The Gradient of the loss function\n",
    "\n",
    "$$   \n",
    "  \\nabla_\\bw~J = \\left[ \\frac{\\partial J}{\\partial w_1} ~~~~ \\frac{\\partial J}{\\partial w_2} ~~~~ \\ldots  ~~~~ \\frac{\\partial J}{\\partial w_d} \\right]^\\top\n",
    "$$\n",
    "\n",
    "* The Gradient for the based $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\bytrue \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "* The Gradient Decent Algorithm (GD)\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)}~ = \\bw^{(step~N)} ~ - \\eta \\nabla_{\\bw} J\n",
    "$$\n",
    "\n",
    "* Visualization of GD, showing $J$ as a function of two $w$-dimensions\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf: Smoke testing\n",
    "\n",
    "Once ready, you can test your regressor via the test stub below, or create your own _test suite_.\n",
    "\n",
    "Be aware that setting the stepsize, $\\eta$, value can be tricky, and you might want to tune `eta0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARN:  This mini smoke-test may produce false-positives and/or\n",
      "       false-negatives..\u001b[0m\n",
      "\n",
      "Iteration 1, Loss: 31.06302760003793\n",
      "Iteration 2, Loss: 29.38835770737716\n",
      "Iteration 3, Loss: 28.86236477654574\n",
      "Iteration 4, Loss: 27.866516696911276\n",
      "Iteration 5, Loss: 26.905339116719666\n",
      "Iteration 6, Loss: 26.435319070027436\n",
      "Iteration 7, Loss: 25.02388531403153\n",
      "Iteration 8, Loss: 24.595330610026373\n",
      "Iteration 9, Loss: 23.748419138837825\n",
      "Iteration 10, Loss: 23.348274942384172\n",
      "\u001b[1;35mINFO:  y_pred = [0.86484733 1.14289115 0.48265928 0.7597091 ]\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  SCORE = \u001b[1;34m-10.871925356928498\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;33mWARN:  your regressor has no coef_/intercept_ atrributes, trying We\\\n",
      "       ights() instead..\u001b[0m\n",
      "\n",
      "\u001b[0;31m       EXCEPTION: 'MyLinReg' object has no attribute 'coef_'\u001b[0m\n",
      "\n",
      "\u001b[1;31mERROR: your regressor also has no Weights() function, giving up!\u001b[0m\n",
      "\n",
      "\u001b[0;31m       EXCEPTION: 'MyLinReg' object has no attribute 'Weights'\u001b[0m\n",
      "\n",
      "\u001b[1;31mERROR: your regressor fails during extraction of bias and weights (\\\n",
      "       but is a COULD)\u001b[0m\n",
      "\n",
      "\u001b[0;31m       EXCEPTION: 'MyLinReg' object has no attribute 'Weights'\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MyLinReg' object has no attribute 'Weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 259\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m Warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis mini smoke-test may produce false-positives and/or\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m false-negatives..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 259\u001b[0m TestMyLinReg()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[58], line 220\u001b[0m, in \u001b[0;36mTestMyLinReg\u001b[1;34m()\u001b[0m\n\u001b[0;32m    218\u001b[0m     Info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficients = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 220\u001b[0m     Err(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour regressor fails during extraction of bias and weights (but is a COULD)\u001b[39m\u001b[38;5;124m\"\u001b[39m, ex)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlibitmal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrintMatrix\n",
      "Cell \u001b[1;32mIn[58], line 115\u001b[0m, in \u001b[0;36mErr\u001b[1;34m(msg, ex)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mErr\u001b[39m(msg, ex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    114\u001b[0m     PrintOutput(msg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlred\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mif\u001b[39;00m ex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ex\n",
      "Cell \u001b[1;32mIn[58], line 216\u001b[0m, in \u001b[0;36mTestMyLinReg\u001b[1;34m()\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    215\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     Err(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour regressor also has no Weights() function, giving up!\u001b[39m\u001b[38;5;124m\"\u001b[39m, ex)\n\u001b[0;32m    217\u001b[0m Info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias         = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    218\u001b[0m Info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficients = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[58], line 115\u001b[0m, in \u001b[0;36mErr\u001b[1;34m(msg, ex)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mErr\u001b[39m(msg, ex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    114\u001b[0m     PrintOutput(msg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlred\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mif\u001b[39;00m ex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ex\n",
      "Cell \u001b[1;32mIn[58], line 205\u001b[0m, in \u001b[0;36mTestMyLinReg\u001b[1;34m()\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         w \u001b[38;5;241m=\u001b[39m regressor\u001b[38;5;241m.\u001b[39mWeights() \u001b[38;5;66;03m# maybe a Weigths function is avalible on you model?\u001b[39;00m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m w\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m,     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only handle vector like w\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms for now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MyLinReg' object has no attribute 'Weights'"
     ]
    }
   ],
   "source": [
    "# Mini smoke test for your linear regressor: TestMyLinReg\n",
    "\n",
    "import sys, os\n",
    "import numpy\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "### SOME NIFTY HELPER FUNS ###\n",
    "\n",
    "def isVector(y, expected_n=-1):\n",
    "    assert isinstance(y, numpy.ndarray), f\"expected type 'numpy.array' but got {type(y)}\"\n",
    "    assert y.ndim==1, f\"expected y.ndim==1 but got {y.ndim}\"\n",
    "    assert expected_n<0 or expected_n==y.shape[0], f\"expected vector of size {expected_n} but got size {y.shape}\"\n",
    "    return True\n",
    "\n",
    "def isMatrix(X, expected_m=-1, expected_n=-1):\n",
    "    assert isinstance(X, numpy.ndarray), f\"expected type 'numpy.array' but got {type(X)}\"\n",
    "    assert X.ndim==2, f\"expected X.ndim==2 but got {X.ndim}\"\n",
    "    assert expected_m<0 or expected_m==y.shape[0], f\"expected matrix of size {expected_m}x{expected_n} but got size {X.shape}\"\n",
    "    assert expected_n<0 or expected_n==y.shape[1], f\"expected vector of size {expected_m}x{expected_n} but got size {X.shape}\"\n",
    "    return True\n",
    "\n",
    "def PrintMatrix(x, label=\"\", precision=12, linewidth=60):\n",
    "    hasFancy = False\n",
    "    try:\n",
    "        # NOTE: how does multiple import behave, any performance issues?\n",
    "        from libitmal.utils import PrintMatrix as FancyPrintMatrix\n",
    "        hasFancy = True\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        FancyPrintMatrix(x, label=label, precision=precision, linewidth=linewidth)\n",
    "    else:\n",
    "        # default simple implementation\n",
    "        print(f\"{label}{' ' if len(label)>0 else ''}{x}\")\n",
    "\n",
    "def Col(color):\n",
    "    hasFancy = False\n",
    "    try:\n",
    "        from libitmal.Utils.colors import Col as FancyCol\n",
    "        hasFancy = True\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import Col from libitmal.Utils.colors, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        return FancyCol(color)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def ColEnd():\n",
    "    hasFancy = False\n",
    "    try:\n",
    "        from libitmal.Utils.colors import ColEnd as FancyColEnd\n",
    "        hasFancy = True\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import Col from libitmal.Utils.colors, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        return FancyColEnd()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def PrintOutput(msg, pre_msg, ex=None, color=\"\", filestream=sys.stdout):\n",
    "\n",
    "    def FormatTxt(txt, linewidth=60, prefix=\"\", replacetabs=True):\n",
    "        assert isinstance(txt, str)\n",
    "        assert isinstance(linewidth, int) and linewidth > 0\n",
    "        assert isinstance(prefix, str)\n",
    "\n",
    "        if replacetabs:\n",
    "            txt = txt.replace(\"\\t\",\"    \")\n",
    "\n",
    "        r = \"\"\n",
    "        n = 0\n",
    "        m = 0\n",
    "        for i in txt:\n",
    "            m += 1\n",
    "            if n >= linewidth:\n",
    "                if not i.isspace() and m < len(txt) and not txt[m].isspace():\n",
    "                    r += \"\\\\\" # add hypen\n",
    "                r += \"\\n\" + prefix\n",
    "                n = 0\n",
    "\n",
    "            if n == 0 and i.isspace():\n",
    "                continue # skip leading space\n",
    "\n",
    "            r += i\n",
    "            n += 1\n",
    "\n",
    "            if i == \"\\n\":\n",
    "                r += prefix\n",
    "                n = 0\n",
    "\n",
    "        return r\n",
    "\n",
    "    col_beg = Col(color)\n",
    "    col_end = ColEnd()\n",
    "\n",
    "    prefix = \"\".ljust(len(pre_msg)) \n",
    "    msg = FormatTxt(msg, prefix=prefix)\n",
    "    \n",
    "    print(f\"{col_beg}{pre_msg}{msg}{col_end}\\n\", file=filestream)\n",
    "\n",
    "    if ex is not None:\n",
    "        #msg += f\"\\n   EXCEPTION: {ex} ({type(ex)})\"\n",
    "        PrintOutput(str(ex), prefix + \"EXCEPTION: \", None, \"red\", filestream)\n",
    "\n",
    "\n",
    "def Warn(msg, ex=None):\n",
    "    PrintOutput(msg, \"WARN:  \", ex, \"lyellow\")\n",
    "\n",
    "\n",
    "def Err(msg, ex=None):\n",
    "    PrintOutput(msg, \"ERROR: \", ex, \"lred\" )\n",
    "    raise Exception(msg) if ex is None else ex\n",
    "\n",
    "\n",
    "def Info(msg):\n",
    "    PrintOutput(msg, \"INFO:  \", None, \"lpurple\")\n",
    "\n",
    "\n",
    "def SimpleAssertInRange(x, expected, eps):\n",
    "    #assert isinstance(x, numpy.ndarray)\n",
    "    #assert isinstance(expected, numpy.ndarray)\n",
    "    #assert x.ndim==1 and expected.ndim==1\n",
    "    #assert x.shape==expected.shape\n",
    "    assert eps>0\n",
    "    assert numpy.allclose(x, expected, eps) # should rtol or atol be set to eps?\n",
    "\n",
    "\n",
    "def GenerateData():\n",
    "    X = numpy.array([[8.34044009e-01],[1.44064899e+00],[2.28749635e-04],[6.04665145e-01]])\n",
    "    y = numpy.array([5.97396028, 7.24897834, 4.86609388, 3.51245674])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def TestMyLinReg():\n",
    "    X, y = GenerateData()\n",
    "\n",
    "    try:\n",
    "        # assume that your regressor class is named 'MyLinReg', please update/change\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor has another name, than 'MyLinReg', please change the name in this smoke test\", ex)\n",
    "\n",
    "    try:\n",
    "        regressor = MyLinReg(max_iter=200)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ for parameter 'max_iter'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(eta0=0.01)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ for parameter 'eta0'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(verbose=False)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'verbose'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(tol=1e-3)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'tol'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(n_iter_no_change=1e-3)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'n_iter_no_change'\", ex)\n",
    "\n",
    "    # create regressor with default hyperparameter values\n",
    "    # to be used for training, prediction and score..\n",
    "    try:\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ with default parameters\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        regressor.fit(X, y)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not fit\", ex)\n",
    "\n",
    "    try:\n",
    "        y_pred = regressor.predict(X)\n",
    "        Info(f\"y_pred = {y_pred}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not predict\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        score  = regressor.score(X, y)\n",
    "        Info(f\"SCORE = {Col('lblue')}{score}{ColEnd()}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails in the score call\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        w    = None # default\n",
    "        bias = None # default\n",
    "        try:\n",
    "            w = regressor.coef_\n",
    "            bias = regressor.intercept_\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Warn(\"your regressor has no coef_/intercept_ atrributes, trying Weights() instead..\", ex)\n",
    "        try:\n",
    "            if w is None:\n",
    "                w = regressor.Weights() # maybe a Weigths function is avalible on you model?\n",
    "                try:\n",
    "                    assert w.ndim == 1,     \"can only handle vector like w's for now\"\n",
    "                    assert w.shape[0] >= 2, \"expected length of to be at least 2, that is one bias one coefficient\"\n",
    "                    bias = w[0]\n",
    "                    w = w[1:]\n",
    "                except Exception as ex:\n",
    "                    w = None\n",
    "                    Err(\"having a hard time concantenating our bias and coefficients, giving up!\", ex)\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Err(\"your regressor also has no Weights() function, giving up!\", ex)\n",
    "        Info(f\"bias         = {bias}\")\n",
    "        Info(f\"coefficients = {w}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails during extraction of bias and weights (but is a COULD)\", ex)\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import PrintMatrix\n",
    "    except Exception as ex:\n",
    "        PrintMatrix = SimplePrintMatrix # fall-back\n",
    "        Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import AssertInRange\n",
    "    except Exception as ex:\n",
    "        AssertInRange = SimpleAssertInRange # fall-back\n",
    "        Warn(\"could not import AssertInRange from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        if w is not None:\n",
    "            if bias is not None:\n",
    "                w = numpy.concatenate(([bias], w)) # re-concat bias an coefficients, may be incorrect for your implementation!\n",
    "            \n",
    "            # TEST VECTOR:\n",
    "            w_expected = numpy.array([4.046879011698, 1.880121487278])\n",
    "            \n",
    "            PrintMatrix(w,          label=\"       w         =\")\n",
    "            PrintMatrix(w_expected, label=\"       w_expected=\")\n",
    "            print()\n",
    "            \n",
    "            eps = 1E-2 # somewhat big epsilon, allowing some slack..\n",
    "            AssertInRange(w, w_expected, eps)\n",
    "            Info(\"Well, good news, your w and the expected w-vector seem to be very close numerically, so the smoke-test has passed!\")\n",
    "            \n",
    "            return regressor\n",
    "        else:\n",
    "            Warn(\"cannot test due to missing w information\")\n",
    "    except Exception as ex:\n",
    "        Err(\"mini-smoketest on your regressor failed\", ex)\n",
    "    \n",
    "    return None\n",
    "\n",
    "Warn(\"This mini smoke-test may produce false-positives and/or\\n false-negatives..\")\n",
    "TestMyLinReg()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qg: [OPTIONAL] More Smoke-Testing\n",
    "\n",
    "Do you dare to compare your custom regressor with the SGD regressor in Scikit-learn on both the IRIS and MNIST datasets?\n",
    "\n",
    "Then run the next smoke-test function, but the code might requre `eta0` anb `max_iter` hyperparamter tuning).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mINFO:  DATA: 'IRIS'\n",
      "       SHAPES: X_train=(112, 4), X_test=(38, 4), y_train=(112,), y_\\\n",
      "       test=(38,)\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  TRAINING['MyLinReg']..\u001b[0m\n",
      "\n",
      "Iteration 1, Loss: 1.6160714285714286\n",
      "Iteration 2, Loss: 1.5771754145408161\n",
      "Iteration 3, Loss: 1.5190361943952762\n",
      "Iteration 4, Loss: 1.3989315672965055\n",
      "Iteration 5, Loss: 1.2793712279296923\n",
      "Iteration 6, Loss: 1.2441794935390509\n",
      "Iteration 7, Loss: 1.2015177216147148\n",
      "Iteration 8, Loss: 1.161434734097129\n",
      "Iteration 9, Loss: 1.1281372446098707\n",
      "Iteration 10, Loss: 1.0938477346825262\n",
      "Iteration 11, Loss: 1.062309491264176\n",
      "Iteration 12, Loss: 1.0311546408923014\n",
      "Iteration 13, Loss: 1.0019927104088642\n",
      "Iteration 14, Loss: 0.9744762045405752\n",
      "Iteration 15, Loss: 0.9452673589302998\n",
      "Iteration 16, Loss: 0.9155258408912541\n",
      "Iteration 17, Loss: 0.8876661075491555\n",
      "Iteration 18, Loss: 0.8589948444896048\n",
      "Iteration 19, Loss: 0.8394001226637193\n",
      "Iteration 20, Loss: 0.8131636180998717\n",
      "Iteration 21, Loss: 0.7917369738250127\n",
      "Iteration 22, Loss: 0.7297222120693473\n",
      "Iteration 23, Loss: 0.7043910937132623\n",
      "Iteration 24, Loss: 0.6863006008342766\n",
      "Iteration 25, Loss: 0.6423107222330834\n",
      "Iteration 26, Loss: 0.6256005820341117\n",
      "Iteration 27, Loss: 0.5937630200861577\n",
      "Iteration 28, Loss: 0.5773135966636768\n",
      "Iteration 29, Loss: 0.5387387180117502\n",
      "Iteration 30, Loss: 0.5090494725973219\n",
      "Iteration 31, Loss: 0.49772319385464797\n",
      "Iteration 32, Loss: 0.48635471582799134\n",
      "Iteration 33, Loss: 0.46275731058812036\n",
      "Iteration 34, Loss: 0.44249951768996165\n",
      "Iteration 35, Loss: 0.42923451449239425\n",
      "Iteration 36, Loss: 0.4176779638157728\n",
      "Iteration 37, Loss: 0.397879095956493\n",
      "Iteration 38, Loss: 0.3829161305637239\n",
      "Iteration 39, Loss: 0.37350010254496385\n",
      "Iteration 40, Loss: 0.3676804290987346\n",
      "Iteration 41, Loss: 0.3499006696252989\n",
      "Iteration 42, Loss: 0.3325093184369151\n",
      "Iteration 43, Loss: 0.318155566603417\n",
      "Iteration 44, Loss: 0.3025430050184556\n",
      "Iteration 45, Loss: 0.29378739262669795\n",
      "Iteration 46, Loss: 0.29063535746384833\n",
      "Iteration 47, Loss: 0.27641209965352376\n",
      "Iteration 48, Loss: 0.26617329312853844\n",
      "Iteration 49, Loss: 0.251652829952581\n",
      "Iteration 50, Loss: 0.24668778381929718\n",
      "Iteration 51, Loss: 0.23272755524793132\n",
      "Iteration 52, Loss: 0.22794048930940689\n",
      "Iteration 53, Loss: 0.21549243424996956\n",
      "Iteration 54, Loss: 0.21861615351354105\n",
      "Iteration 55, Loss: 0.21547360456044903\n",
      "Iteration 56, Loss: 0.21139715538567558\n",
      "Iteration 57, Loss: 0.20849479195454318\n",
      "Iteration 58, Loss: 0.21108654256688578\n",
      "Iteration 59, Loss: 0.2194325099274453\n",
      "Iteration 60, Loss: 0.22310043731480422\n",
      "Iteration 61, Loss: 0.20783566607494036\n",
      "Iteration 62, Loss: 0.19133563155925762\n",
      "Iteration 63, Loss: 0.19127012501863014\n",
      "Iteration 64, Loss: 0.19163667108021096\n",
      "Iteration 65, Loss: 0.18470382937287902\n",
      "Iteration 66, Loss: 0.17214393669176983\n",
      "Iteration 67, Loss: 0.16688700215528546\n",
      "Iteration 68, Loss: 0.16880666732793867\n",
      "Iteration 69, Loss: 0.16569674461035108\n",
      "Iteration 70, Loss: 0.15459888501524066\n",
      "Iteration 71, Loss: 0.15021571969342482\n",
      "Iteration 72, Loss: 0.15116632902773622\n",
      "Iteration 73, Loss: 0.1413557956399302\n",
      "Iteration 74, Loss: 0.1414244186774582\n",
      "Iteration 75, Loss: 0.1372628237570853\n",
      "Iteration 76, Loss: 0.12790296210537558\n",
      "Iteration 77, Loss: 0.12667445301663163\n",
      "Iteration 78, Loss: 0.11845089985511151\n",
      "Iteration 79, Loss: 0.11112979653717223\n",
      "Iteration 80, Loss: 0.11970039422743868\n",
      "Iteration 81, Loss: 0.12089053579503363\n",
      "Iteration 82, Loss: 0.11997539131110531\n",
      "Iteration 83, Loss: 0.12322326921787538\n",
      "Iteration 84, Loss: 0.12320057536142907\n",
      "Iteration 85, Loss: 0.11543312035810731\n",
      "Iteration 86, Loss: 0.1142248175844763\n",
      "Iteration 87, Loss: 0.10738021399703965\n",
      "Iteration 88, Loss: 0.10122717306571882\n",
      "Iteration 89, Loss: 0.10556840563878787\n",
      "Iteration 90, Loss: 0.09946571162085129\n",
      "Iteration 91, Loss: 0.10315290056533719\n",
      "Iteration 92, Loss: 0.1049402358756448\n",
      "Iteration 93, Loss: 0.09884692116636454\n",
      "Iteration 94, Loss: 0.09977274566008\n",
      "Iteration 95, Loss: 0.10196325481817062\n",
      "Iteration 96, Loss: 0.10098095007886469\n",
      "Iteration 97, Loss: 0.09517083779911799\n",
      "Iteration 98, Loss: 0.0902007818484065\n",
      "Iteration 99, Loss: 0.0896798883488618\n",
      "Iteration 100, Loss: 0.08893095586286012\n",
      "Iteration 101, Loss: 0.08501415743893889\n",
      "Iteration 102, Loss: 0.08294365665809035\n",
      "Iteration 103, Loss: 0.08445204048206494\n",
      "Iteration 104, Loss: 0.08596754817272419\n",
      "Iteration 105, Loss: 0.08198629375711139\n",
      "Iteration 106, Loss: 0.08117965547661059\n",
      "Iteration 107, Loss: 0.08104386601692186\n",
      "Iteration 108, Loss: 0.0773478883734048\n",
      "Iteration 109, Loss: 0.08167320193850312\n",
      "Iteration 110, Loss: 0.08134110332814236\n",
      "Iteration 111, Loss: 0.07760753884054392\n",
      "Iteration 112, Loss: 0.07907155654037552\n",
      "Iteration 113, Loss: 0.0758291983551934\n",
      "Iteration 114, Loss: 0.0730092784978984\n",
      "Iteration 115, Loss: 0.07139029371711947\n",
      "Iteration 116, Loss: 0.07182877067854257\n",
      "Iteration 117, Loss: 0.07074993631430246\n",
      "Iteration 118, Loss: 0.07056801056713688\n",
      "Iteration 119, Loss: 0.07023572875451588\n",
      "Iteration 120, Loss: 0.06984752660767672\n",
      "Iteration 121, Loss: 0.069197495241056\n",
      "Iteration 122, Loss: 0.06884447218214283\n",
      "Stopping early at iteration 122 due to no improvement.\n",
      "y_pred_test=[-0.0279  1.4936  1.4351  0.0574  1.8204  1.6239 -0.0385\n",
      "              1.4864  0.092   0.7614  0.8366  2.0279 -0.0339  1.669\n",
      "              0.1163  0.0331  0.0395  0.0479  1.9197  1.2376  1.825\n",
      "              1.0975  1.4124  1.0239  1.6291  0.068   0.2229  2.2133\n",
      "              0.1046  1.537   0.0483  1.6454 -0.1586  1.7683  1.5071\n",
      "              1.0321  1.6036  1.4864]\n",
      "\n",
      "\u001b[1;35mINFO:  SCORE['MyLinReg'] = \u001b[1;34m0.897\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  TRAINING['SGDRegressor']..\u001b[0m\n",
      "\n",
      "y_pred_test=[-0.0841  1.5093  1.4471 -0.0002  1.7732  1.6055 -0.08\n",
      "              1.5758  0.1247  0.8441  0.9404  1.9251 -0.0761  1.6853\n",
      "             -0.1291 -0.0502 -0.1754 -0.0797  1.8942  1.2166  1.7983\n",
      "              1.1622  1.4422  0.9958  1.6641 -0.0044  0.1501  2.1044\n",
      "             -0.0666  1.4145 -0.0503  1.7839 -0.2308  1.8063  1.402\n",
      "              1.0451  1.5893  1.5758]\n",
      "\n",
      "\u001b[1;35mINFO:  SCORE['SGDRegressor'] = \u001b[1;34m0.915\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  ##############################################\n",
      "       \u001b[0m\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "\u001b[1;35mINFO:  DATA: 'MNIST'\n",
      "       SHAPES: X_train=(52500, 784), X_test=(17500, 784), y_train=(\\\n",
      "       52500,), y_test=(17500,)\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  TRAINING['MyLinReg']..\u001b[0m\n",
      "\n",
      "Iteration 1, Loss: 28.280209523809525\n",
      "Iteration 2, Loss: 28.234184114009953\n",
      "Iteration 3, Loss: 27.840781023719405\n",
      "Iteration 4, Loss: 28.80620519655915\n",
      "Iteration 5, Loss: 28.372773024372286\n",
      "Iteration 6, Loss: 27.59573175363594\n",
      "Iteration 7, Loss: 27.74101535737999\n",
      "Iteration 8, Loss: 27.602325169053493\n",
      "Iteration 9, Loss: 26.82436361055566\n",
      "Iteration 10, Loss: 26.902438293852\n",
      "Iteration 11, Loss: 26.96378700958668\n",
      "Iteration 12, Loss: 26.488923133718036\n",
      "Iteration 13, Loss: 25.597152866855627\n",
      "Iteration 14, Loss: 26.02847015135136\n",
      "Iteration 15, Loss: 25.96804302586108\n",
      "Iteration 16, Loss: 25.750655438344474\n",
      "Iteration 17, Loss: 25.98958818126655\n",
      "Iteration 18, Loss: 25.510628890336942\n",
      "Iteration 19, Loss: 25.491637047183477\n",
      "Iteration 20, Loss: 25.56193962572679\n",
      "Iteration 21, Loss: 25.580490969679275\n",
      "Iteration 22, Loss: 25.831329155614743\n",
      "Iteration 23, Loss: 25.368236912111733\n",
      "Iteration 24, Loss: 24.685464651477226\n",
      "Iteration 25, Loss: 24.007668902398958\n",
      "Iteration 26, Loss: 23.628084108014534\n",
      "Iteration 27, Loss: 24.092925266459567\n",
      "Iteration 28, Loss: 23.79376853443428\n",
      "Iteration 29, Loss: 23.700595424636756\n",
      "Iteration 30, Loss: 23.88742760863241\n",
      "Iteration 31, Loss: 23.504551819907878\n",
      "Iteration 32, Loss: 23.58254009049738\n",
      "Iteration 33, Loss: 23.534172322795754\n",
      "Iteration 34, Loss: 23.50154098951214\n",
      "Iteration 35, Loss: 23.36740961061142\n",
      "Iteration 36, Loss: 23.341379324871134\n",
      "Iteration 37, Loss: 23.148444584862027\n",
      "Iteration 38, Loss: 23.132992395342047\n",
      "Iteration 39, Loss: 23.21032105500994\n",
      "Iteration 40, Loss: 23.131108140525527\n",
      "Iteration 41, Loss: 23.016669262123884\n",
      "Iteration 42, Loss: 23.146044853153175\n",
      "Iteration 43, Loss: 23.110811355666847\n",
      "Iteration 44, Loss: 23.167469004487078\n",
      "Iteration 45, Loss: 22.820276536453374\n",
      "Iteration 46, Loss: 22.963282478335813\n",
      "Iteration 47, Loss: 22.984711256001013\n",
      "Iteration 48, Loss: 23.383566204731157\n",
      "Iteration 49, Loss: 23.411258893040447\n",
      "Iteration 50, Loss: 23.326594745690873\n",
      "Iteration 51, Loss: 23.154137978466622\n",
      "Iteration 52, Loss: 23.2949576970493\n",
      "Iteration 53, Loss: 22.447409296408953\n",
      "Iteration 54, Loss: 22.344738737924796\n",
      "Iteration 55, Loss: 22.373433067889398\n",
      "Iteration 56, Loss: 22.666361088680457\n",
      "Iteration 57, Loss: 22.38628504263974\n",
      "Iteration 58, Loss: 22.2995742660889\n",
      "Iteration 59, Loss: 22.26388096758093\n",
      "Iteration 60, Loss: 22.509652143886928\n",
      "Iteration 61, Loss: 22.42907784314188\n",
      "Iteration 62, Loss: 22.594230558367208\n",
      "Iteration 63, Loss: 22.630563402502577\n",
      "Iteration 64, Loss: 22.620330416769278\n",
      "Iteration 65, Loss: 22.559876158969804\n",
      "Iteration 66, Loss: 22.413022327845926\n",
      "Iteration 67, Loss: 22.43154597565984\n",
      "Iteration 68, Loss: 22.196852278630093\n",
      "Iteration 69, Loss: 21.916799638497476\n",
      "Iteration 70, Loss: 21.896895218037724\n",
      "Iteration 71, Loss: 21.956260801482397\n",
      "Iteration 72, Loss: 21.842838230079032\n",
      "Iteration 73, Loss: 21.975702498375394\n",
      "Iteration 74, Loss: 22.00531972479061\n",
      "Iteration 75, Loss: 22.191557619124552\n",
      "Iteration 76, Loss: 22.17985761336121\n",
      "Iteration 77, Loss: 21.91663550627915\n",
      "Iteration 78, Loss: 21.78126612299279\n",
      "Iteration 79, Loss: 22.00090789656357\n",
      "Iteration 80, Loss: 22.066957388409705\n",
      "Iteration 81, Loss: 22.277306256172924\n",
      "Iteration 82, Loss: 22.360686734470537\n",
      "Iteration 83, Loss: 21.995888260881575\n",
      "Iteration 84, Loss: 22.15715293390602\n",
      "Iteration 85, Loss: 22.247191684683973\n",
      "Iteration 86, Loss: 22.150036628093115\n",
      "Iteration 87, Loss: 21.94802407210006\n",
      "Iteration 88, Loss: 21.874373121353873\n",
      "Iteration 89, Loss: 21.322978416827123\n",
      "Iteration 90, Loss: 20.974990715065903\n",
      "Iteration 91, Loss: 21.009075994374065\n",
      "Iteration 92, Loss: 20.95918081104816\n",
      "Iteration 93, Loss: 20.990973697226398\n",
      "Iteration 94, Loss: 20.78430388204899\n",
      "Iteration 95, Loss: 20.481539305571044\n",
      "Iteration 96, Loss: 20.469241375070286\n",
      "Iteration 97, Loss: 20.395003183281954\n",
      "Iteration 98, Loss: 20.25747753578336\n",
      "Iteration 99, Loss: 20.208835979096698\n",
      "Iteration 100, Loss: 20.171588471916266\n",
      "Iteration 101, Loss: 20.117399332927256\n",
      "Iteration 102, Loss: 20.264834331784908\n",
      "Iteration 103, Loss: 20.26092317286322\n",
      "Iteration 104, Loss: 20.19798254498109\n",
      "Iteration 105, Loss: 20.387372373817254\n",
      "Iteration 106, Loss: 20.108935156160978\n",
      "Iteration 107, Loss: 19.81705286380313\n",
      "Iteration 108, Loss: 19.85959491429803\n",
      "Iteration 109, Loss: 19.697921316774494\n",
      "Iteration 110, Loss: 19.75511634739815\n",
      "Iteration 111, Loss: 19.879907741049003\n",
      "Iteration 112, Loss: 19.777804120709586\n",
      "Iteration 113, Loss: 19.85014975242134\n",
      "Iteration 114, Loss: 19.77863689808748\n",
      "Iteration 115, Loss: 19.764165350693073\n",
      "Iteration 116, Loss: 19.677144880749022\n",
      "Iteration 117, Loss: 19.858416086977968\n",
      "Iteration 118, Loss: 19.827696324145016\n",
      "Iteration 119, Loss: 19.760540544044492\n",
      "Iteration 120, Loss: 19.852404621007548\n",
      "Iteration 121, Loss: 19.801025813278162\n",
      "Iteration 122, Loss: 19.80277470171992\n",
      "Iteration 123, Loss: 19.75258416071541\n",
      "Iteration 124, Loss: 19.776908261504843\n",
      "Iteration 125, Loss: 19.85582619861453\n",
      "Iteration 126, Loss: 20.007808565623726\n",
      "Iteration 127, Loss: 20.05397358757577\n",
      "Iteration 128, Loss: 19.884120692690907\n",
      "Iteration 129, Loss: 19.55720056557067\n",
      "Iteration 130, Loss: 19.56885256373971\n",
      "Iteration 131, Loss: 19.553789797035794\n",
      "Iteration 132, Loss: 19.493250604085652\n",
      "Iteration 133, Loss: 19.438689310275024\n",
      "Iteration 134, Loss: 19.32843885245671\n",
      "Iteration 135, Loss: 19.471165197503076\n",
      "Iteration 136, Loss: 19.52358011617075\n",
      "Iteration 137, Loss: 48.43446204465979\n",
      "Iteration 138, Loss: 47.84965794766958\n",
      "Iteration 139, Loss: 47.85536482275811\n",
      "Iteration 140, Loss: 47.84966026579363\n",
      "Iteration 141, Loss: 47.813209548059724\n",
      "Iteration 142, Loss: 47.81803750446819\n",
      "Iteration 143, Loss: 47.70081154522097\n",
      "Iteration 144, Loss: 47.42572455954005\n",
      "Iteration 145, Loss: 47.366164532370895\n",
      "Iteration 146, Loss: 47.7209480305142\n",
      "Iteration 147, Loss: 47.708495384862104\n",
      "Iteration 148, Loss: 47.140852736427455\n",
      "Iteration 149, Loss: 47.284289000697676\n",
      "Iteration 150, Loss: 47.65913224902708\n",
      "Iteration 151, Loss: 47.168709589383035\n",
      "Iteration 152, Loss: 46.858967022202236\n",
      "Iteration 153, Loss: 46.41987630290424\n",
      "Iteration 154, Loss: 47.61396887299298\n",
      "Iteration 155, Loss: 48.600190148261355\n",
      "Iteration 156, Loss: 48.48382125996056\n",
      "Iteration 157, Loss: 48.419195342045384\n",
      "Iteration 158, Loss: 47.611921593207384\n",
      "Iteration 159, Loss: 47.448955061170466\n",
      "Iteration 160, Loss: 47.259350263080655\n",
      "Iteration 161, Loss: 46.90381400529777\n",
      "Iteration 162, Loss: 46.890743312299875\n",
      "Iteration 163, Loss: 46.82179677614752\n",
      "Iteration 164, Loss: 46.500555804807966\n",
      "Iteration 165, Loss: 46.489091810458234\n",
      "Iteration 166, Loss: 46.58919754769449\n",
      "Iteration 167, Loss: 47.02769570426822\n",
      "Iteration 168, Loss: 46.73791491569736\n",
      "Iteration 169, Loss: 46.70248210539309\n",
      "Iteration 170, Loss: 46.68858636702943\n",
      "Iteration 171, Loss: 47.06573256946093\n",
      "Iteration 172, Loss: 46.06395959139207\n",
      "Iteration 173, Loss: 45.65960607562898\n",
      "Iteration 174, Loss: 43.64074594574371\n",
      "Iteration 175, Loss: 43.48040398866652\n",
      "Iteration 176, Loss: 43.50997782433399\n",
      "Iteration 177, Loss: 43.17522593572794\n",
      "Iteration 178, Loss: 43.012888311473034\n",
      "Iteration 179, Loss: 42.87133697601143\n",
      "Iteration 180, Loss: 42.838591519163835\n",
      "Iteration 181, Loss: 42.80056181676723\n",
      "Iteration 182, Loss: 42.77265928955173\n",
      "Iteration 183, Loss: 42.85363756646604\n",
      "Iteration 184, Loss: 42.87423913179408\n",
      "Iteration 185, Loss: 42.97595827204308\n",
      "Iteration 186, Loss: 42.7568749374904\n",
      "Iteration 187, Loss: 43.244553948231406\n",
      "Iteration 188, Loss: 43.08080271481593\n",
      "Iteration 189, Loss: 43.10439321417148\n",
      "Iteration 190, Loss: 43.129418814297075\n",
      "Iteration 191, Loss: 43.20368238607747\n",
      "Iteration 192, Loss: 43.11290935586016\n",
      "Iteration 193, Loss: 43.12596112487339\n",
      "Iteration 194, Loss: 43.16350746243106\n",
      "Iteration 195, Loss: 43.05107429186754\n",
      "Iteration 196, Loss: 42.93964658606219\n",
      "Iteration 197, Loss: 42.6259850067581\n",
      "Iteration 198, Loss: 42.6224528165292\n",
      "Iteration 199, Loss: 42.581770673747414\n",
      "Iteration 200, Loss: 42.52419341636742\n",
      "Iteration 201, Loss: 42.63193801578303\n",
      "Iteration 202, Loss: 42.58906224440202\n",
      "Iteration 203, Loss: 42.54497028603227\n",
      "Iteration 204, Loss: 42.329662865767745\n",
      "Iteration 205, Loss: 42.27550858452674\n",
      "Iteration 206, Loss: 42.1454961938338\n",
      "Iteration 207, Loss: 42.19875018306224\n",
      "Iteration 208, Loss: 42.04495805725574\n",
      "Iteration 209, Loss: 42.32347879597387\n",
      "Iteration 210, Loss: 42.237502040284156\n",
      "Iteration 211, Loss: 42.1487862637646\n",
      "Iteration 212, Loss: 41.86246946853323\n",
      "Iteration 213, Loss: 41.86514028768939\n",
      "Iteration 214, Loss: 41.67012499459835\n",
      "Iteration 215, Loss: 41.16089254195969\n",
      "Iteration 216, Loss: 41.1994484720186\n",
      "Iteration 217, Loss: 41.120239295819204\n",
      "Iteration 218, Loss: 41.07424581257516\n",
      "Iteration 219, Loss: 40.95488298469479\n",
      "Iteration 220, Loss: 41.038720693410845\n",
      "Iteration 221, Loss: 41.00002751961276\n",
      "Iteration 222, Loss: 41.30870733741036\n",
      "Iteration 223, Loss: 41.82197205979291\n",
      "Iteration 224, Loss: 42.499175480600805\n",
      "Iteration 225, Loss: 42.79368415244004\n",
      "Iteration 226, Loss: 42.66536873351558\n",
      "Iteration 227, Loss: 42.60579136870267\n",
      "Iteration 228, Loss: 42.56837068849438\n",
      "Iteration 229, Loss: 41.745806769018884\n",
      "Iteration 230, Loss: 41.77015984202837\n",
      "Iteration 231, Loss: 41.57014440624455\n",
      "Iteration 232, Loss: 41.56333201753334\n",
      "Iteration 233, Loss: 41.53627895943425\n",
      "Iteration 234, Loss: 41.51186138618235\n",
      "Iteration 235, Loss: 41.558530100927264\n",
      "Iteration 236, Loss: 41.07676459511644\n",
      "Iteration 237, Loss: 41.07792629652103\n",
      "Iteration 238, Loss: 41.005591945906616\n",
      "Iteration 239, Loss: 41.00947134991887\n",
      "Iteration 240, Loss: 40.93381452592111\n",
      "Iteration 241, Loss: 41.033170639444265\n",
      "Iteration 242, Loss: 41.01051529896467\n",
      "Iteration 243, Loss: 41.024308019638525\n",
      "Iteration 244, Loss: 41.11618760398622\n",
      "Iteration 245, Loss: 41.00821905230099\n",
      "Iteration 246, Loss: 40.69914349521797\n",
      "Iteration 247, Loss: 40.62113881821951\n",
      "Iteration 248, Loss: 40.54769816115191\n",
      "Iteration 249, Loss: 40.74443749557596\n",
      "Iteration 250, Loss: 41.02756042352789\n",
      "Iteration 251, Loss: 41.369966266396865\n",
      "Iteration 252, Loss: 41.43921887604276\n",
      "Iteration 253, Loss: 41.431024617594446\n",
      "Iteration 254, Loss: 41.632760823016746\n",
      "Iteration 255, Loss: 41.9872682597023\n",
      "Iteration 256, Loss: 42.2776945276631\n",
      "Iteration 257, Loss: 42.203117573249216\n",
      "Iteration 258, Loss: 42.16537395206895\n",
      "Iteration 259, Loss: 42.216920130247296\n",
      "Iteration 260, Loss: 42.135012996197105\n",
      "Iteration 261, Loss: 41.337512258462716\n",
      "Iteration 262, Loss: 40.76164853995837\n",
      "Iteration 263, Loss: 40.733876172411115\n",
      "Iteration 264, Loss: 41.310406823697676\n",
      "Iteration 265, Loss: 40.6267011747382\n",
      "Iteration 266, Loss: 40.553603018447205\n",
      "Iteration 267, Loss: 40.542749252018645\n",
      "Iteration 268, Loss: 40.67312637301666\n",
      "Iteration 269, Loss: 40.95288247917235\n",
      "Iteration 270, Loss: 40.898799537548996\n",
      "Iteration 271, Loss: 40.96721538654163\n",
      "Iteration 272, Loss: 40.92792893228953\n",
      "Iteration 273, Loss: 41.151613144125804\n",
      "Iteration 274, Loss: 40.82628853576444\n",
      "Iteration 275, Loss: 40.666961866217136\n",
      "Iteration 276, Loss: 40.78685731474436\n",
      "Iteration 277, Loss: 40.08094029616643\n",
      "Iteration 278, Loss: 39.542231995728734\n",
      "Iteration 279, Loss: 39.73399896913364\n",
      "Iteration 280, Loss: 39.47716796771069\n",
      "Iteration 281, Loss: 39.44865919599512\n",
      "Iteration 282, Loss: 39.435504679497825\n",
      "Iteration 283, Loss: 39.420773667913075\n",
      "Iteration 284, Loss: 39.02723881308734\n",
      "Iteration 285, Loss: 38.79872501225558\n",
      "Iteration 286, Loss: 38.99336650119578\n",
      "Iteration 287, Loss: 38.98016162014524\n",
      "Iteration 288, Loss: 38.94534676133351\n",
      "Iteration 289, Loss: 38.91891253498998\n",
      "Iteration 290, Loss: 38.594177017433935\n",
      "Iteration 291, Loss: 38.58587691631434\n",
      "Iteration 292, Loss: 38.566662563912125\n",
      "Iteration 293, Loss: 38.54071520280581\n",
      "Iteration 294, Loss: 38.62239304720746\n",
      "Iteration 295, Loss: 38.60222290289364\n",
      "Iteration 296, Loss: 39.02926995877875\n",
      "Iteration 297, Loss: 38.98554176745134\n",
      "Iteration 298, Loss: 38.989303312228984\n",
      "Iteration 299, Loss: 38.82620921560952\n",
      "Iteration 300, Loss: 38.70026716922796\n",
      "Iteration 301, Loss: 38.49157467863659\n",
      "Iteration 302, Loss: 38.557563316446995\n",
      "Iteration 303, Loss: 38.81741238244954\n",
      "Iteration 304, Loss: 38.78228234991952\n",
      "Iteration 305, Loss: 38.730961817852986\n",
      "Iteration 306, Loss: 38.98322100462642\n",
      "Iteration 307, Loss: 39.19997245061584\n",
      "Iteration 308, Loss: 39.18913064969311\n",
      "Iteration 309, Loss: 39.14893527994869\n",
      "Iteration 310, Loss: 39.153694746380864\n",
      "Iteration 311, Loss: 39.02581880464024\n",
      "Iteration 312, Loss: 38.87681754395165\n",
      "Iteration 313, Loss: 38.77806372812803\n",
      "Iteration 314, Loss: 39.116858919449264\n",
      "Iteration 315, Loss: 39.30504920446355\n",
      "Iteration 316, Loss: 39.44290294882804\n",
      "Iteration 317, Loss: 39.270272819657784\n",
      "Iteration 318, Loss: 39.26939974467184\n",
      "Iteration 319, Loss: 39.162191995551936\n",
      "Iteration 320, Loss: 39.16563374827548\n",
      "Iteration 321, Loss: 39.277676967155074\n",
      "Iteration 322, Loss: 39.26894993379752\n",
      "Iteration 323, Loss: 39.42213950935257\n",
      "Iteration 324, Loss: 39.420271105670686\n",
      "Iteration 325, Loss: 39.32236777622953\n",
      "Iteration 326, Loss: 39.38414309482767\n",
      "Iteration 327, Loss: 39.38098114389273\n",
      "Iteration 328, Loss: 39.47844393403969\n",
      "Iteration 329, Loss: 39.469321497462786\n",
      "Iteration 330, Loss: 39.19374405050492\n",
      "Iteration 331, Loss: 39.231703746501964\n",
      "Iteration 332, Loss: 39.393750659322635\n",
      "Iteration 333, Loss: 39.535435568040555\n",
      "Iteration 334, Loss: 39.49989086205621\n",
      "Iteration 335, Loss: 39.2717860446041\n",
      "Iteration 336, Loss: 39.422791994282804\n",
      "Iteration 337, Loss: 39.38658526574331\n",
      "Iteration 338, Loss: 39.434068830046535\n",
      "Iteration 339, Loss: 39.27693945735049\n",
      "Iteration 340, Loss: 39.130850810218426\n",
      "Iteration 341, Loss: 39.31408425788557\n",
      "Iteration 342, Loss: 39.34683951165788\n",
      "Iteration 343, Loss: 39.499166694749874\n",
      "Iteration 344, Loss: 39.670491675753354\n",
      "Iteration 345, Loss: 39.94684267522237\n",
      "Iteration 346, Loss: 39.86974037486801\n",
      "Iteration 347, Loss: 39.85904053542725\n",
      "Iteration 348, Loss: 39.859729440588836\n",
      "Iteration 349, Loss: 39.84534060297138\n",
      "Iteration 350, Loss: 39.957531565166285\n",
      "Iteration 351, Loss: 40.44382763584176\n",
      "Iteration 352, Loss: 40.70091103836019\n",
      "Iteration 353, Loss: 40.72872646632679\n",
      "Iteration 354, Loss: 40.915477804637916\n",
      "Iteration 355, Loss: 40.783453745289314\n",
      "Iteration 356, Loss: 40.4072536936102\n",
      "Iteration 357, Loss: 40.37584033656293\n",
      "Iteration 358, Loss: 40.496374757540885\n",
      "Iteration 359, Loss: 40.72654874359434\n",
      "Iteration 360, Loss: 41.038761359296494\n",
      "Iteration 361, Loss: 41.027870921077835\n",
      "Iteration 362, Loss: 41.50883750547874\n",
      "Iteration 363, Loss: 41.19240023449403\n",
      "Iteration 364, Loss: 41.296468404969445\n",
      "Iteration 365, Loss: 41.270017678695666\n",
      "Iteration 366, Loss: 65.65418726119651\n",
      "Iteration 367, Loss: 63.560615567439044\n",
      "Iteration 368, Loss: 62.84750075926271\n",
      "Iteration 369, Loss: 62.80397713994925\n",
      "Iteration 370, Loss: 62.77886143273534\n",
      "Iteration 371, Loss: 62.98878434669743\n",
      "Iteration 372, Loss: 63.32035090883876\n",
      "Iteration 373, Loss: 63.279392013642784\n",
      "Iteration 374, Loss: 63.27846681937048\n",
      "Iteration 375, Loss: 65.86393624507559\n",
      "Iteration 376, Loss: 65.4844621542114\n",
      "Iteration 377, Loss: 65.4413516707805\n",
      "Iteration 378, Loss: 65.72637271461927\n",
      "Iteration 379, Loss: 57.198443748640045\n",
      "Iteration 380, Loss: 56.31818001499644\n",
      "Iteration 381, Loss: 56.41881716799704\n",
      "Iteration 382, Loss: 55.007169519718474\n",
      "Iteration 383, Loss: 54.972291877975756\n",
      "Iteration 384, Loss: 54.3328461895781\n",
      "Iteration 385, Loss: 54.47908747293674\n",
      "Iteration 386, Loss: 54.48674486772083\n",
      "Iteration 387, Loss: 54.47724946635702\n",
      "Iteration 388, Loss: 54.43156145043093\n",
      "Iteration 389, Loss: 54.419703550309805\n",
      "Iteration 390, Loss: 54.358389153031744\n",
      "Iteration 391, Loss: 53.672146713927035\n",
      "Iteration 392, Loss: 53.627489848722384\n",
      "Iteration 393, Loss: 53.98486152327341\n",
      "Iteration 394, Loss: 53.91810617205715\n",
      "Iteration 395, Loss: 53.89570962631083\n",
      "Iteration 396, Loss: 53.8936008008559\n",
      "Iteration 397, Loss: 53.862335634872416\n",
      "Iteration 398, Loss: 53.86253768327955\n",
      "Iteration 399, Loss: 53.79356871677497\n",
      "Iteration 400, Loss: 53.74022868394461\n",
      "Iteration 401, Loss: 53.753865612133914\n",
      "Iteration 402, Loss: 53.79093705414791\n",
      "Iteration 403, Loss: 53.87799985411957\n",
      "Iteration 404, Loss: 53.87473613331243\n",
      "Iteration 405, Loss: 53.0822932825032\n",
      "Iteration 406, Loss: 53.20120204696195\n",
      "Iteration 407, Loss: 53.27396134440598\n",
      "Iteration 408, Loss: 53.15230332309515\n",
      "Iteration 409, Loss: 53.12181783955508\n",
      "Iteration 410, Loss: 53.140004330904546\n",
      "Iteration 411, Loss: 53.127363112769025\n",
      "Iteration 412, Loss: 51.96898498135436\n",
      "Iteration 413, Loss: 51.94664817456902\n",
      "Iteration 414, Loss: 51.940557048614004\n",
      "Iteration 415, Loss: 51.860513496299205\n",
      "Iteration 416, Loss: 51.86211975023096\n",
      "Iteration 417, Loss: 52.04236297345531\n",
      "Iteration 418, Loss: 51.980119734351966\n",
      "Iteration 419, Loss: 51.97305106224788\n",
      "Iteration 420, Loss: 51.93314391685677\n",
      "Iteration 421, Loss: 51.94127821554217\n",
      "Iteration 422, Loss: 52.032615837768454\n",
      "Iteration 423, Loss: 51.94220157354846\n",
      "Iteration 424, Loss: 51.33684081282724\n",
      "Iteration 425, Loss: 51.3411202177702\n",
      "Iteration 426, Loss: 51.43163528610198\n",
      "Iteration 427, Loss: 49.82834623856054\n",
      "Iteration 428, Loss: 49.79988045155141\n",
      "Iteration 429, Loss: 49.80479530659292\n",
      "Iteration 430, Loss: 50.30042027004117\n",
      "Iteration 431, Loss: 50.328740286254536\n",
      "Iteration 432, Loss: 50.473977797582734\n",
      "Iteration 433, Loss: 49.956074488289524\n",
      "Iteration 434, Loss: 49.49980676980513\n",
      "Iteration 435, Loss: 49.68795252220504\n",
      "Iteration 436, Loss: 49.598625424998616\n",
      "Iteration 437, Loss: 49.4221478191845\n",
      "Iteration 438, Loss: 49.401018598137945\n",
      "Iteration 439, Loss: 49.39548989581807\n",
      "Iteration 440, Loss: 49.363830741292254\n",
      "Iteration 441, Loss: 49.14548058918107\n",
      "Iteration 442, Loss: 49.09317013585502\n",
      "Iteration 443, Loss: 48.69554559244593\n",
      "Iteration 444, Loss: 48.85563029022363\n",
      "Iteration 445, Loss: 48.63864989424057\n",
      "Iteration 446, Loss: 48.64663951837201\n",
      "Iteration 447, Loss: 48.64726220911675\n",
      "Iteration 448, Loss: 48.56901280787295\n",
      "Iteration 449, Loss: 48.388168011756925\n",
      "Iteration 450, Loss: 48.21046715325753\n",
      "Iteration 451, Loss: 48.2168297834322\n",
      "Iteration 452, Loss: 48.198587500218515\n",
      "Iteration 453, Loss: 48.181759277295434\n",
      "Iteration 454, Loss: 48.1097583004374\n",
      "Iteration 455, Loss: 47.79392577590996\n",
      "Iteration 456, Loss: 47.64992594312834\n",
      "Iteration 457, Loss: 47.59485185019601\n",
      "Iteration 458, Loss: 47.718058378096686\n",
      "Iteration 459, Loss: 48.22156752904275\n",
      "Iteration 460, Loss: 47.422640244860915\n",
      "Iteration 461, Loss: 47.53474571448553\n",
      "Iteration 462, Loss: 46.75910638365278\n",
      "Iteration 463, Loss: 46.79761648266097\n",
      "Iteration 464, Loss: 47.12199864200591\n",
      "Iteration 465, Loss: 47.59625910888237\n",
      "Iteration 466, Loss: 47.007515569976356\n",
      "Iteration 467, Loss: 46.98581531180122\n",
      "Iteration 468, Loss: 46.582580132977085\n",
      "Iteration 469, Loss: 47.048415949724536\n",
      "Iteration 470, Loss: 46.94593155862581\n",
      "Iteration 471, Loss: 46.916047003447225\n",
      "Iteration 472, Loss: 46.941829848657946\n",
      "Iteration 473, Loss: 46.90355085094332\n",
      "Iteration 474, Loss: 46.88353296501698\n",
      "Iteration 475, Loss: 46.87565620263936\n",
      "Iteration 476, Loss: 46.705455558839404\n",
      "Iteration 477, Loss: 46.92374061324953\n",
      "Iteration 478, Loss: 46.36511628156479\n",
      "Iteration 479, Loss: 46.37662729584445\n",
      "Iteration 480, Loss: 45.7758111325139\n",
      "Iteration 481, Loss: 45.92058940649214\n",
      "Iteration 482, Loss: 46.10698268584395\n",
      "Iteration 483, Loss: 46.09028556147027\n",
      "Iteration 484, Loss: 46.11564710472903\n",
      "Iteration 485, Loss: 46.260036705787044\n",
      "Iteration 486, Loss: 46.30994819441108\n",
      "Iteration 487, Loss: 46.079906292648886\n",
      "Iteration 488, Loss: 45.77262033147406\n",
      "Iteration 489, Loss: 45.67083120854731\n",
      "Iteration 490, Loss: 45.61894861589205\n",
      "Iteration 491, Loss: 45.5747330470198\n",
      "Iteration 492, Loss: 45.506803785852156\n",
      "Iteration 493, Loss: 45.49100431265582\n",
      "Iteration 494, Loss: 45.2415730065247\n",
      "Iteration 495, Loss: 45.21916794714632\n",
      "Iteration 496, Loss: 45.197466855097964\n",
      "Iteration 497, Loss: 45.15959220441597\n",
      "Iteration 498, Loss: 45.23452471917371\n",
      "Iteration 499, Loss: 45.07525065627856\n",
      "Iteration 500, Loss: 45.00909837358309\n",
      "Iteration 501, Loss: 44.96232160924033\n",
      "Iteration 502, Loss: 44.988304829480555\n",
      "Iteration 503, Loss: 44.968078437909234\n",
      "Iteration 504, Loss: 44.952046705860624\n",
      "Iteration 505, Loss: 44.946183132132916\n",
      "Iteration 506, Loss: 44.88707921660526\n",
      "Iteration 507, Loss: 44.78642292131448\n",
      "Iteration 508, Loss: 44.807543206348164\n",
      "Iteration 509, Loss: 44.80064506066709\n",
      "Iteration 510, Loss: 52.60169464547016\n",
      "Iteration 511, Loss: 52.4072578055208\n",
      "Iteration 512, Loss: 52.0909225911457\n",
      "Iteration 513, Loss: 52.02198964287578\n",
      "Iteration 514, Loss: 51.44955450711009\n",
      "Iteration 515, Loss: 51.30214566993915\n",
      "Iteration 516, Loss: 50.90125423678273\n",
      "Iteration 517, Loss: 50.32058000288574\n",
      "Iteration 518, Loss: 49.82288735556939\n",
      "Iteration 519, Loss: 49.90933775433489\n",
      "Iteration 520, Loss: 49.90832730202896\n",
      "Iteration 521, Loss: 49.88311124260595\n",
      "Iteration 522, Loss: 49.826296527442494\n",
      "Iteration 523, Loss: 48.654062878492695\n",
      "Iteration 524, Loss: 48.62623890866614\n",
      "Iteration 525, Loss: 48.63803355924507\n",
      "Iteration 526, Loss: 48.6366018158351\n",
      "Iteration 527, Loss: 48.420034339006676\n",
      "Iteration 528, Loss: 48.33648969905924\n",
      "Iteration 529, Loss: 48.05092525406185\n",
      "Iteration 530, Loss: 47.99579369513754\n",
      "Iteration 531, Loss: 48.01240149791003\n",
      "Iteration 532, Loss: 47.76421397889953\n",
      "Iteration 533, Loss: 47.7662080783844\n",
      "Iteration 534, Loss: 46.97863611392856\n",
      "Iteration 535, Loss: 46.84479932190451\n",
      "Iteration 536, Loss: 46.754084695351274\n",
      "Iteration 537, Loss: 46.990512525715125\n",
      "Iteration 538, Loss: 47.35260076733229\n",
      "Iteration 539, Loss: 47.2910699553047\n",
      "Iteration 540, Loss: 47.01905574575875\n",
      "Iteration 541, Loss: 46.45799255983646\n",
      "Iteration 542, Loss: 46.77325994900288\n",
      "Iteration 543, Loss: 46.678674737166546\n",
      "Iteration 544, Loss: 46.451010587053354\n",
      "Iteration 545, Loss: 46.271853160839136\n",
      "Iteration 546, Loss: 46.15397195140849\n",
      "Iteration 547, Loss: 46.38222734402042\n",
      "Iteration 548, Loss: 46.02114073604894\n",
      "Iteration 549, Loss: 44.92429642105874\n",
      "Iteration 550, Loss: 44.90681578479494\n",
      "Iteration 551, Loss: 44.634517796130275\n",
      "Iteration 552, Loss: 44.656585253855766\n",
      "Iteration 553, Loss: 44.81199803508517\n",
      "Iteration 554, Loss: 44.783282193298135\n",
      "Iteration 555, Loss: 44.36598195070507\n",
      "Iteration 556, Loss: 44.29420029730585\n",
      "Iteration 557, Loss: 44.15328239231871\n",
      "Iteration 558, Loss: 44.33018481280518\n",
      "Iteration 559, Loss: 44.145202043201905\n",
      "Iteration 560, Loss: 43.77306181099534\n",
      "Iteration 561, Loss: 43.758502735326296\n",
      "Iteration 562, Loss: 43.57350479617821\n",
      "Iteration 563, Loss: 43.65335829453261\n",
      "Iteration 564, Loss: 43.65072399940153\n",
      "Iteration 565, Loss: 44.109329103467424\n",
      "Iteration 566, Loss: 44.093461145408405\n",
      "Iteration 567, Loss: 43.98396418350972\n",
      "Iteration 568, Loss: 43.75683791473334\n",
      "Iteration 569, Loss: 43.40928980056829\n",
      "Iteration 570, Loss: 43.47292098174734\n",
      "Iteration 571, Loss: 43.46355481279007\n",
      "Iteration 572, Loss: 42.50946741713774\n",
      "Iteration 573, Loss: 42.439513814158374\n",
      "Iteration 574, Loss: 42.414974610429255\n",
      "Iteration 575, Loss: 42.35198412978564\n",
      "Iteration 576, Loss: 42.330765344065235\n",
      "Iteration 577, Loss: 42.39050621851232\n",
      "Iteration 578, Loss: 42.45184075773083\n",
      "Iteration 579, Loss: 42.45517319886878\n",
      "Iteration 580, Loss: 42.43545044665479\n",
      "Iteration 581, Loss: 42.505068513963955\n",
      "Iteration 582, Loss: 42.48681331452402\n",
      "Iteration 583, Loss: 42.41962517618862\n",
      "Iteration 584, Loss: 42.57338853923997\n",
      "Iteration 585, Loss: 42.545236440323585\n",
      "Iteration 586, Loss: 42.49288201971281\n",
      "Iteration 587, Loss: 42.427662561900185\n",
      "Iteration 588, Loss: 42.428395315028965\n",
      "Iteration 589, Loss: 42.409461977581394\n",
      "Iteration 590, Loss: 42.370753750071835\n",
      "Iteration 591, Loss: 42.36081734667884\n",
      "Iteration 592, Loss: 42.34684655769923\n",
      "Iteration 593, Loss: 42.364118765399986\n",
      "Iteration 594, Loss: 42.361622664844006\n",
      "Iteration 595, Loss: 42.434011198856176\n",
      "Iteration 596, Loss: 42.42939249246771\n",
      "Iteration 597, Loss: 42.35848311617223\n",
      "Iteration 598, Loss: 42.39372073040679\n",
      "Iteration 599, Loss: 42.08736295899207\n",
      "Iteration 600, Loss: 42.056051341121055\n",
      "Iteration 601, Loss: 41.975834914001155\n",
      "Iteration 602, Loss: 41.921485749193415\n",
      "Iteration 603, Loss: 42.054542903383734\n",
      "Iteration 604, Loss: 42.04785873810629\n",
      "Iteration 605, Loss: 42.02055245809109\n",
      "Iteration 606, Loss: 41.86586424729198\n",
      "Iteration 607, Loss: 41.85120986011663\n",
      "Iteration 608, Loss: 41.794126315475175\n",
      "Iteration 609, Loss: 41.59429240620332\n",
      "Iteration 610, Loss: 40.90716181339969\n",
      "Iteration 611, Loss: 40.96184237735509\n",
      "Iteration 612, Loss: 40.95983242138232\n",
      "Iteration 613, Loss: 40.88362472336142\n",
      "Iteration 614, Loss: 40.88045697823323\n",
      "Iteration 615, Loss: 40.84208908624816\n",
      "Iteration 616, Loss: 40.9335462680646\n",
      "Iteration 617, Loss: 40.925646810547086\n",
      "Iteration 618, Loss: 40.85708989114798\n",
      "Iteration 619, Loss: 40.89513040529974\n",
      "Iteration 620, Loss: 40.873683214709374\n",
      "Iteration 621, Loss: 40.875918465997955\n",
      "Iteration 622, Loss: 40.87059401285335\n",
      "Iteration 623, Loss: 40.8599731128866\n",
      "Iteration 624, Loss: 40.8484068689661\n",
      "Iteration 625, Loss: 40.8581349312966\n",
      "Iteration 626, Loss: 40.77070976601914\n",
      "Iteration 627, Loss: 40.710812093625\n",
      "Iteration 628, Loss: 40.989056153060325\n",
      "Iteration 629, Loss: 40.8515463445383\n",
      "Iteration 630, Loss: 40.74542950822079\n",
      "Iteration 631, Loss: 40.73333055376032\n",
      "Iteration 632, Loss: 40.747805801180576\n",
      "Iteration 633, Loss: 40.967009053332646\n",
      "Iteration 634, Loss: 40.90239164703603\n",
      "Iteration 635, Loss: 40.68896046183757\n",
      "Iteration 636, Loss: 40.74106425970977\n",
      "Iteration 637, Loss: 40.729985750387186\n",
      "Iteration 638, Loss: 40.1521571616241\n",
      "Iteration 639, Loss: 40.12552517773866\n",
      "Iteration 640, Loss: 39.992078389091006\n",
      "Iteration 641, Loss: 39.6385492035165\n",
      "Iteration 642, Loss: 39.671036128755404\n",
      "Iteration 643, Loss: 41.78066800686671\n",
      "Iteration 644, Loss: 41.73933061497811\n",
      "Iteration 645, Loss: 41.66473505408708\n",
      "Iteration 646, Loss: 41.65211073043664\n",
      "Iteration 647, Loss: 41.41132121253083\n",
      "Iteration 648, Loss: 41.460615196123406\n",
      "Iteration 649, Loss: 41.42064930594715\n",
      "Iteration 650, Loss: 41.70414763862782\n",
      "Iteration 651, Loss: 41.69445488058231\n",
      "Iteration 652, Loss: 41.62218897107371\n",
      "Iteration 653, Loss: 41.335536097151405\n",
      "Iteration 654, Loss: 41.36599618027252\n",
      "Iteration 655, Loss: 41.30683783055708\n",
      "Iteration 656, Loss: 41.430709565996494\n",
      "Iteration 657, Loss: 40.812254274309275\n",
      "Iteration 658, Loss: 40.79360637698966\n",
      "Iteration 659, Loss: 40.73343975150344\n",
      "Iteration 660, Loss: 40.64789704925919\n",
      "Iteration 661, Loss: 40.55231930787797\n",
      "Iteration 662, Loss: 40.558657597993914\n",
      "Iteration 663, Loss: 40.55915078942182\n",
      "Iteration 664, Loss: 40.55085827331743\n",
      "Iteration 665, Loss: 40.62488159034904\n",
      "Iteration 666, Loss: 40.626312288179385\n",
      "Iteration 667, Loss: 40.65767339148045\n",
      "Iteration 668, Loss: 40.66678573514771\n",
      "Iteration 669, Loss: 40.676542216277646\n",
      "Iteration 670, Loss: 40.647310004994786\n",
      "Iteration 671, Loss: 40.90376578178174\n",
      "Iteration 672, Loss: 40.91654541070981\n",
      "Iteration 673, Loss: 40.93763932370863\n",
      "Iteration 674, Loss: 41.041016819931414\n",
      "Iteration 675, Loss: 41.04382397102825\n",
      "Iteration 676, Loss: 41.009331480666624\n",
      "Iteration 677, Loss: 41.014155925821754\n",
      "Iteration 678, Loss: 40.993503887714155\n",
      "Iteration 679, Loss: 40.99971499490859\n",
      "Iteration 680, Loss: 40.56674025595976\n",
      "Iteration 681, Loss: 40.61364906395668\n",
      "Iteration 682, Loss: 40.58545453131378\n",
      "Iteration 683, Loss: 40.47948047969704\n",
      "Iteration 684, Loss: 40.43265986268873\n",
      "Iteration 685, Loss: 40.480026878529266\n",
      "Iteration 686, Loss: 40.4872098014177\n",
      "Iteration 687, Loss: 40.45974874360297\n",
      "Iteration 688, Loss: 39.94246150344108\n",
      "Iteration 689, Loss: 39.94398779745253\n",
      "Iteration 690, Loss: 39.949580336033485\n",
      "Iteration 691, Loss: 39.97081300493958\n",
      "Iteration 692, Loss: 39.961597361736054\n",
      "Iteration 693, Loss: 39.93074125161369\n",
      "Iteration 694, Loss: 39.91762661167583\n",
      "Iteration 695, Loss: 39.77457567804162\n",
      "Iteration 696, Loss: 39.75798873062585\n",
      "Iteration 697, Loss: 39.745502656596045\n",
      "Iteration 698, Loss: 39.73212694690801\n",
      "Iteration 699, Loss: 39.695445448155766\n",
      "Iteration 700, Loss: 40.03021711465685\n",
      "Iteration 701, Loss: 40.0285858887186\n",
      "Iteration 702, Loss: 39.995175356187545\n",
      "Iteration 703, Loss: 40.06100315954105\n",
      "Iteration 704, Loss: 39.82500755276166\n",
      "Iteration 705, Loss: 39.82898275774707\n",
      "Iteration 706, Loss: 39.84096299860008\n",
      "Iteration 707, Loss: 39.785242298467764\n",
      "Iteration 708, Loss: 39.868242174092735\n",
      "Iteration 709, Loss: 39.576569301300104\n",
      "Iteration 710, Loss: 39.66716775839683\n",
      "Iteration 711, Loss: 39.65691774175218\n",
      "Iteration 712, Loss: 39.56642004471646\n",
      "Iteration 713, Loss: 39.52189797706138\n",
      "Iteration 714, Loss: 39.52688483277455\n",
      "Iteration 715, Loss: 39.47329195938117\n",
      "Iteration 716, Loss: 39.564909782012755\n",
      "Iteration 717, Loss: 39.728829771173835\n",
      "Iteration 718, Loss: 39.737268272415385\n",
      "Iteration 719, Loss: 39.72793465818891\n",
      "Iteration 720, Loss: 39.53844469506848\n",
      "Iteration 721, Loss: 39.46156306086561\n",
      "Iteration 722, Loss: 39.495349800436706\n",
      "Iteration 723, Loss: 39.53359481218023\n",
      "Iteration 724, Loss: 39.399077652000734\n",
      "Iteration 725, Loss: 39.38008652609631\n",
      "Iteration 726, Loss: 39.390202742423746\n",
      "Iteration 727, Loss: 39.39966806915262\n",
      "Iteration 728, Loss: 39.40369996084665\n",
      "Iteration 729, Loss: 39.38640143774124\n",
      "Iteration 730, Loss: 38.957649337886494\n",
      "Iteration 731, Loss: 38.86085340497483\n",
      "Iteration 732, Loss: 38.85215421002954\n",
      "Iteration 733, Loss: 38.80901848823882\n",
      "Iteration 734, Loss: 38.58756706393096\n",
      "Iteration 735, Loss: 40.39408253977325\n",
      "Iteration 736, Loss: 40.28676898951251\n",
      "Iteration 737, Loss: 40.40643568740906\n",
      "Iteration 738, Loss: 40.36347272307444\n",
      "Iteration 739, Loss: 40.428076225030104\n",
      "Iteration 740, Loss: 40.41579038809978\n",
      "Iteration 741, Loss: 40.38658717786099\n",
      "Iteration 742, Loss: 40.11943389996343\n",
      "Iteration 743, Loss: 39.84254778803938\n",
      "Iteration 744, Loss: 39.99284988682405\n",
      "Iteration 745, Loss: 39.88394506184634\n",
      "Iteration 746, Loss: 39.865677739631785\n",
      "Iteration 747, Loss: 39.86892750981976\n",
      "Iteration 748, Loss: 39.85825091495354\n",
      "Iteration 749, Loss: 40.78112313439902\n",
      "Iteration 750, Loss: 40.795744526341245\n",
      "Iteration 751, Loss: 40.801038739987746\n",
      "Iteration 752, Loss: 40.73008281375161\n",
      "Iteration 753, Loss: 40.672017095929654\n",
      "Iteration 754, Loss: 40.50452213350052\n",
      "Iteration 755, Loss: 40.516173850420124\n",
      "Iteration 756, Loss: 40.594106886923576\n",
      "Iteration 757, Loss: 40.335050941911405\n",
      "Iteration 758, Loss: 40.26583345395566\n",
      "Iteration 759, Loss: 40.26257901904765\n",
      "Iteration 760, Loss: 40.15790423798471\n",
      "Iteration 761, Loss: 39.969915342603116\n",
      "Iteration 762, Loss: 39.991822634526756\n",
      "Iteration 763, Loss: 40.02288315904398\n",
      "Iteration 764, Loss: 40.02551623814485\n",
      "Iteration 765, Loss: 40.01034822201201\n",
      "Iteration 766, Loss: 40.00588542201722\n",
      "Iteration 767, Loss: 40.0018276333538\n",
      "Iteration 768, Loss: 39.89533545669215\n",
      "Iteration 769, Loss: 39.59753045038651\n",
      "Iteration 770, Loss: 39.60386952553455\n",
      "Iteration 771, Loss: 39.64777420419266\n",
      "Iteration 772, Loss: 39.64392406807506\n",
      "Iteration 773, Loss: 39.51500797018977\n",
      "Iteration 774, Loss: 39.51242927200965\n",
      "Iteration 775, Loss: 39.5226680389721\n",
      "Iteration 776, Loss: 39.52188258818168\n",
      "Iteration 777, Loss: 39.51748871474814\n",
      "Iteration 778, Loss: 39.514835620687805\n",
      "Iteration 779, Loss: 39.51310603615346\n",
      "Iteration 780, Loss: 40.00316406387304\n",
      "Iteration 781, Loss: 40.00824065163154\n",
      "Iteration 782, Loss: 40.08057866837613\n",
      "Iteration 783, Loss: 40.010869709966684\n",
      "Iteration 784, Loss: 39.96370033109214\n",
      "Iteration 785, Loss: 40.00149502045095\n",
      "Iteration 786, Loss: 39.97623969810166\n",
      "Iteration 787, Loss: 40.01923782752085\n",
      "Iteration 788, Loss: 39.97582840904464\n",
      "Iteration 789, Loss: 40.036933850433286\n",
      "Iteration 790, Loss: 40.117279168691944\n",
      "Iteration 791, Loss: 40.36732944755791\n",
      "Iteration 792, Loss: 40.45858838656825\n",
      "Iteration 793, Loss: 40.458009749486315\n",
      "Iteration 794, Loss: 40.46958309342117\n",
      "Iteration 795, Loss: 40.35900240649104\n",
      "Iteration 796, Loss: 40.14877562904265\n",
      "Iteration 797, Loss: 40.164170658401744\n",
      "Iteration 798, Loss: 39.848478814864606\n",
      "Iteration 799, Loss: 39.81291230965216\n",
      "Iteration 800, Loss: 39.79848516632442\n",
      "Iteration 801, Loss: 39.82114444124834\n",
      "Iteration 802, Loss: 39.79966179822935\n",
      "Iteration 803, Loss: 39.68943237962865\n",
      "Iteration 804, Loss: 39.68772792764571\n",
      "Iteration 805, Loss: 39.65844994719761\n",
      "Iteration 806, Loss: 39.71588446705327\n",
      "Iteration 807, Loss: 39.66638052457424\n",
      "Iteration 808, Loss: 39.59224852749106\n",
      "Iteration 809, Loss: 39.754961646938526\n",
      "Iteration 810, Loss: 39.7386085634741\n",
      "Iteration 811, Loss: 39.85873037858265\n",
      "Iteration 812, Loss: 39.54673095055591\n",
      "Iteration 813, Loss: 39.546487451311584\n",
      "Iteration 814, Loss: 39.47485820355742\n",
      "Iteration 815, Loss: 39.49539335742682\n",
      "Iteration 816, Loss: 39.48705546061414\n",
      "Iteration 817, Loss: 39.40090987593302\n",
      "Iteration 818, Loss: 39.393893951577724\n",
      "Iteration 819, Loss: 39.38902133499774\n",
      "Iteration 820, Loss: 39.456448053629884\n",
      "Iteration 821, Loss: 39.41799572036974\n",
      "Iteration 822, Loss: 39.485012046756765\n",
      "Iteration 823, Loss: 39.50768942707346\n",
      "Iteration 824, Loss: 39.49673291310371\n",
      "Iteration 825, Loss: 39.24038010383656\n",
      "Iteration 826, Loss: 39.24755042039853\n",
      "Iteration 827, Loss: 39.24621176583849\n",
      "Iteration 828, Loss: 39.25471061550078\n",
      "Iteration 829, Loss: 39.234468348604665\n",
      "Iteration 830, Loss: 39.253408782305\n",
      "Iteration 831, Loss: 39.178709755265\n",
      "Iteration 832, Loss: 39.170370190509416\n",
      "Iteration 833, Loss: 38.73370734124498\n",
      "Iteration 834, Loss: 38.721700228795406\n",
      "Iteration 835, Loss: 38.79869914913337\n",
      "Iteration 836, Loss: 38.81648306269461\n",
      "Iteration 837, Loss: 38.82378562309947\n",
      "Iteration 838, Loss: 39.01622827400792\n",
      "Iteration 839, Loss: 39.02151606693262\n",
      "Iteration 840, Loss: 39.011624176360435\n",
      "Iteration 841, Loss: 38.99475400486168\n",
      "Iteration 842, Loss: 38.99326871534667\n",
      "Iteration 843, Loss: 38.98290763332947\n",
      "Iteration 844, Loss: 38.93117853068925\n",
      "Iteration 845, Loss: 38.939955953018504\n",
      "Iteration 846, Loss: 38.95047128211474\n",
      "Iteration 847, Loss: 38.93534100907474\n",
      "Iteration 848, Loss: 39.045613367724734\n",
      "Iteration 849, Loss: 39.18685769032315\n",
      "Iteration 850, Loss: 39.181383670312336\n",
      "Iteration 851, Loss: 39.27030704552767\n",
      "Iteration 852, Loss: 39.26766711898266\n",
      "Iteration 853, Loss: 39.17467425354504\n",
      "Iteration 854, Loss: 39.18876066647183\n",
      "Iteration 855, Loss: 39.090668345624174\n",
      "Iteration 856, Loss: 39.097496765542424\n",
      "Iteration 857, Loss: 39.171123803673694\n",
      "Iteration 858, Loss: 39.19141364567768\n",
      "Iteration 859, Loss: 39.20320556982161\n",
      "Iteration 860, Loss: 38.37835707889479\n",
      "Iteration 861, Loss: 38.37763847648783\n",
      "Iteration 862, Loss: 38.191672755488845\n",
      "Iteration 863, Loss: 38.231919811046296\n",
      "Iteration 864, Loss: 38.09325835059311\n",
      "Iteration 865, Loss: 41.08286096253241\n",
      "Iteration 866, Loss: 40.97107599769537\n",
      "Iteration 867, Loss: 40.52938168822114\n",
      "Iteration 868, Loss: 40.366838740995966\n",
      "Iteration 869, Loss: 40.096808616741754\n",
      "Iteration 870, Loss: 39.98868188190576\n",
      "Iteration 871, Loss: 40.50016726261947\n",
      "Iteration 872, Loss: 40.52416254039794\n",
      "Iteration 873, Loss: 40.269334568541915\n",
      "Iteration 874, Loss: 39.33462035657516\n",
      "Iteration 875, Loss: 39.27386617564652\n",
      "Iteration 876, Loss: 39.235218692153715\n",
      "Iteration 877, Loss: 222.0286704275846\n",
      "Iteration 878, Loss: 220.91987554079907\n",
      "Iteration 879, Loss: 218.88622023390386\n",
      "Iteration 880, Loss: 219.1548885299869\n",
      "Iteration 881, Loss: 218.23777138735676\n",
      "Iteration 882, Loss: 218.47632138274844\n",
      "Iteration 883, Loss: 217.80681589256702\n",
      "Iteration 884, Loss: 218.24156443519894\n",
      "Iteration 885, Loss: 218.4083577780226\n",
      "Iteration 886, Loss: 218.28479711537997\n",
      "Iteration 887, Loss: 218.2514746630085\n",
      "Iteration 888, Loss: 218.28896017308884\n",
      "Iteration 889, Loss: 216.60595755863974\n",
      "Iteration 890, Loss: 216.38130340187442\n",
      "Iteration 891, Loss: 216.02868075996645\n",
      "Iteration 892, Loss: 215.9197229339077\n",
      "Iteration 893, Loss: 213.0171637722665\n",
      "Iteration 894, Loss: 213.79865134683092\n",
      "Iteration 895, Loss: 213.640396582595\n",
      "Iteration 896, Loss: 213.51644101707225\n",
      "Iteration 897, Loss: 213.66163970716966\n",
      "Iteration 898, Loss: 213.3199516429893\n",
      "Iteration 899, Loss: 213.36526951126945\n",
      "Iteration 900, Loss: 213.36674405022055\n",
      "Iteration 901, Loss: 210.98078871765392\n",
      "Iteration 902, Loss: 211.26890283490044\n",
      "Iteration 903, Loss: 212.31636129918698\n",
      "Iteration 904, Loss: 210.71211300022006\n",
      "Iteration 905, Loss: 210.2874757698317\n",
      "Iteration 906, Loss: 209.54102062114853\n",
      "Iteration 907, Loss: 209.81945247111491\n",
      "Iteration 908, Loss: 209.83058731330627\n",
      "Iteration 909, Loss: 196.13737653720716\n",
      "Iteration 910, Loss: 196.1921821849604\n",
      "Iteration 911, Loss: 196.20027254488795\n",
      "Iteration 912, Loss: 196.2909010331372\n",
      "Iteration 913, Loss: 196.4311922041002\n",
      "Iteration 914, Loss: 196.55266476526538\n",
      "Iteration 915, Loss: 197.31773621994705\n",
      "Iteration 916, Loss: 197.25573865236157\n",
      "Iteration 917, Loss: 197.49527188385173\n",
      "Iteration 918, Loss: 197.50470440089572\n",
      "Iteration 919, Loss: 194.09661200438845\n",
      "Iteration 920, Loss: 191.63327164123194\n",
      "Iteration 921, Loss: 190.87566243620654\n",
      "Iteration 922, Loss: 190.3200652009075\n",
      "Iteration 923, Loss: 190.3583874695724\n",
      "Iteration 924, Loss: 190.40013582676056\n",
      "Iteration 925, Loss: 190.26844934467695\n",
      "Iteration 926, Loss: 189.66651244478777\n",
      "Iteration 927, Loss: 188.1101682988992\n",
      "Iteration 928, Loss: 188.1259780780494\n",
      "Iteration 929, Loss: 187.2999225785424\n",
      "Iteration 930, Loss: 187.2596541772079\n",
      "Iteration 931, Loss: 186.03204873960786\n",
      "Iteration 932, Loss: 186.04179438367188\n",
      "Iteration 933, Loss: 185.98556968053012\n",
      "Iteration 934, Loss: 185.8932016137502\n",
      "Iteration 935, Loss: 185.69489351848782\n",
      "Iteration 936, Loss: 185.7034027666987\n",
      "Iteration 937, Loss: 185.60259874487508\n",
      "Iteration 938, Loss: 185.60558914664384\n",
      "Iteration 939, Loss: 185.5393845630533\n",
      "Iteration 940, Loss: 185.7746456166828\n",
      "Iteration 941, Loss: 183.03167933054723\n",
      "Iteration 942, Loss: 183.16444777192984\n",
      "Iteration 943, Loss: 183.12946642341709\n",
      "Iteration 944, Loss: 183.1185550355203\n",
      "Iteration 945, Loss: 181.73691055502724\n",
      "Iteration 946, Loss: 181.8032266405374\n",
      "Iteration 947, Loss: 181.62234268116234\n",
      "Iteration 948, Loss: 182.26080146960842\n",
      "Iteration 949, Loss: 182.6283896649282\n",
      "Iteration 950, Loss: 182.66617482569714\n",
      "Iteration 951, Loss: 181.041850578816\n",
      "Iteration 952, Loss: 181.12556230456912\n",
      "Iteration 953, Loss: 180.90631457212046\n",
      "Iteration 954, Loss: 179.3211704111474\n",
      "Iteration 955, Loss: 178.9251311764256\n",
      "Iteration 956, Loss: 178.9305136490186\n",
      "Iteration 957, Loss: 178.55175238812703\n",
      "Iteration 958, Loss: 178.89213401070936\n",
      "Iteration 959, Loss: 178.78202161721043\n",
      "Iteration 960, Loss: 178.66777649113405\n",
      "Iteration 961, Loss: 178.6159979449396\n",
      "Iteration 962, Loss: 178.54675822789466\n",
      "Iteration 963, Loss: 177.83196305458128\n",
      "Iteration 964, Loss: 177.9509737239875\n",
      "Iteration 965, Loss: 177.95638815168118\n",
      "Iteration 966, Loss: 177.72263700199125\n",
      "Iteration 967, Loss: 177.6691201063538\n",
      "Iteration 968, Loss: 173.50675457720203\n",
      "Iteration 969, Loss: 173.44276715596564\n",
      "Iteration 970, Loss: 194.8743185247122\n",
      "Iteration 971, Loss: 194.91116353572315\n",
      "Iteration 972, Loss: 194.46700536638758\n",
      "Iteration 973, Loss: 179.30433135259534\n",
      "Iteration 974, Loss: 178.87210334055973\n",
      "Iteration 975, Loss: 179.64443029807254\n",
      "Iteration 976, Loss: 179.5927693554467\n",
      "Iteration 977, Loss: 177.29884986410576\n",
      "Iteration 978, Loss: 177.55116460343515\n",
      "Iteration 979, Loss: 175.98620968196656\n",
      "Iteration 980, Loss: 175.80984351872408\n",
      "Iteration 981, Loss: 175.1542402670509\n",
      "Iteration 982, Loss: 175.23774241246275\n",
      "Iteration 983, Loss: 175.63960878719354\n",
      "Iteration 984, Loss: 175.3344436437782\n",
      "Iteration 985, Loss: 175.66385636237126\n",
      "Iteration 986, Loss: 174.05570177584528\n",
      "Iteration 987, Loss: 173.99002639268323\n",
      "Iteration 988, Loss: 173.90604284325534\n",
      "Iteration 989, Loss: 173.6630487194085\n",
      "Iteration 990, Loss: 172.50428163721787\n",
      "Iteration 991, Loss: 172.4921387577773\n",
      "Iteration 992, Loss: 172.44269775069444\n",
      "Iteration 993, Loss: 172.76485046114783\n",
      "Iteration 994, Loss: 171.99094791991422\n",
      "Iteration 995, Loss: 171.42755704944085\n",
      "Iteration 996, Loss: 171.39248316339652\n",
      "Iteration 997, Loss: 171.5548295159011\n",
      "Iteration 998, Loss: 172.03233203070462\n",
      "Iteration 999, Loss: 172.219002479237\n",
      "Iteration 1000, Loss: 171.46528203049294\n",
      "y_pred_test=[7.1267 ... 7.68  ]\n",
      "\n",
      "\u001b[1;35mINFO:  SCORE['MyLinReg'] = \u001b[1;34m-28.944\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  TRAINING['SGDRegressor']..\u001b[0m\n",
      "\n",
      "y_pred_test=[-3.8607e+10 ...  2.1351e+10]\n",
      "\n",
      "\u001b[1;35mINFO:  SCORE['SGDRegressor'] = \u001b[1;34m-55859515566698398744576.000\u001b\\\n",
      "       [0m\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  ##############################################\n",
      "       \u001b[0m\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "try:\n",
    "    from libitmal import dataloaders\n",
    "except Exception as ex:\n",
    "    Err(\"can not import dataloaders form libitmal, and then I can not run the TestAndCompareRegressors smoke-test, sorry!\", ex)\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\",  dataloaders.IRIS_GetDataSet,  1E-2),\n",
    "              (\"MNIST\", dataloaders.MNIST_GetDataSet, 1E-3)]:\n",
    "        \n",
    "        # NOTE: f-tuble is (<name>, <data-loader-function-pointer>, <eps0>)\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        Info(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        eta0 = f[2] # an adaptive learning rate is really needed here!\n",
    "        regressor0 = MyLinReg(eta0=eta0, max_iter=1000)\n",
    "        regressor1 = SGDRegressor()    \n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            Info(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            \n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            \n",
    "            PrintMatrix(y_pred_test, label=\"y_pred_test=\", precision=4)\n",
    "            print()\n",
    "            \n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            Info(f\"SCORE['{r[0]}'] = {Col('lblue')}{r2:0.3f}{ColEnd()}\")\n",
    "            \n",
    "        Info(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow...\n",
    "TestAndCompareRegressors()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qh Conclusion\n",
    "\n",
    "As always, take some time to fine-tune your regressor, perhaps just some code-refactoring, cleaning out 'bad' code, and summarize all your findings\n",
    " above. \n",
    "\n",
    "In other words, write a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2022-12-22| CEF, initial draft. \n",
    "2023-02-26| CEF, first release.\n",
    "2023-02-28| CEF, fix a few issues related to import from libitmal, added Info and color output.\n",
    "2024-09-19| CEF, major overhaul, change math/text and code snippets.\n",
    "2024-09-25| CEF, final fixes, tests, and proof-reading. Moved early stopping and learning graphs to a later excercise.\n",
    "2024-10-04| CEF, clarified Qa with respect to what-is-to-be implemented and what-is-to-be described in text only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
