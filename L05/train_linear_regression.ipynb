{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "(In the following you need not present your journal in the Qa+b+c+ etc. order. You could just present the final code with test and comments.)\n",
    "\n",
    "## Training Your Own Linear Regressor\n",
    "\n",
    "Create a linear regressor, with a Scikit-learn compatible fit-predict interface. You should implement every detail of the linear regressor in Python, using whatever libraries, say `numpy`, you want (except a linear regressor itself).\n",
    "\n",
    "Below is a primitive _get-started_ skeleton for your implementation. Keep the class name `MyLinReg`, which is used in the test sequence later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class MyLinReg():\n",
    "    def __init__(self, eta0=0.01, max_iter=10, tol=1e-3, n_iter_no_change=5, verbose=True):\n",
    "        self.eta0 = eta0\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MyLinReg.__str__(): hi!\"\n",
    "    \n",
    "    # def update_weights(self):     \n",
    "    #     Y_pred = self.predict( self.X ) \n",
    "          \n",
    "    #     # Calculate gradients\n",
    "    #     dW = - (2 / self.m) * self.X.T.dot(self.Y - Y_pred)\n",
    "    #     db = - (2 / self.m) * np.sum(self.Y - Y_pred)\n",
    "          \n",
    "    #     # update weights       \n",
    "    #     self.W = self.W - self.eta0 * dW       \n",
    "    #     self.b = self.b - self.eta0 * db           \n",
    "    #     return self\n",
    "    def Weights(self): \n",
    "        # Pick one random sample (stochastic)\n",
    "        i = np.random.randint(0, self.m)  # Random index\n",
    "        X_i = self.X[i].reshape(1, -1)  # Make it a row vector\n",
    "        Y_i = self.Y[i]  # Corresponding target\n",
    "\n",
    "        # Compute prediction\n",
    "        Y_pred = self.predict(X_i)\n",
    "\n",
    "        # Compute gradient (SGD formula)\n",
    "        dW = X_i.T.dot(Y_pred - Y_i)  # Gradient for weights\n",
    "        db =  - (2 / self.m) * np.sum(self.Y - Y_pred)  # Gradient for bias\n",
    "\n",
    "        # Update weights\n",
    "        self.W = self.W - self.eta0 * dW\n",
    "        self.b = self.b - self.eta0 * db\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.m, self.n = X.shape\n",
    "         # weight initialization           \n",
    "        self.W = np.zeros( self.n )           \n",
    "        self.b = 0          \n",
    "        self.X = X           \n",
    "        self.Y = y \n",
    "\n",
    "        best_loss = float('inf')\n",
    "        no_change_count = 0\n",
    "                    \n",
    "        # gradient descent learning                  \n",
    "        for i in range(self.max_iter):\n",
    "            Y_pred = self.predict(self.X)\n",
    "            loss = np.mean((self.Y - Y_pred) ** 2)  # Mean Squared Error (MSE)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Iteration {i+1}, Loss: {loss}\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if abs(best_loss - loss) < self.tol:\n",
    "                no_change_count += 1\n",
    "                if no_change_count >= self.n_iter_no_change:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Stopping early at iteration {i+1} due to no improvement.\")\n",
    "                    break\n",
    "            else:\n",
    "                no_change_count = 0  # Reset count if loss decreases\n",
    "\n",
    "            best_loss = loss\n",
    "\n",
    "            self.Weights()               \n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the output for given input X.\"\"\"\n",
    "        return X.dot(self.W) + self.b\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        \"\"\"Compute R^2 (coefficient of determination) score.\"\"\"\n",
    "        y_pred = self.predict(X)  # Get model predictions\n",
    "\n",
    "        ss_total = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "        ss_residual = np.sum((y_true - y_pred) ** 2)  # Residual sum of squares\n",
    "        \n",
    "        r2_score = 1 - (ss_residual / ss_total)  # Compute R^2\n",
    "\n",
    "        return r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TODO list\n",
    "\n",
    "You must investigate and describe all major details for a linear regressor, and implement at least the following concepts (MUST):\n",
    "\n",
    "### Qa: Concepts and Implementations MUSTS\n",
    "\n",
    "* Implement: the `fit-predict` interface, for a one-dimensional output only, \n",
    "* Implement: a $R^2$ score function (re-use existing code or perhaps just inherit it), \n",
    "* Implement: loss function based on (R)MSE,\n",
    "* Implement: setting of the number of iterations and learning rate ($\\eta$) via parameters in the constructor (the signature of your `__init__` must include the named parameters `eta0` and `max_iter`),\n",
    "* (in a later exercise we will also add `tol`, `n_iter_no_change` and `verbose` to the constructor),\n",
    "* Implement: the batch-gradient decent algorithm (GD),\n",
    "* Implement: constant learning rate (maybe also adaptive learning rate if you are brave),\n",
    "* Implement: stochastic gradient descent (SGD),\n",
    "* Describe in text: epochs vs iterations,\n",
    "* Describe in text: compare the numerical optimization with the Closed-form solution.\n",
    "\n",
    "### Qb: [OPTIONAL] Additional Concepts and Implementations\n",
    "\n",
    "And perhaps you could include (SHOULD/COULD):\n",
    "\n",
    "* (stochastic) mini-bach gradient decent, \n",
    "* interface to your bias and weights via `intercept_` and `coef_` attributes on your linear regressor `class`,\n",
    "* get/set functionality of your regressor, such that it is fully compatible with other Scikit-learn algorithms, try it out in say a `cross_val_score()` call from Scikit-learn,\n",
    "* test in via the smoke tests at the end of this Notebook,\n",
    "* testing it on MNIST data.\n",
    "\n",
    "With the following no-no's (WONT):\n",
    "\n",
    "* no learning graphs, no early stopping (we will do this in a later exercise),\n",
    "* no multi-linear regression,\n",
    "* no reuse of the Scikit-learn regressor,\n",
    "* no `C/C++` optimized implementation with a _thin_ Python interface (nifty, but out-of-scope for this cause),\n",
    "* no copy-paste of code from other sources WITHOUT a clear cite/reference for your source.\n",
    "\n",
    "### Qc: Testing and Test Data\n",
    "\n",
    "Use mainly very low-dimensional data for testing, say the IRIS set, since it might be very slow. Or create a simple low-dimensionality data generator.\n",
    "\n",
    "(There is a _micro_ data set in the function `GenerateData` in the smoke tests functions below, but better is to opt for an realistic data set.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 5901425308.702746\n",
      "Iteration 2, Loss: 4335763329.341322\n",
      "Iteration 3, Loss: 2003718100.4523504\n",
      "Iteration 4, Loss: 1944755307.8720984\n",
      "Iteration 5, Loss: 2095806339.7383912\n",
      "Iteration 6, Loss: 2270508450.4573355\n",
      "Iteration 7, Loss: 1903901966.5123832\n",
      "Iteration 8, Loss: 1957798773.066677\n",
      "Iteration 9, Loss: 2063849450.4554574\n",
      "Iteration 10, Loss: 2171364200.0252748\n",
      "Iteration 11, Loss: 2367103555.7009087\n",
      "Iteration 12, Loss: 2628825704.0413\n",
      "Iteration 13, Loss: 2637920744.553008\n",
      "Iteration 14, Loss: 2793549456.3557396\n",
      "Iteration 15, Loss: 2733301418.8243523\n",
      "Iteration 16, Loss: 2712339281.825598\n",
      "Iteration 17, Loss: 1895975934.0664654\n",
      "Iteration 18, Loss: 2113716456.2215264\n",
      "Iteration 19, Loss: 2069053038.6889246\n",
      "Iteration 20, Loss: 2325174866.7964807\n",
      "Iteration 21, Loss: 2684322022.127235\n",
      "Iteration 22, Loss: 1850690070.8892996\n",
      "Iteration 23, Loss: 1880185449.3386695\n",
      "Iteration 24, Loss: 2129916856.1657488\n",
      "Iteration 25, Loss: 1840066393.1459935\n",
      "Iteration 26, Loss: 1914430470.605385\n",
      "Iteration 27, Loss: 1845558095.5241153\n",
      "Iteration 28, Loss: 1938153221.6486058\n",
      "Iteration 29, Loss: 2031777494.4200537\n",
      "Iteration 30, Loss: 2173105950.3133845\n",
      "Iteration 31, Loss: 1806690072.567707\n",
      "Iteration 32, Loss: 2090153173.6684773\n",
      "Iteration 33, Loss: 1894750233.9564514\n",
      "Iteration 34, Loss: 2270035729.9043193\n",
      "Iteration 35, Loss: 2018208421.8601327\n",
      "Iteration 36, Loss: 2173753281.8964806\n",
      "Iteration 37, Loss: 2003143219.2765656\n",
      "Iteration 38, Loss: 2345306002.470519\n",
      "Iteration 39, Loss: 1825898908.5428765\n",
      "Iteration 40, Loss: 2026953722.0299847\n",
      "Iteration 41, Loss: 2250102705.640086\n",
      "Iteration 42, Loss: 1816530989.6630259\n",
      "Iteration 43, Loss: 1886535502.9924202\n",
      "Iteration 44, Loss: 1986735164.7779021\n",
      "Iteration 45, Loss: 1823271190.607759\n",
      "Iteration 46, Loss: 1962847812.971473\n",
      "Iteration 47, Loss: 2072435583.9360902\n",
      "Iteration 48, Loss: 1706089481.9318423\n",
      "Iteration 49, Loss: 1727994658.8020532\n",
      "Iteration 50, Loss: 1836262428.2852082\n",
      "Iteration 51, Loss: 2148639280.4345074\n",
      "Iteration 52, Loss: 2275585885.0810943\n",
      "Iteration 53, Loss: 2447089001.2220116\n",
      "Iteration 54, Loss: 3232728742.933725\n",
      "Iteration 55, Loss: 1644116275.9682856\n",
      "Iteration 56, Loss: 1565311363.9603605\n",
      "Iteration 57, Loss: 2027510974.6554303\n",
      "Iteration 58, Loss: 2175914500.872764\n",
      "Iteration 59, Loss: 1725464065.2690797\n",
      "Iteration 60, Loss: 1791263547.794258\n",
      "Iteration 61, Loss: 1857410739.0443594\n",
      "Iteration 62, Loss: 2130985421.857732\n",
      "Iteration 63, Loss: 2828225417.5931168\n",
      "Iteration 64, Loss: 1599331187.7466583\n",
      "Iteration 65, Loss: 1623488911.3625274\n",
      "Iteration 66, Loss: 1709310615.9235032\n",
      "Iteration 67, Loss: 1719455228.9680235\n",
      "Iteration 68, Loss: 1971368653.7710552\n",
      "Iteration 69, Loss: 1594867204.703488\n",
      "Iteration 70, Loss: 1688230373.4505298\n",
      "Iteration 71, Loss: 1801633705.350176\n",
      "Iteration 72, Loss: 2073594907.4015596\n",
      "Iteration 73, Loss: 1582285050.8753085\n",
      "Iteration 74, Loss: 1651984794.8117964\n",
      "Iteration 75, Loss: 1860381270.3743947\n",
      "Iteration 76, Loss: 1642377901.3155987\n",
      "Iteration 77, Loss: 1817942590.8175354\n",
      "Iteration 78, Loss: 1598278045.5176814\n",
      "Iteration 79, Loss: 1628191271.329114\n",
      "Iteration 80, Loss: 1703286708.1968954\n",
      "Iteration 81, Loss: 1761355826.3589303\n",
      "Iteration 82, Loss: 1874048602.8882868\n",
      "Iteration 83, Loss: 1969263796.4532537\n",
      "Iteration 84, Loss: 1951921989.0789406\n",
      "Iteration 85, Loss: 2001129934.1870918\n",
      "Iteration 86, Loss: 1564812301.9640477\n",
      "Iteration 87, Loss: 1626044931.1131632\n",
      "Iteration 88, Loss: 1710393965.745408\n",
      "Iteration 89, Loss: 1823317697.6545067\n",
      "Iteration 90, Loss: 1933199023.0835755\n",
      "Iteration 91, Loss: 2047562637.564364\n",
      "Iteration 92, Loss: 2870247510.469723\n",
      "Iteration 93, Loss: 17685648451.80253\n",
      "Iteration 94, Loss: 2239497370.8226995\n",
      "Iteration 95, Loss: 1804509639.2612283\n",
      "Iteration 96, Loss: 1489773841.3604329\n",
      "Iteration 97, Loss: 1500782590.3523192\n",
      "Iteration 98, Loss: 1570229076.7934115\n",
      "Iteration 99, Loss: 1636718802.7812083\n",
      "Iteration 100, Loss: 1782480665.1716402\n",
      "Iteration 101, Loss: 1876534964.9968452\n",
      "Iteration 102, Loss: 2025966472.8697064\n",
      "Iteration 103, Loss: 1793204092.9657695\n",
      "Iteration 104, Loss: 1849681470.4738085\n",
      "Iteration 105, Loss: 1973440252.5942376\n",
      "Iteration 106, Loss: 1813930890.3238826\n",
      "Iteration 107, Loss: 1930068064.5997007\n",
      "Iteration 108, Loss: 1986864707.2481294\n",
      "Iteration 109, Loss: 2037154934.600455\n",
      "Iteration 110, Loss: 1304581868.6237133\n",
      "Iteration 111, Loss: 1306632546.0848906\n",
      "Iteration 112, Loss: 1309395664.0409875\n",
      "Iteration 113, Loss: 1327301535.3218029\n",
      "Iteration 114, Loss: 1614308607.7396407\n",
      "Iteration 115, Loss: 1359069760.8119872\n",
      "Iteration 116, Loss: 1398875247.5977616\n",
      "Iteration 117, Loss: 1324180642.916986\n",
      "Iteration 118, Loss: 1361074105.645801\n",
      "Iteration 119, Loss: 1555740089.006858\n",
      "Iteration 120, Loss: 1380772159.4231138\n",
      "Iteration 121, Loss: 1309489429.0320566\n",
      "Iteration 122, Loss: 1366411437.1798246\n",
      "Iteration 123, Loss: 1309526500.185916\n",
      "Iteration 124, Loss: 1492407149.8700635\n",
      "Iteration 125, Loss: 1667070555.538716\n",
      "Iteration 126, Loss: 1728907031.9425952\n",
      "Iteration 127, Loss: 1676132363.0975945\n",
      "Iteration 128, Loss: 1759211722.9610357\n",
      "Iteration 129, Loss: 1702669533.0489345\n",
      "Iteration 130, Loss: 1594191772.4531343\n",
      "Iteration 131, Loss: 1590703128.6119647\n",
      "Iteration 132, Loss: 1458967850.0998309\n",
      "Iteration 133, Loss: 1380186887.2202847\n",
      "Iteration 134, Loss: 1397424221.413516\n",
      "Iteration 135, Loss: 1413212709.7045884\n",
      "Iteration 136, Loss: 1273721853.198528\n",
      "Iteration 137, Loss: 1290320029.737913\n",
      "Iteration 138, Loss: 1489818826.9371016\n",
      "Iteration 139, Loss: 1509413755.8481646\n",
      "Iteration 140, Loss: 1522742082.7251315\n",
      "Iteration 141, Loss: 1419477349.7585871\n",
      "Iteration 142, Loss: 1440316609.317241\n",
      "Iteration 143, Loss: 1319357009.964021\n",
      "Iteration 144, Loss: 1320004825.4359603\n",
      "Iteration 145, Loss: 1227759626.0940444\n",
      "Iteration 146, Loss: 1414770708.3823156\n",
      "Iteration 147, Loss: 1570983621.6349337\n",
      "Iteration 148, Loss: 1647887700.180384\n",
      "Iteration 149, Loss: 1519293342.8726544\n",
      "Iteration 150, Loss: 1249131301.2061534\n",
      "Iteration 151, Loss: 1415237286.9543152\n",
      "Iteration 152, Loss: 1560139004.3252745\n",
      "Iteration 153, Loss: 1580476321.6141477\n",
      "Iteration 154, Loss: 1531012449.5157568\n",
      "Iteration 155, Loss: 1322008253.6730363\n",
      "Iteration 156, Loss: 1321009620.2373552\n",
      "Iteration 157, Loss: 1343291192.3824108\n",
      "Iteration 158, Loss: 1263261663.9031005\n",
      "Iteration 159, Loss: 1298018706.3475127\n",
      "Iteration 160, Loss: 1321406935.7204485\n",
      "Iteration 161, Loss: 1316793202.810077\n",
      "Iteration 162, Loss: 1395061232.7116091\n",
      "Iteration 163, Loss: 1246800414.5027883\n",
      "Iteration 164, Loss: 1285282298.716694\n",
      "Iteration 165, Loss: 1166459703.7539098\n",
      "Iteration 166, Loss: 1304543729.0165381\n",
      "Iteration 167, Loss: 1372832885.7615964\n",
      "Iteration 168, Loss: 1252452374.357026\n",
      "Iteration 169, Loss: 1256218742.613527\n",
      "Iteration 170, Loss: 1272605203.6864853\n",
      "Iteration 171, Loss: 1273125450.3960786\n",
      "Iteration 172, Loss: 1270786428.6072161\n",
      "Iteration 173, Loss: 1277822141.320847\n",
      "Iteration 174, Loss: 1200601590.1904988\n",
      "Iteration 175, Loss: 1172978415.1472108\n",
      "Iteration 176, Loss: 1201057220.2804186\n",
      "Iteration 177, Loss: 1308527054.8839824\n",
      "Iteration 178, Loss: 1367129608.1089876\n",
      "Iteration 179, Loss: 1364151077.9336286\n",
      "Iteration 180, Loss: 1236888390.5496886\n",
      "Iteration 181, Loss: 1173498055.5918727\n",
      "Iteration 182, Loss: 1171243127.8880348\n",
      "Iteration 183, Loss: 1759376498.7629564\n",
      "Iteration 184, Loss: 1095536077.6265793\n",
      "Iteration 185, Loss: 1648732655.1030018\n",
      "Iteration 186, Loss: 1627350354.725504\n",
      "Iteration 187, Loss: 1311661738.840031\n",
      "Iteration 188, Loss: 1107780581.8860416\n",
      "Iteration 189, Loss: 1200419687.6855927\n",
      "Iteration 190, Loss: 1320956861.8543572\n",
      "Iteration 191, Loss: 1177827861.0117414\n",
      "Iteration 192, Loss: 1229101847.3882248\n",
      "Iteration 193, Loss: 1289413951.7650678\n",
      "Iteration 194, Loss: 1158386108.8949916\n",
      "Iteration 195, Loss: 1282407780.0205424\n",
      "Iteration 196, Loss: 1324101806.7678444\n",
      "Iteration 197, Loss: 1327345151.4581683\n",
      "Iteration 198, Loss: 1160884673.493656\n",
      "Iteration 199, Loss: 1168374223.127613\n",
      "Iteration 200, Loss: 1409384781.9029021\n",
      "Iteration 201, Loss: 1404102521.74554\n",
      "Iteration 202, Loss: 1452149392.334063\n",
      "Iteration 203, Loss: 1436876405.5823872\n",
      "Iteration 204, Loss: 1447748048.2313075\n",
      "Iteration 205, Loss: 1468538473.3243556\n",
      "Iteration 206, Loss: 1432546927.6244404\n",
      "Iteration 207, Loss: 1409618372.007757\n",
      "Iteration 208, Loss: 1408312673.5470068\n",
      "Iteration 209, Loss: 1318565370.082155\n",
      "Iteration 210, Loss: 1053535386.4224007\n",
      "Iteration 211, Loss: 1453051981.723168\n",
      "Iteration 212, Loss: 1098351834.9028807\n",
      "Iteration 213, Loss: 1400175839.2885025\n",
      "Iteration 214, Loss: 1406454522.3191893\n",
      "Iteration 215, Loss: 1370870546.7229493\n",
      "Iteration 216, Loss: 1397289734.996732\n",
      "Iteration 217, Loss: 1410384604.7687325\n",
      "Iteration 218, Loss: 1324525180.57857\n",
      "Iteration 219, Loss: 1206650942.6766162\n",
      "Iteration 220, Loss: 1160664381.265999\n",
      "Iteration 221, Loss: 1097403556.026706\n",
      "Iteration 222, Loss: 1518321282.2719147\n",
      "Iteration 223, Loss: 1167031716.6568954\n",
      "Iteration 224, Loss: 1295213997.311373\n",
      "Iteration 225, Loss: 1098912961.0842812\n",
      "Iteration 226, Loss: 1777491617.6880922\n",
      "Iteration 227, Loss: 1257549852.662434\n",
      "Iteration 228, Loss: 1212712117.396269\n",
      "Iteration 229, Loss: 1408379489.1001482\n",
      "Iteration 230, Loss: 1491693297.3755429\n",
      "Iteration 231, Loss: 1483152898.2675803\n",
      "Iteration 232, Loss: 1411552335.9782388\n",
      "Iteration 233, Loss: 1404656583.935195\n",
      "Iteration 234, Loss: 1174631320.6653883\n",
      "Iteration 235, Loss: 1120248127.4206634\n",
      "Iteration 236, Loss: 1378666891.2950873\n",
      "Iteration 237, Loss: 1153935654.276301\n",
      "Iteration 238, Loss: 1118751570.4417357\n",
      "Iteration 239, Loss: 1130870554.3479733\n",
      "Iteration 240, Loss: 1186095481.2418926\n",
      "Iteration 241, Loss: 1166381599.6532598\n",
      "Iteration 242, Loss: 1150165376.6941938\n",
      "Iteration 243, Loss: 1205228737.590213\n",
      "Iteration 244, Loss: 1192373914.7958038\n",
      "Iteration 245, Loss: 1162169194.01314\n",
      "Iteration 246, Loss: 1135307887.509772\n",
      "Iteration 247, Loss: 1358504598.2844746\n",
      "Iteration 248, Loss: 1408013174.8277233\n",
      "Iteration 249, Loss: 1286907420.2611501\n",
      "Iteration 250, Loss: 1337223999.2103434\n",
      "Iteration 251, Loss: 1376450956.7004082\n",
      "Iteration 252, Loss: 1160187106.4631395\n",
      "Iteration 253, Loss: 1136819041.7607632\n",
      "Iteration 254, Loss: 1179008107.7474327\n",
      "Iteration 255, Loss: 1293008471.2071724\n",
      "Iteration 256, Loss: 1336144285.0068889\n",
      "Iteration 257, Loss: 1381474836.381095\n",
      "Iteration 258, Loss: 1415587170.244223\n",
      "Iteration 259, Loss: 1132200116.829759\n",
      "Iteration 260, Loss: 1096839499.0502255\n",
      "Iteration 261, Loss: 1102282773.1158102\n",
      "Iteration 262, Loss: 1154475600.3777485\n",
      "Iteration 263, Loss: 1321965449.5814602\n",
      "Iteration 264, Loss: 1374474739.801444\n",
      "Iteration 265, Loss: 1342085157.978765\n",
      "Iteration 266, Loss: 1332116344.8833828\n",
      "Iteration 267, Loss: 1079759601.2426295\n",
      "Iteration 268, Loss: 1409246926.6236706\n",
      "Iteration 269, Loss: 1340898244.9649632\n",
      "Iteration 270, Loss: 1112571930.12953\n",
      "Iteration 271, Loss: 1370374372.8605986\n",
      "Iteration 272, Loss: 1343699834.30062\n",
      "Iteration 273, Loss: 1188968669.302844\n",
      "Iteration 274, Loss: 1330304599.434249\n",
      "Iteration 275, Loss: 1378915675.471873\n",
      "Iteration 276, Loss: 1356123233.5942397\n",
      "Iteration 277, Loss: 1206354927.3098981\n",
      "Iteration 278, Loss: 1217297602.5751586\n",
      "Iteration 279, Loss: 1240983653.3577743\n",
      "Iteration 280, Loss: 1291459394.289426\n",
      "Iteration 281, Loss: 1288504846.3436828\n",
      "Iteration 282, Loss: 1282585325.7513256\n",
      "Iteration 283, Loss: 1271889291.546796\n",
      "Iteration 284, Loss: 1485704515.514162\n",
      "Iteration 285, Loss: 1165797771.8691275\n",
      "Iteration 286, Loss: 1089210822.3926697\n",
      "Iteration 287, Loss: 1963877144.7751331\n",
      "Iteration 288, Loss: 1843135961.0065248\n",
      "Iteration 289, Loss: 1110914380.1008036\n",
      "Iteration 290, Loss: 1119409050.240106\n",
      "Iteration 291, Loss: 1108702013.0881538\n",
      "Iteration 292, Loss: 1103663432.102797\n",
      "Iteration 293, Loss: 1258044046.5586667\n",
      "Iteration 294, Loss: 1125830166.251384\n",
      "Iteration 295, Loss: 1128885195.7733755\n",
      "Iteration 296, Loss: 1433210191.4507065\n",
      "Iteration 297, Loss: 1456505775.450089\n",
      "Iteration 298, Loss: 1427538886.858006\n",
      "Iteration 299, Loss: 1371377808.728669\n",
      "Iteration 300, Loss: 1178851011.5876806\n",
      "Iteration 301, Loss: 1180439606.2388175\n",
      "Iteration 302, Loss: 1159149947.9852853\n",
      "Iteration 303, Loss: 1278957794.6627529\n",
      "Iteration 304, Loss: 1249453037.603425\n",
      "Iteration 305, Loss: 1137519786.7731674\n",
      "Iteration 306, Loss: 1255643514.8699176\n",
      "Iteration 307, Loss: 1264863331.8982546\n",
      "Iteration 308, Loss: 1192893038.4278982\n",
      "Iteration 309, Loss: 1253486404.1080022\n",
      "Iteration 310, Loss: 1179426112.0878673\n",
      "Iteration 311, Loss: 1313117357.7575986\n",
      "Iteration 312, Loss: 1360994644.4510279\n",
      "Iteration 313, Loss: 1254436913.3888445\n",
      "Iteration 314, Loss: 1620425301.9009202\n",
      "Iteration 315, Loss: 1143134722.7768795\n",
      "Iteration 316, Loss: 1223150386.6508005\n",
      "Iteration 317, Loss: 1260330673.6648395\n",
      "Iteration 318, Loss: 1392732572.5482404\n",
      "Iteration 319, Loss: 1495565780.7953222\n",
      "Iteration 320, Loss: 1231508099.7269363\n",
      "Iteration 321, Loss: 1173640022.1245184\n",
      "Iteration 322, Loss: 1674216033.0759456\n",
      "Iteration 323, Loss: 1676010506.2199006\n",
      "Iteration 324, Loss: 1682615315.213612\n",
      "Iteration 325, Loss: 1408045002.470378\n",
      "Iteration 326, Loss: 1408726935.7748253\n",
      "Iteration 327, Loss: 1482927841.5354464\n",
      "Iteration 328, Loss: 1491716554.04692\n",
      "Iteration 329, Loss: 1440312177.726448\n",
      "Iteration 330, Loss: 1152105530.1367707\n",
      "Iteration 331, Loss: 1482895343.4594743\n",
      "Iteration 332, Loss: 1261746151.1097887\n",
      "Iteration 333, Loss: 1156016820.896708\n",
      "Iteration 334, Loss: 1536311404.7629948\n",
      "Iteration 335, Loss: 1535662280.4014583\n",
      "Iteration 336, Loss: 1249209654.2711782\n",
      "Iteration 337, Loss: 1173126968.8767521\n",
      "Iteration 338, Loss: 1174452057.433083\n",
      "Iteration 339, Loss: 1151853490.842406\n",
      "Iteration 340, Loss: 1158573193.1035764\n",
      "Iteration 341, Loss: 1331203086.778333\n",
      "Iteration 342, Loss: 1328013373.1251578\n",
      "Iteration 343, Loss: 1476028655.0561829\n",
      "Iteration 344, Loss: 1158113143.3306866\n",
      "Iteration 345, Loss: 1168345903.9822426\n",
      "Iteration 346, Loss: 1172255714.9604955\n",
      "Iteration 347, Loss: 1177351756.2443607\n",
      "Iteration 348, Loss: 1335468194.5382738\n",
      "Iteration 349, Loss: 1395193738.6650412\n",
      "Iteration 350, Loss: 1387045544.3819475\n",
      "Iteration 351, Loss: 1388907843.184302\n",
      "Iteration 352, Loss: 1253451089.417507\n",
      "Iteration 353, Loss: 1165921595.4090314\n",
      "Iteration 354, Loss: 1169156898.5959816\n",
      "Iteration 355, Loss: 1743411274.5046074\n",
      "Iteration 356, Loss: 1634535826.0931292\n",
      "Iteration 357, Loss: 1212570732.0720723\n",
      "Iteration 358, Loss: 1365841919.9056327\n",
      "Iteration 359, Loss: 1180537085.2671392\n",
      "Iteration 360, Loss: 1174419057.6969428\n",
      "Iteration 361, Loss: 1183756889.9794183\n",
      "Iteration 362, Loss: 1161466695.3214562\n",
      "Iteration 363, Loss: 1423787820.772239\n",
      "Iteration 364, Loss: 1464206683.6224542\n",
      "Iteration 365, Loss: 1227938455.1915138\n",
      "Iteration 366, Loss: 1494719302.9529402\n",
      "Iteration 367, Loss: 1487791381.5347722\n",
      "Iteration 368, Loss: 1553713053.8073115\n",
      "Iteration 369, Loss: 1554504392.3848195\n",
      "Iteration 370, Loss: 1426962941.021118\n",
      "Iteration 371, Loss: 1117262241.0506554\n",
      "Iteration 372, Loss: 1126317385.9466777\n",
      "Iteration 373, Loss: 1090377103.2204225\n",
      "Iteration 374, Loss: 1192281959.1618962\n",
      "Iteration 375, Loss: 1223646386.335253\n",
      "Iteration 376, Loss: 1072711225.1054018\n",
      "Iteration 377, Loss: 1064926977.971419\n",
      "Iteration 378, Loss: 1189547295.7994535\n",
      "Iteration 379, Loss: 1226252297.841854\n",
      "Iteration 380, Loss: 1151001033.944014\n",
      "Iteration 381, Loss: 1129384003.815415\n",
      "Iteration 382, Loss: 1123932437.7546105\n",
      "Iteration 383, Loss: 1930209038.2027497\n",
      "Iteration 384, Loss: 1094236105.1099772\n",
      "Iteration 385, Loss: 1212286911.4903464\n",
      "Iteration 386, Loss: 1637156979.6158621\n",
      "Iteration 387, Loss: 1626950930.9056065\n",
      "Iteration 388, Loss: 1347681237.6512072\n",
      "Iteration 389, Loss: 2083646719.7824206\n",
      "Iteration 390, Loss: 1941191195.7876596\n",
      "Iteration 391, Loss: 1113909599.087139\n",
      "Iteration 392, Loss: 1769118798.3641336\n",
      "Iteration 393, Loss: 1137066284.5006344\n",
      "Iteration 394, Loss: 1530820878.1735036\n",
      "Iteration 395, Loss: 1386636379.35846\n",
      "Iteration 396, Loss: 1167176329.9325144\n",
      "Iteration 397, Loss: 2004529745.3529825\n",
      "Iteration 398, Loss: 1299446448.676005\n",
      "Iteration 399, Loss: 1402546644.747698\n",
      "Iteration 400, Loss: 1490286544.1865273\n",
      "Iteration 401, Loss: 1592769763.0318983\n",
      "Iteration 402, Loss: 1596440573.2485678\n",
      "Iteration 403, Loss: 1604033315.4383976\n",
      "Iteration 404, Loss: 1301930313.9545162\n",
      "Iteration 405, Loss: 4497145188.91054\n",
      "Iteration 406, Loss: 2838319966.9420123\n",
      "Iteration 407, Loss: 1486265713.2131417\n",
      "Iteration 408, Loss: 1456318024.9117675\n",
      "Iteration 409, Loss: 1488646071.5501938\n",
      "Iteration 410, Loss: 1206124765.4975529\n",
      "Iteration 411, Loss: 1260775171.9698536\n",
      "Iteration 412, Loss: 1434935484.4787488\n",
      "Iteration 413, Loss: 1504810113.2957783\n",
      "Iteration 414, Loss: 1309089370.9698691\n",
      "Iteration 415, Loss: 1313827337.4584117\n",
      "Iteration 416, Loss: 1250024755.1507568\n",
      "Iteration 417, Loss: 1311340154.5040326\n",
      "Iteration 418, Loss: 1241945232.7428896\n",
      "Iteration 419, Loss: 1457889541.4634938\n",
      "Iteration 420, Loss: 1548435409.977233\n",
      "Iteration 421, Loss: 1543729517.0923107\n",
      "Iteration 422, Loss: 1531926678.1560829\n",
      "Iteration 423, Loss: 1253248709.0301468\n",
      "Iteration 424, Loss: 1416615202.7636974\n",
      "Iteration 425, Loss: 1544314312.8973327\n",
      "Iteration 426, Loss: 1678786706.1770263\n",
      "Iteration 427, Loss: 1260043608.225119\n",
      "Iteration 428, Loss: 1563271289.9634008\n",
      "Iteration 429, Loss: 1291357623.1555\n",
      "Iteration 430, Loss: 1495666824.0295281\n",
      "Iteration 431, Loss: 1577894666.2015016\n",
      "Iteration 432, Loss: 1693423407.3099558\n",
      "Iteration 433, Loss: 1713737343.9786546\n",
      "Iteration 434, Loss: 1762304707.6326613\n",
      "Iteration 435, Loss: 1228869482.3904748\n",
      "Iteration 436, Loss: 1740502418.8566914\n",
      "Iteration 437, Loss: 1261013703.8138566\n",
      "Iteration 438, Loss: 1455242602.1223445\n",
      "Iteration 439, Loss: 1552718194.153362\n",
      "Iteration 440, Loss: 1370783223.8541818\n",
      "Iteration 441, Loss: 1370489930.580813\n",
      "Iteration 442, Loss: 1553999460.5604732\n",
      "Iteration 443, Loss: 1549620954.2044315\n",
      "Iteration 444, Loss: 1300563622.612317\n",
      "Iteration 445, Loss: 1516992313.6097205\n",
      "Iteration 446, Loss: 1318458760.04031\n",
      "Iteration 447, Loss: 1687636653.2378504\n",
      "Iteration 448, Loss: 1378813371.681339\n",
      "Iteration 449, Loss: 1300282009.764448\n",
      "Iteration 450, Loss: 1312121097.8170025\n",
      "Iteration 451, Loss: 1417561157.6601295\n",
      "Iteration 452, Loss: 1415758063.003139\n",
      "Iteration 453, Loss: 1399681822.1204538\n",
      "Iteration 454, Loss: 1507131902.283377\n",
      "Iteration 455, Loss: 1439577199.904693\n",
      "Iteration 456, Loss: 1626143228.2477634\n",
      "Iteration 457, Loss: 1773371290.05285\n",
      "Iteration 458, Loss: 1950101773.6872878\n",
      "Iteration 459, Loss: 1450625883.3229668\n",
      "Iteration 460, Loss: 4610342611.84823\n",
      "Iteration 461, Loss: 4030021332.74452\n",
      "Iteration 462, Loss: 3893243008.2679143\n",
      "Iteration 463, Loss: 2580277809.751429\n",
      "Iteration 464, Loss: 1307780601.6906588\n",
      "Iteration 465, Loss: 1294060347.0702395\n",
      "Iteration 466, Loss: 1448695351.9104211\n",
      "Iteration 467, Loss: 1600084010.6831691\n",
      "Iteration 468, Loss: 1444231389.744108\n",
      "Iteration 469, Loss: 1421033969.3798273\n",
      "Iteration 470, Loss: 1473676938.0950158\n",
      "Iteration 471, Loss: 1633798217.1797607\n",
      "Iteration 472, Loss: 1419210876.2797568\n",
      "Iteration 473, Loss: 1471115866.8225079\n",
      "Iteration 474, Loss: 1548523110.1599448\n",
      "Iteration 475, Loss: 1691166647.1605468\n",
      "Iteration 476, Loss: 1820344045.374745\n",
      "Iteration 477, Loss: 1836466600.0331318\n",
      "Iteration 478, Loss: 1857552111.2305887\n",
      "Iteration 479, Loss: 1376970229.1333146\n",
      "Iteration 480, Loss: 1389004246.6127431\n",
      "Iteration 481, Loss: 1237453019.8162522\n",
      "Iteration 482, Loss: 1308432943.501317\n",
      "Iteration 483, Loss: 1453260142.7808292\n",
      "Iteration 484, Loss: 1255760500.723234\n",
      "Iteration 485, Loss: 1524507493.6008718\n",
      "Iteration 486, Loss: 1376794287.5976694\n",
      "Iteration 487, Loss: 1329815473.1297832\n",
      "Iteration 488, Loss: 1295471986.8759897\n",
      "Iteration 489, Loss: 1298127035.0873365\n",
      "Iteration 490, Loss: 1562352546.944119\n",
      "Iteration 491, Loss: 1611621728.7828074\n",
      "Iteration 492, Loss: 1254558738.0996125\n",
      "Iteration 493, Loss: 1678975172.0347817\n",
      "Iteration 494, Loss: 1765874535.8453524\n",
      "Iteration 495, Loss: 1821048379.425263\n",
      "Iteration 496, Loss: 1773086126.1338801\n",
      "Iteration 497, Loss: 1810820969.0844054\n",
      "Iteration 498, Loss: 1502263912.048439\n",
      "Iteration 499, Loss: 1919576031.396288\n",
      "Iteration 500, Loss: 1908340727.164345\n",
      "Iteration 501, Loss: 1937245954.8955135\n",
      "Iteration 502, Loss: 1247731351.6956577\n",
      "Iteration 503, Loss: 1294456869.0896187\n",
      "Iteration 504, Loss: 1463338778.2446227\n",
      "Iteration 505, Loss: 1257642328.4393187\n",
      "Iteration 506, Loss: 1277757630.7612543\n",
      "Iteration 507, Loss: 1367862260.846123\n",
      "Iteration 508, Loss: 1384393898.373297\n",
      "Iteration 509, Loss: 1384256769.5000188\n",
      "Iteration 510, Loss: 1520279297.5573857\n",
      "Iteration 511, Loss: 1209030503.3312695\n",
      "Iteration 512, Loss: 1223754317.3149545\n",
      "Iteration 513, Loss: 1230400402.757967\n",
      "Iteration 514, Loss: 1303967086.9864757\n",
      "Iteration 515, Loss: 1261272683.6022723\n",
      "Iteration 516, Loss: 1407631472.460943\n",
      "Iteration 517, Loss: 1385265956.2272418\n",
      "Iteration 518, Loss: 1301558193.5070436\n",
      "Iteration 519, Loss: 1300507647.127875\n",
      "Iteration 520, Loss: 1205157534.555491\n",
      "Iteration 521, Loss: 1548795458.6827085\n",
      "Iteration 522, Loss: 1245004785.5657883\n",
      "Iteration 523, Loss: 1265473762.8055336\n",
      "Iteration 524, Loss: 1158565235.0543926\n",
      "Iteration 525, Loss: 1750187465.634387\n",
      "Iteration 526, Loss: 1756641325.194601\n",
      "Iteration 527, Loss: 1753815888.8930054\n",
      "Iteration 528, Loss: 1704312480.2978928\n",
      "Iteration 529, Loss: 1665099576.4206638\n",
      "Iteration 530, Loss: 1661281923.7573996\n",
      "Iteration 531, Loss: 1164666615.2905567\n",
      "Iteration 532, Loss: 1166952941.986701\n",
      "Iteration 533, Loss: 1179086735.498045\n",
      "Iteration 534, Loss: 1250958202.765007\n",
      "Iteration 535, Loss: 1167198129.1173584\n",
      "Iteration 536, Loss: 1160237180.6678557\n",
      "Iteration 537, Loss: 1779716063.278327\n",
      "Iteration 538, Loss: 1765831776.567889\n",
      "Iteration 539, Loss: 1845206470.1648118\n",
      "Iteration 540, Loss: 1610015492.9582427\n",
      "Iteration 541, Loss: 1470219574.696234\n",
      "Iteration 542, Loss: 1095597859.3947976\n",
      "Iteration 543, Loss: 1078829533.3547914\n",
      "Iteration 544, Loss: 1078284895.8808706\n",
      "Iteration 545, Loss: 1067533901.8419805\n",
      "Iteration 546, Loss: 1312031270.492539\n",
      "Iteration 547, Loss: 1307369416.8372638\n",
      "Iteration 548, Loss: 1298899065.3653016\n",
      "Iteration 549, Loss: 1332827419.6739902\n",
      "Iteration 550, Loss: 1096492069.2115061\n",
      "Iteration 551, Loss: 1338303066.7410548\n",
      "Iteration 552, Loss: 1113718200.9735696\n",
      "Iteration 553, Loss: 1243905294.3690417\n",
      "Iteration 554, Loss: 1229035085.151254\n",
      "Iteration 555, Loss: 1130807361.2472084\n",
      "Iteration 556, Loss: 1117287017.0100207\n",
      "Iteration 557, Loss: 1237843300.4967449\n",
      "Iteration 558, Loss: 1217263371.4194896\n",
      "Iteration 559, Loss: 1278190590.2496479\n",
      "Iteration 560, Loss: 1268012606.256256\n",
      "Iteration 561, Loss: 1099156851.9479077\n",
      "Iteration 562, Loss: 1088373894.564662\n",
      "Iteration 563, Loss: 1021572871.9772927\n",
      "Iteration 564, Loss: 1439585590.9700394\n",
      "Iteration 565, Loss: 1441558974.2111828\n",
      "Iteration 566, Loss: 1100160770.3463569\n",
      "Iteration 567, Loss: 1093569294.6182234\n",
      "Iteration 568, Loss: 1176004500.1014755\n",
      "Iteration 569, Loss: 1109043946.00017\n",
      "Iteration 570, Loss: 1154445506.7897377\n",
      "Iteration 571, Loss: 1268989039.3462636\n",
      "Iteration 572, Loss: 1304032335.6734972\n",
      "Iteration 573, Loss: 1331842754.3801544\n",
      "Iteration 574, Loss: 1166709612.8991058\n",
      "Iteration 575, Loss: 1159283766.726871\n",
      "Iteration 576, Loss: 1235129164.172437\n",
      "Iteration 577, Loss: 1266119518.510466\n",
      "Iteration 578, Loss: 1024407182.1655699\n",
      "Iteration 579, Loss: 1014895072.9160166\n",
      "Iteration 580, Loss: 1012005341.7304562\n",
      "Iteration 581, Loss: 1002030771.6174052\n",
      "Iteration 582, Loss: 1027402792.3942072\n",
      "Iteration 583, Loss: 1216433806.0545342\n",
      "Iteration 584, Loss: 1253399222.782198\n",
      "Iteration 585, Loss: 1166370121.6044257\n",
      "Iteration 586, Loss: 1193605158.5776138\n",
      "Iteration 587, Loss: 1511846932.3886693\n",
      "Iteration 588, Loss: 1459456279.070154\n",
      "Iteration 589, Loss: 1219395834.1976519\n",
      "Iteration 590, Loss: 1198132707.7593346\n",
      "Iteration 591, Loss: 1040705397.9834702\n",
      "Iteration 592, Loss: 1347231894.7944489\n",
      "Iteration 593, Loss: 1088993492.8912868\n",
      "Iteration 594, Loss: 1358138313.4179738\n",
      "Iteration 595, Loss: 1084683467.8467798\n",
      "Iteration 596, Loss: 2054828975.375111\n",
      "Iteration 597, Loss: 1064011194.133286\n",
      "Iteration 598, Loss: 1057023273.3521832\n",
      "Iteration 599, Loss: 1321971320.5757558\n",
      "Iteration 600, Loss: 1105976901.2955186\n",
      "Iteration 601, Loss: 1140411996.7396107\n",
      "Iteration 602, Loss: 1182131544.7663305\n",
      "Iteration 603, Loss: 1297089556.3086002\n",
      "Iteration 604, Loss: 1172262084.8381512\n",
      "Iteration 605, Loss: 1205149761.269362\n",
      "Iteration 606, Loss: 1191385249.500991\n",
      "Iteration 607, Loss: 1302582645.6554453\n",
      "Iteration 608, Loss: 1250735725.9414985\n",
      "Iteration 609, Loss: 1218310646.916085\n",
      "Iteration 610, Loss: 1295898940.3607705\n",
      "Iteration 611, Loss: 1151100072.7455032\n",
      "Iteration 612, Loss: 1208496194.3743422\n",
      "Iteration 613, Loss: 1185537585.9278877\n",
      "Iteration 614, Loss: 1179603686.337069\n",
      "Iteration 615, Loss: 1292566404.060152\n",
      "Iteration 616, Loss: 1334928027.4326844\n",
      "Iteration 617, Loss: 1216577893.136743\n",
      "Iteration 618, Loss: 1184700429.093066\n",
      "Iteration 619, Loss: 1190889846.6271737\n",
      "Iteration 620, Loss: 1336220404.6273832\n",
      "Iteration 621, Loss: 1228993074.7239642\n",
      "Iteration 622, Loss: 1134807262.8534064\n",
      "Iteration 623, Loss: 1128293716.0097566\n",
      "Iteration 624, Loss: 1245275089.283856\n",
      "Iteration 625, Loss: 1309018352.6065047\n",
      "Iteration 626, Loss: 1290709254.7572906\n",
      "Iteration 627, Loss: 1270188115.6669257\n",
      "Iteration 628, Loss: 1142810539.5664792\n",
      "Iteration 629, Loss: 1165879021.8573236\n",
      "Iteration 630, Loss: 1140514774.0192893\n",
      "Iteration 631, Loss: 1270118601.830487\n",
      "Iteration 632, Loss: 1182006142.8312895\n",
      "Iteration 633, Loss: 1098069115.2628376\n",
      "Iteration 634, Loss: 1188198021.595206\n",
      "Iteration 635, Loss: 1136291156.077709\n",
      "Iteration 636, Loss: 1130138585.6198294\n",
      "Iteration 637, Loss: 1157044977.695952\n",
      "Iteration 638, Loss: 1102256423.006347\n",
      "Iteration 639, Loss: 1131259984.315936\n",
      "Iteration 640, Loss: 1097845596.249494\n",
      "Iteration 641, Loss: 1089679463.5690813\n",
      "Iteration 642, Loss: 1079497359.193479\n",
      "Iteration 643, Loss: 1057453436.6477286\n",
      "Iteration 644, Loss: 1018877342.0188739\n",
      "Iteration 645, Loss: 1026411034.3044617\n",
      "Iteration 646, Loss: 1096953134.1862235\n",
      "Iteration 647, Loss: 1157350133.1119843\n",
      "Iteration 648, Loss: 1054220702.8377055\n",
      "Iteration 649, Loss: 1121477630.2246373\n",
      "Iteration 650, Loss: 974363620.4263549\n",
      "Iteration 651, Loss: 1049483797.0144875\n",
      "Iteration 652, Loss: 1008783544.8551894\n",
      "Iteration 653, Loss: 1033249514.2532014\n",
      "Iteration 654, Loss: 1002362163.073147\n",
      "Iteration 655, Loss: 1030564167.2624326\n",
      "Iteration 656, Loss: 1008182434.2412379\n",
      "Iteration 657, Loss: 1069511855.5563756\n",
      "Iteration 658, Loss: 1274939699.181043\n",
      "Iteration 659, Loss: 1126741125.9347124\n",
      "Iteration 660, Loss: 1158502020.7051892\n",
      "Iteration 661, Loss: 1081098257.2540174\n",
      "Iteration 662, Loss: 1896992143.7433085\n",
      "Iteration 663, Loss: 1431450731.4854836\n",
      "Iteration 664, Loss: 1278241828.0859263\n",
      "Iteration 665, Loss: 1107302134.9140005\n",
      "Iteration 666, Loss: 1198815881.3478553\n",
      "Iteration 667, Loss: 1225714008.879836\n",
      "Iteration 668, Loss: 1187401984.695242\n",
      "Iteration 669, Loss: 1245555133.5263922\n",
      "Iteration 670, Loss: 1270077083.7569106\n",
      "Iteration 671, Loss: 1259383316.8709674\n",
      "Iteration 672, Loss: 1063978208.5796468\n",
      "Iteration 673, Loss: 1037081081.3089896\n",
      "Iteration 674, Loss: 1165166176.0489829\n",
      "Iteration 675, Loss: 1231140611.271461\n",
      "Iteration 676, Loss: 1014707355.6014979\n",
      "Iteration 677, Loss: 1577046897.3837848\n",
      "Iteration 678, Loss: 1500797826.965988\n",
      "Iteration 679, Loss: 1103208298.122954\n",
      "Iteration 680, Loss: 1340520843.4244988\n",
      "Iteration 681, Loss: 1326995976.0409458\n",
      "Iteration 682, Loss: 1363190287.8608942\n",
      "Iteration 683, Loss: 1098275682.1814961\n",
      "Iteration 684, Loss: 1185926073.73781\n",
      "Iteration 685, Loss: 1201815124.7188458\n",
      "Iteration 686, Loss: 1251666319.461301\n",
      "Iteration 687, Loss: 1401795444.9140742\n",
      "Iteration 688, Loss: 1040074077.6068699\n",
      "Iteration 689, Loss: 1023917953.2300289\n",
      "Iteration 690, Loss: 1018812337.9817096\n",
      "Iteration 691, Loss: 1040874858.3846567\n",
      "Iteration 692, Loss: 1078377820.6450222\n",
      "Iteration 693, Loss: 1303898863.8593147\n",
      "Iteration 694, Loss: 1335965731.34538\n",
      "Iteration 695, Loss: 1007630608.5027043\n",
      "Iteration 696, Loss: 1135162741.3394146\n",
      "Iteration 697, Loss: 1106547745.4703753\n",
      "Iteration 698, Loss: 1191598354.610649\n",
      "Iteration 699, Loss: 1129612320.8936117\n",
      "Iteration 700, Loss: 1105395385.91074\n",
      "Iteration 701, Loss: 1233183056.2077768\n",
      "Iteration 702, Loss: 1050812451.3345615\n",
      "Iteration 703, Loss: 1183900214.327049\n",
      "Iteration 704, Loss: 1252858639.0593152\n",
      "Iteration 705, Loss: 1281058805.8876684\n",
      "Iteration 706, Loss: 1067726222.1895349\n",
      "Iteration 707, Loss: 1176929360.1474402\n",
      "Iteration 708, Loss: 1116572353.5981238\n",
      "Iteration 709, Loss: 1263262286.058998\n",
      "Iteration 710, Loss: 1075440943.7107685\n",
      "Iteration 711, Loss: 1184768431.5247521\n",
      "Iteration 712, Loss: 1127200405.8868828\n",
      "Iteration 713, Loss: 1228337040.2502277\n",
      "Iteration 714, Loss: 1208017247.4485462\n",
      "Iteration 715, Loss: 1279386607.5495894\n",
      "Iteration 716, Loss: 1198955076.048282\n",
      "Iteration 717, Loss: 1127504710.6123962\n",
      "Iteration 718, Loss: 1097878006.7641587\n",
      "Iteration 719, Loss: 1829425606.1486766\n",
      "Iteration 720, Loss: 1365868683.1674206\n",
      "Iteration 721, Loss: 2133011175.2569463\n",
      "Iteration 722, Loss: 1963416226.1557963\n",
      "Iteration 723, Loss: 1561839977.1422741\n",
      "Iteration 724, Loss: 1499682983.8835218\n",
      "Iteration 725, Loss: 1260532764.4606042\n",
      "Iteration 726, Loss: 1257562848.5318332\n",
      "Iteration 727, Loss: 1232550149.324872\n",
      "Iteration 728, Loss: 1154493862.041923\n",
      "Iteration 729, Loss: 1247880385.9552028\n",
      "Iteration 730, Loss: 1226294982.1526423\n",
      "Iteration 731, Loss: 1284000246.7530997\n",
      "Iteration 732, Loss: 1028377488.3218538\n",
      "Iteration 733, Loss: 1264586551.162347\n",
      "Iteration 734, Loss: 1134968748.7209454\n",
      "Iteration 735, Loss: 1155006760.2947013\n",
      "Iteration 736, Loss: 1132229987.7166846\n",
      "Iteration 737, Loss: 1108515282.8860595\n",
      "Iteration 738, Loss: 1085160322.7394783\n",
      "Iteration 739, Loss: 980157999.5553647\n",
      "Iteration 740, Loss: 980359008.9670744\n",
      "Iteration 741, Loss: 1306746657.4972475\n",
      "Iteration 742, Loss: 1148483874.9223614\n",
      "Iteration 743, Loss: 1622038684.8675659\n",
      "Iteration 744, Loss: 1024361446.9831492\n",
      "Iteration 745, Loss: 1022924862.8595113\n",
      "Iteration 746, Loss: 1600614910.1506052\n",
      "Iteration 747, Loss: 1165372080.749754\n",
      "Iteration 748, Loss: 1157718630.0823112\n",
      "Iteration 749, Loss: 1249079461.9692662\n",
      "Iteration 750, Loss: 1303411238.9574747\n",
      "Iteration 751, Loss: 1108908096.591728\n",
      "Iteration 752, Loss: 1102437879.504368\n",
      "Iteration 753, Loss: 1265985986.6751246\n",
      "Iteration 754, Loss: 1137801079.2521455\n",
      "Iteration 755, Loss: 1225347112.0260792\n",
      "Iteration 756, Loss: 1194413256.5974972\n",
      "Iteration 757, Loss: 1163433267.6692698\n",
      "Iteration 758, Loss: 1076118475.843782\n",
      "Iteration 759, Loss: 1321474881.2001145\n",
      "Iteration 760, Loss: 1376454090.8186278\n",
      "Iteration 761, Loss: 1096017829.233093\n",
      "Iteration 762, Loss: 1104945010.8422668\n",
      "Iteration 763, Loss: 1222299412.2156198\n",
      "Iteration 764, Loss: 1205903415.272426\n",
      "Iteration 765, Loss: 1248693256.2917912\n",
      "Iteration 766, Loss: 1087180059.4686787\n",
      "Iteration 767, Loss: 1784961727.244191\n",
      "Iteration 768, Loss: 1504795643.8086019\n",
      "Iteration 769, Loss: 1187516484.9463048\n",
      "Iteration 770, Loss: 1085888417.328948\n",
      "Iteration 771, Loss: 1087584201.524107\n",
      "Iteration 772, Loss: 1355394772.8446963\n",
      "Iteration 773, Loss: 1231623887.4276576\n",
      "Iteration 774, Loss: 1333961428.9516401\n",
      "Iteration 775, Loss: 1380778951.2637258\n",
      "Iteration 776, Loss: 1094839842.3501287\n",
      "Iteration 777, Loss: 1083389223.335248\n",
      "Iteration 778, Loss: 1134915252.0878758\n",
      "Iteration 779, Loss: 1095310523.7357452\n",
      "Iteration 780, Loss: 1109898656.8565173\n",
      "Iteration 781, Loss: 1095049064.3908129\n",
      "Iteration 782, Loss: 1070516735.2170426\n",
      "Iteration 783, Loss: 1022676070.0121286\n",
      "Iteration 784, Loss: 1020887648.7402654\n",
      "Iteration 785, Loss: 1256946654.7313013\n",
      "Iteration 786, Loss: 1199529503.9558058\n",
      "Iteration 787, Loss: 1138401673.327379\n",
      "Iteration 788, Loss: 1202406166.4088106\n",
      "Iteration 789, Loss: 1169281829.9709508\n",
      "Iteration 790, Loss: 1198207391.1196795\n",
      "Iteration 791, Loss: 1242910127.1681821\n",
      "Iteration 792, Loss: 1159031423.0451107\n",
      "Iteration 793, Loss: 1202613814.3068933\n",
      "Iteration 794, Loss: 1162321530.101378\n",
      "Iteration 795, Loss: 1080515512.2462897\n",
      "Iteration 796, Loss: 1378408292.8544316\n",
      "Iteration 797, Loss: 1172977167.9105337\n",
      "Iteration 798, Loss: 1102642650.7879267\n",
      "Iteration 799, Loss: 1044032887.1642306\n",
      "Iteration 800, Loss: 1253878619.473219\n",
      "Iteration 801, Loss: 1030256165.3563032\n",
      "Iteration 802, Loss: 1715785481.6217687\n",
      "Iteration 803, Loss: 1536728059.5047092\n",
      "Iteration 804, Loss: 1173080100.1376817\n",
      "Iteration 805, Loss: 1341739095.0849679\n",
      "Iteration 806, Loss: 1098446508.3515687\n",
      "Iteration 807, Loss: 1435609485.4777346\n",
      "Iteration 808, Loss: 1410823202.3372025\n",
      "Iteration 809, Loss: 1199634099.2353826\n",
      "Iteration 810, Loss: 1179172747.031659\n",
      "Iteration 811, Loss: 1168664911.9563913\n",
      "Iteration 812, Loss: 1113859522.8197753\n",
      "Iteration 813, Loss: 1163562566.686346\n",
      "Iteration 814, Loss: 1155110374.2040114\n",
      "Iteration 815, Loss: 1305929234.2300828\n",
      "Iteration 816, Loss: 1136217903.9031508\n",
      "Iteration 817, Loss: 1205164301.6479666\n",
      "Iteration 818, Loss: 1194794061.911937\n",
      "Iteration 819, Loss: 1207325550.7750087\n",
      "Iteration 820, Loss: 1234394715.0507035\n",
      "Iteration 821, Loss: 1090532510.1929133\n",
      "Iteration 822, Loss: 1277208705.6253824\n",
      "Iteration 823, Loss: 1140043933.0776622\n",
      "Iteration 824, Loss: 1296815788.3889117\n",
      "Iteration 825, Loss: 1321802200.8626015\n",
      "Iteration 826, Loss: 1338008773.5342658\n",
      "Iteration 827, Loss: 1269775810.838011\n",
      "Iteration 828, Loss: 1117102313.1611025\n",
      "Iteration 829, Loss: 1176224637.0592582\n",
      "Iteration 830, Loss: 1171358192.9499326\n",
      "Iteration 831, Loss: 1161647435.3591607\n",
      "Iteration 832, Loss: 1150310418.516093\n",
      "Iteration 833, Loss: 1203030594.8847303\n",
      "Iteration 834, Loss: 1279198889.826757\n",
      "Iteration 835, Loss: 1319658720.9846337\n",
      "Iteration 836, Loss: 1109356101.5509481\n",
      "Iteration 837, Loss: 1312186068.0374496\n",
      "Iteration 838, Loss: 1300328676.9392948\n",
      "Iteration 839, Loss: 1072756276.8524278\n",
      "Iteration 840, Loss: 1301390779.5356903\n",
      "Iteration 841, Loss: 1311542352.430441\n",
      "Iteration 842, Loss: 1276898185.1560733\n",
      "Iteration 843, Loss: 1287796253.7472217\n",
      "Iteration 844, Loss: 1098226314.219937\n",
      "Iteration 845, Loss: 1083650048.3251183\n",
      "Iteration 846, Loss: 1263446990.5541182\n",
      "Iteration 847, Loss: 1031086497.9481307\n",
      "Iteration 848, Loss: 1119040260.6247241\n",
      "Iteration 849, Loss: 1111341915.1210506\n",
      "Iteration 850, Loss: 1024827997.5520078\n",
      "Iteration 851, Loss: 1053669719.4789869\n",
      "Iteration 852, Loss: 1039613646.1638695\n",
      "Iteration 853, Loss: 1689204781.6423156\n",
      "Iteration 854, Loss: 1587199413.0375245\n",
      "Iteration 855, Loss: 1480405537.6164932\n",
      "Iteration 856, Loss: 1291082915.905914\n",
      "Iteration 857, Loss: 1029961469.6986798\n",
      "Iteration 858, Loss: 1174505602.524524\n",
      "Iteration 859, Loss: 1124073190.6200929\n",
      "Iteration 860, Loss: 1219635733.800423\n",
      "Iteration 861, Loss: 1283510470.805448\n",
      "Iteration 862, Loss: 1296476229.4445405\n",
      "Iteration 863, Loss: 1195824172.3603647\n",
      "Iteration 864, Loss: 1261116401.3703303\n",
      "Iteration 865, Loss: 1236868005.0414202\n",
      "Iteration 866, Loss: 1219201296.1422822\n",
      "Iteration 867, Loss: 1239107166.6188662\n",
      "Iteration 868, Loss: 1156867858.8165655\n",
      "Iteration 869, Loss: 1142693514.2350552\n",
      "Iteration 870, Loss: 1159393895.2760377\n",
      "Iteration 871, Loss: 1186533057.7898023\n",
      "Iteration 872, Loss: 1129710177.2780144\n",
      "Iteration 873, Loss: 1227649126.699728\n",
      "Iteration 874, Loss: 1120074460.5472531\n",
      "Iteration 875, Loss: 1183162629.8474665\n",
      "Iteration 876, Loss: 1153748444.769808\n",
      "Iteration 877, Loss: 1187573426.490043\n",
      "Iteration 878, Loss: 1149089884.8403468\n",
      "Iteration 879, Loss: 1146244765.8685422\n",
      "Iteration 880, Loss: 1140134012.075676\n",
      "Iteration 881, Loss: 1123098882.4593954\n",
      "Iteration 882, Loss: 1173850624.4018395\n",
      "Iteration 883, Loss: 1205065170.1881094\n",
      "Iteration 884, Loss: 1076035328.2375488\n",
      "Iteration 885, Loss: 1104878618.5361135\n",
      "Iteration 886, Loss: 1379011627.9590652\n",
      "Iteration 887, Loss: 1377287813.7597094\n",
      "Iteration 888, Loss: 1336517137.7603512\n",
      "Iteration 889, Loss: 1258927072.7320337\n",
      "Iteration 890, Loss: 1053487774.9283255\n",
      "Iteration 891, Loss: 1054968522.4112145\n",
      "Iteration 892, Loss: 1192155368.083249\n",
      "Iteration 893, Loss: 1294756083.1718974\n",
      "Iteration 894, Loss: 1297282887.9858224\n",
      "Iteration 895, Loss: 1101473202.2712045\n",
      "Iteration 896, Loss: 1091870291.0296028\n",
      "Iteration 897, Loss: 1083997764.696571\n",
      "Iteration 898, Loss: 1099965514.6363864\n",
      "Iteration 899, Loss: 1078962317.9959068\n",
      "Iteration 900, Loss: 1668410125.149158\n",
      "Iteration 901, Loss: 1532907623.496227\n",
      "Iteration 902, Loss: 1439722286.8425362\n",
      "Iteration 903, Loss: 1069291004.809804\n",
      "Iteration 904, Loss: 1154098028.4984343\n",
      "Iteration 905, Loss: 1213861081.2860956\n",
      "Iteration 906, Loss: 1176582439.089775\n",
      "Iteration 907, Loss: 1089431986.399649\n",
      "Iteration 908, Loss: 1077497347.9674635\n",
      "Iteration 909, Loss: 1046240514.8704602\n",
      "Iteration 910, Loss: 1066457554.830462\n",
      "Iteration 911, Loss: 1018817203.3763118\n",
      "Iteration 912, Loss: 1103590321.2124076\n",
      "Iteration 913, Loss: 1233849073.6189106\n",
      "Iteration 914, Loss: 1198056958.9424388\n",
      "Iteration 915, Loss: 1069336770.1295277\n",
      "Iteration 916, Loss: 1028224187.2183998\n",
      "Iteration 917, Loss: 1099104644.072564\n",
      "Iteration 918, Loss: 1026455513.2410482\n",
      "Iteration 919, Loss: 1133015151.3811195\n",
      "Iteration 920, Loss: 1150301045.5324845\n",
      "Iteration 921, Loss: 1005994220.8857769\n",
      "Iteration 922, Loss: 1087854516.2905705\n",
      "Iteration 923, Loss: 1515970034.3453662\n",
      "Iteration 924, Loss: 1300575401.8031151\n",
      "Iteration 925, Loss: 1045976410.7241236\n",
      "Iteration 926, Loss: 1966083632.673199\n",
      "Iteration 927, Loss: 1768905190.1346104\n",
      "Iteration 928, Loss: 1537071025.3297753\n",
      "Iteration 929, Loss: 1328873246.677978\n",
      "Iteration 930, Loss: 1219740086.9668577\n",
      "Iteration 931, Loss: 1186410142.775184\n",
      "Iteration 932, Loss: 1226301437.682067\n",
      "Iteration 933, Loss: 1040552887.4395195\n",
      "Iteration 934, Loss: 1114598631.8288064\n",
      "Iteration 935, Loss: 1144386452.22356\n",
      "Iteration 936, Loss: 1193518383.6137483\n",
      "Iteration 937, Loss: 1157001700.676295\n",
      "Iteration 938, Loss: 1110488757.488088\n",
      "Iteration 939, Loss: 1667622385.037597\n",
      "Iteration 940, Loss: 1282426205.437637\n",
      "Iteration 941, Loss: 1070465020.2105533\n",
      "Iteration 942, Loss: 1029850899.8730456\n",
      "Iteration 943, Loss: 1220790578.9949675\n",
      "Iteration 944, Loss: 1060345811.1833583\n",
      "Iteration 945, Loss: 1948747309.5111017\n",
      "Iteration 946, Loss: 1022355655.5997791\n",
      "Iteration 947, Loss: 4559918913.501246\n",
      "Iteration 948, Loss: 1067725159.0298663\n",
      "Iteration 949, Loss: 1300187042.6522136\n",
      "Iteration 950, Loss: 1349100489.2645915\n",
      "Iteration 951, Loss: 1373944256.7990005\n",
      "Iteration 952, Loss: 1336479413.0208683\n",
      "Iteration 953, Loss: 1061371794.156224\n",
      "Iteration 954, Loss: 1415311037.5089047\n",
      "Iteration 955, Loss: 1374475686.7539628\n",
      "Iteration 956, Loss: 1343574963.4549901\n",
      "Iteration 957, Loss: 1190130870.398537\n",
      "Iteration 958, Loss: 1152696156.7434814\n",
      "Iteration 959, Loss: 1213082068.0448976\n",
      "Iteration 960, Loss: 1198963168.98163\n",
      "Iteration 961, Loss: 1163602189.8009262\n",
      "Iteration 962, Loss: 1180784054.5615337\n",
      "Iteration 963, Loss: 1235663983.9540732\n",
      "Iteration 964, Loss: 1193043438.8119903\n",
      "Iteration 965, Loss: 1188296274.601673\n",
      "Iteration 966, Loss: 1159615541.4288025\n",
      "Iteration 967, Loss: 1196145163.9255455\n",
      "Iteration 968, Loss: 1175780097.0792572\n",
      "Iteration 969, Loss: 1207704677.3387232\n",
      "Iteration 970, Loss: 1233562556.8199372\n",
      "Iteration 971, Loss: 1296785342.1056705\n",
      "Iteration 972, Loss: 1327669504.6486182\n",
      "Iteration 973, Loss: 1323248665.2339168\n",
      "Iteration 974, Loss: 1267461301.110083\n",
      "Iteration 975, Loss: 1228972108.7667086\n",
      "Iteration 976, Loss: 1563048197.038639\n",
      "Iteration 977, Loss: 1502208155.9294941\n",
      "Iteration 978, Loss: 1263709327.487758\n",
      "Iteration 979, Loss: 1317323815.546212\n",
      "Iteration 980, Loss: 1358521168.3523903\n",
      "Iteration 981, Loss: 1323373049.9946344\n",
      "Iteration 982, Loss: 1340358316.8713243\n",
      "Iteration 983, Loss: 1052427684.7745273\n",
      "Iteration 984, Loss: 1087918834.9072585\n",
      "Iteration 985, Loss: 1073957076.031034\n",
      "Iteration 986, Loss: 1067878436.567016\n",
      "Iteration 987, Loss: 1341412509.239268\n",
      "Iteration 988, Loss: 1165388825.9618692\n",
      "Iteration 989, Loss: 1078283797.6718054\n",
      "Iteration 990, Loss: 1199137257.0084567\n",
      "Iteration 991, Loss: 1180411426.2739387\n",
      "Iteration 992, Loss: 1305507436.3716145\n",
      "Iteration 993, Loss: 1313373998.2924545\n",
      "Iteration 994, Loss: 1253007540.8896484\n",
      "Iteration 995, Loss: 1229208908.2780378\n",
      "Iteration 996, Loss: 1215264558.484282\n",
      "Iteration 997, Loss: 1537616501.189911\n",
      "Iteration 998, Loss: 1462864880.2630959\n",
      "Iteration 999, Loss: 1064515030.2754273\n",
      "Iteration 1000, Loss: 1105758381.374944\n",
      "Iteration 1001, Loss: 1084606790.306185\n",
      "Iteration 1002, Loss: 1336521380.9309053\n",
      "Iteration 1003, Loss: 1307075144.4797683\n",
      "Iteration 1004, Loss: 1031838328.1097068\n",
      "Iteration 1005, Loss: 1130068139.5408146\n",
      "Iteration 1006, Loss: 1140447910.8038442\n",
      "Iteration 1007, Loss: 1121218601.8968246\n",
      "Iteration 1008, Loss: 1105722635.467065\n",
      "Iteration 1009, Loss: 1134099448.808043\n",
      "Iteration 1010, Loss: 1106939540.4341488\n",
      "Iteration 1011, Loss: 1130018911.483141\n",
      "Iteration 1012, Loss: 1111112615.601913\n",
      "Iteration 1013, Loss: 1264548626.1527288\n",
      "Iteration 1014, Loss: 1176960499.770612\n",
      "Iteration 1015, Loss: 1165493030.4423242\n",
      "Iteration 1016, Loss: 1128279439.813786\n",
      "Iteration 1017, Loss: 1149776991.2771165\n",
      "Iteration 1018, Loss: 1241661807.261926\n",
      "Iteration 1019, Loss: 1285364667.0984674\n",
      "Iteration 1020, Loss: 1067834050.4417703\n",
      "Iteration 1021, Loss: 1062004626.8695639\n",
      "Iteration 1022, Loss: 2146266481.4827645\n",
      "Iteration 1023, Loss: 1063443739.1962649\n",
      "Iteration 1024, Loss: 1410159060.6055186\n",
      "Iteration 1025, Loss: 1239140041.4448552\n",
      "Iteration 1026, Loss: 1268035347.4782307\n",
      "Iteration 1027, Loss: 1250038140.1667325\n",
      "Iteration 1028, Loss: 1328049930.9498613\n",
      "Iteration 1029, Loss: 1306474785.8334098\n",
      "Iteration 1030, Loss: 1070401283.6160045\n",
      "Iteration 1031, Loss: 3490587479.266222\n",
      "Iteration 1032, Loss: 2213885679.4870887\n",
      "Iteration 1033, Loss: 2159921272.0894833\n",
      "Iteration 1034, Loss: 1623935380.660638\n",
      "Iteration 1035, Loss: 1273196301.1309633\n",
      "Iteration 1036, Loss: 1175115914.411716\n",
      "Iteration 1037, Loss: 1056643846.2050394\n",
      "Iteration 1038, Loss: 1056063712.5939\n",
      "Iteration 1039, Loss: 2549522300.7716403\n",
      "Iteration 1040, Loss: 2277979449.845764\n",
      "Iteration 1041, Loss: 1703384146.5178933\n",
      "Iteration 1042, Loss: 1155527257.6506834\n",
      "Iteration 1043, Loss: 1845193594.18225\n",
      "Iteration 1044, Loss: 1492312032.6513927\n",
      "Iteration 1045, Loss: 1181912032.9883811\n",
      "Iteration 1046, Loss: 1217724891.7137847\n",
      "Iteration 1047, Loss: 1301762914.729343\n",
      "Iteration 1048, Loss: 1185874072.529712\n",
      "Iteration 1049, Loss: 1322231893.23935\n",
      "Iteration 1050, Loss: 1149597387.095478\n",
      "Iteration 1051, Loss: 1264031162.0907414\n",
      "Iteration 1052, Loss: 1249660045.02108\n",
      "Iteration 1053, Loss: 1660741537.426165\n",
      "Iteration 1054, Loss: 1617800594.6053464\n",
      "Iteration 1055, Loss: 1621792788.8820467\n",
      "Iteration 1056, Loss: 1174068782.5851614\n",
      "Iteration 1057, Loss: 1383795936.300299\n",
      "Iteration 1058, Loss: 1414119742.9692266\n",
      "Iteration 1059, Loss: 1203360480.340673\n",
      "Iteration 1060, Loss: 1156754460.3889835\n",
      "Iteration 1061, Loss: 1317966272.613843\n",
      "Iteration 1062, Loss: 1353377238.164801\n",
      "Iteration 1063, Loss: 1371832181.4577038\n",
      "Iteration 1064, Loss: 1152512077.5313401\n",
      "Iteration 1065, Loss: 1473534148.0235527\n",
      "Iteration 1066, Loss: 1343597166.433196\n",
      "Iteration 1067, Loss: 1173658434.7598264\n",
      "Iteration 1068, Loss: 1468312343.7874537\n",
      "Iteration 1069, Loss: 1198658485.1246428\n",
      "Iteration 1070, Loss: 1343953827.813646\n",
      "Iteration 1071, Loss: 1287110118.6814296\n",
      "Iteration 1072, Loss: 1443861666.75039\n",
      "Iteration 1073, Loss: 1456414514.9195898\n",
      "Iteration 1074, Loss: 1550700353.5558887\n",
      "Iteration 1075, Loss: 1552496233.203373\n",
      "Iteration 1076, Loss: 1319174764.1984785\n",
      "Iteration 1077, Loss: 1411846432.3458025\n",
      "Iteration 1078, Loss: 1190734007.852297\n",
      "Iteration 1079, Loss: 1141701406.8088799\n",
      "Iteration 1080, Loss: 1175104527.2224383\n",
      "Iteration 1081, Loss: 1172260548.2958074\n",
      "Iteration 1082, Loss: 1221896044.312261\n",
      "Iteration 1083, Loss: 1214532953.9973035\n",
      "Iteration 1084, Loss: 1254684073.360845\n",
      "Iteration 1085, Loss: 1255899389.2701254\n",
      "Iteration 1086, Loss: 1376986893.9810655\n",
      "Iteration 1087, Loss: 1140762127.903655\n",
      "Iteration 1088, Loss: 1134855702.1192474\n",
      "Iteration 1089, Loss: 1260627689.9287827\n",
      "Iteration 1090, Loss: 1326744034.0392046\n",
      "Iteration 1091, Loss: 1380336901.4932272\n",
      "Iteration 1092, Loss: 1259314981.755938\n",
      "Iteration 1093, Loss: 1366697567.2706082\n",
      "Iteration 1094, Loss: 1256757058.7211552\n",
      "Iteration 1095, Loss: 1217792667.1152818\n",
      "Iteration 1096, Loss: 1224435256.1062512\n",
      "Iteration 1097, Loss: 1212948947.5721505\n",
      "Iteration 1098, Loss: 1311125551.230974\n",
      "Iteration 1099, Loss: 1279782630.2566235\n",
      "Iteration 1100, Loss: 1177928401.1307466\n",
      "Iteration 1101, Loss: 1302386834.2278767\n",
      "Iteration 1102, Loss: 1392380272.525267\n",
      "Iteration 1103, Loss: 1452599236.1915138\n",
      "Iteration 1104, Loss: 1461622052.8276403\n",
      "Iteration 1105, Loss: 1235611597.7383993\n",
      "Iteration 1106, Loss: 1226922502.164144\n",
      "Iteration 1107, Loss: 1346405000.1614883\n",
      "Iteration 1108, Loss: 1420680623.1587577\n",
      "Iteration 1109, Loss: 1236708208.5230937\n",
      "Iteration 1110, Loss: 1328007177.8097374\n",
      "Iteration 1111, Loss: 1197597908.8880339\n",
      "Iteration 1112, Loss: 1191275022.1824303\n",
      "Iteration 1113, Loss: 1193660283.8503823\n",
      "Iteration 1114, Loss: 1188470633.0866027\n",
      "Iteration 1115, Loss: 1333276223.4683917\n",
      "Iteration 1116, Loss: 1117465428.3993983\n",
      "Iteration 1117, Loss: 1093908838.1304362\n",
      "Iteration 1118, Loss: 1074983094.977302\n",
      "Iteration 1119, Loss: 1074071458.374556\n",
      "Iteration 1120, Loss: 1179573225.342393\n",
      "Iteration 1121, Loss: 1335388575.8584895\n",
      "Iteration 1122, Loss: 1314195872.0504076\n",
      "Iteration 1123, Loss: 1292444331.3518546\n",
      "Iteration 1124, Loss: 1284880414.8082547\n",
      "Iteration 1125, Loss: 1050922385.2030772\n",
      "Iteration 1126, Loss: 1507993068.3124547\n",
      "Iteration 1127, Loss: 1475759686.2676723\n",
      "Iteration 1128, Loss: 1057268197.667784\n",
      "Iteration 1129, Loss: 1227507111.0779352\n",
      "Iteration 1130, Loss: 1215091703.2639642\n",
      "Iteration 1131, Loss: 1212137386.5022237\n",
      "Iteration 1132, Loss: 1244687680.5725222\n",
      "Iteration 1133, Loss: 1165170864.3934956\n",
      "Iteration 1134, Loss: 1188587930.2139904\n",
      "Iteration 1135, Loss: 1182495275.121627\n",
      "Iteration 1136, Loss: 1061248902.8529456\n",
      "Iteration 1137, Loss: 1055419037.7616472\n",
      "Iteration 1138, Loss: 1053320488.9416833\n",
      "Iteration 1139, Loss: 1077707321.1314414\n",
      "Iteration 1140, Loss: 1182781364.0021038\n",
      "Iteration 1141, Loss: 1210043142.7896168\n",
      "Iteration 1142, Loss: 1119732241.2651384\n",
      "Iteration 1143, Loss: 1031741065.8185271\n",
      "Iteration 1144, Loss: 1035776875.7294276\n",
      "Iteration 1145, Loss: 1021799981.1059203\n",
      "Iteration 1146, Loss: 1118156486.2194617\n",
      "Iteration 1147, Loss: 1123698300.2173193\n",
      "Iteration 1148, Loss: 1099660695.9549997\n",
      "Iteration 1149, Loss: 1075846403.599451\n",
      "Iteration 1150, Loss: 1101766781.9233072\n",
      "Iteration 1151, Loss: 1075880566.9880168\n",
      "Iteration 1152, Loss: 1125578802.8346324\n",
      "Iteration 1153, Loss: 1069696715.7626805\n",
      "Iteration 1154, Loss: 1016744687.2698433\n",
      "Iteration 1155, Loss: 1029135978.1130438\n",
      "Iteration 1156, Loss: 1006731141.4858167\n",
      "Iteration 1157, Loss: 1124712361.6848693\n",
      "Iteration 1158, Loss: 1089607453.6579368\n",
      "Iteration 1159, Loss: 1038246352.5997663\n",
      "Iteration 1160, Loss: 1040165239.0356977\n",
      "Iteration 1161, Loss: 1049233554.1887032\n",
      "Iteration 1162, Loss: 1203819033.870401\n",
      "Iteration 1163, Loss: 1403606769.0121536\n",
      "Iteration 1164, Loss: 1323397522.28248\n",
      "Iteration 1165, Loss: 1341193887.9806426\n",
      "Iteration 1166, Loss: 1196497045.58183\n",
      "Iteration 1167, Loss: 1122667991.63732\n",
      "Iteration 1168, Loss: 1092834949.863443\n",
      "Iteration 1169, Loss: 1549731000.934252\n",
      "Iteration 1170, Loss: 1331636464.3384228\n",
      "Iteration 1171, Loss: 1165762259.9899294\n",
      "Iteration 1172, Loss: 1222732507.9858308\n",
      "Iteration 1173, Loss: 1187924976.6896746\n",
      "Iteration 1174, Loss: 1099143626.8406055\n",
      "Iteration 1175, Loss: 1116571032.3969307\n",
      "Iteration 1176, Loss: 1228847778.947036\n",
      "Iteration 1177, Loss: 1195063265.2182286\n",
      "Iteration 1178, Loss: 1144240678.4555395\n",
      "Iteration 1179, Loss: 1142579476.2092705\n",
      "Iteration 1180, Loss: 1145673684.628649\n",
      "Iteration 1181, Loss: 1158690029.603357\n",
      "Iteration 1182, Loss: 1078710586.1082268\n",
      "Iteration 1183, Loss: 1054691575.0308617\n",
      "Iteration 1184, Loss: 1030965317.6539958\n",
      "Iteration 1185, Loss: 956038568.9041406\n",
      "Iteration 1186, Loss: 944983707.2526963\n",
      "Iteration 1187, Loss: 938951414.8518212\n",
      "Iteration 1188, Loss: 2563652239.3932114\n",
      "Iteration 1189, Loss: 1031229156.3651003\n",
      "Iteration 1190, Loss: 995429121.6448102\n",
      "Iteration 1191, Loss: 1089990744.54349\n",
      "Iteration 1192, Loss: 1021854802.506453\n",
      "Iteration 1193, Loss: 986253354.42386\n",
      "Iteration 1194, Loss: 2310987317.672098\n",
      "Iteration 1195, Loss: 980896850.9329097\n",
      "Iteration 1196, Loss: 968482672.8936366\n",
      "Iteration 1197, Loss: 3093491148.5239005\n",
      "Iteration 1198, Loss: 2104419837.5621834\n",
      "Iteration 1199, Loss: 1985110863.7099254\n",
      "Iteration 1200, Loss: 995221347.6470599\n",
      "Iteration 1201, Loss: 1072936093.0559413\n",
      "Iteration 1202, Loss: 1109662858.898624\n",
      "Iteration 1203, Loss: 1236826021.5397754\n",
      "Iteration 1204, Loss: 1201718169.9199097\n",
      "Iteration 1205, Loss: 1178395050.6771579\n",
      "Iteration 1206, Loss: 1053432928.2671286\n",
      "Iteration 1207, Loss: 1040263301.0301278\n",
      "Iteration 1208, Loss: 1124881832.1247108\n",
      "Iteration 1209, Loss: 1181060028.9055753\n",
      "Iteration 1210, Loss: 1161818099.3231094\n",
      "Iteration 1211, Loss: 1165039382.3819816\n",
      "Iteration 1212, Loss: 1184206513.4583628\n",
      "Iteration 1213, Loss: 1201962944.6401842\n",
      "Iteration 1214, Loss: 956551424.3969954\n",
      "Iteration 1215, Loss: 944818744.0319736\n",
      "Iteration 1216, Loss: 942281749.3446382\n",
      "Iteration 1217, Loss: 1370475168.7283072\n",
      "Iteration 1218, Loss: 1280571208.1073835\n",
      "Iteration 1219, Loss: 1193093591.5637083\n",
      "Iteration 1220, Loss: 1175829981.571073\n",
      "Iteration 1221, Loss: 1132452510.381596\n",
      "Iteration 1222, Loss: 1080892444.287603\n",
      "Iteration 1223, Loss: 1095351203.5023162\n",
      "Iteration 1224, Loss: 1045404499.8410774\n",
      "Iteration 1225, Loss: 965400531.8523259\n",
      "Iteration 1226, Loss: 1197771630.0813048\n",
      "Iteration 1227, Loss: 1054865309.708976\n",
      "Iteration 1228, Loss: 1041727234.961633\n",
      "Iteration 1229, Loss: 933878998.3589554\n",
      "Iteration 1230, Loss: 944184713.706059\n",
      "Iteration 1231, Loss: 2300771610.181464\n",
      "Iteration 1232, Loss: 1917827758.8000135\n",
      "Iteration 1233, Loss: 1433977654.1004725\n",
      "Iteration 1234, Loss: 1344341184.6255143\n",
      "Iteration 1235, Loss: 913470326.7794895\n",
      "Iteration 1236, Loss: 1193778857.110565\n",
      "Iteration 1237, Loss: 1077259032.1528203\n",
      "Iteration 1238, Loss: 1046920272.8123503\n",
      "Iteration 1239, Loss: 1622081193.143919\n",
      "Iteration 1240, Loss: 1131318835.7813406\n",
      "Iteration 1241, Loss: 1111588993.3086622\n",
      "Iteration 1242, Loss: 1302144607.8890834\n",
      "Iteration 1243, Loss: 1240197055.4589322\n",
      "Iteration 1244, Loss: 933379640.9784486\n",
      "Iteration 1245, Loss: 2428535447.1434226\n",
      "Iteration 1246, Loss: 1691468547.7687058\n",
      "Iteration 1247, Loss: 1003523019.8085693\n",
      "Iteration 1248, Loss: 975036731.9312727\n",
      "Iteration 1249, Loss: 961771755.1224018\n",
      "Iteration 1250, Loss: 980053402.6224624\n",
      "Iteration 1251, Loss: 1149552576.345748\n",
      "Iteration 1252, Loss: 1187707443.8679452\n",
      "Iteration 1253, Loss: 1175238466.077511\n",
      "Iteration 1254, Loss: 1104587966.319736\n",
      "Iteration 1255, Loss: 1063593156.6680428\n",
      "Iteration 1256, Loss: 1077149649.1570106\n",
      "Iteration 1257, Loss: 932295721.1556476\n",
      "Iteration 1258, Loss: 920830918.7686183\n",
      "Iteration 1259, Loss: 940737858.2707152\n",
      "Iteration 1260, Loss: 1153045102.3385794\n",
      "Iteration 1261, Loss: 994826999.8402034\n",
      "Iteration 1262, Loss: 1054424385.9923067\n",
      "Iteration 1263, Loss: 1027718668.262456\n",
      "Iteration 1264, Loss: 1026478159.8943383\n",
      "Iteration 1265, Loss: 990619163.7607687\n",
      "Iteration 1266, Loss: 1993073280.5274684\n",
      "Iteration 1267, Loss: 1191731630.461539\n",
      "Iteration 1268, Loss: 1023576464.5119672\n",
      "Iteration 1269, Loss: 969549553.8372442\n",
      "Iteration 1270, Loss: 1034966676.0393378\n",
      "Iteration 1271, Loss: 1041500801.4321597\n",
      "Iteration 1272, Loss: 1062955202.5403695\n",
      "Iteration 1273, Loss: 952354511.0633961\n",
      "Iteration 1274, Loss: 925793985.6277412\n",
      "Iteration 1275, Loss: 907531065.0089484\n",
      "Iteration 1276, Loss: 1065199305.2816641\n",
      "Iteration 1277, Loss: 1154648538.8445473\n",
      "Iteration 1278, Loss: 1048361358.597013\n",
      "Iteration 1279, Loss: 998717059.1910034\n",
      "Iteration 1280, Loss: 960453218.9340087\n",
      "Iteration 1281, Loss: 1009266704.2237074\n",
      "Iteration 1282, Loss: 978301317.2511436\n",
      "Iteration 1283, Loss: 963465151.7751961\n",
      "Iteration 1284, Loss: 925392639.018028\n",
      "Iteration 1285, Loss: 874299624.0128202\n",
      "Iteration 1286, Loss: 1045403812.6610714\n",
      "Iteration 1287, Loss: 1124238948.0337124\n",
      "Iteration 1288, Loss: 967238292.4389\n",
      "Iteration 1289, Loss: 966100566.5677094\n",
      "Iteration 1290, Loss: 902631685.4505117\n",
      "Iteration 1291, Loss: 996811390.8702244\n",
      "Iteration 1292, Loss: 1163897824.015027\n",
      "Iteration 1293, Loss: 967779048.5912272\n",
      "Iteration 1294, Loss: 939841175.6909578\n",
      "Iteration 1295, Loss: 906391929.7282666\n",
      "Iteration 1296, Loss: 889899157.5918366\n",
      "Iteration 1297, Loss: 2531077840.9685545\n",
      "Iteration 1298, Loss: 1656036386.4145253\n",
      "Iteration 1299, Loss: 1257259245.8303683\n",
      "Iteration 1300, Loss: 1019494645.672516\n",
      "Iteration 1301, Loss: 1491968360.6148212\n",
      "Iteration 1302, Loss: 918653420.0637114\n",
      "Iteration 1303, Loss: 1427394068.5649326\n",
      "Iteration 1304, Loss: 1316003971.4553535\n",
      "Iteration 1305, Loss: 1322395903.9279532\n",
      "Iteration 1306, Loss: 1219481548.48972\n",
      "Iteration 1307, Loss: 953960349.4122853\n",
      "Iteration 1308, Loss: 931369028.9624255\n",
      "Iteration 1309, Loss: 902936759.06853\n",
      "Iteration 1310, Loss: 908182495.7029063\n",
      "Iteration 1311, Loss: 895024237.2923787\n",
      "Iteration 1312, Loss: 1064572068.2179129\n",
      "Iteration 1313, Loss: 1026152235.1708188\n",
      "Iteration 1314, Loss: 976292096.691995\n",
      "Iteration 1315, Loss: 917736729.5006764\n",
      "Iteration 1316, Loss: 1003490746.3819922\n",
      "Iteration 1317, Loss: 1013998384.6638935\n",
      "Iteration 1318, Loss: 964732686.2049404\n",
      "Iteration 1319, Loss: 1001422529.3032808\n",
      "Iteration 1320, Loss: 1026126072.1233045\n",
      "Iteration 1321, Loss: 1014681946.4658557\n",
      "Iteration 1322, Loss: 977330154.004961\n",
      "Iteration 1323, Loss: 1173352432.5713563\n",
      "Iteration 1324, Loss: 984274913.489117\n",
      "Iteration 1325, Loss: 918615167.324429\n",
      "Iteration 1326, Loss: 917761702.3264482\n",
      "Iteration 1327, Loss: 886139916.8771394\n",
      "Iteration 1328, Loss: 891425264.7531295\n",
      "Iteration 1329, Loss: 865011970.8318659\n",
      "Iteration 1330, Loss: 915110229.8151075\n",
      "Iteration 1331, Loss: 974967412.2622857\n",
      "Iteration 1332, Loss: 864633388.883909\n",
      "Iteration 1333, Loss: 843759980.3514397\n",
      "Iteration 1334, Loss: 836408456.7247455\n",
      "Iteration 1335, Loss: 828188827.7633883\n",
      "Iteration 1336, Loss: 820616446.1295786\n",
      "Iteration 1337, Loss: 1010914951.4687253\n",
      "Iteration 1338, Loss: 955823084.505387\n",
      "Iteration 1339, Loss: 944764341.8470327\n",
      "Iteration 1340, Loss: 949351412.7422346\n",
      "Iteration 1341, Loss: 1012166957.7621032\n",
      "Iteration 1342, Loss: 1070942926.2588931\n",
      "Iteration 1343, Loss: 993629595.86438\n",
      "Iteration 1344, Loss: 971276431.7891154\n",
      "Iteration 1345, Loss: 968694068.8017341\n",
      "Iteration 1346, Loss: 1385424924.939146\n",
      "Iteration 1347, Loss: 882486820.1318709\n",
      "Iteration 1348, Loss: 3027164981.928226\n",
      "Iteration 1349, Loss: 2287289960.580117\n",
      "Iteration 1350, Loss: 1591310889.894947\n",
      "Iteration 1351, Loss: 1568234836.8889172\n",
      "Iteration 1352, Loss: 1320658916.326121\n",
      "Iteration 1353, Loss: 902601306.2724849\n",
      "Iteration 1354, Loss: 885061673.7927574\n",
      "Iteration 1355, Loss: 880709987.4275174\n",
      "Iteration 1356, Loss: 849540209.8695879\n",
      "Iteration 1357, Loss: 845348453.5447328\n",
      "Iteration 1358, Loss: 1489111664.386321\n",
      "Iteration 1359, Loss: 914593635.4274445\n",
      "Iteration 1360, Loss: 892006979.2982266\n",
      "Iteration 1361, Loss: 864096978.8655865\n",
      "Iteration 1362, Loss: 841539562.4483371\n",
      "Iteration 1363, Loss: 1359404705.6535318\n",
      "Iteration 1364, Loss: 1173702114.0004163\n",
      "Iteration 1365, Loss: 848153218.6448498\n",
      "Iteration 1366, Loss: 1009878434.1452957\n",
      "Iteration 1367, Loss: 960477357.7899582\n",
      "Iteration 1368, Loss: 987565688.0063038\n",
      "Iteration 1369, Loss: 884817898.1827196\n",
      "Iteration 1370, Loss: 855015414.3470651\n",
      "Iteration 1371, Loss: 2745938613.962672\n",
      "Iteration 1372, Loss: 925128393.9019761\n",
      "Iteration 1373, Loss: 4730446651.243963\n",
      "Iteration 1374, Loss: 1731486668.6694028\n",
      "Iteration 1375, Loss: 924264186.001997\n",
      "Iteration 1376, Loss: 934171329.6370718\n",
      "Iteration 1377, Loss: 941334450.8811263\n",
      "Iteration 1378, Loss: 1398808184.897523\n",
      "Iteration 1379, Loss: 1310869733.4381218\n",
      "Iteration 1380, Loss: 916214804.082729\n",
      "Iteration 1381, Loss: 2608162514.7061453\n",
      "Iteration 1382, Loss: 1169557137.8050911\n",
      "Iteration 1383, Loss: 967148523.9064939\n",
      "Iteration 1384, Loss: 1286740616.3537107\n",
      "Iteration 1385, Loss: 970275739.0502175\n",
      "Iteration 1386, Loss: 995037712.8986037\n",
      "Iteration 1387, Loss: 965160243.610413\n",
      "Iteration 1388, Loss: 950799274.7112099\n",
      "Iteration 1389, Loss: 1154467156.931443\n",
      "Iteration 1390, Loss: 1031199344.632969\n",
      "Iteration 1391, Loss: 1108010356.2624376\n",
      "Iteration 1392, Loss: 1101579453.5433831\n",
      "Iteration 1393, Loss: 1000790800.0464661\n",
      "Iteration 1394, Loss: 1066798041.3467718\n",
      "Iteration 1395, Loss: 1176496912.845919\n",
      "Iteration 1396, Loss: 991727765.3073802\n",
      "Iteration 1397, Loss: 1073926203.7338822\n",
      "Iteration 1398, Loss: 1040804988.5503607\n",
      "Iteration 1399, Loss: 985527485.0457604\n",
      "Iteration 1400, Loss: 1167010566.2238426\n",
      "Iteration 1401, Loss: 1180163568.73322\n",
      "Iteration 1402, Loss: 1128804856.195761\n",
      "Iteration 1403, Loss: 1142297265.3197217\n",
      "Iteration 1404, Loss: 1056527122.659255\n",
      "Iteration 1405, Loss: 1265921539.0852444\n",
      "Iteration 1406, Loss: 1010357402.9648458\n",
      "Iteration 1407, Loss: 1128101152.84919\n",
      "Iteration 1408, Loss: 1092930463.895435\n",
      "Iteration 1409, Loss: 1190249641.0262272\n",
      "Iteration 1410, Loss: 1201130105.9988966\n",
      "Iteration 1411, Loss: 1216483083.17442\n",
      "Iteration 1412, Loss: 1007069291.4078023\n",
      "Iteration 1413, Loss: 990133189.5575119\n",
      "Iteration 1414, Loss: 933584540.215221\n",
      "Iteration 1415, Loss: 999021659.199882\n",
      "Iteration 1416, Loss: 1060171930.6246487\n",
      "Iteration 1417, Loss: 1017463839.45368\n",
      "Iteration 1418, Loss: 1019117246.3060344\n",
      "Iteration 1419, Loss: 1204239173.6327512\n",
      "Iteration 1420, Loss: 1013159113.7385162\n",
      "Iteration 1421, Loss: 1051008574.0605747\n",
      "Iteration 1422, Loss: 957833153.293935\n",
      "Iteration 1423, Loss: 942825614.8482003\n",
      "Iteration 1424, Loss: 1140468420.324075\n",
      "Iteration 1425, Loss: 963624395.0127676\n",
      "Iteration 1426, Loss: 941402340.7547863\n",
      "Iteration 1427, Loss: 1052731962.4922965\n",
      "Iteration 1428, Loss: 1004627896.667633\n",
      "Iteration 1429, Loss: 1192767297.4881482\n",
      "Iteration 1430, Loss: 1004362670.8698145\n",
      "Iteration 1431, Loss: 1017035634.7641839\n",
      "Iteration 1432, Loss: 1140530453.6066089\n",
      "Iteration 1433, Loss: 1496165604.1029258\n",
      "Iteration 1434, Loss: 1405042787.7054079\n",
      "Iteration 1435, Loss: 958511692.6390835\n",
      "Iteration 1436, Loss: 948125634.8839487\n",
      "Iteration 1437, Loss: 915853291.1398165\n",
      "Iteration 1438, Loss: 1225832647.811384\n",
      "Iteration 1439, Loss: 1186429946.114935\n",
      "Iteration 1440, Loss: 926602684.5194641\n",
      "Iteration 1441, Loss: 922037409.1505737\n",
      "Iteration 1442, Loss: 909025804.40674\n",
      "Iteration 1443, Loss: 1190846727.507345\n",
      "Iteration 1444, Loss: 1085420142.3076894\n",
      "Iteration 1445, Loss: 1017254818.4312191\n",
      "Iteration 1446, Loss: 1981973301.757887\n",
      "Iteration 1447, Loss: 1769570304.9970772\n",
      "Iteration 1448, Loss: 1480282076.444516\n",
      "Iteration 1449, Loss: 1341858705.029804\n",
      "Iteration 1450, Loss: 991053737.6865914\n",
      "Iteration 1451, Loss: 1283660849.081621\n",
      "Iteration 1452, Loss: 1073921587.008382\n",
      "Iteration 1453, Loss: 1136804242.9601755\n",
      "Iteration 1454, Loss: 1082858745.79469\n",
      "Iteration 1455, Loss: 1797990561.2498808\n",
      "Iteration 1456, Loss: 1375682901.4304671\n",
      "Iteration 1457, Loss: 1138352306.0274212\n",
      "Iteration 1458, Loss: 1107155215.6696863\n",
      "Iteration 1459, Loss: 1132800253.2407362\n",
      "Iteration 1460, Loss: 1036313792.4429278\n",
      "Iteration 1461, Loss: 1151949732.8128223\n",
      "Iteration 1462, Loss: 1578084344.799521\n",
      "Iteration 1463, Loss: 1508682458.8066113\n",
      "Iteration 1464, Loss: 1243611049.6448288\n",
      "Iteration 1465, Loss: 1204551465.9670975\n",
      "Iteration 1466, Loss: 975899138.1197174\n",
      "Iteration 1467, Loss: 1229396293.196724\n",
      "Iteration 1468, Loss: 1246147167.5550025\n",
      "Iteration 1469, Loss: 1035182049.4048443\n",
      "Iteration 1470, Loss: 1119702589.5536563\n",
      "Iteration 1471, Loss: 1189722174.2663913\n",
      "Iteration 1472, Loss: 1161195436.2260003\n",
      "Iteration 1473, Loss: 1184044314.4665856\n",
      "Iteration 1474, Loss: 1145345845.4845781\n",
      "Iteration 1475, Loss: 1113600016.5695972\n",
      "Iteration 1476, Loss: 1173752242.9510884\n",
      "Iteration 1477, Loss: 979870473.282071\n",
      "Iteration 1478, Loss: 983498325.8320985\n",
      "Iteration 1479, Loss: 1015735728.9367044\n",
      "Iteration 1480, Loss: 1405480524.4967542\n",
      "Iteration 1481, Loss: 1050106863.2514453\n",
      "Iteration 1482, Loss: 1013552175.8777833\n",
      "Iteration 1483, Loss: 5267415510.292307\n",
      "Iteration 1484, Loss: 8920899362.729572\n",
      "Iteration 1485, Loss: 62873709150.64396\n",
      "Iteration 1486, Loss: 26815275888.772182\n",
      "Iteration 1487, Loss: 6486724019.482142\n",
      "Iteration 1488, Loss: 2987619972.872719\n",
      "Iteration 1489, Loss: 1281639486.100307\n",
      "Iteration 1490, Loss: 1316868131.114374\n",
      "Iteration 1491, Loss: 1304300380.803854\n",
      "Iteration 1492, Loss: 1170559439.006991\n",
      "Iteration 1493, Loss: 1076940039.0254605\n",
      "Iteration 1494, Loss: 1086042438.5446234\n",
      "Iteration 1495, Loss: 1314446848.1005974\n",
      "Iteration 1496, Loss: 1235153592.6971822\n",
      "Iteration 1497, Loss: 1137307829.1522968\n",
      "Iteration 1498, Loss: 1266810631.71915\n",
      "Iteration 1499, Loss: 1240897576.3933597\n",
      "Iteration 1500, Loss: 1622588011.2832148\n",
      "Iteration 1501, Loss: 1576608072.5741246\n",
      "Iteration 1502, Loss: 1149010414.8723407\n",
      "Iteration 1503, Loss: 1153775234.927995\n",
      "Iteration 1504, Loss: 1374156332.2990258\n",
      "Iteration 1505, Loss: 1127752862.9572263\n",
      "Iteration 1506, Loss: 1113614905.6915545\n",
      "Iteration 1507, Loss: 1527223232.047416\n",
      "Iteration 1508, Loss: 1429588928.2671988\n",
      "Iteration 1509, Loss: 1156410388.9692404\n",
      "Iteration 1510, Loss: 1124150677.845213\n",
      "Iteration 1511, Loss: 1230612543.165622\n",
      "Iteration 1512, Loss: 1336061037.918809\n",
      "Iteration 1513, Loss: 1269930178.4299238\n",
      "Iteration 1514, Loss: 1263296136.0307813\n",
      "Iteration 1515, Loss: 1185335838.5055277\n",
      "Iteration 1516, Loss: 1214363387.825783\n",
      "Iteration 1517, Loss: 1180704383.2074568\n",
      "Iteration 1518, Loss: 1183673449.0499384\n",
      "Iteration 1519, Loss: 1245690040.1864886\n",
      "Iteration 1520, Loss: 1537441051.5432742\n",
      "Iteration 1521, Loss: 1129732068.5363498\n",
      "Iteration 1522, Loss: 1418912153.2513676\n",
      "Iteration 1523, Loss: 1154089962.989422\n",
      "Iteration 1524, Loss: 1442664279.3425622\n",
      "Iteration 1525, Loss: 1400718793.0529428\n",
      "Iteration 1526, Loss: 1457669698.2327237\n",
      "Iteration 1527, Loss: 1516076936.8811316\n",
      "Iteration 1528, Loss: 1492409297.4645216\n",
      "Iteration 1529, Loss: 1465106288.7583277\n",
      "Iteration 1530, Loss: 1436053273.4632087\n",
      "Iteration 1531, Loss: 1419278224.293335\n",
      "Iteration 1532, Loss: 1085867448.4460607\n",
      "Iteration 1533, Loss: 1063981590.9531902\n",
      "Iteration 1534, Loss: 1170178441.6091352\n",
      "Iteration 1535, Loss: 1195047368.0536265\n",
      "Iteration 1536, Loss: 1083730753.5643187\n",
      "Iteration 1537, Loss: 1212625764.010793\n",
      "Iteration 1538, Loss: 1243472086.0920973\n",
      "Iteration 1539, Loss: 1218119834.8643017\n",
      "Iteration 1540, Loss: 1205701042.290284\n",
      "Iteration 1541, Loss: 1538880889.7276063\n",
      "Iteration 1542, Loss: 1452382477.9339466\n",
      "Iteration 1543, Loss: 1351966114.1581726\n",
      "Iteration 1544, Loss: 1311042425.8058004\n",
      "Iteration 1545, Loss: 1100578673.7706525\n",
      "Iteration 1546, Loss: 1093428380.168206\n",
      "Iteration 1547, Loss: 1131395708.8333251\n",
      "Iteration 1548, Loss: 1151233944.6507056\n",
      "Iteration 1549, Loss: 1128847393.5320234\n",
      "Iteration 1550, Loss: 1151159619.23407\n",
      "Iteration 1551, Loss: 1172459810.847706\n",
      "Iteration 1552, Loss: 1135675603.2814105\n",
      "Iteration 1553, Loss: 1103181581.308761\n",
      "Iteration 1554, Loss: 1067856370.7804059\n",
      "Iteration 1555, Loss: 1099879212.0295804\n",
      "Iteration 1556, Loss: 994408257.6878136\n",
      "Iteration 1557, Loss: 955320159.7275981\n",
      "Iteration 1558, Loss: 1432014423.9585261\n",
      "Iteration 1559, Loss: 1342584076.691251\n",
      "Iteration 1560, Loss: 976200606.8440243\n",
      "Iteration 1561, Loss: 1212020189.8758965\n",
      "Iteration 1562, Loss: 1219622565.904917\n",
      "Iteration 1563, Loss: 1170769639.6409416\n",
      "Iteration 1564, Loss: 1143024887.3780274\n",
      "Iteration 1565, Loss: 1104282019.4402156\n",
      "Iteration 1566, Loss: 1397074197.7343369\n",
      "Iteration 1567, Loss: 1417430486.9575262\n",
      "Iteration 1568, Loss: 1099863866.2345421\n",
      "Iteration 1569, Loss: 1021650193.8455025\n",
      "Iteration 1570, Loss: 999128824.8991325\n",
      "Iteration 1571, Loss: 1015614740.6898462\n",
      "Iteration 1572, Loss: 973405823.5332483\n",
      "Iteration 1573, Loss: 944305938.4279698\n",
      "Iteration 1574, Loss: 930783062.5382736\n",
      "Iteration 1575, Loss: 1073352633.0644692\n",
      "Iteration 1576, Loss: 1160188800.686709\n",
      "Iteration 1577, Loss: 1002353965.3012102\n",
      "Iteration 1578, Loss: 982074463.0744113\n",
      "Iteration 1579, Loss: 1133759360.1308308\n",
      "Iteration 1580, Loss: 1040064823.6007617\n",
      "Iteration 1581, Loss: 1243835386.0699697\n",
      "Iteration 1582, Loss: 1197102728.199056\n",
      "Iteration 1583, Loss: 1004868420.657652\n",
      "Iteration 1584, Loss: 1075548214.7127314\n",
      "Iteration 1585, Loss: 1092357680.0576627\n",
      "Iteration 1586, Loss: 1448386434.4409618\n",
      "Iteration 1587, Loss: 1392875223.9634826\n",
      "Iteration 1588, Loss: 1324706966.0449014\n",
      "Iteration 1589, Loss: 1022423252.4913634\n",
      "Iteration 1590, Loss: 997347133.9501398\n",
      "Iteration 1591, Loss: 1010937530.2762719\n",
      "Iteration 1592, Loss: 1947968608.3574798\n",
      "Iteration 1593, Loss: 956665569.9392886\n",
      "Iteration 1594, Loss: 979061163.0597347\n",
      "Iteration 1595, Loss: 1186021689.3980026\n",
      "Iteration 1596, Loss: 1099488770.3619084\n",
      "Iteration 1597, Loss: 1075433334.643425\n",
      "Iteration 1598, Loss: 1041234408.7924374\n",
      "Iteration 1599, Loss: 1006005848.9728582\n",
      "Iteration 1600, Loss: 954703026.1273162\n",
      "Iteration 1601, Loss: 927112044.3933047\n",
      "Iteration 1602, Loss: 1157975736.2485454\n",
      "Iteration 1603, Loss: 1014569469.2194636\n",
      "Iteration 1604, Loss: 1152812974.0457609\n",
      "Iteration 1605, Loss: 1000053841.1921947\n",
      "Iteration 1606, Loss: 979203310.7484287\n",
      "Iteration 1607, Loss: 1064392074.4378666\n",
      "Iteration 1608, Loss: 1597718853.8763487\n",
      "Iteration 1609, Loss: 1380531541.4130666\n",
      "Iteration 1610, Loss: 1295422606.427747\n",
      "Iteration 1611, Loss: 987161235.1619915\n",
      "Iteration 1612, Loss: 1930168393.979459\n",
      "Iteration 1613, Loss: 1715155429.8846924\n",
      "Iteration 1614, Loss: 1003116544.2205604\n",
      "Iteration 1615, Loss: 994992488.0279311\n",
      "Iteration 1616, Loss: 982644575.8026766\n",
      "Iteration 1617, Loss: 1101341994.1195168\n",
      "Iteration 1618, Loss: 1046062520.994092\n",
      "Iteration 1619, Loss: 1131512793.8671615\n",
      "Iteration 1620, Loss: 1093505048.5315266\n",
      "Iteration 1621, Loss: 1061623311.6746428\n",
      "Iteration 1622, Loss: 1039740698.1296171\n",
      "Iteration 1623, Loss: 1007122964.3627721\n",
      "Iteration 1624, Loss: 2024602742.4295683\n",
      "Iteration 1625, Loss: 2028421445.9872792\n",
      "Iteration 1626, Loss: 1207543514.7689226\n",
      "Iteration 1627, Loss: 1196654052.3375866\n",
      "Iteration 1628, Loss: 1124093485.0306187\n",
      "Iteration 1629, Loss: 1032424392.6892018\n",
      "Iteration 1630, Loss: 1048463541.113324\n",
      "Iteration 1631, Loss: 1046747685.3304203\n",
      "Iteration 1632, Loss: 1122304633.6089861\n",
      "Iteration 1633, Loss: 1111601203.5247695\n",
      "Iteration 1634, Loss: 933239153.0493568\n",
      "Iteration 1635, Loss: 2825307760.0218863\n",
      "Iteration 1636, Loss: 1531739364.2043633\n",
      "Iteration 1637, Loss: 1327047946.8860996\n",
      "Iteration 1638, Loss: 1245574712.31624\n",
      "Iteration 1639, Loss: 1026294728.4323542\n",
      "Iteration 1640, Loss: 1101892169.1747313\n",
      "Iteration 1641, Loss: 1077781892.9794111\n",
      "Iteration 1642, Loss: 1142751174.073228\n",
      "Iteration 1643, Loss: 1167302513.3623257\n",
      "Iteration 1644, Loss: 1022497538.7865094\n",
      "Iteration 1645, Loss: 971807748.6278039\n",
      "Iteration 1646, Loss: 1584031715.296297\n",
      "Iteration 1647, Loss: 1300460147.1587367\n",
      "Iteration 1648, Loss: 1108707862.2355084\n",
      "Iteration 1649, Loss: 1119859404.9044776\n",
      "Iteration 1650, Loss: 1193954350.096789\n",
      "Iteration 1651, Loss: 1161834500.2644138\n",
      "Iteration 1652, Loss: 1084747124.4287496\n",
      "Iteration 1653, Loss: 1284576710.0049415\n",
      "Iteration 1654, Loss: 1272454168.5100703\n",
      "Iteration 1655, Loss: 1108104733.431886\n",
      "Iteration 1656, Loss: 1148162969.5589721\n",
      "Iteration 1657, Loss: 1065109549.6090255\n",
      "Iteration 1658, Loss: 1378876576.8074539\n",
      "Iteration 1659, Loss: 1074151350.639604\n",
      "Iteration 1660, Loss: 1297083119.2929978\n",
      "Iteration 1661, Loss: 1105118966.027964\n",
      "Iteration 1662, Loss: 1056082011.351732\n",
      "Iteration 1663, Loss: 1063315229.865941\n",
      "Iteration 1664, Loss: 1059670174.2422516\n",
      "Iteration 1665, Loss: 1054902703.6122772\n",
      "Iteration 1666, Loss: 1044684026.3968276\n",
      "Iteration 1667, Loss: 1083031381.046644\n",
      "Iteration 1668, Loss: 1124334863.279635\n",
      "Iteration 1669, Loss: 1137833311.6648583\n",
      "Iteration 1670, Loss: 1203834597.9983003\n",
      "Iteration 1671, Loss: 1084866898.916679\n",
      "Iteration 1672, Loss: 1009085850.8237177\n",
      "Iteration 1673, Loss: 995161245.0064534\n",
      "Iteration 1674, Loss: 982699522.5730475\n",
      "Iteration 1675, Loss: 972163489.5347296\n",
      "Iteration 1676, Loss: 972366417.1758684\n",
      "Iteration 1677, Loss: 959150458.4432615\n",
      "Iteration 1678, Loss: 1414932839.3706107\n",
      "Iteration 1679, Loss: 1009509095.9008585\n",
      "Iteration 1680, Loss: 1037949713.0823689\n",
      "Iteration 1681, Loss: 1217610607.8622866\n",
      "Iteration 1682, Loss: 1078386698.559971\n",
      "Iteration 1683, Loss: 1229468012.0214741\n",
      "Iteration 1684, Loss: 1019511594.2382158\n",
      "Iteration 1685, Loss: 1015712944.2464545\n",
      "Iteration 1686, Loss: 1401928183.4523017\n",
      "Iteration 1687, Loss: 1025209809.9021956\n",
      "Iteration 1688, Loss: 2544976598.9438624\n",
      "Iteration 1689, Loss: 1343939995.8239179\n",
      "Iteration 1690, Loss: 1372783264.8614352\n",
      "Iteration 1691, Loss: 1314144101.9459834\n",
      "Iteration 1692, Loss: 1083716966.617051\n",
      "Iteration 1693, Loss: 1319052034.057635\n",
      "Iteration 1694, Loss: 1338530908.4725296\n",
      "Iteration 1695, Loss: 1216877517.581505\n",
      "Iteration 1696, Loss: 1342905091.1634846\n",
      "Iteration 1697, Loss: 1400850324.6124418\n",
      "Iteration 1698, Loss: 1442640105.43347\n",
      "Iteration 1699, Loss: 1105671057.777162\n",
      "Iteration 1700, Loss: 1133332191.5279856\n",
      "Iteration 1701, Loss: 1255947340.136651\n",
      "Iteration 1702, Loss: 1352180561.0594807\n",
      "Iteration 1703, Loss: 1362479167.8315897\n",
      "Iteration 1704, Loss: 1392316766.841671\n",
      "Iteration 1705, Loss: 1135624512.4866192\n",
      "Iteration 1706, Loss: 1303396677.161939\n",
      "Iteration 1707, Loss: 1192797864.394195\n",
      "Iteration 1708, Loss: 1324392505.8427377\n",
      "Iteration 1709, Loss: 1255420703.2181997\n",
      "Iteration 1710, Loss: 1157005798.1949499\n",
      "Iteration 1711, Loss: 1398416395.7712202\n",
      "Iteration 1712, Loss: 1142356766.112461\n",
      "Iteration 1713, Loss: 1490706506.3577044\n",
      "Iteration 1714, Loss: 1558482687.2849128\n",
      "Iteration 1715, Loss: 1548124891.4006567\n",
      "Iteration 1716, Loss: 1613254409.3756115\n",
      "Iteration 1717, Loss: 1166764843.715442\n",
      "Iteration 1718, Loss: 1212335199.33325\n",
      "Iteration 1719, Loss: 1141695822.9462667\n",
      "Iteration 1720, Loss: 1192739761.8702898\n",
      "Iteration 1721, Loss: 1181405920.3329442\n",
      "Iteration 1722, Loss: 1370469612.46663\n",
      "Iteration 1723, Loss: 1293150565.7828095\n",
      "Iteration 1724, Loss: 1175893628.0022464\n",
      "Iteration 1725, Loss: 1161690682.1786313\n",
      "Iteration 1726, Loss: 1183167182.4441502\n",
      "Iteration 1727, Loss: 1860286724.6316519\n",
      "Iteration 1728, Loss: 1942693064.2180123\n",
      "Iteration 1729, Loss: 1302949555.9006727\n",
      "Iteration 1730, Loss: 1165217109.044359\n",
      "Iteration 1731, Loss: 1226895300.4613836\n",
      "Iteration 1732, Loss: 1266982019.5375385\n",
      "Iteration 1733, Loss: 1313232964.0901144\n",
      "Iteration 1734, Loss: 1278996207.1331139\n",
      "Iteration 1735, Loss: 1277964022.2287023\n",
      "Iteration 1736, Loss: 1375581691.7798538\n",
      "Iteration 1737, Loss: 1397509604.4277554\n",
      "Iteration 1738, Loss: 1242341806.994009\n",
      "Iteration 1739, Loss: 1233881511.3435006\n",
      "Iteration 1740, Loss: 1184966737.4815056\n",
      "Iteration 1741, Loss: 1187833076.3677638\n",
      "Iteration 1742, Loss: 1131730723.8702583\n",
      "Iteration 1743, Loss: 1521821409.3506281\n",
      "Iteration 1744, Loss: 1238928515.416175\n",
      "Iteration 1745, Loss: 1282477493.5628731\n",
      "Iteration 1746, Loss: 1367763592.8661911\n",
      "Iteration 1747, Loss: 1426012370.9771445\n",
      "Iteration 1748, Loss: 1203042945.5122087\n",
      "Iteration 1749, Loss: 1498325405.8398693\n",
      "Iteration 1750, Loss: 1181559592.392125\n",
      "Iteration 1751, Loss: 1230987080.819147\n",
      "Iteration 1752, Loss: 1385193480.4945724\n",
      "Iteration 1753, Loss: 1445086910.1046786\n",
      "Iteration 1754, Loss: 1438926605.8660553\n",
      "Iteration 1755, Loss: 1162301169.6429646\n",
      "Iteration 1756, Loss: 1268128105.125641\n",
      "Iteration 1757, Loss: 1262224746.0898445\n",
      "Iteration 1758, Loss: 1259542531.204524\n",
      "Iteration 1759, Loss: 1340407328.274164\n",
      "Iteration 1760, Loss: 1304995297.4704328\n",
      "Iteration 1761, Loss: 1302685318.72546\n",
      "Iteration 1762, Loss: 1112928976.1448505\n",
      "Iteration 1763, Loss: 1376485968.3166769\n",
      "Iteration 1764, Loss: 1420692726.6817963\n",
      "Iteration 1765, Loss: 1163907747.226888\n",
      "Iteration 1766, Loss: 1361546182.5085363\n",
      "Iteration 1767, Loss: 1414799260.4059699\n",
      "Iteration 1768, Loss: 1328577929.8395684\n",
      "Iteration 1769, Loss: 1426835846.6225607\n",
      "Iteration 1770, Loss: 1430388667.0752716\n",
      "Iteration 1771, Loss: 1318571839.492025\n",
      "Iteration 1772, Loss: 1346635104.4071572\n",
      "Iteration 1773, Loss: 1326738204.5781636\n",
      "Iteration 1774, Loss: 1177167908.9452379\n",
      "Iteration 1775, Loss: 1358881452.2132518\n",
      "Iteration 1776, Loss: 1430131816.2271006\n",
      "Iteration 1777, Loss: 1196115254.3813007\n",
      "Iteration 1778, Loss: 1504310607.7905564\n",
      "Iteration 1779, Loss: 1524902845.6386633\n",
      "Iteration 1780, Loss: 1493651909.9542892\n",
      "Iteration 1781, Loss: 1503690846.8217764\n",
      "Iteration 1782, Loss: 1510001692.7835479\n",
      "Iteration 1783, Loss: 1199251053.0141964\n",
      "Iteration 1784, Loss: 1206677590.2701793\n",
      "Iteration 1785, Loss: 1425100513.747325\n",
      "Iteration 1786, Loss: 1334809876.3341124\n",
      "Iteration 1787, Loss: 1446156833.284553\n",
      "Iteration 1788, Loss: 1344057038.043085\n",
      "Iteration 1789, Loss: 1260274896.3785706\n",
      "Iteration 1790, Loss: 1343721414.1427054\n",
      "Iteration 1791, Loss: 1216116446.5274541\n",
      "Iteration 1792, Loss: 1474091801.1304467\n",
      "Iteration 1793, Loss: 1370284518.0668273\n",
      "Iteration 1794, Loss: 1515351593.6141047\n",
      "Iteration 1795, Loss: 1596838177.5023844\n",
      "Iteration 1796, Loss: 1428459525.7095883\n",
      "Iteration 1797, Loss: 1292595368.7584667\n",
      "Iteration 1798, Loss: 1417327395.943551\n",
      "Iteration 1799, Loss: 1359313038.9063253\n",
      "Iteration 1800, Loss: 1373136801.802077\n",
      "Iteration 1801, Loss: 1384665753.032585\n",
      "Iteration 1802, Loss: 1402105880.027314\n",
      "Iteration 1803, Loss: 1407747723.110169\n",
      "Iteration 1804, Loss: 1157946045.1209266\n",
      "Iteration 1805, Loss: 1294907308.7795637\n",
      "Iteration 1806, Loss: 1586711545.2098613\n",
      "Iteration 1807, Loss: 1356443910.2240472\n",
      "Iteration 1808, Loss: 1491464795.0527651\n",
      "Iteration 1809, Loss: 1573015670.788952\n",
      "Iteration 1810, Loss: 1490723086.9862702\n",
      "Iteration 1811, Loss: 1286242131.918825\n",
      "Iteration 1812, Loss: 1198987378.001082\n",
      "Iteration 1813, Loss: 1367650923.2295778\n",
      "Iteration 1814, Loss: 1320008904.6335797\n",
      "Iteration 1815, Loss: 1275141073.033722\n",
      "Iteration 1816, Loss: 1353767272.6972823\n",
      "Iteration 1817, Loss: 1579774150.9313602\n",
      "Iteration 1818, Loss: 1599479807.235657\n",
      "Iteration 1819, Loss: 1474978459.2499514\n",
      "Iteration 1820, Loss: 1482875356.491029\n",
      "Iteration 1821, Loss: 1234817527.917046\n",
      "Iteration 1822, Loss: 1188890752.7595532\n",
      "Iteration 1823, Loss: 1327769236.337583\n",
      "Iteration 1824, Loss: 1288649855.229807\n",
      "Iteration 1825, Loss: 1315584897.9251626\n",
      "Iteration 1826, Loss: 1366503143.5247898\n",
      "Iteration 1827, Loss: 1379142877.2132077\n",
      "Iteration 1828, Loss: 1437776804.9268587\n",
      "Iteration 1829, Loss: 1150623429.1099539\n",
      "Iteration 1830, Loss: 1523226371.3951113\n",
      "Iteration 1831, Loss: 1519448490.4164195\n",
      "Iteration 1832, Loss: 1144048261.5461106\n",
      "Iteration 1833, Loss: 1186766109.746757\n",
      "Iteration 1834, Loss: 1371905961.363758\n",
      "Iteration 1835, Loss: 1376115887.7816052\n",
      "Iteration 1836, Loss: 1155547569.83539\n",
      "Iteration 1837, Loss: 1407195015.281072\n",
      "Iteration 1838, Loss: 1188954375.4977407\n",
      "Iteration 1839, Loss: 1504505523.954112\n",
      "Iteration 1840, Loss: 1226661640.6553514\n",
      "Iteration 1841, Loss: 1960169825.086412\n",
      "Iteration 1842, Loss: 1201445079.2313628\n",
      "Iteration 1843, Loss: 1183395188.821762\n",
      "Iteration 1844, Loss: 1297360092.450536\n",
      "Iteration 1845, Loss: 1383331472.6985884\n",
      "Iteration 1846, Loss: 1447752941.7384171\n",
      "Iteration 1847, Loss: 1414446243.6305027\n",
      "Iteration 1848, Loss: 1381499380.9122398\n",
      "Iteration 1849, Loss: 1384076226.6832154\n",
      "Iteration 1850, Loss: 1278286254.6430948\n",
      "Iteration 1851, Loss: 1287841765.1939309\n",
      "Iteration 1852, Loss: 1385778679.3090103\n",
      "Iteration 1853, Loss: 1396903471.7124166\n",
      "Iteration 1854, Loss: 1132019708.395128\n",
      "Iteration 1855, Loss: 1118602653.0184326\n",
      "Iteration 1856, Loss: 1164911580.6561744\n",
      "Iteration 1857, Loss: 1165528314.8148117\n",
      "Iteration 1858, Loss: 1323230178.9254444\n",
      "Iteration 1859, Loss: 1318472392.6788785\n",
      "Iteration 1860, Loss: 1409384977.2542572\n",
      "Iteration 1861, Loss: 1389264258.1511095\n",
      "Iteration 1862, Loss: 1393358802.0960503\n",
      "Iteration 1863, Loss: 1153224794.5508957\n",
      "Iteration 1864, Loss: 1118835769.4395187\n",
      "Iteration 1865, Loss: 1112557042.1167161\n",
      "Iteration 1866, Loss: 1144986935.2748215\n",
      "Iteration 1867, Loss: 1141613914.4703248\n",
      "Iteration 1868, Loss: 1203624676.257566\n",
      "Iteration 1869, Loss: 1310107551.2445774\n",
      "Iteration 1870, Loss: 1259460106.5132568\n",
      "Iteration 1871, Loss: 1242735148.1633492\n",
      "Iteration 1872, Loss: 1229186527.9453156\n",
      "Iteration 1873, Loss: 1212141160.7402482\n",
      "Iteration 1874, Loss: 1258605060.2857642\n",
      "Iteration 1875, Loss: 1218735804.7450528\n",
      "Iteration 1876, Loss: 1131350257.2312055\n",
      "Iteration 1877, Loss: 1561691498.5263479\n",
      "Iteration 1878, Loss: 1205498217.7464013\n",
      "Iteration 1879, Loss: 1128279465.4969327\n",
      "Iteration 1880, Loss: 1308599572.5320153\n",
      "Iteration 1881, Loss: 1100471446.3714619\n",
      "Iteration 1882, Loss: 1088030933.298837\n",
      "Iteration 1883, Loss: 1138966869.3390687\n",
      "Iteration 1884, Loss: 1193289152.664425\n",
      "Iteration 1885, Loss: 1204490277.8019783\n",
      "Iteration 1886, Loss: 1312714215.666511\n",
      "Iteration 1887, Loss: 1093920933.3206892\n",
      "Iteration 1888, Loss: 1383409957.4688616\n",
      "Iteration 1889, Loss: 1279197057.7604773\n",
      "Iteration 1890, Loss: 1199611817.380499\n",
      "Iteration 1891, Loss: 1126698270.5094264\n",
      "Iteration 1892, Loss: 1501054605.5194452\n",
      "Iteration 1893, Loss: 1360508373.0976439\n",
      "Iteration 1894, Loss: 1402493081.398749\n",
      "Iteration 1895, Loss: 1410869137.923954\n",
      "Iteration 1896, Loss: 1467566339.2900693\n",
      "Iteration 1897, Loss: 1443523010.0609012\n",
      "Iteration 1898, Loss: 1149479801.291832\n",
      "Iteration 1899, Loss: 1323577379.6136143\n",
      "Iteration 1900, Loss: 1327032370.2993994\n",
      "Iteration 1901, Loss: 1272368698.9051685\n",
      "Iteration 1902, Loss: 1252271625.7609763\n",
      "Iteration 1903, Loss: 1156937583.5843492\n",
      "Iteration 1904, Loss: 1272630897.4068348\n",
      "Iteration 1905, Loss: 1262884256.7393606\n",
      "Iteration 1906, Loss: 1261960790.9722593\n",
      "Iteration 1907, Loss: 1110733020.6935554\n",
      "Iteration 1908, Loss: 1099907230.8890262\n",
      "Iteration 1909, Loss: 2306607976.3189754\n",
      "Iteration 1910, Loss: 1111328391.230945\n",
      "Iteration 1911, Loss: 1091996759.8678608\n",
      "Iteration 1912, Loss: 1100815708.2578168\n",
      "Iteration 1913, Loss: 1533844702.1735287\n",
      "Iteration 1914, Loss: 1537488557.3930695\n",
      "Iteration 1915, Loss: 1194603986.7252235\n",
      "Iteration 1916, Loss: 1322443670.9947212\n",
      "Iteration 1917, Loss: 1317504697.292399\n",
      "Iteration 1918, Loss: 1374036750.6922088\n",
      "Iteration 1919, Loss: 1129754168.6138098\n",
      "Iteration 1920, Loss: 1705844821.7301626\n",
      "Iteration 1921, Loss: 1708940225.7867603\n",
      "Iteration 1922, Loss: 1642445339.7941802\n",
      "Iteration 1923, Loss: 1668780225.463907\n",
      "Iteration 1924, Loss: 1111203402.010343\n",
      "Iteration 1925, Loss: 1143014816.6457314\n",
      "Iteration 1926, Loss: 1144742633.1608644\n",
      "Iteration 1927, Loss: 1290203421.6271257\n",
      "Iteration 1928, Loss: 1282084302.293871\n",
      "Iteration 1929, Loss: 1270786770.6520772\n",
      "Iteration 1930, Loss: 1258864926.1546705\n",
      "Iteration 1931, Loss: 1336191024.6130774\n",
      "Iteration 1932, Loss: 1141858124.0162003\n",
      "Iteration 1933, Loss: 1187288552.6458387\n",
      "Iteration 1934, Loss: 1205047819.6740966\n",
      "Iteration 1935, Loss: 1196240450.7307177\n",
      "Iteration 1936, Loss: 1070530683.625985\n",
      "Iteration 1937, Loss: 1078706412.4848166\n",
      "Iteration 1938, Loss: 1058353139.7547221\n",
      "Iteration 1939, Loss: 1050123889.4863483\n",
      "Iteration 1940, Loss: 2037084345.5068238\n",
      "Iteration 1941, Loss: 1897574484.300539\n",
      "Iteration 1942, Loss: 1775691985.9886189\n",
      "Iteration 1943, Loss: 1449519857.5614977\n",
      "Iteration 1944, Loss: 1393154431.186214\n",
      "Iteration 1945, Loss: 1248539511.0742564\n",
      "Iteration 1946, Loss: 1245337464.7356741\n",
      "Iteration 1947, Loss: 1120913677.6302295\n",
      "Iteration 1948, Loss: 1113634141.7228696\n",
      "Iteration 1949, Loss: 1708920847.384566\n",
      "Iteration 1950, Loss: 1356015721.3883095\n",
      "Iteration 1951, Loss: 1402410073.487957\n",
      "Iteration 1952, Loss: 1253700478.1300108\n",
      "Iteration 1953, Loss: 1067025876.8992959\n",
      "Iteration 1954, Loss: 1060923626.8385378\n",
      "Iteration 1955, Loss: 1052570249.212705\n",
      "Iteration 1956, Loss: 1208724887.2075145\n",
      "Iteration 1957, Loss: 1268751087.7730715\n",
      "Iteration 1958, Loss: 1266814228.3282645\n",
      "Iteration 1959, Loss: 1269299455.520714\n",
      "Iteration 1960, Loss: 1261187754.1597624\n",
      "Iteration 1961, Loss: 1256072354.0935857\n",
      "Iteration 1962, Loss: 1181334141.1381786\n",
      "Iteration 1963, Loss: 1145774975.192929\n",
      "Iteration 1964, Loss: 1334423692.9866211\n",
      "Iteration 1965, Loss: 1393082813.293313\n",
      "Iteration 1966, Loss: 1317994195.9491286\n",
      "Iteration 1967, Loss: 1335160445.4057708\n",
      "Iteration 1968, Loss: 1093639402.29089\n",
      "Iteration 1969, Loss: 1492934845.541536\n",
      "Iteration 1970, Loss: 1473680093.2311077\n",
      "Iteration 1971, Loss: 1188460412.3664153\n",
      "Iteration 1972, Loss: 1296784363.0852804\n",
      "Iteration 1973, Loss: 1276067326.0226254\n",
      "Iteration 1974, Loss: 1092887355.9913468\n",
      "Iteration 1975, Loss: 1093646251.380376\n",
      "Iteration 1976, Loss: 1334716066.032023\n",
      "Iteration 1977, Loss: 1362649764.440579\n",
      "Iteration 1978, Loss: 1242078732.3262997\n",
      "Iteration 1979, Loss: 1299102954.1299613\n",
      "Iteration 1980, Loss: 1180492475.0263324\n",
      "Iteration 1981, Loss: 1167471519.257734\n",
      "Iteration 1982, Loss: 1118138565.7267923\n",
      "Iteration 1983, Loss: 1146008080.158054\n",
      "Iteration 1984, Loss: 1156718234.580098\n",
      "Iteration 1985, Loss: 1027182234.3875978\n",
      "Iteration 1986, Loss: 1015050995.489437\n",
      "Iteration 1987, Loss: 1030770829.26827\n",
      "Iteration 1988, Loss: 1021938168.1086533\n",
      "Iteration 1989, Loss: 1011966894.5634286\n",
      "Iteration 1990, Loss: 1225683569.0893457\n",
      "Iteration 1991, Loss: 1020932717.2840761\n",
      "Iteration 1992, Loss: 1244943738.5785384\n",
      "Iteration 1993, Loss: 1184177019.145738\n",
      "Iteration 1994, Loss: 1100347102.465584\n",
      "Iteration 1995, Loss: 1084358126.3407702\n",
      "Iteration 1996, Loss: 1118192582.9290226\n",
      "Iteration 1997, Loss: 1255574038.2128098\n",
      "Iteration 1998, Loss: 1225770398.6456604\n",
      "Iteration 1999, Loss: 1200766746.7903566\n",
      "Iteration 2000, Loss: 1212283772.9950933\n",
      "Iteration 2001, Loss: 1169885875.663945\n",
      "Iteration 2002, Loss: 1103295802.9312217\n",
      "Iteration 2003, Loss: 1235014357.745759\n",
      "Iteration 2004, Loss: 1122882114.7718153\n",
      "Iteration 2005, Loss: 1272544866.2370949\n",
      "Iteration 2006, Loss: 1238564522.263033\n",
      "Iteration 2007, Loss: 1280473768.528025\n",
      "Iteration 2008, Loss: 1299612183.775432\n",
      "Iteration 2009, Loss: 1262158291.6975784\n",
      "Iteration 2010, Loss: 1084984391.159248\n",
      "Iteration 2011, Loss: 1242528970.5492003\n",
      "Iteration 2012, Loss: 1096490624.6023238\n",
      "Iteration 2013, Loss: 1077137562.523674\n",
      "Iteration 2014, Loss: 991320723.8571788\n",
      "Iteration 2015, Loss: 980298561.0933322\n",
      "Iteration 2016, Loss: 1151962433.982014\n",
      "Iteration 2017, Loss: 990501453.4453154\n",
      "Iteration 2018, Loss: 1064832362.2442384\n",
      "Iteration 2019, Loss: 1092359482.2767885\n",
      "Iteration 2020, Loss: 1070552676.8016955\n",
      "Iteration 2021, Loss: 1046162077.4814048\n",
      "Iteration 2022, Loss: 1056939013.0341281\n",
      "Iteration 2023, Loss: 1000055879.8595058\n",
      "Iteration 2024, Loss: 984860180.1730453\n",
      "Iteration 2025, Loss: 952875362.0107964\n",
      "Iteration 2026, Loss: 940728990.5109054\n",
      "Iteration 2027, Loss: 1728364555.0584354\n",
      "Iteration 2028, Loss: 986355264.9269148\n",
      "Iteration 2029, Loss: 1305418924.1599479\n",
      "Iteration 2030, Loss: 1251295935.3013072\n",
      "Iteration 2031, Loss: 972897324.5289772\n",
      "Iteration 2032, Loss: 969466322.4945202\n",
      "Iteration 2033, Loss: 998693388.194428\n",
      "Iteration 2034, Loss: 953453842.1305863\n",
      "Iteration 2035, Loss: 1178620763.5940063\n",
      "Iteration 2036, Loss: 1194477571.2416794\n",
      "Iteration 2037, Loss: 1013707969.3284508\n",
      "Iteration 2038, Loss: 1083686137.3420565\n",
      "Iteration 2039, Loss: 1532884759.232812\n",
      "Iteration 2040, Loss: 1435100810.134315\n",
      "Iteration 2041, Loss: 1383683687.946345\n",
      "Iteration 2042, Loss: 1302769768.788003\n",
      "Iteration 2043, Loss: 1009573569.7914555\n",
      "Iteration 2044, Loss: 982500263.8884879\n",
      "Iteration 2045, Loss: 968015158.8229877\n",
      "Iteration 2046, Loss: 949634028.0097166\n",
      "Iteration 2047, Loss: 932293792.0880605\n",
      "Iteration 2048, Loss: 1081604251.0771654\n",
      "Iteration 2049, Loss: 991253690.0097468\n",
      "Iteration 2050, Loss: 924228587.6137906\n",
      "Iteration 2051, Loss: 1340113613.294608\n",
      "Iteration 2052, Loss: 1350544275.8063982\n",
      "Iteration 2053, Loss: 1072436132.4072571\n",
      "Iteration 2054, Loss: 1079416073.0681078\n",
      "Iteration 2055, Loss: 1050764143.2356699\n",
      "Iteration 2056, Loss: 1024257545.5630603\n",
      "Iteration 2057, Loss: 1200732629.7912433\n",
      "Iteration 2058, Loss: 1104898424.7279885\n",
      "Iteration 2059, Loss: 1038747807.2512518\n",
      "Iteration 2060, Loss: 1048563632.6017172\n",
      "Iteration 2061, Loss: 1207172257.1104245\n",
      "Iteration 2062, Loss: 1142011053.6010056\n",
      "Iteration 2063, Loss: 1023913745.9770355\n",
      "Iteration 2064, Loss: 1181063681.2630517\n",
      "Iteration 2065, Loss: 1044144049.971959\n",
      "Iteration 2066, Loss: 1933731790.7212594\n",
      "Iteration 2067, Loss: 1761548700.5310056\n",
      "Iteration 2068, Loss: 983044050.4010859\n",
      "Iteration 2069, Loss: 1102185155.2210002\n",
      "Iteration 2070, Loss: 979046150.7606938\n",
      "Iteration 2071, Loss: 1429487661.950232\n",
      "Iteration 2072, Loss: 1332645890.264703\n",
      "Iteration 2073, Loss: 1282264361.03005\n",
      "Iteration 2074, Loss: 1109361571.2676113\n",
      "Iteration 2075, Loss: 1128323662.1640217\n",
      "Iteration 2076, Loss: 1083087609.201758\n",
      "Iteration 2077, Loss: 1046133523.4990807\n",
      "Iteration 2078, Loss: 1099011743.0154238\n",
      "Iteration 2079, Loss: 1170665496.7669392\n",
      "Iteration 2080, Loss: 945735838.0223024\n",
      "Iteration 2081, Loss: 1017696263.6424885\n",
      "Iteration 2082, Loss: 1095199086.787782\n",
      "Iteration 2083, Loss: 1069227023.4609339\n",
      "Iteration 2084, Loss: 1125144240.549983\n",
      "Iteration 2085, Loss: 1168825951.1725566\n",
      "Iteration 2086, Loss: 1034805546.2132297\n",
      "Iteration 2087, Loss: 1118525769.6713302\n",
      "Iteration 2088, Loss: 1214242543.1153092\n",
      "Iteration 2089, Loss: 1143431782.2332962\n",
      "Iteration 2090, Loss: 1110259317.6011887\n",
      "Iteration 2091, Loss: 1076414858.353757\n",
      "Iteration 2092, Loss: 1079799360.1976352\n",
      "Iteration 2093, Loss: 943159536.1568437\n",
      "Iteration 2094, Loss: 1229723323.8661528\n",
      "Iteration 2095, Loss: 1218125763.9113858\n",
      "Iteration 2096, Loss: 1191903565.03455\n",
      "Iteration 2097, Loss: 952481946.3812898\n",
      "Iteration 2098, Loss: 1016621747.2051316\n",
      "Iteration 2099, Loss: 974229233.7622927\n",
      "Iteration 2100, Loss: 1168418294.9297385\n",
      "Iteration 2101, Loss: 1178917433.1194048\n",
      "Iteration 2102, Loss: 1155376555.626855\n",
      "Iteration 2103, Loss: 1015249791.7301164\n",
      "Iteration 2104, Loss: 996290860.1775124\n",
      "Iteration 2105, Loss: 1006883453.4364309\n",
      "Iteration 2106, Loss: 1283628588.727613\n",
      "Iteration 2107, Loss: 1103783701.7110746\n",
      "Iteration 2108, Loss: 1097812812.8139644\n",
      "Iteration 2109, Loss: 1089097039.5402803\n",
      "Iteration 2110, Loss: 991084741.4680315\n",
      "Iteration 2111, Loss: 984942464.7232825\n",
      "Iteration 2112, Loss: 973600051.8400724\n",
      "Iteration 2113, Loss: 963730698.871307\n",
      "Iteration 2114, Loss: 1110079704.595555\n",
      "Iteration 2115, Loss: 962245995.0577402\n",
      "Iteration 2116, Loss: 951480556.7136226\n",
      "Iteration 2117, Loss: 1219287581.285775\n",
      "Iteration 2118, Loss: 1161831115.0190787\n",
      "Iteration 2119, Loss: 1023519643.1045045\n",
      "Iteration 2120, Loss: 1031706505.8061748\n",
      "Iteration 2121, Loss: 966204930.500076\n",
      "Iteration 2122, Loss: 947318891.2470684\n",
      "Iteration 2123, Loss: 964962315.0011185\n",
      "Iteration 2124, Loss: 945032642.6862031\n",
      "Iteration 2125, Loss: 1058072087.3095291\n",
      "Iteration 2126, Loss: 1151016775.9620721\n",
      "Iteration 2127, Loss: 1050139817.6911855\n",
      "Iteration 2128, Loss: 1040086854.3261764\n",
      "Iteration 2129, Loss: 1021657492.1069196\n",
      "Iteration 2130, Loss: 1038142286.5493859\n",
      "Iteration 2131, Loss: 1807989648.9372375\n",
      "Iteration 2132, Loss: 972175047.1820754\n",
      "Iteration 2133, Loss: 2415662603.930661\n",
      "Iteration 2134, Loss: 1577343667.13519\n",
      "Iteration 2135, Loss: 1503218429.6584501\n",
      "Iteration 2136, Loss: 1057087586.8391241\n",
      "Iteration 2137, Loss: 944837270.0654386\n",
      "Iteration 2138, Loss: 1258744355.2298565\n",
      "Iteration 2139, Loss: 1142817984.743692\n",
      "Iteration 2140, Loss: 1155760540.9161227\n",
      "Iteration 2141, Loss: 999267129.1237066\n",
      "Iteration 2142, Loss: 1146822579.5809996\n",
      "Iteration 2143, Loss: 1069071715.5452245\n",
      "Iteration 2144, Loss: 1034743383.3937352\n",
      "Iteration 2145, Loss: 1003387910.5023423\n",
      "Iteration 2146, Loss: 1005804521.5775617\n",
      "Iteration 2147, Loss: 961065087.2680843\n",
      "Iteration 2148, Loss: 1022960505.8491315\n",
      "Iteration 2149, Loss: 939229417.2138585\n",
      "Iteration 2150, Loss: 919882697.6162709\n",
      "Iteration 2151, Loss: 1728875136.9536002\n",
      "Iteration 2152, Loss: 1610144830.380321\n",
      "Iteration 2153, Loss: 1085585874.167141\n",
      "Iteration 2154, Loss: 1595499669.76171\n",
      "Iteration 2155, Loss: 936451827.1688586\n",
      "Iteration 2156, Loss: 922681055.8478475\n",
      "Iteration 2157, Loss: 917560764.6406296\n",
      "Iteration 2158, Loss: 904362364.3440577\n",
      "Iteration 2159, Loss: 1213804829.508784\n",
      "Iteration 2160, Loss: 935411891.4776919\n",
      "Iteration 2161, Loss: 918357941.1740998\n",
      "Iteration 2162, Loss: 920743038.6927675\n",
      "Iteration 2163, Loss: 1173670809.86706\n",
      "Iteration 2164, Loss: 984608668.65232\n",
      "Iteration 2165, Loss: 942888932.2216667\n",
      "Iteration 2166, Loss: 926118768.1082809\n",
      "Iteration 2167, Loss: 905200499.6415514\n",
      "Iteration 2168, Loss: 1345423839.4290013\n",
      "Iteration 2169, Loss: 1165015119.3355155\n",
      "Iteration 2170, Loss: 991200745.1627649\n",
      "Iteration 2171, Loss: 1000971825.9596845\n",
      "Iteration 2172, Loss: 1006590132.6387645\n",
      "Iteration 2173, Loss: 895730782.7497708\n",
      "Iteration 2174, Loss: 889211969.470756\n",
      "Iteration 2175, Loss: 1045424731.3662051\n",
      "Iteration 2176, Loss: 997862239.6280183\n",
      "Iteration 2177, Loss: 948515830.898213\n",
      "Iteration 2178, Loss: 992824263.4675813\n",
      "Iteration 2179, Loss: 1027739722.7064513\n",
      "Iteration 2180, Loss: 999463466.099169\n",
      "Iteration 2181, Loss: 1142037973.95952\n",
      "Iteration 2182, Loss: 1108868832.7926145\n",
      "Iteration 2183, Loss: 1111270392.9805129\n",
      "Iteration 2184, Loss: 1094392072.92724\n",
      "Iteration 2185, Loss: 1030366485.8491951\n",
      "Iteration 2186, Loss: 984544845.5969577\n",
      "Iteration 2187, Loss: 908720571.9870881\n",
      "Iteration 2188, Loss: 1089930215.9519975\n",
      "Iteration 2189, Loss: 970138951.0168439\n",
      "Iteration 2190, Loss: 972650768.9601998\n",
      "Iteration 2191, Loss: 954293710.0257404\n",
      "Iteration 2192, Loss: 938660045.444567\n",
      "Iteration 2193, Loss: 1084259227.9202392\n",
      "Iteration 2194, Loss: 976428962.5599539\n",
      "Iteration 2195, Loss: 975533471.9340092\n",
      "Iteration 2196, Loss: 891132874.0585881\n",
      "Iteration 2197, Loss: 895939411.7107283\n",
      "Iteration 2198, Loss: 898447545.5006804\n",
      "Iteration 2199, Loss: 999881913.2818155\n",
      "Iteration 2200, Loss: 1151410037.7334225\n",
      "Iteration 2201, Loss: 1108922171.3947701\n",
      "Iteration 2202, Loss: 1082351081.125284\n",
      "Iteration 2203, Loss: 1023012991.0974348\n",
      "Iteration 2204, Loss: 1087717281.9678242\n",
      "Iteration 2205, Loss: 1012815060.6198101\n",
      "Iteration 2206, Loss: 1021617583.6217129\n",
      "Iteration 2207, Loss: 1000026067.5741712\n",
      "Iteration 2208, Loss: 894661161.5777419\n",
      "Iteration 2209, Loss: 852817364.2336042\n",
      "Iteration 2210, Loss: 850683863.7584416\n",
      "Iteration 2211, Loss: 848578382.521214\n",
      "Iteration 2212, Loss: 837338737.7162592\n",
      "Iteration 2213, Loss: 835482548.0806764\n",
      "Iteration 2214, Loss: 824792958.4756682\n",
      "Iteration 2215, Loss: 820695404.3420924\n",
      "Iteration 2216, Loss: 1011648914.1728112\n",
      "Iteration 2217, Loss: 915116023.1079519\n",
      "Iteration 2218, Loss: 884780589.6357919\n",
      "Iteration 2219, Loss: 1100206330.9684343\n",
      "Iteration 2220, Loss: 1082823834.3079267\n",
      "Iteration 2221, Loss: 856989116.8134326\n",
      "Iteration 2222, Loss: 833162608.8095183\n",
      "Iteration 2223, Loss: 813513063.3391124\n",
      "Iteration 2224, Loss: 803763264.8977472\n",
      "Iteration 2225, Loss: 5178593435.301429\n",
      "Iteration 2226, Loss: 3544886356.6520076\n",
      "Iteration 2227, Loss: 2387891376.099176\n",
      "Iteration 2228, Loss: 2109840496.860947\n",
      "Iteration 2229, Loss: 1445141912.2240095\n",
      "Iteration 2230, Loss: 908098462.9848619\n",
      "Iteration 2231, Loss: 892463990.4614874\n",
      "Iteration 2232, Loss: 4020663119.825775\n",
      "Iteration 2233, Loss: 1509303473.4692743\n",
      "Iteration 2234, Loss: 882425778.6264873\n",
      "Iteration 2235, Loss: 893479576.9124434\n",
      "Iteration 2236, Loss: 904788254.6519634\n",
      "Iteration 2237, Loss: 5104720188.778764\n",
      "Iteration 2238, Loss: 4856809558.559655\n",
      "Iteration 2239, Loss: 3962154983.749904\n",
      "Iteration 2240, Loss: 1992056468.4467516\n",
      "Iteration 2241, Loss: 1412288748.7426178\n",
      "Iteration 2242, Loss: 1055518405.4647301\n",
      "Iteration 2243, Loss: 927346988.6835686\n",
      "Iteration 2244, Loss: 897229553.3251202\n",
      "Iteration 2245, Loss: 876925056.7015479\n",
      "Iteration 2246, Loss: 868511222.9146014\n",
      "Iteration 2247, Loss: 1029781181.1007878\n",
      "Iteration 2248, Loss: 1657413913.0333915\n",
      "Iteration 2249, Loss: 895989568.6446651\n",
      "Iteration 2250, Loss: 1556939305.3977132\n",
      "Iteration 2251, Loss: 931748885.1874231\n",
      "Iteration 2252, Loss: 2230753719.137605\n",
      "Iteration 2253, Loss: 985962903.0668917\n",
      "Iteration 2254, Loss: 3876469901.067967\n",
      "Iteration 2255, Loss: 2740262839.5210443\n",
      "Iteration 2256, Loss: 2695880627.1784973\n",
      "Iteration 2257, Loss: 2647362405.652678\n",
      "Iteration 2258, Loss: 2000841578.4337919\n",
      "Iteration 2259, Loss: 1328499424.6856651\n",
      "Iteration 2260, Loss: 1212045603.6278615\n",
      "Iteration 2261, Loss: 894332320.9691674\n",
      "Iteration 2262, Loss: 1371979249.5301275\n",
      "Iteration 2263, Loss: 1369472138.4921305\n",
      "Iteration 2264, Loss: 1215572315.4524202\n",
      "Iteration 2265, Loss: 898431132.990516\n",
      "Iteration 2266, Loss: 885064358.3882113\n",
      "Iteration 2267, Loss: 1224393607.927488\n",
      "Iteration 2268, Loss: 947289518.2886164\n",
      "Iteration 2269, Loss: 917626655.7128218\n",
      "Iteration 2270, Loss: 2768496141.423367\n",
      "Iteration 2271, Loss: 4140948680.745782\n",
      "Iteration 2272, Loss: 1892058020.483816\n",
      "Iteration 2273, Loss: 1666014957.9239922\n",
      "Iteration 2274, Loss: 924877709.1223733\n",
      "Iteration 2275, Loss: 914588732.935885\n",
      "Iteration 2276, Loss: 3579557586.895319\n",
      "Iteration 2277, Loss: 4549323041.585509\n",
      "Iteration 2278, Loss: 961490321.7392753\n",
      "Iteration 2279, Loss: 949735849.4093584\n",
      "Iteration 2280, Loss: 924724767.8187405\n",
      "Iteration 2281, Loss: 909252069.5658782\n",
      "Iteration 2282, Loss: 1080540053.6901255\n",
      "Iteration 2283, Loss: 1000193265.8672267\n",
      "Iteration 2284, Loss: 1058358580.3595283\n",
      "Iteration 2285, Loss: 1220759788.1059465\n",
      "Iteration 2286, Loss: 997504507.6004535\n",
      "Iteration 2287, Loss: 1257477745.1427858\n",
      "Iteration 2288, Loss: 1306234970.0080202\n",
      "Iteration 2289, Loss: 975654947.1332252\n",
      "Iteration 2290, Loss: 965751398.6555889\n",
      "Iteration 2291, Loss: 4167567671.710333\n",
      "Iteration 2292, Loss: 1291526506.9655104\n",
      "Iteration 2293, Loss: 993476645.5779679\n",
      "Iteration 2294, Loss: 980721985.4897201\n",
      "Iteration 2295, Loss: 969416276.1783983\n",
      "Iteration 2296, Loss: 957886133.1434045\n",
      "Iteration 2297, Loss: 1201531482.226471\n",
      "Iteration 2298, Loss: 1042005798.516291\n",
      "Iteration 2299, Loss: 1124062000.7933547\n",
      "Iteration 2300, Loss: 1092230731.8068736\n",
      "Iteration 2301, Loss: 1058264947.0410279\n",
      "Iteration 2302, Loss: 1103106396.9167957\n",
      "Iteration 2303, Loss: 1048344547.5947808\n",
      "Iteration 2304, Loss: 1054801851.5740477\n",
      "Iteration 2305, Loss: 1066987346.2128206\n",
      "Iteration 2306, Loss: 1092772875.9005754\n",
      "Iteration 2307, Loss: 1072244246.0419397\n",
      "Iteration 2308, Loss: 1133925418.6925309\n",
      "Iteration 2309, Loss: 1203525949.4290924\n",
      "Iteration 2310, Loss: 1195110238.3113585\n",
      "Iteration 2311, Loss: 1138722686.5288353\n",
      "Iteration 2312, Loss: 997507683.7227632\n",
      "Iteration 2313, Loss: 1017638212.1298355\n",
      "Iteration 2314, Loss: 948756856.3764583\n",
      "Iteration 2315, Loss: 1172413495.8390548\n",
      "Iteration 2316, Loss: 1180979368.7106671\n",
      "Iteration 2317, Loss: 1049075703.5994197\n",
      "Iteration 2318, Loss: 1151394103.2020595\n",
      "Iteration 2319, Loss: 1178079387.3956351\n",
      "Iteration 2320, Loss: 1146095373.778023\n",
      "Iteration 2321, Loss: 1113742210.8755784\n",
      "Iteration 2322, Loss: 967205684.6731077\n",
      "Iteration 2323, Loss: 1127815164.8402052\n",
      "Iteration 2324, Loss: 1008057306.7885013\n",
      "Iteration 2325, Loss: 1303861676.2785728\n",
      "Iteration 2326, Loss: 1139034126.0727246\n",
      "Iteration 2327, Loss: 1056212251.5449797\n",
      "Iteration 2328, Loss: 1066576287.2533424\n",
      "Iteration 2329, Loss: 970649428.1510286\n",
      "Iteration 2330, Loss: 970245869.857263\n",
      "Iteration 2331, Loss: 990170683.0160183\n",
      "Iteration 2332, Loss: 1215566456.5821157\n",
      "Iteration 2333, Loss: 1023004137.9433103\n",
      "Iteration 2334, Loss: 1005597007.8442712\n",
      "Iteration 2335, Loss: 1428337935.5184097\n",
      "Iteration 2336, Loss: 1336951062.482754\n",
      "Iteration 2337, Loss: 1000566647.0760128\n",
      "Iteration 2338, Loss: 4584764185.286046\n",
      "Iteration 2339, Loss: 1081930906.7330194\n",
      "Iteration 2340, Loss: 1204613797.0249002\n",
      "Iteration 2341, Loss: 1029616908.4527963\n",
      "Iteration 2342, Loss: 1527434914.3878586\n",
      "Iteration 2343, Loss: 1127646138.8032386\n",
      "Iteration 2344, Loss: 1149415792.522721\n",
      "Iteration 2345, Loss: 1186216598.2998242\n",
      "Iteration 2346, Loss: 1119991473.6321447\n",
      "Iteration 2347, Loss: 1235672012.0374734\n",
      "Iteration 2348, Loss: 1278821620.5081184\n",
      "Iteration 2349, Loss: 1200654598.0068452\n",
      "Iteration 2350, Loss: 1152755745.79335\n",
      "Iteration 2351, Loss: 1174962200.4129562\n",
      "Iteration 2352, Loss: 1156956679.3960931\n",
      "Iteration 2353, Loss: 1267977958.1314418\n",
      "Iteration 2354, Loss: 1307202293.10592\n",
      "Iteration 2355, Loss: 1339516955.7792687\n",
      "Iteration 2356, Loss: 1063826757.0303371\n",
      "Iteration 2357, Loss: 1446329014.127952\n",
      "Iteration 2358, Loss: 1428971011.7323303\n",
      "Iteration 2359, Loss: 1116092746.8127134\n",
      "Iteration 2360, Loss: 1200359491.4731057\n",
      "Iteration 2361, Loss: 1097936248.6299663\n",
      "Iteration 2362, Loss: 1093733353.491406\n",
      "Iteration 2363, Loss: 1140657652.8868976\n",
      "Iteration 2364, Loss: 1458169117.374743\n",
      "Iteration 2365, Loss: 1070599609.0985256\n",
      "Iteration 2366, Loss: 1181611033.2845254\n",
      "Iteration 2367, Loss: 1680757940.6495686\n",
      "Iteration 2368, Loss: 1618655185.314199\n",
      "Iteration 2369, Loss: 1206601753.3808131\n",
      "Iteration 2370, Loss: 1195349180.214672\n",
      "Iteration 2371, Loss: 1249901247.2699387\n",
      "Iteration 2372, Loss: 1237402191.2069066\n",
      "Iteration 2373, Loss: 1537137309.470049\n",
      "Iteration 2374, Loss: 1509723629.252121\n",
      "Iteration 2375, Loss: 1173833062.5695987\n",
      "Iteration 2376, Loss: 1159201752.8541532\n",
      "Iteration 2377, Loss: 1257768582.2662718\n",
      "Iteration 2378, Loss: 1319687901.7537556\n",
      "Iteration 2379, Loss: 1327446401.0526972\n",
      "Iteration 2380, Loss: 1214744274.354411\n",
      "Iteration 2381, Loss: 1273923540.3969955\n",
      "Iteration 2382, Loss: 1115414150.3463233\n",
      "Iteration 2383, Loss: 1071191040.7704747\n",
      "Iteration 2384, Loss: 1120343779.7587473\n",
      "Iteration 2385, Loss: 1999806773.6861076\n",
      "Iteration 2386, Loss: 1107184937.8202956\n",
      "Iteration 2387, Loss: 1141503446.1327753\n",
      "Iteration 2388, Loss: 1727925024.7185078\n",
      "Iteration 2389, Loss: 1702733697.6448905\n",
      "Iteration 2390, Loss: 1476767082.224276\n",
      "Iteration 2391, Loss: 1161930123.0274315\n",
      "Iteration 2392, Loss: 1351342441.059889\n",
      "Iteration 2393, Loss: 1402791886.6661491\n",
      "Iteration 2394, Loss: 1104080136.8918808\n",
      "Iteration 2395, Loss: 1478239932.0385635\n",
      "Iteration 2396, Loss: 1487134684.6389654\n",
      "Iteration 2397, Loss: 1171058440.8353946\n",
      "Iteration 2398, Loss: 1587774738.0678275\n",
      "Iteration 2399, Loss: 1205089613.6298823\n",
      "Iteration 2400, Loss: 1273711945.8804324\n",
      "Iteration 2401, Loss: 1400512544.8817766\n",
      "Iteration 2402, Loss: 1268128046.4089744\n",
      "Iteration 2403, Loss: 1140928436.7647417\n",
      "Iteration 2404, Loss: 1516614912.5799916\n",
      "Iteration 2405, Loss: 1545170912.9191723\n",
      "Iteration 2406, Loss: 1532753190.4883358\n",
      "Iteration 2407, Loss: 1543374449.3962023\n",
      "Iteration 2408, Loss: 1547485020.2702808\n",
      "Iteration 2409, Loss: 1540959024.503683\n",
      "Iteration 2410, Loss: 1214689243.3384027\n",
      "Iteration 2411, Loss: 1107819502.025133\n",
      "Iteration 2412, Loss: 1360166958.4719892\n",
      "Iteration 2413, Loss: 1366471096.467317\n",
      "Iteration 2414, Loss: 1239177001.8920767\n",
      "Iteration 2415, Loss: 1198242913.5520103\n",
      "Iteration 2416, Loss: 1234354338.5344522\n",
      "Iteration 2417, Loss: 1145912479.1151168\n",
      "Iteration 2418, Loss: 1224826381.220058\n",
      "Iteration 2419, Loss: 1094932407.10936\n",
      "Iteration 2420, Loss: 1560360767.0839508\n",
      "Iteration 2421, Loss: 1557107314.460105\n",
      "Iteration 2422, Loss: 1460140322.91355\n",
      "Iteration 2423, Loss: 1165536475.988049\n",
      "Iteration 2424, Loss: 5658230586.463182\n",
      "Iteration 2425, Loss: 1220678485.366957\n",
      "Iteration 2426, Loss: 1301317491.895575\n",
      "Iteration 2427, Loss: 1239211787.8585284\n",
      "Iteration 2428, Loss: 1222870063.9505172\n",
      "Iteration 2429, Loss: 1258208754.913623\n",
      "Iteration 2430, Loss: 1306338037.5938847\n",
      "Iteration 2431, Loss: 1345431984.1662128\n",
      "Iteration 2432, Loss: 1252225185.5830784\n",
      "Iteration 2433, Loss: 1292177430.8699868\n",
      "Iteration 2434, Loss: 1336474622.784403\n",
      "Iteration 2435, Loss: 1092206578.209754\n",
      "Iteration 2436, Loss: 1075914480.3670394\n",
      "Iteration 2437, Loss: 1564526078.2742453\n",
      "Iteration 2438, Loss: 1163159722.8614926\n",
      "Iteration 2439, Loss: 1311619578.8046525\n",
      "Iteration 2440, Loss: 1308087070.4647338\n",
      "Iteration 2441, Loss: 1287209658.6876268\n",
      "Iteration 2442, Loss: 1280481466.692554\n",
      "Iteration 2443, Loss: 1363912757.3002238\n",
      "Iteration 2444, Loss: 1349514270.0262547\n",
      "Iteration 2445, Loss: 1328952305.8070807\n",
      "Iteration 2446, Loss: 1072961131.5973592\n",
      "Iteration 2447, Loss: 1321043599.893984\n",
      "Iteration 2448, Loss: 1303243006.116329\n",
      "Iteration 2449, Loss: 1328377417.6431887\n",
      "Iteration 2450, Loss: 1125603688.898599\n",
      "Iteration 2451, Loss: 1168410268.8338184\n",
      "Iteration 2452, Loss: 1196843492.9422433\n",
      "Iteration 2453, Loss: 1191652246.6438103\n",
      "Iteration 2454, Loss: 1167822218.372103\n",
      "Iteration 2455, Loss: 1194785614.9396296\n",
      "Iteration 2456, Loss: 1298908482.3290055\n",
      "Iteration 2457, Loss: 1191753231.5430162\n",
      "Iteration 2458, Loss: 1195631004.4897175\n",
      "Iteration 2459, Loss: 1151662278.6387165\n",
      "Iteration 2460, Loss: 1139440936.752606\n",
      "Iteration 2461, Loss: 1200081918.4568222\n",
      "Iteration 2462, Loss: 1193318139.0854447\n",
      "Iteration 2463, Loss: 1151813861.1940193\n",
      "Iteration 2464, Loss: 1194394247.0388029\n",
      "Iteration 2465, Loss: 1692871327.4197404\n",
      "Iteration 2466, Loss: 1758250052.548461\n",
      "Iteration 2467, Loss: 1121206422.9478042\n",
      "Iteration 2468, Loss: 1255598246.722817\n",
      "Iteration 2469, Loss: 1164955612.2990086\n",
      "Iteration 2470, Loss: 1218049947.7825792\n",
      "Iteration 2471, Loss: 1158120829.279047\n",
      "Iteration 2472, Loss: 1252572517.7951694\n",
      "Iteration 2473, Loss: 1231466712.1560175\n",
      "Iteration 2474, Loss: 1302653658.2459202\n",
      "Iteration 2475, Loss: 1179586247.8084009\n",
      "Iteration 2476, Loss: 1315151944.0958443\n",
      "Iteration 2477, Loss: 1272914172.9284961\n",
      "Iteration 2478, Loss: 1316015027.0700288\n",
      "Iteration 2479, Loss: 1302977799.4489064\n",
      "Iteration 2480, Loss: 1254163822.8766882\n",
      "Iteration 2481, Loss: 1522602454.2316005\n",
      "Iteration 2482, Loss: 1246480818.1141808\n",
      "Iteration 2483, Loss: 1135991427.9978743\n",
      "Iteration 2484, Loss: 1140448206.2353556\n",
      "Iteration 2485, Loss: 1222451159.6079988\n",
      "Iteration 2486, Loss: 1268279451.1604207\n",
      "Iteration 2487, Loss: 1263707159.646\n",
      "Iteration 2488, Loss: 1163982046.685164\n",
      "Iteration 2489, Loss: 1197477580.6063704\n",
      "Iteration 2490, Loss: 1310491971.6867208\n",
      "Iteration 2491, Loss: 1441676247.8070652\n",
      "Iteration 2492, Loss: 1424406463.4215767\n",
      "Iteration 2493, Loss: 1411347787.6363573\n",
      "Iteration 2494, Loss: 1465113951.8439097\n",
      "Iteration 2495, Loss: 1182859336.797093\n",
      "Iteration 2496, Loss: 1160370774.6764014\n",
      "Iteration 2497, Loss: 1076669319.0256789\n",
      "Iteration 2498, Loss: 1184306348.0830417\n",
      "Iteration 2499, Loss: 1182324381.28947\n",
      "Iteration 2500, Loss: 1222327849.905971\n",
      "Iteration 2501, Loss: 1185683015.4994347\n",
      "Iteration 2502, Loss: 1756397510.2709484\n",
      "Iteration 2503, Loss: 1397207710.6360054\n",
      "Iteration 2504, Loss: 1128112599.401182\n",
      "Iteration 2505, Loss: 2310099205.1195264\n",
      "Iteration 2506, Loss: 2154104702.588577\n",
      "Iteration 2507, Loss: 2114526230.7001944\n",
      "Iteration 2508, Loss: 1612128427.279776\n",
      "Iteration 2509, Loss: 1606393930.422802\n",
      "Iteration 2510, Loss: 1244177027.3052952\n",
      "Iteration 2511, Loss: 1320227621.7019339\n",
      "Iteration 2512, Loss: 1130770584.051546\n",
      "Iteration 2513, Loss: 2213493667.0867276\n",
      "Iteration 2514, Loss: 2317614057.2631917\n",
      "Iteration 2515, Loss: 2229341826.800084\n",
      "Iteration 2516, Loss: 1238309521.4365854\n",
      "Iteration 2517, Loss: 1231353067.702786\n",
      "Iteration 2518, Loss: 1134131998.2443433\n",
      "Iteration 2519, Loss: 1197534876.2009625\n",
      "Iteration 2520, Loss: 1256085473.4380224\n",
      "Iteration 2521, Loss: 1254885509.7341087\n",
      "Iteration 2522, Loss: 1222879007.068619\n",
      "Iteration 2523, Loss: 1299496560.1967356\n",
      "Iteration 2524, Loss: 1505456555.65959\n",
      "Iteration 2525, Loss: 1242743744.289567\n",
      "Iteration 2526, Loss: 1281840213.0230873\n",
      "Iteration 2527, Loss: 1216075948.4348128\n",
      "Iteration 2528, Loss: 1286688927.7719736\n",
      "Iteration 2529, Loss: 1387723173.9532201\n",
      "Iteration 2530, Loss: 1346384444.3477087\n",
      "Iteration 2531, Loss: 1145564808.1940339\n",
      "Iteration 2532, Loss: 1512203863.4242399\n",
      "Iteration 2533, Loss: 1547802529.5080547\n",
      "Iteration 2534, Loss: 1224704062.1480048\n",
      "Iteration 2535, Loss: 1375360140.1473386\n",
      "Iteration 2536, Loss: 1469850458.2394228\n",
      "Iteration 2537, Loss: 1535774027.8439565\n",
      "Iteration 2538, Loss: 1262030274.1883886\n",
      "Iteration 2539, Loss: 1524147116.4319422\n",
      "Iteration 2540, Loss: 1528886974.8151355\n",
      "Iteration 2541, Loss: 1560440958.649322\n",
      "Iteration 2542, Loss: 1266050116.339198\n",
      "Iteration 2543, Loss: 1307730853.542708\n",
      "Iteration 2544, Loss: 1356704314.3881261\n",
      "Iteration 2545, Loss: 1366765567.395718\n",
      "Iteration 2546, Loss: 1256172594.2771816\n",
      "Iteration 2547, Loss: 1369383370.0180645\n",
      "Iteration 2548, Loss: 1121384643.812774\n",
      "Iteration 2549, Loss: 1478386459.0769925\n",
      "Iteration 2550, Loss: 1131414379.5055547\n",
      "Iteration 2551, Loss: 1652568471.661315\n",
      "Iteration 2552, Loss: 1158909994.5546005\n",
      "Iteration 2553, Loss: 1241604393.639425\n",
      "Iteration 2554, Loss: 1330164831.6297345\n",
      "Iteration 2555, Loss: 1217198023.0674858\n",
      "Iteration 2556, Loss: 1331764094.0208433\n",
      "Iteration 2557, Loss: 1602669515.606505\n",
      "Iteration 2558, Loss: 1365873168.7336357\n",
      "Iteration 2559, Loss: 1278625379.5693877\n",
      "Iteration 2560, Loss: 1185731776.694735\n",
      "Iteration 2561, Loss: 1209025506.6079907\n",
      "Iteration 2562, Loss: 1215229099.6195118\n",
      "Iteration 2563, Loss: 1361014429.069927\n",
      "Iteration 2564, Loss: 1480000864.141739\n",
      "Iteration 2565, Loss: 1380807890.539259\n",
      "Iteration 2566, Loss: 1187129279.6193109\n",
      "Iteration 2567, Loss: 1150607516.5871892\n",
      "Iteration 2568, Loss: 2414065849.383294\n",
      "Iteration 2569, Loss: 1462589150.1760364\n",
      "Iteration 2570, Loss: 1480152909.843355\n",
      "Iteration 2571, Loss: 1486278381.6985044\n",
      "Iteration 2572, Loss: 1348722032.6941495\n",
      "Iteration 2573, Loss: 1336363678.5306597\n",
      "Iteration 2574, Loss: 1394637458.5388212\n",
      "Iteration 2575, Loss: 1366912419.3810425\n",
      "Iteration 2576, Loss: 1446656126.8085225\n",
      "Iteration 2577, Loss: 1552165762.6676342\n",
      "Iteration 2578, Loss: 1557980489.9210153\n",
      "Iteration 2579, Loss: 1214443235.7233477\n",
      "Iteration 2580, Loss: 1345736226.57061\n",
      "Iteration 2581, Loss: 1487281416.5379825\n",
      "Iteration 2582, Loss: 1504211323.499333\n",
      "Iteration 2583, Loss: 1252858935.5055337\n",
      "Iteration 2584, Loss: 2775928669.895408\n",
      "Iteration 2585, Loss: 2508838959.6960793\n",
      "Iteration 2586, Loss: 2375709631.764291\n",
      "Iteration 2587, Loss: 1245905478.24963\n",
      "Iteration 2588, Loss: 1702713685.1905596\n",
      "Iteration 2589, Loss: 1663345034.5464826\n",
      "Iteration 2590, Loss: 1477886559.5428555\n",
      "Iteration 2591, Loss: 1405484818.5516753\n",
      "Iteration 2592, Loss: 1268933156.9593506\n",
      "Iteration 2593, Loss: 2351908514.1509\n",
      "Iteration 2594, Loss: 1880203312.0015016\n",
      "Iteration 2595, Loss: 1327183715.9625475\n",
      "Iteration 2596, Loss: 1358508442.3138402\n",
      "Iteration 2597, Loss: 1392550506.5771275\n",
      "Iteration 2598, Loss: 1385261689.194835\n",
      "Iteration 2599, Loss: 1536251226.210274\n",
      "Iteration 2600, Loss: 1574437459.9365623\n",
      "Iteration 2601, Loss: 1532891966.7907877\n",
      "Iteration 2602, Loss: 1402815873.244754\n",
      "Iteration 2603, Loss: 1363156582.0290546\n",
      "Iteration 2604, Loss: 1466280512.5558877\n",
      "Iteration 2605, Loss: 1543380430.5873864\n",
      "Iteration 2606, Loss: 1462349365.3142977\n",
      "Iteration 2607, Loss: 1281000415.1003823\n",
      "Iteration 2608, Loss: 1256115269.409797\n",
      "Iteration 2609, Loss: 1264062794.6029363\n",
      "Iteration 2610, Loss: 1268892991.2175198\n",
      "Iteration 2611, Loss: 1532951464.494938\n",
      "Iteration 2612, Loss: 1260724580.3297052\n",
      "Iteration 2613, Loss: 1428419154.1702518\n",
      "Iteration 2614, Loss: 1562528252.723306\n",
      "Iteration 2615, Loss: 1371344944.4805079\n",
      "Iteration 2616, Loss: 1309421984.6725993\n",
      "Iteration 2617, Loss: 1623874684.883284\n",
      "Iteration 2618, Loss: 1418288268.742752\n",
      "Iteration 2619, Loss: 1539260358.9343977\n",
      "Iteration 2620, Loss: 1423170851.40502\n",
      "Iteration 2621, Loss: 1449651315.0999885\n",
      "Iteration 2622, Loss: 1568636145.9334812\n",
      "Iteration 2623, Loss: 1592051294.6252167\n",
      "Iteration 2624, Loss: 1523716521.74147\n",
      "Iteration 2625, Loss: 1356338506.5801377\n",
      "Iteration 2626, Loss: 1301417023.8732893\n",
      "Iteration 2627, Loss: 1265850741.793798\n",
      "Iteration 2628, Loss: 1337965335.092166\n",
      "Iteration 2629, Loss: 1428053016.0531747\n",
      "Iteration 2630, Loss: 1582703387.0832603\n",
      "Iteration 2631, Loss: 1614260417.3545747\n",
      "Iteration 2632, Loss: 1702167928.4700046\n",
      "Iteration 2633, Loss: 1234442222.9341335\n",
      "Iteration 2634, Loss: 1219350430.3796175\n",
      "Iteration 2635, Loss: 1223358343.9633188\n",
      "Iteration 2636, Loss: 1233843428.865946\n",
      "Iteration 2637, Loss: 1298304887.863102\n",
      "Iteration 2638, Loss: 1199842798.9287643\n",
      "Iteration 2639, Loss: 1266851325.8456843\n",
      "Iteration 2640, Loss: 1184472019.7138138\n",
      "Iteration 2641, Loss: 1459457567.4579682\n",
      "Iteration 2642, Loss: 1489592319.3327909\n",
      "Iteration 2643, Loss: 1365862379.2580676\n",
      "Iteration 2644, Loss: 1377246626.876806\n",
      "Iteration 2645, Loss: 1382829950.5441837\n",
      "Iteration 2646, Loss: 1405808856.8201873\n",
      "Iteration 2647, Loss: 1346769667.5110698\n",
      "Iteration 2648, Loss: 1293373063.898147\n",
      "Iteration 2649, Loss: 1372405516.623999\n",
      "Iteration 2650, Loss: 1382302639.159928\n",
      "Iteration 2651, Loss: 1386352524.2645237\n",
      "Iteration 2652, Loss: 1385025689.2915843\n",
      "Iteration 2653, Loss: 1294172813.658753\n",
      "Iteration 2654, Loss: 1226932728.1600723\n",
      "Iteration 2655, Loss: 1382444252.28858\n",
      "Iteration 2656, Loss: 1320634208.3116271\n",
      "Iteration 2657, Loss: 1372468591.801548\n",
      "Iteration 2658, Loss: 1309582993.5534873\n",
      "Iteration 2659, Loss: 1214956906.011205\n",
      "Iteration 2660, Loss: 1230355708.1185648\n",
      "Iteration 2661, Loss: 1267199711.5561018\n",
      "Iteration 2662, Loss: 1263436571.5980983\n",
      "Iteration 2663, Loss: 1332605190.7644708\n",
      "Iteration 2664, Loss: 1317923341.9500885\n",
      "Iteration 2665, Loss: 1375876104.7296402\n",
      "Iteration 2666, Loss: 1405885207.1967525\n",
      "Iteration 2667, Loss: 1080991007.7301638\n",
      "Iteration 2668, Loss: 2403222539.313942\n",
      "Iteration 2669, Loss: 1122774256.093336\n",
      "Iteration 2670, Loss: 1121858384.9664924\n",
      "Iteration 2671, Loss: 1239055412.0490985\n",
      "Iteration 2672, Loss: 1305686500.445035\n",
      "Iteration 2673, Loss: 1379999196.7960994\n",
      "Iteration 2674, Loss: 1119598306.663373\n",
      "Iteration 2675, Loss: 1423357490.1479623\n",
      "Iteration 2676, Loss: 1268179173.5481505\n",
      "Iteration 2677, Loss: 1264520379.261146\n",
      "Iteration 2678, Loss: 1284464400.8728082\n",
      "Iteration 2679, Loss: 1207716292.8606763\n",
      "Iteration 2680, Loss: 1319235986.1843765\n",
      "Iteration 2681, Loss: 1116388058.319394\n",
      "Iteration 2682, Loss: 1107981785.229011\n",
      "Iteration 2683, Loss: 1142986067.6732748\n",
      "Iteration 2684, Loss: 1258174561.2901852\n",
      "Iteration 2685, Loss: 1179783020.89691\n",
      "Iteration 2686, Loss: 1285369434.1144204\n",
      "Iteration 2687, Loss: 1268078012.525118\n",
      "Iteration 2688, Loss: 1082132126.7045507\n",
      "Iteration 2689, Loss: 1657467745.5866823\n",
      "Iteration 2690, Loss: 1164361777.904587\n",
      "Iteration 2691, Loss: 1957610215.6894028\n",
      "Iteration 2692, Loss: 2042834365.1247973\n",
      "Iteration 2693, Loss: 1983773488.3507183\n",
      "Iteration 2694, Loss: 1567055450.9038496\n",
      "Iteration 2695, Loss: 1122721865.5015824\n",
      "Iteration 2696, Loss: 1464574221.8675659\n",
      "Iteration 2697, Loss: 1129795458.7948086\n",
      "Iteration 2698, Loss: 1191276701.859423\n",
      "Iteration 2699, Loss: 1337516543.1468475\n",
      "Iteration 2700, Loss: 1431845947.014865\n",
      "Iteration 2701, Loss: 1245806798.3799322\n",
      "Iteration 2702, Loss: 1250099192.0467818\n",
      "Iteration 2703, Loss: 1330219054.289853\n",
      "Iteration 2704, Loss: 1264505151.79576\n",
      "Iteration 2705, Loss: 1121033753.9504642\n",
      "Iteration 2706, Loss: 1316334551.006006\n",
      "Iteration 2707, Loss: 1327159477.6558647\n",
      "Iteration 2708, Loss: 1419795873.4895337\n",
      "Iteration 2709, Loss: 1277480445.9427607\n",
      "Iteration 2710, Loss: 1292247978.0697627\n",
      "Iteration 2711, Loss: 1292212170.8647442\n",
      "Iteration 2712, Loss: 1208266189.4145455\n",
      "Iteration 2713, Loss: 1205311475.3942478\n",
      "Iteration 2714, Loss: 1304280827.368318\n",
      "Iteration 2715, Loss: 1585569554.6982157\n",
      "Iteration 2716, Loss: 1537136600.7901561\n",
      "Iteration 2717, Loss: 1215000914.3430514\n",
      "Iteration 2718, Loss: 1211685941.2697816\n",
      "Iteration 2719, Loss: 1331683670.417146\n",
      "Iteration 2720, Loss: 1352307220.5307019\n",
      "Iteration 2721, Loss: 1462732070.2176673\n",
      "Iteration 2722, Loss: 1530232224.1084678\n",
      "Iteration 2723, Loss: 1380621478.5567234\n",
      "Iteration 2724, Loss: 1423128695.180914\n",
      "Iteration 2725, Loss: 1255272077.4005044\n",
      "Iteration 2726, Loss: 1671815862.7069054\n",
      "Iteration 2727, Loss: 1223329290.7345684\n",
      "Iteration 2728, Loss: 1385424556.2326846\n",
      "Iteration 2729, Loss: 1171904728.1539884\n",
      "Iteration 2730, Loss: 1173301102.7924957\n",
      "Iteration 2731, Loss: 1182773429.3696582\n",
      "Iteration 2732, Loss: 1256022860.3598108\n",
      "Iteration 2733, Loss: 1373192043.40594\n",
      "Iteration 2734, Loss: 1416276178.9343503\n",
      "Iteration 2735, Loss: 1242459733.1447997\n",
      "Iteration 2736, Loss: 1250409224.9750228\n",
      "Iteration 2737, Loss: 1278008173.0634475\n",
      "Iteration 2738, Loss: 1163074481.9080434\n",
      "Iteration 2739, Loss: 1173377332.3652577\n",
      "Iteration 2740, Loss: 1883894172.330528\n",
      "Iteration 2741, Loss: 1794903202.0247777\n",
      "Iteration 2742, Loss: 1812284517.9539797\n",
      "Iteration 2743, Loss: 1839735994.8437383\n",
      "Iteration 2744, Loss: 1707505000.7652314\n",
      "Iteration 2745, Loss: 1700962619.4504743\n",
      "Iteration 2746, Loss: 1665372107.3268607\n",
      "Iteration 2747, Loss: 1162658059.0459657\n",
      "Iteration 2748, Loss: 1181824219.7606728\n",
      "Iteration 2749, Loss: 1911179766.1806455\n",
      "Iteration 2750, Loss: 1998714440.57723\n",
      "Iteration 2751, Loss: 1125435052.1875362\n",
      "Iteration 2752, Loss: 1410104011.1012642\n",
      "Iteration 2753, Loss: 1199330884.725154\n",
      "Iteration 2754, Loss: 1293838853.559435\n",
      "Iteration 2755, Loss: 1355636344.8730834\n",
      "Iteration 2756, Loss: 1360631250.38058\n",
      "Iteration 2757, Loss: 1353689052.1167872\n",
      "Iteration 2758, Loss: 1351338940.2541282\n",
      "Iteration 2759, Loss: 1426536231.4224327\n",
      "Iteration 2760, Loss: 1416906394.913108\n",
      "Iteration 2761, Loss: 1130655542.2845392\n",
      "Iteration 2762, Loss: 1588225846.6484509\n",
      "Iteration 2763, Loss: 1140777642.1258945\n",
      "Iteration 2764, Loss: 1125116036.8238308\n",
      "Iteration 2765, Loss: 1385988840.7980895\n",
      "Iteration 2766, Loss: 1362118907.4209573\n",
      "Iteration 2767, Loss: 1192089083.4813573\n",
      "Iteration 2768, Loss: 1221017941.1651225\n",
      "Iteration 2769, Loss: 1115113260.7619627\n",
      "Iteration 2770, Loss: 1101991686.958826\n",
      "Iteration 2771, Loss: 1223226320.919702\n",
      "Iteration 2772, Loss: 1345066826.850445\n",
      "Iteration 2773, Loss: 1334974408.786844\n",
      "Iteration 2774, Loss: 1408471651.4305305\n",
      "Iteration 2775, Loss: 1165535413.1163926\n",
      "Iteration 2776, Loss: 1551513505.6531916\n",
      "Iteration 2777, Loss: 1252585632.7891247\n",
      "Iteration 2778, Loss: 1929562508.1991334\n",
      "Iteration 2779, Loss: 1289247555.4294565\n",
      "Iteration 2780, Loss: 1309835326.73621\n",
      "Iteration 2781, Loss: 1434540175.4235349\n",
      "Iteration 2782, Loss: 1316621368.9813695\n",
      "Iteration 2783, Loss: 1314621519.1930904\n",
      "Iteration 2784, Loss: 1317318207.6250148\n",
      "Iteration 2785, Loss: 1315539626.7960145\n",
      "Iteration 2786, Loss: 1189412561.2255116\n",
      "Iteration 2787, Loss: 1122225890.045397\n",
      "Iteration 2788, Loss: 1231759900.4495852\n",
      "Iteration 2789, Loss: 1216045768.8970249\n",
      "Iteration 2790, Loss: 1262789447.1714306\n",
      "Iteration 2791, Loss: 1224158161.0749989\n",
      "Iteration 2792, Loss: 1349451350.0962179\n",
      "Iteration 2793, Loss: 1345125971.7695992\n",
      "Iteration 2794, Loss: 1127586204.300592\n",
      "Iteration 2795, Loss: 4340416384.400936\n",
      "Iteration 2796, Loss: 3689161550.522696\n",
      "Iteration 2797, Loss: 1761988780.447096\n",
      "Iteration 2798, Loss: 2782577145.807939\n",
      "Iteration 2799, Loss: 2214138071.459815\n",
      "Iteration 2800, Loss: 2148689918.1725254\n",
      "Iteration 2801, Loss: 1379747503.0814161\n",
      "Iteration 2802, Loss: 1172393003.8082755\n",
      "Iteration 2803, Loss: 1695642900.8332162\n",
      "Iteration 2804, Loss: 1242434208.1785548\n",
      "Iteration 2805, Loss: 1326363183.4467523\n",
      "Iteration 2806, Loss: 1403024820.847318\n",
      "Iteration 2807, Loss: 1414122913.834129\n",
      "Iteration 2808, Loss: 1513331023.9348838\n",
      "Iteration 2809, Loss: 1597722506.5403874\n",
      "Iteration 2810, Loss: 1245181288.1753929\n",
      "Iteration 2811, Loss: 1427995902.6421719\n",
      "Iteration 2812, Loss: 1477307313.7167392\n",
      "Iteration 2813, Loss: 1543669880.9158452\n",
      "Iteration 2814, Loss: 1609253505.0349216\n",
      "Iteration 2815, Loss: 1348414955.286112\n",
      "Iteration 2816, Loss: 1176618325.6998394\n",
      "Iteration 2817, Loss: 1177843027.4664848\n",
      "Iteration 2818, Loss: 1308311463.3098946\n",
      "Iteration 2819, Loss: 1372638445.523798\n",
      "Iteration 2820, Loss: 1469423795.1777995\n",
      "Iteration 2821, Loss: 1440508216.7916727\n",
      "Iteration 2822, Loss: 1531775091.3638341\n",
      "Iteration 2823, Loss: 1277731381.762508\n",
      "Iteration 2824, Loss: 1288222378.3242102\n",
      "Iteration 2825, Loss: 1260673103.527496\n",
      "Iteration 2826, Loss: 1394341316.9264297\n",
      "Iteration 2827, Loss: 1454130903.2645197\n",
      "Iteration 2828, Loss: 1175638901.7319148\n",
      "Iteration 2829, Loss: 1137404273.486199\n",
      "Iteration 2830, Loss: 1246309311.844727\n",
      "Iteration 2831, Loss: 1332336343.6418397\n",
      "Iteration 2832, Loss: 1249745217.3350472\n",
      "Iteration 2833, Loss: 1244257400.1005921\n",
      "Iteration 2834, Loss: 1242258823.0346186\n",
      "Iteration 2835, Loss: 1291618128.4811707\n",
      "Iteration 2836, Loss: 1140607731.2541318\n",
      "Iteration 2837, Loss: 1222153449.4969783\n",
      "Iteration 2838, Loss: 1277889953.230317\n",
      "Iteration 2839, Loss: 1309875739.1882935\n",
      "Iteration 2840, Loss: 1390244910.9831905\n",
      "Iteration 2841, Loss: 1381270663.201621\n",
      "Iteration 2842, Loss: 1192002407.4341097\n",
      "Iteration 2843, Loss: 1146455913.7198856\n",
      "Iteration 2844, Loss: 1410413536.6070771\n",
      "Iteration 2845, Loss: 1414430837.611103\n",
      "Iteration 2846, Loss: 1420508439.082601\n",
      "Iteration 2847, Loss: 1160881821.9407437\n",
      "Iteration 2848, Loss: 1099712917.905225\n",
      "Iteration 2849, Loss: 1360252892.174457\n",
      "Iteration 2850, Loss: 1351872525.0596814\n",
      "Iteration 2851, Loss: 1337638292.1421332\n",
      "Iteration 2852, Loss: 1323048589.591496\n",
      "Iteration 2853, Loss: 1370264515.2601607\n",
      "Iteration 2854, Loss: 1066954307.7473508\n",
      "Iteration 2855, Loss: 1105252211.161056\n",
      "Iteration 2856, Loss: 1361380198.4171515\n",
      "Iteration 2857, Loss: 1310881764.2224808\n",
      "Iteration 2858, Loss: 1150558273.9568474\n",
      "Iteration 2859, Loss: 1231289151.6401982\n",
      "Iteration 2860, Loss: 1226998556.0175424\n",
      "Iteration 2861, Loss: 1075683004.6042645\n",
      "Iteration 2862, Loss: 1063815351.6458538\n",
      "Iteration 2863, Loss: 1055682256.3479329\n",
      "Iteration 2864, Loss: 1043930790.2171551\n",
      "Iteration 2865, Loss: 1287418504.0486722\n",
      "Iteration 2866, Loss: 1311522651.2096696\n",
      "Iteration 2867, Loss: 1350716437.9703176\n",
      "Iteration 2868, Loss: 1307327993.4142404\n",
      "Iteration 2869, Loss: 1293560091.4453592\n",
      "Iteration 2870, Loss: 1038596521.7820728\n",
      "Iteration 2871, Loss: 1319670784.0959888\n",
      "Iteration 2872, Loss: 1098166446.97726\n",
      "Iteration 2873, Loss: 1130841603.0341432\n",
      "Iteration 2874, Loss: 1316347806.5983262\n",
      "Iteration 2875, Loss: 1235449103.7448206\n",
      "Iteration 2876, Loss: 1107582765.9721224\n",
      "Iteration 2877, Loss: 1099146793.3441575\n",
      "Iteration 2878, Loss: 1133112532.5026155\n",
      "Iteration 2879, Loss: 1188279805.5089922\n",
      "Iteration 2880, Loss: 1175741569.8830671\n",
      "Iteration 2881, Loss: 1297967660.503839\n",
      "Iteration 2882, Loss: 1082931570.3100576\n",
      "Iteration 2883, Loss: 1082343532.4773607\n",
      "Iteration 2884, Loss: 1111450544.4836538\n",
      "Iteration 2885, Loss: 1138675023.4148579\n",
      "Iteration 2886, Loss: 1126869368.796286\n",
      "Iteration 2887, Loss: 1158188487.13392\n",
      "Iteration 2888, Loss: 1244344627.5165222\n",
      "Iteration 2889, Loss: 1236233235.2814581\n",
      "Iteration 2890, Loss: 1092298611.1285238\n",
      "Iteration 2891, Loss: 1064713192.329461\n",
      "Iteration 2892, Loss: 1091587416.7497225\n",
      "Iteration 2893, Loss: 1192901480.5929918\n",
      "Iteration 2894, Loss: 1111884046.2081368\n",
      "Iteration 2895, Loss: 1897844344.1224055\n",
      "Iteration 2896, Loss: 1850943728.518631\n",
      "Iteration 2897, Loss: 1801094647.0875137\n",
      "Iteration 2898, Loss: 1696140305.4075727\n",
      "Iteration 2899, Loss: 1632550423.8274224\n",
      "Iteration 2900, Loss: 1527462558.420964\n",
      "Iteration 2901, Loss: 1437928352.4149697\n",
      "Iteration 2902, Loss: 1006325666.7855387\n",
      "Iteration 2903, Loss: 1019230295.1780676\n",
      "Iteration 2904, Loss: 1003449602.7223376\n",
      "Iteration 2905, Loss: 1084854862.417321\n",
      "Iteration 2906, Loss: 1101453182.801271\n",
      "Iteration 2907, Loss: 1189521884.169808\n",
      "Iteration 2908, Loss: 1176303466.4355083\n",
      "Iteration 2909, Loss: 1203109117.2820091\n",
      "Iteration 2910, Loss: 1164917395.74253\n",
      "Iteration 2911, Loss: 1091531544.0846252\n",
      "Iteration 2912, Loss: 1066900161.9987948\n",
      "Iteration 2913, Loss: 1045093749.2595806\n",
      "Iteration 2914, Loss: 1025857905.755031\n",
      "Iteration 2915, Loss: 1059909490.0484416\n",
      "Iteration 2916, Loss: 1079817262.3099072\n",
      "Iteration 2917, Loss: 1159453180.7137141\n",
      "Iteration 2918, Loss: 1061622727.6384078\n",
      "Iteration 2919, Loss: 1051526938.9649372\n",
      "Iteration 2920, Loss: 1019813811.6381907\n",
      "Iteration 2921, Loss: 1432892887.8477683\n",
      "Iteration 2922, Loss: 1007772196.1481112\n",
      "Iteration 2923, Loss: 1448119406.9516082\n",
      "Iteration 2924, Loss: 1094816463.1143866\n",
      "Iteration 2925, Loss: 1050524298.4645624\n",
      "Iteration 2926, Loss: 1024645050.239195\n",
      "Iteration 2927, Loss: 3861186753.687022\n",
      "Iteration 2928, Loss: 5502552953.162502\n",
      "Iteration 2929, Loss: 2988337330.9058204\n",
      "Iteration 2930, Loss: 2460611772.422002\n",
      "Iteration 2931, Loss: 18268437925.71553\n",
      "Iteration 2932, Loss: 13975937156.571087\n",
      "Iteration 2933, Loss: 11903527954.223381\n",
      "Iteration 2934, Loss: 3039867700.639855\n",
      "Iteration 2935, Loss: 2624741650.9806967\n",
      "Iteration 2936, Loss: 1321117966.1018746\n",
      "Iteration 2937, Loss: 1099813904.8685107\n",
      "Iteration 2938, Loss: 1087511457.2436132\n",
      "Iteration 2939, Loss: 1103556942.3763282\n",
      "Iteration 2940, Loss: 1063896295.1150604\n",
      "Iteration 2941, Loss: 1040635947.6466881\n",
      "Iteration 2942, Loss: 1023497703.8617252\n",
      "Iteration 2943, Loss: 1011806564.5100667\n",
      "Iteration 2944, Loss: 1463871303.0854738\n",
      "Iteration 2945, Loss: 1046279944.8509625\n",
      "Iteration 2946, Loss: 1017004514.5335687\n",
      "Iteration 2947, Loss: 990677936.496515\n",
      "Iteration 2948, Loss: 976417740.4667729\n",
      "Iteration 2949, Loss: 981791030.1897788\n",
      "Iteration 2950, Loss: 1376576319.5026598\n",
      "Iteration 2951, Loss: 1098877850.436924\n",
      "Iteration 2952, Loss: 1002286670.8867583\n",
      "Iteration 2953, Loss: 1026947353.7683946\n",
      "Iteration 2954, Loss: 1044572005.381184\n",
      "Iteration 2955, Loss: 1032242429.7746605\n",
      "Iteration 2956, Loss: 999095183.2538052\n",
      "Iteration 2957, Loss: 1330405088.7898257\n",
      "Iteration 2958, Loss: 978749360.5993228\n",
      "Iteration 2959, Loss: 965664911.18692\n",
      "Iteration 2960, Loss: 1418583601.8102648\n",
      "Iteration 2961, Loss: 1173956811.8081136\n",
      "Iteration 2962, Loss: 1023492758.9080639\n",
      "Iteration 2963, Loss: 1006234506.5036494\n",
      "Iteration 2964, Loss: 1329009444.4458923\n",
      "Iteration 2965, Loss: 1032931493.3421116\n",
      "Iteration 2966, Loss: 1174550422.539036\n",
      "Iteration 2967, Loss: 1198751895.6362002\n",
      "Iteration 2968, Loss: 1086449237.6209893\n",
      "Iteration 2969, Loss: 1814859415.832744\n",
      "Iteration 2970, Loss: 1684509443.6245215\n",
      "Iteration 2971, Loss: 1238137069.6031053\n",
      "Iteration 2972, Loss: 1080063898.3005166\n",
      "Iteration 2973, Loss: 1017646576.6166158\n",
      "Iteration 2974, Loss: 997565866.9724126\n",
      "Iteration 2975, Loss: 986980034.2365845\n",
      "Iteration 2976, Loss: 1646232756.8073773\n",
      "Iteration 2977, Loss: 1443958141.911572\n",
      "Iteration 2978, Loss: 1368023559.266849\n",
      "Iteration 2979, Loss: 1309930575.7359002\n",
      "Iteration 2980, Loss: 1191107648.691361\n",
      "Iteration 2981, Loss: 1175377702.7629488\n",
      "Iteration 2982, Loss: 1089926887.4235582\n",
      "Iteration 2983, Loss: 1106693388.277945\n",
      "Iteration 2984, Loss: 1185564228.4805272\n",
      "Iteration 2985, Loss: 1235197747.102984\n",
      "Iteration 2986, Loss: 1027123098.2271386\n",
      "Iteration 2987, Loss: 1046598923.6032058\n",
      "Iteration 2988, Loss: 1133208020.5482686\n",
      "Iteration 2989, Loss: 1122265482.1539645\n",
      "Iteration 2990, Loss: 1251308997.1602728\n",
      "Iteration 2991, Loss: 1012400397.2397766\n",
      "Iteration 2992, Loss: 1433700723.4983335\n",
      "Iteration 2993, Loss: 1049282187.12381\n",
      "Iteration 2994, Loss: 1061471625.5493294\n",
      "Iteration 2995, Loss: 1052470718.2816085\n",
      "Iteration 2996, Loss: 1077022796.3745835\n",
      "Iteration 2997, Loss: 1115515524.1846976\n",
      "Iteration 2998, Loss: 1072439850.2579637\n",
      "Iteration 2999, Loss: 1055119032.4290757\n",
      "Iteration 3000, Loss: 1525155798.7541604\n",
      "Iteration 3001, Loss: 1432662897.6199934\n",
      "Iteration 3002, Loss: 1362225221.106016\n",
      "Iteration 3003, Loss: 1058051313.3074038\n",
      "Iteration 3004, Loss: 1089993279.88101\n",
      "Iteration 3005, Loss: 1343705008.8438823\n",
      "Iteration 3006, Loss: 1310288464.5103507\n",
      "Iteration 3007, Loss: 1323165600.1998386\n",
      "Iteration 3008, Loss: 1041075272.5971084\n",
      "Iteration 3009, Loss: 1047223995.1955622\n",
      "Iteration 3010, Loss: 1036245625.0708736\n",
      "Iteration 3011, Loss: 1066307242.2028111\n",
      "Iteration 3012, Loss: 1052872853.255374\n",
      "Iteration 3013, Loss: 1341448219.0533304\n",
      "Iteration 3014, Loss: 1294396124.979824\n",
      "Iteration 3015, Loss: 1067556107.4940383\n",
      "Iteration 3016, Loss: 1189322925.317844\n",
      "Iteration 3017, Loss: 1165679006.6613185\n",
      "Iteration 3018, Loss: 1138724790.2871404\n",
      "Iteration 3019, Loss: 1122399640.5499375\n",
      "Iteration 3020, Loss: 1093609284.0013952\n",
      "Iteration 3021, Loss: 998827243.922776\n",
      "Iteration 3022, Loss: 2122012108.9678833\n",
      "Iteration 3023, Loss: 1026492968.2236534\n",
      "Iteration 3024, Loss: 1008913859.9415972\n",
      "Iteration 3025, Loss: 1290443523.214578\n",
      "Iteration 3026, Loss: 1271813547.9927592\n",
      "Iteration 3027, Loss: 1072408500.8934767\n",
      "Iteration 3028, Loss: 1168511720.2478132\n",
      "Iteration 3029, Loss: 1203340888.1575117\n",
      "Iteration 3030, Loss: 1071658193.6735461\n",
      "Iteration 3031, Loss: 1304789566.0094225\n",
      "Iteration 3032, Loss: 1332316447.7979655\n",
      "Iteration 3033, Loss: 1281468280.999168\n",
      "Iteration 3034, Loss: 1028302111.7260607\n",
      "Iteration 3035, Loss: 1206040535.0371804\n",
      "Iteration 3036, Loss: 1186483456.5428526\n",
      "Iteration 3037, Loss: 1280662129.7855148\n",
      "Iteration 3038, Loss: 1252910954.8248928\n",
      "Iteration 3039, Loss: 1098666822.1349916\n",
      "Iteration 3040, Loss: 1088617953.5497484\n",
      "Iteration 3041, Loss: 1011339382.0935224\n",
      "Iteration 3042, Loss: 1352190799.0602143\n",
      "Iteration 3043, Loss: 1364629834.656324\n",
      "Iteration 3044, Loss: 1159310771.8792353\n",
      "Iteration 3045, Loss: 1107092121.096415\n",
      "Iteration 3046, Loss: 1211916370.8663507\n",
      "Iteration 3047, Loss: 1107468611.6253643\n",
      "Iteration 3048, Loss: 1398021619.1133022\n",
      "Iteration 3049, Loss: 1096864413.8834918\n",
      "Iteration 3050, Loss: 1312443956.4914231\n",
      "Iteration 3051, Loss: 1259705451.4080744\n",
      "Iteration 3052, Loss: 1261108924.7590127\n",
      "Iteration 3053, Loss: 1286119553.5744138\n",
      "Iteration 3054, Loss: 1074285430.1602774\n",
      "Iteration 3055, Loss: 1216431985.1963222\n",
      "Iteration 3056, Loss: 1356447647.956604\n",
      "Iteration 3057, Loss: 1338114601.6333327\n",
      "Iteration 3058, Loss: 1087334516.8809464\n",
      "Iteration 3059, Loss: 1238991219.7576184\n",
      "Iteration 3060, Loss: 1302586283.5038536\n",
      "Iteration 3061, Loss: 1366330417.5666332\n",
      "Iteration 3062, Loss: 1115518129.6127462\n",
      "Iteration 3063, Loss: 1122040008.4641187\n",
      "Iteration 3064, Loss: 1238584522.8087327\n",
      "Iteration 3065, Loss: 1128376542.473053\n",
      "Iteration 3066, Loss: 1101207675.235178\n",
      "Iteration 3067, Loss: 1129332132.5102427\n",
      "Iteration 3068, Loss: 1127395908.425183\n",
      "Iteration 3069, Loss: 1105482508.6399438\n",
      "Iteration 3070, Loss: 1312171392.4893303\n",
      "Iteration 3071, Loss: 1219041901.6166584\n",
      "Iteration 3072, Loss: 1290230376.807634\n",
      "Iteration 3073, Loss: 1258426308.9548917\n",
      "Iteration 3074, Loss: 1246296821.4741023\n",
      "Iteration 3075, Loss: 1182975161.6510842\n",
      "Iteration 3076, Loss: 1307946735.5566657\n",
      "Iteration 3077, Loss: 1291919043.6446116\n",
      "Iteration 3078, Loss: 1129300870.8472\n",
      "Iteration 3079, Loss: 1274225164.9989216\n",
      "Iteration 3080, Loss: 1122838432.180287\n",
      "Iteration 3081, Loss: 2338508995.200956\n",
      "Iteration 3082, Loss: 2520045091.527036\n",
      "Iteration 3083, Loss: 10671136060.685253\n",
      "Iteration 3084, Loss: 10144678645.417442\n",
      "Iteration 3085, Loss: 37796474273.206795\n",
      "Iteration 3086, Loss: 35181166421.25634\n",
      "Iteration 3087, Loss: 38380113302.20047\n",
      "Iteration 3088, Loss: 34865742829.2359\n",
      "Iteration 3089, Loss: 20383296477.915886\n",
      "Iteration 3090, Loss: 11513761765.607508\n",
      "Iteration 3091, Loss: 10670692566.500717\n",
      "Iteration 3092, Loss: 8516671277.070144\n",
      "Iteration 3093, Loss: 1235090945.6044683\n",
      "Iteration 3094, Loss: 1205173646.2599854\n",
      "Iteration 3095, Loss: 1220982337.7937534\n",
      "Iteration 3096, Loss: 1399015919.40308\n",
      "Iteration 3097, Loss: 1430080233.8796532\n",
      "Iteration 3098, Loss: 1207991352.0148878\n",
      "Iteration 3099, Loss: 1290900948.8164487\n",
      "Iteration 3100, Loss: 1162292462.9815226\n",
      "Iteration 3101, Loss: 2192725789.01402\n",
      "Iteration 3102, Loss: 2177489114.760309\n",
      "Iteration 3103, Loss: 1180373316.7010403\n",
      "Iteration 3104, Loss: 2389522824.713583\n",
      "Iteration 3105, Loss: 2336598750.241165\n",
      "Iteration 3106, Loss: 2320126885.179923\n",
      "Iteration 3107, Loss: 1670932228.4454753\n",
      "Iteration 3108, Loss: 1545287793.846587\n",
      "Iteration 3109, Loss: 1628456138.0890064\n",
      "Iteration 3110, Loss: 1181519553.556439\n",
      "Iteration 3111, Loss: 1320681594.38841\n",
      "Iteration 3112, Loss: 1231076926.4312663\n",
      "Iteration 3113, Loss: 1229505253.6247332\n",
      "Iteration 3114, Loss: 1229887051.4731767\n",
      "Iteration 3115, Loss: 1267079225.1221488\n",
      "Iteration 3116, Loss: 1442603396.1765497\n",
      "Iteration 3117, Loss: 1522803631.2040684\n",
      "Iteration 3118, Loss: 1266572744.7258806\n",
      "Iteration 3119, Loss: 1251275851.8482373\n",
      "Iteration 3120, Loss: 1197129546.286022\n",
      "Iteration 3121, Loss: 1468393456.9908912\n",
      "Iteration 3122, Loss: 1366630051.3066392\n",
      "Iteration 3123, Loss: 1371730441.3650913\n",
      "Iteration 3124, Loss: 1383979521.1728947\n",
      "Iteration 3125, Loss: 1168837881.8354547\n",
      "Iteration 3126, Loss: 1179294032.3070028\n",
      "Iteration 3127, Loss: 1508301640.6910923\n",
      "Iteration 3128, Loss: 1180774867.0299523\n",
      "Iteration 3129, Loss: 1234788319.0388513\n",
      "Iteration 3130, Loss: 1314712337.432052\n",
      "Iteration 3131, Loss: 1168387746.96967\n",
      "Iteration 3132, Loss: 1172897410.102728\n",
      "Iteration 3133, Loss: 1183839930.6096332\n",
      "Iteration 3134, Loss: 1425618290.214354\n",
      "Iteration 3135, Loss: 1407106778.2332463\n",
      "Iteration 3136, Loss: 1411395482.9977877\n",
      "Iteration 3137, Loss: 1321596064.1219058\n",
      "Iteration 3138, Loss: 1345935445.1895015\n",
      "Iteration 3139, Loss: 1327946809.1805983\n",
      "Iteration 3140, Loss: 1342443492.1607199\n",
      "Iteration 3141, Loss: 1289622768.6667178\n",
      "Iteration 3142, Loss: 1423897603.022247\n",
      "Iteration 3143, Loss: 1185223682.8196645\n",
      "Iteration 3144, Loss: 1191169333.686499\n",
      "Iteration 3145, Loss: 1189498504.1479716\n",
      "Iteration 3146, Loss: 1400604752.8787851\n",
      "Iteration 3147, Loss: 1221460192.4065511\n",
      "Iteration 3148, Loss: 1275574479.4203703\n",
      "Iteration 3149, Loss: 1335576996.959957\n",
      "Iteration 3150, Loss: 1188194331.131567\n",
      "Iteration 3151, Loss: 1193515158.15679\n",
      "Iteration 3152, Loss: 1183982015.338861\n",
      "Iteration 3153, Loss: 1216854894.802394\n",
      "Iteration 3154, Loss: 1209052727.8780904\n",
      "Iteration 3155, Loss: 1170112964.3795192\n",
      "Iteration 3156, Loss: 1285462647.7772088\n",
      "Iteration 3157, Loss: 1146129276.120067\n",
      "Iteration 3158, Loss: 1913691060.6559258\n",
      "Iteration 3159, Loss: 1217963762.4916334\n",
      "Iteration 3160, Loss: 1153755487.9680307\n",
      "Iteration 3161, Loss: 1198286328.4133878\n",
      "Iteration 3162, Loss: 1359796168.2676077\n",
      "Iteration 3163, Loss: 1160342918.5385237\n",
      "Iteration 3164, Loss: 1163464971.4615235\n",
      "Iteration 3165, Loss: 1574016267.2800515\n",
      "Iteration 3166, Loss: 1139819333.7180426\n",
      "Iteration 3167, Loss: 1133803287.177184\n",
      "Iteration 3168, Loss: 1134372157.217274\n",
      "Iteration 3169, Loss: 1280410744.7945373\n",
      "Iteration 3170, Loss: 1320816411.101257\n",
      "Iteration 3171, Loss: 1281776200.7453983\n",
      "Iteration 3172, Loss: 1248424268.5173686\n",
      "Iteration 3173, Loss: 1134390576.401428\n",
      "Iteration 3174, Loss: 1287959424.1215627\n",
      "Iteration 3175, Loss: 1123040869.7881668\n",
      "Iteration 3176, Loss: 1487289360.6455114\n",
      "Iteration 3177, Loss: 1421958395.2737837\n",
      "Iteration 3178, Loss: 1145845879.3240268\n",
      "Iteration 3179, Loss: 1342873877.3628602\n",
      "Iteration 3180, Loss: 1141320786.5448222\n",
      "Iteration 3181, Loss: 1206605266.3273919\n",
      "Iteration 3182, Loss: 1238955478.456777\n",
      "Iteration 3183, Loss: 1205000419.2203376\n",
      "Iteration 3184, Loss: 1189262244.2692385\n",
      "Iteration 3185, Loss: 1175149379.2489643\n",
      "Iteration 3186, Loss: 1261049826.8703444\n",
      "Iteration 3187, Loss: 1254666675.5662758\n",
      "Iteration 3188, Loss: 1151378548.637368\n",
      "Iteration 3189, Loss: 1329020007.202851\n",
      "Iteration 3190, Loss: 1431898468.6819544\n",
      "Iteration 3191, Loss: 1327049376.5768616\n",
      "Iteration 3192, Loss: 1207098858.5781703\n",
      "Iteration 3193, Loss: 1356056040.5012407\n",
      "Iteration 3194, Loss: 1487331373.1185873\n",
      "Iteration 3195, Loss: 1563364691.7669387\n",
      "Iteration 3196, Loss: 1496371777.1505215\n",
      "Iteration 3197, Loss: 1577677314.7750993\n",
      "Iteration 3198, Loss: 1613870684.136624\n",
      "Iteration 3199, Loss: 1397014544.6440954\n",
      "Iteration 3200, Loss: 1414444139.591918\n",
      "Iteration 3201, Loss: 1330592846.0814002\n",
      "Iteration 3202, Loss: 1473448654.5227845\n",
      "Iteration 3203, Loss: 1478533880.0745847\n",
      "Iteration 3204, Loss: 1348040154.7645352\n",
      "Iteration 3205, Loss: 1210953064.407064\n",
      "Iteration 3206, Loss: 1270727748.969373\n",
      "Iteration 3207, Loss: 1290247071.0772543\n",
      "Iteration 3208, Loss: 1288458308.2282274\n",
      "Iteration 3209, Loss: 1248765168.4274201\n",
      "Iteration 3210, Loss: 1379370464.7656074\n",
      "Iteration 3211, Loss: 1322141124.9514403\n",
      "Iteration 3212, Loss: 1435740716.951872\n",
      "Iteration 3213, Loss: 1432208290.1715944\n",
      "Iteration 3214, Loss: 1420850416.9742026\n",
      "Iteration 3215, Loss: 1124706804.5319908\n",
      "Iteration 3216, Loss: 1316976938.4058712\n",
      "Iteration 3217, Loss: 1172002450.6781542\n",
      "Iteration 3218, Loss: 1310570240.2403593\n",
      "Iteration 3219, Loss: 1309134099.5748487\n",
      "Iteration 3220, Loss: 1307598791.911308\n",
      "Iteration 3221, Loss: 1218787776.298676\n",
      "Iteration 3222, Loss: 1196799300.170096\n",
      "Iteration 3223, Loss: 1307296864.4585137\n",
      "Iteration 3224, Loss: 1093375690.9372118\n",
      "Iteration 3225, Loss: 1815374934.861835\n",
      "Iteration 3226, Loss: 1141416723.069068\n",
      "Iteration 3227, Loss: 1277888741.0073287\n",
      "Iteration 3228, Loss: 1272458876.22356\n",
      "Iteration 3229, Loss: 1311246461.6132524\n",
      "Iteration 3230, Loss: 1211101921.223887\n",
      "Iteration 3231, Loss: 1490607252.50686\n",
      "Iteration 3232, Loss: 1209901440.7000542\n",
      "Iteration 3233, Loss: 1397503365.671062\n",
      "Iteration 3234, Loss: 1459755923.678909\n",
      "Iteration 3235, Loss: 1460643962.703047\n",
      "Iteration 3236, Loss: 1473134047.0272334\n",
      "Iteration 3237, Loss: 1495790439.9247007\n",
      "Iteration 3238, Loss: 1195969080.2265956\n",
      "Iteration 3239, Loss: 1120270247.6428347\n",
      "Iteration 3240, Loss: 2399376184.3857913\n",
      "Iteration 3241, Loss: 1148432927.8957944\n",
      "Iteration 3242, Loss: 2522200433.7965155\n",
      "Iteration 3243, Loss: 1203570975.310217\n",
      "Iteration 3244, Loss: 1210942417.7555113\n",
      "Iteration 3245, Loss: 1360401530.8306985\n",
      "Iteration 3246, Loss: 1471412497.2304375\n",
      "Iteration 3247, Loss: 1470020880.537637\n",
      "Iteration 3248, Loss: 1196294573.5631585\n",
      "Iteration 3249, Loss: 1160941361.467216\n",
      "Iteration 3250, Loss: 1125196736.3259025\n",
      "Iteration 3251, Loss: 1122422202.3901694\n",
      "Iteration 3252, Loss: 1483702006.0999136\n",
      "Iteration 3253, Loss: 1452828660.9725752\n",
      "Iteration 3254, Loss: 1297092495.1086378\n",
      "Iteration 3255, Loss: 1137867450.173421\n",
      "Iteration 3256, Loss: 1474771995.3554072\n",
      "Iteration 3257, Loss: 1203673845.9365273\n",
      "Iteration 3258, Loss: 1294994384.316926\n",
      "Iteration 3259, Loss: 1167849728.8358133\n",
      "Iteration 3260, Loss: 1340168091.2603338\n",
      "Iteration 3261, Loss: 1269519201.6731758\n",
      "Iteration 3262, Loss: 1289217622.3203838\n",
      "Iteration 3263, Loss: 1335864222.154295\n",
      "Iteration 3264, Loss: 1195883326.6888676\n",
      "Iteration 3265, Loss: 1376781919.2910502\n",
      "Iteration 3266, Loss: 1428699835.8630009\n",
      "Iteration 3267, Loss: 1169712495.5760002\n",
      "Iteration 3268, Loss: 1209916144.2007987\n",
      "Iteration 3269, Loss: 1210254172.6986156\n",
      "Iteration 3270, Loss: 1145993517.0529814\n",
      "Iteration 3271, Loss: 1352442744.476546\n",
      "Iteration 3272, Loss: 1157682612.8866231\n",
      "Iteration 3273, Loss: 1157720673.7874951\n",
      "Iteration 3274, Loss: 1294847502.6183197\n",
      "Iteration 3275, Loss: 1345416833.408725\n",
      "Iteration 3276, Loss: 1342711597.4172387\n",
      "Iteration 3277, Loss: 1185163117.8570573\n",
      "Iteration 3278, Loss: 1320305125.2262788\n",
      "Iteration 3279, Loss: 1309209844.5396287\n",
      "Iteration 3280, Loss: 1363948530.0450928\n",
      "Iteration 3281, Loss: 1425482253.3660064\n",
      "Iteration 3282, Loss: 1409270676.6015856\n",
      "Iteration 3283, Loss: 1415899258.8397589\n",
      "Iteration 3284, Loss: 1257461074.7594962\n",
      "Iteration 3285, Loss: 1286222184.862313\n",
      "Iteration 3286, Loss: 1280799764.3645983\n",
      "Iteration 3287, Loss: 1322809689.531924\n",
      "Iteration 3288, Loss: 1139009348.5665884\n",
      "Iteration 3289, Loss: 1241665405.5504503\n",
      "Iteration 3290, Loss: 1190955511.337596\n",
      "Iteration 3291, Loss: 1246875322.2077353\n",
      "Iteration 3292, Loss: 1128060417.8966794\n",
      "Iteration 3293, Loss: 1438561954.3326128\n",
      "Iteration 3294, Loss: 1202554469.501131\n",
      "Iteration 3295, Loss: 1209594225.160605\n",
      "Iteration 3296, Loss: 1256514577.6646984\n",
      "Iteration 3297, Loss: 1357024261.4521654\n",
      "Iteration 3298, Loss: 1158625462.6181\n",
      "Iteration 3299, Loss: 1217210323.9476395\n",
      "Iteration 3300, Loss: 1324094892.9617555\n",
      "Iteration 3301, Loss: 1161641956.4047396\n",
      "Iteration 3302, Loss: 1579218133.2719345\n",
      "Iteration 3303, Loss: 1263113034.0937767\n",
      "Iteration 3304, Loss: 1255773355.0177412\n",
      "Iteration 3305, Loss: 1245679829.4686089\n",
      "Iteration 3306, Loss: 1190445292.6524634\n",
      "Iteration 3307, Loss: 1209286147.7766895\n",
      "Iteration 3308, Loss: 1233395845.3699496\n",
      "Iteration 3309, Loss: 1320927307.1263127\n",
      "Iteration 3310, Loss: 1123099155.44812\n",
      "Iteration 3311, Loss: 1377270009.3282986\n",
      "Iteration 3312, Loss: 1376837759.9815457\n",
      "Iteration 3313, Loss: 1165920264.7808726\n",
      "Iteration 3314, Loss: 1134009800.5496423\n",
      "Iteration 3315, Loss: 1122701832.6418312\n",
      "Iteration 3316, Loss: 1912492272.2750928\n",
      "Iteration 3317, Loss: 1200039639.661123\n",
      "Iteration 3318, Loss: 1293373375.4216845\n",
      "Iteration 3319, Loss: 1383803035.7188895\n",
      "Iteration 3320, Loss: 1334864436.0773096\n",
      "Iteration 3321, Loss: 1227296058.6159961\n",
      "Iteration 3322, Loss: 1238714699.5765934\n",
      "Iteration 3323, Loss: 1285676896.775055\n",
      "Iteration 3324, Loss: 1288925922.7070773\n",
      "Iteration 3325, Loss: 1288006286.2160208\n",
      "Iteration 3326, Loss: 1337434396.3089058\n",
      "Iteration 3327, Loss: 1245883721.36741\n",
      "Iteration 3328, Loss: 1093331570.3548508\n",
      "Iteration 3329, Loss: 1098403764.8853197\n",
      "Iteration 3330, Loss: 1295281417.492498\n",
      "Iteration 3331, Loss: 1283112531.9978805\n",
      "Iteration 3332, Loss: 1280630843.5482483\n",
      "Iteration 3333, Loss: 1323112845.0241294\n",
      "Iteration 3334, Loss: 1238015314.3251252\n",
      "Iteration 3335, Loss: 1174358769.5332346\n",
      "Iteration 3336, Loss: 1321555973.207812\n",
      "Iteration 3337, Loss: 1320833385.0786254\n",
      "Iteration 3338, Loss: 1295031231.2931693\n",
      "Iteration 3339, Loss: 1344759797.5531905\n",
      "Iteration 3340, Loss: 1190583529.711935\n",
      "Iteration 3341, Loss: 1646702653.829968\n",
      "Iteration 3342, Loss: 1373475901.6447577\n",
      "Iteration 3343, Loss: 1436731130.7252977\n",
      "Iteration 3344, Loss: 1491310213.8021736\n",
      "Iteration 3345, Loss: 1178197264.0788772\n",
      "Iteration 3346, Loss: 1152647678.6056657\n",
      "Iteration 3347, Loss: 1092789222.0624413\n",
      "Iteration 3348, Loss: 1087694280.2002435\n",
      "Iteration 3349, Loss: 1131855028.7409651\n",
      "Iteration 3350, Loss: 1261974119.0130005\n",
      "Iteration 3351, Loss: 1073143123.9669766\n",
      "Iteration 3352, Loss: 1368928686.9532998\n",
      "Iteration 3353, Loss: 1107324451.4577856\n",
      "Iteration 3354, Loss: 1084878961.671568\n",
      "Iteration 3355, Loss: 1368342740.5322976\n",
      "Iteration 3356, Loss: 1380034109.8108766\n",
      "Iteration 3357, Loss: 1183482179.1309717\n",
      "Iteration 3358, Loss: 1185635763.9909537\n",
      "Iteration 3359, Loss: 1762804029.725217\n",
      "Iteration 3360, Loss: 1152895347.4743314\n",
      "Iteration 3361, Loss: 1379578764.7056994\n",
      "Iteration 3362, Loss: 1263665150.92445\n",
      "Iteration 3363, Loss: 1236908074.1823559\n",
      "Iteration 3364, Loss: 1236435985.5886812\n",
      "Iteration 3365, Loss: 1356474401.6511438\n",
      "Iteration 3366, Loss: 1316002396.3388627\n",
      "Iteration 3367, Loss: 1309560289.5961516\n",
      "Iteration 3368, Loss: 1294800997.7405734\n",
      "Iteration 3369, Loss: 1305130776.1134977\n",
      "Iteration 3370, Loss: 1223216696.8953435\n",
      "Iteration 3371, Loss: 1216766595.1064599\n",
      "Iteration 3372, Loss: 1219466248.7286355\n",
      "Iteration 3373, Loss: 1320890273.7817328\n",
      "Iteration 3374, Loss: 1379228996.8813105\n",
      "Iteration 3375, Loss: 1136394076.8753886\n",
      "Iteration 3376, Loss: 2408551336.7237725\n",
      "Iteration 3377, Loss: 2235138911.1472197\n",
      "Iteration 3378, Loss: 2152270161.2807364\n",
      "Iteration 3379, Loss: 1821962495.0605054\n",
      "Iteration 3380, Loss: 1592171553.3638487\n",
      "Iteration 3381, Loss: 1544239501.8667731\n",
      "Iteration 3382, Loss: 1360917890.8539119\n",
      "Iteration 3383, Loss: 1227455482.1721084\n",
      "Iteration 3384, Loss: 1167036889.9535701\n",
      "Iteration 3385, Loss: 2691373722.416479\n",
      "Iteration 3386, Loss: 1474106283.5245204\n",
      "Iteration 3387, Loss: 1430094999.0572102\n",
      "Iteration 3388, Loss: 1286844342.0965\n",
      "Iteration 3389, Loss: 1165684642.7912393\n",
      "Iteration 3390, Loss: 1532019814.370682\n",
      "Iteration 3391, Loss: 1272144320.1110964\n",
      "Iteration 3392, Loss: 1464155483.2844775\n",
      "Iteration 3393, Loss: 1530819115.2077348\n",
      "Iteration 3394, Loss: 1616090318.3063536\n",
      "Iteration 3395, Loss: 1611778332.8961632\n",
      "Iteration 3396, Loss: 1588599947.5852892\n",
      "Iteration 3397, Loss: 1510146099.4310784\n",
      "Iteration 3398, Loss: 1236257738.7297406\n",
      "Iteration 3399, Loss: 1359550577.3358998\n",
      "Iteration 3400, Loss: 1311968991.7053678\n",
      "Iteration 3401, Loss: 1239214617.1914382\n",
      "Iteration 3402, Loss: 1188913475.9048874\n",
      "Iteration 3403, Loss: 2330296967.868262\n",
      "Iteration 3404, Loss: 1491545293.0601375\n",
      "Iteration 3405, Loss: 1351368848.3421595\n",
      "Iteration 3406, Loss: 1528044541.0682142\n",
      "Iteration 3407, Loss: 1353753213.9379249\n",
      "Iteration 3408, Loss: 1315262076.1131473\n",
      "Iteration 3409, Loss: 1351666886.2706668\n",
      "Iteration 3410, Loss: 1244860599.9035861\n",
      "Iteration 3411, Loss: 1238329782.3678703\n",
      "Iteration 3412, Loss: 1506200926.6741455\n",
      "Iteration 3413, Loss: 1264260011.620411\n",
      "Iteration 3414, Loss: 1298953685.6303325\n",
      "Iteration 3415, Loss: 1404477888.4301631\n",
      "Iteration 3416, Loss: 1284747622.898877\n",
      "Iteration 3417, Loss: 1402080199.1404288\n",
      "Iteration 3418, Loss: 1513011285.389901\n",
      "Iteration 3419, Loss: 1537875283.798551\n",
      "Iteration 3420, Loss: 1558944300.7204404\n",
      "Iteration 3421, Loss: 1622315580.8805392\n",
      "Iteration 3422, Loss: 1655750529.2829323\n",
      "Iteration 3423, Loss: 1600382231.708628\n",
      "Iteration 3424, Loss: 1607007208.8690052\n",
      "Iteration 3425, Loss: 1451454262.7948916\n",
      "Iteration 3426, Loss: 1262858552.0122962\n",
      "Iteration 3427, Loss: 1344916809.1329703\n",
      "Iteration 3428, Loss: 1355711538.4430397\n",
      "Iteration 3429, Loss: 1230522374.872342\n",
      "Iteration 3430, Loss: 1228034636.7132103\n",
      "Iteration 3431, Loss: 1143687424.232928\n",
      "Iteration 3432, Loss: 1263825463.1744833\n",
      "Iteration 3433, Loss: 1247313402.8699884\n",
      "Iteration 3434, Loss: 1724667271.6988287\n",
      "Iteration 3435, Loss: 1181716273.872794\n",
      "Iteration 3436, Loss: 1335308778.9842687\n",
      "Iteration 3437, Loss: 1273258930.4254034\n",
      "Iteration 3438, Loss: 1425731018.493022\n",
      "Iteration 3439, Loss: 1337108890.7202942\n",
      "Iteration 3440, Loss: 1391595680.1115146\n",
      "Iteration 3441, Loss: 1400668020.5431666\n",
      "Iteration 3442, Loss: 1409240408.0327933\n",
      "Iteration 3443, Loss: 1470961610.6359127\n",
      "Iteration 3444, Loss: 1510903552.4420412\n",
      "Iteration 3445, Loss: 1547550600.7543929\n",
      "Iteration 3446, Loss: 1162252279.9550073\n",
      "Iteration 3447, Loss: 1210120837.4457247\n",
      "Iteration 3448, Loss: 1214284720.123392\n",
      "Iteration 3449, Loss: 1219384736.9446898\n",
      "Iteration 3450, Loss: 1179636860.1353931\n",
      "Iteration 3451, Loss: 1181795404.5506608\n",
      "Iteration 3452, Loss: 1088356050.0668342\n",
      "Iteration 3453, Loss: 1099784730.4765613\n",
      "Iteration 3454, Loss: 1155614197.4372635\n",
      "Iteration 3455, Loss: 1214239355.314772\n",
      "Iteration 3456, Loss: 1209917635.289748\n",
      "Iteration 3457, Loss: 1245749354.6149645\n",
      "Iteration 3458, Loss: 1315217235.6775572\n",
      "Iteration 3459, Loss: 1357891790.2503395\n",
      "Iteration 3460, Loss: 1145250753.9077628\n",
      "Iteration 3461, Loss: 1263937236.8127627\n",
      "Iteration 3462, Loss: 1149954411.4940414\n",
      "Iteration 3463, Loss: 1143461780.5295281\n",
      "Iteration 3464, Loss: 1188100775.9795475\n",
      "Iteration 3465, Loss: 1213108125.5161023\n",
      "Iteration 3466, Loss: 1144132877.4027195\n",
      "Iteration 3467, Loss: 1244016637.837545\n",
      "Iteration 3468, Loss: 1070241914.4955873\n",
      "Iteration 3469, Loss: 1433178283.5934262\n",
      "Iteration 3470, Loss: 1236932473.9853551\n",
      "Iteration 3471, Loss: 1222988764.17776\n",
      "Iteration 3472, Loss: 1216059803.4556398\n",
      "Iteration 3473, Loss: 1110495416.6142902\n",
      "Iteration 3474, Loss: 1301799501.8054523\n",
      "Iteration 3475, Loss: 1173759213.8967214\n",
      "Iteration 3476, Loss: 1168585826.4871626\n",
      "Iteration 3477, Loss: 1154666629.11604\n",
      "Iteration 3478, Loss: 1180368377.6519303\n",
      "Iteration 3479, Loss: 1083223564.8420582\n",
      "Iteration 3480, Loss: 1344998950.8515868\n",
      "Iteration 3481, Loss: 1384840851.317967\n",
      "Iteration 3482, Loss: 1367794413.59973\n",
      "Iteration 3483, Loss: 1084435153.1953745\n",
      "Iteration 3484, Loss: 1505003405.189884\n",
      "Iteration 3485, Loss: 1066786432.4753709\n",
      "Iteration 3486, Loss: 2401371033.357996\n",
      "Iteration 3487, Loss: 1165004583.6027248\n",
      "Iteration 3488, Loss: 1278302407.001168\n",
      "Iteration 3489, Loss: 1266326054.7422986\n",
      "Iteration 3490, Loss: 1319712213.3022053\n",
      "Iteration 3491, Loss: 1331821011.7884943\n",
      "Iteration 3492, Loss: 1085502884.8225577\n",
      "Iteration 3493, Loss: 1138414310.9054327\n",
      "Iteration 3494, Loss: 1107892763.8852768\n",
      "Iteration 3495, Loss: 1098962731.0085151\n",
      "Iteration 3496, Loss: 1181676361.06617\n",
      "Iteration 3497, Loss: 1283459753.5315077\n",
      "Iteration 3498, Loss: 1236140659.9819632\n",
      "Iteration 3499, Loss: 1075216845.5946503\n",
      "Iteration 3500, Loss: 1327501057.8823187\n",
      "Iteration 3501, Loss: 1233140387.6867785\n",
      "Iteration 3502, Loss: 1144152797.4226117\n",
      "Iteration 3503, Loss: 1201880985.8097994\n",
      "Iteration 3504, Loss: 1145503120.9088264\n",
      "Iteration 3505, Loss: 1188512144.766879\n",
      "Iteration 3506, Loss: 1201192161.5867841\n",
      "Iteration 3507, Loss: 1321963544.135771\n",
      "Iteration 3508, Loss: 1370825261.488078\n",
      "Iteration 3509, Loss: 1141345954.2807379\n",
      "Iteration 3510, Loss: 1135381469.3298771\n",
      "Iteration 3511, Loss: 2040125695.9687643\n",
      "Iteration 3512, Loss: 1909928160.3266063\n",
      "Iteration 3513, Loss: 2047446756.9318504\n",
      "Iteration 3514, Loss: 1374728499.4204352\n",
      "Iteration 3515, Loss: 1156434504.2358103\n",
      "Iteration 3516, Loss: 1327364064.6676817\n",
      "Iteration 3517, Loss: 1196430177.7632456\n",
      "Iteration 3518, Loss: 1226603456.151388\n",
      "Iteration 3519, Loss: 1684483093.373893\n",
      "Iteration 3520, Loss: 1665879440.03168\n",
      "Iteration 3521, Loss: 1263210291.7323372\n",
      "Iteration 3522, Loss: 1211497950.2454553\n",
      "Iteration 3523, Loss: 1216790485.9873667\n",
      "Iteration 3524, Loss: 1340125511.9929857\n",
      "Iteration 3525, Loss: 1134580149.902524\n",
      "Iteration 3526, Loss: 1373847419.0619035\n",
      "Iteration 3527, Loss: 1142706734.5431879\n",
      "Iteration 3528, Loss: 2614563998.434669\n",
      "Iteration 3529, Loss: 2463865506.7712665\n",
      "Iteration 3530, Loss: 1195072892.389056\n",
      "Iteration 3531, Loss: 1155807969.6712863\n",
      "Iteration 3532, Loss: 1670847352.3303435\n",
      "Iteration 3533, Loss: 1433314949.018948\n",
      "Iteration 3534, Loss: 1194265008.1675837\n",
      "Iteration 3535, Loss: 1174446557.225475\n",
      "Iteration 3536, Loss: 1214323701.797484\n",
      "Iteration 3537, Loss: 1233644141.5389066\n",
      "Iteration 3538, Loss: 1382427642.8975744\n",
      "Iteration 3539, Loss: 1443928784.5582967\n",
      "Iteration 3540, Loss: 1506166774.3098915\n",
      "Iteration 3541, Loss: 1529988735.4634125\n",
      "Iteration 3542, Loss: 1255532356.1907682\n",
      "Iteration 3543, Loss: 4129826673.429804\n",
      "Iteration 3544, Loss: 2850711644.854126\n",
      "Iteration 3545, Loss: 2513860484.3024035\n",
      "Iteration 3546, Loss: 1792223375.4577687\n",
      "Iteration 3547, Loss: 1854054567.163759\n",
      "Iteration 3548, Loss: 1954511180.6675005\n",
      "Iteration 3549, Loss: 1887362649.2167754\n",
      "Iteration 3550, Loss: 1175503715.3476977\n",
      "Iteration 3551, Loss: 1480175174.7140093\n",
      "Iteration 3552, Loss: 1195677687.493672\n",
      "Iteration 3553, Loss: 1388205639.132035\n",
      "Iteration 3554, Loss: 1393865998.0052385\n",
      "Iteration 3555, Loss: 1182290579.4137678\n",
      "Iteration 3556, Loss: 1174878447.1379318\n",
      "Iteration 3557, Loss: 1184362246.9102595\n",
      "Iteration 3558, Loss: 1182290840.9512076\n",
      "Iteration 3559, Loss: 1353829105.4511352\n",
      "Iteration 3560, Loss: 1407609996.204108\n",
      "Iteration 3561, Loss: 1464178005.2642355\n",
      "Iteration 3562, Loss: 1464626671.5959785\n",
      "Iteration 3563, Loss: 1378432878.1464024\n",
      "Iteration 3564, Loss: 1248134580.2276757\n",
      "Iteration 3565, Loss: 1121578928.3501282\n",
      "Iteration 3566, Loss: 1111556308.6296048\n",
      "Iteration 3567, Loss: 1162346476.3006265\n",
      "Iteration 3568, Loss: 1286019068.0220342\n",
      "Iteration 3569, Loss: 1357064695.7005653\n",
      "Iteration 3570, Loss: 1218556040.9162042\n",
      "Iteration 3571, Loss: 1215609280.3547113\n",
      "Iteration 3572, Loss: 1306766754.474918\n",
      "Iteration 3573, Loss: 1344442922.1247196\n",
      "Iteration 3574, Loss: 1324685212.4207096\n",
      "Iteration 3575, Loss: 1305790103.5835452\n",
      "Iteration 3576, Loss: 1345927491.1087904\n",
      "Iteration 3577, Loss: 1308922910.605571\n",
      "Iteration 3578, Loss: 1286951007.946806\n",
      "Iteration 3579, Loss: 1415397332.292179\n",
      "Iteration 3580, Loss: 1413145185.2538142\n",
      "Iteration 3581, Loss: 1255186766.6602683\n",
      "Iteration 3582, Loss: 1115945384.5368483\n",
      "Iteration 3583, Loss: 1204068099.4706018\n",
      "Iteration 3584, Loss: 1658597956.7979424\n",
      "Iteration 3585, Loss: 1617352410.1432636\n",
      "Iteration 3586, Loss: 1426092980.8050132\n",
      "Iteration 3587, Loss: 1477862859.9596353\n",
      "Iteration 3588, Loss: 1152355071.4102037\n",
      "Iteration 3589, Loss: 1062593535.345283\n",
      "Iteration 3590, Loss: 1054158188.9680021\n",
      "Iteration 3591, Loss: 1054238640.4259788\n",
      "Iteration 3592, Loss: 1150882071.812069\n",
      "Iteration 3593, Loss: 1739463506.7717664\n",
      "Iteration 3594, Loss: 1069208054.4466307\n",
      "Iteration 3595, Loss: 1068708025.6444889\n",
      "Iteration 3596, Loss: 1064555941.7872453\n",
      "Iteration 3597, Loss: 1160395791.6909354\n",
      "Iteration 3598, Loss: 1404049352.0628316\n",
      "Iteration 3599, Loss: 1397572407.5618854\n",
      "Iteration 3600, Loss: 1104835856.3273118\n",
      "Iteration 3601, Loss: 2071275787.3407602\n",
      "Iteration 3602, Loss: 1187622329.8554616\n",
      "Iteration 3603, Loss: 1301422011.7853773\n",
      "Iteration 3604, Loss: 1167892113.6554062\n",
      "Iteration 3605, Loss: 1574240507.90986\n",
      "Iteration 3606, Loss: 1186600792.5960329\n",
      "Iteration 3607, Loss: 1408444235.5077417\n",
      "Iteration 3608, Loss: 1202727735.6841514\n",
      "Iteration 3609, Loss: 1422876663.7000093\n",
      "Iteration 3610, Loss: 1470228849.3031418\n",
      "Iteration 3611, Loss: 1196170381.3896828\n",
      "Iteration 3612, Loss: 1384842609.156187\n",
      "Iteration 3613, Loss: 1212712415.6983964\n",
      "Iteration 3614, Loss: 1417562045.289202\n",
      "Iteration 3615, Loss: 1499438536.7817972\n",
      "Iteration 3616, Loss: 1302752228.3136046\n",
      "Iteration 3617, Loss: 1306641232.4204464\n",
      "Iteration 3618, Loss: 1300934955.7440379\n",
      "Iteration 3619, Loss: 1283129378.4403589\n",
      "Iteration 3620, Loss: 1300698406.1489832\n",
      "Iteration 3621, Loss: 1444254787.1441746\n",
      "Iteration 3622, Loss: 1472962263.1235971\n",
      "Iteration 3623, Loss: 1242717199.9111898\n",
      "Iteration 3624, Loss: 1463513072.5272424\n",
      "Iteration 3625, Loss: 1323026495.036032\n",
      "Iteration 3626, Loss: 1422641883.7891607\n",
      "Iteration 3627, Loss: 1384409240.7413266\n",
      "Iteration 3628, Loss: 1382844372.3472226\n",
      "Iteration 3629, Loss: 1444924638.1231513\n",
      "Iteration 3630, Loss: 1513695771.886652\n",
      "Iteration 3631, Loss: 1246064170.807989\n",
      "Iteration 3632, Loss: 1150691198.4370174\n",
      "Iteration 3633, Loss: 1520794537.6147892\n",
      "Iteration 3634, Loss: 1278702623.152433\n",
      "Iteration 3635, Loss: 1427572668.882772\n",
      "Iteration 3636, Loss: 1489217889.4821906\n",
      "Iteration 3637, Loss: 1385482016.1206114\n",
      "Iteration 3638, Loss: 1353481233.046455\n",
      "Iteration 3639, Loss: 1472379034.4361608\n",
      "Iteration 3640, Loss: 1461498277.226719\n",
      "Iteration 3641, Loss: 1466255171.0073133\n",
      "Iteration 3642, Loss: 1278008502.678691\n",
      "Iteration 3643, Loss: 1295302125.7928302\n",
      "Iteration 3644, Loss: 1298966075.4508412\n",
      "Iteration 3645, Loss: 1346845360.8710995\n",
      "Iteration 3646, Loss: 1287146151.3856628\n",
      "Iteration 3647, Loss: 1314035013.1830962\n",
      "Iteration 3648, Loss: 1410120116.133006\n",
      "Iteration 3649, Loss: 1195618327.8034284\n",
      "Iteration 3650, Loss: 1301174953.9361718\n",
      "Iteration 3651, Loss: 1162429359.7450185\n",
      "Iteration 3652, Loss: 1205572346.0566013\n",
      "Iteration 3653, Loss: 1214628322.7973914\n",
      "Iteration 3654, Loss: 1231544156.7770107\n",
      "Iteration 3655, Loss: 1148875213.0950499\n",
      "Iteration 3656, Loss: 1291351451.0767643\n",
      "Iteration 3657, Loss: 1379091822.0867658\n",
      "Iteration 3658, Loss: 1378925517.5840776\n",
      "Iteration 3659, Loss: 1460325750.012282\n",
      "Iteration 3660, Loss: 1277746549.0964475\n",
      "Iteration 3661, Loss: 1348027897.9977415\n",
      "Iteration 3662, Loss: 1401790498.9344392\n",
      "Iteration 3663, Loss: 1155993620.953758\n",
      "Iteration 3664, Loss: 1146989977.610938\n",
      "Iteration 3665, Loss: 1437792029.734785\n",
      "Iteration 3666, Loss: 1184047181.214646\n",
      "Iteration 3667, Loss: 1169265597.4037087\n",
      "Iteration 3668, Loss: 1218513988.0539804\n",
      "Iteration 3669, Loss: 1249350288.9170232\n",
      "Iteration 3670, Loss: 1300021694.3430119\n",
      "Iteration 3671, Loss: 1309107464.23561\n",
      "Iteration 3672, Loss: 1388651499.1440465\n",
      "Iteration 3673, Loss: 1155557404.821192\n",
      "Iteration 3674, Loss: 1538807539.4340403\n",
      "Iteration 3675, Loss: 1522342022.0751286\n",
      "Iteration 3676, Loss: 1536640525.8480194\n",
      "Iteration 3677, Loss: 1462116610.6617467\n",
      "Iteration 3678, Loss: 1392247949.6961265\n",
      "Iteration 3679, Loss: 1342563914.5449433\n",
      "Iteration 3680, Loss: 1333053654.9404576\n",
      "Iteration 3681, Loss: 1319075929.2917275\n",
      "Iteration 3682, Loss: 1287514826.9616263\n",
      "Iteration 3683, Loss: 1139910328.1682854\n",
      "Iteration 3684, Loss: 1255360634.261786\n",
      "Iteration 3685, Loss: 1245056843.4813867\n",
      "Iteration 3686, Loss: 1105862007.9377036\n",
      "Iteration 3687, Loss: 1482655743.1262069\n",
      "Iteration 3688, Loss: 1465500485.766003\n",
      "Iteration 3689, Loss: 1366333379.9804263\n",
      "Iteration 3690, Loss: 1231045260.470647\n",
      "Iteration 3691, Loss: 1220748663.2363405\n",
      "Iteration 3692, Loss: 1083475222.6341174\n",
      "Iteration 3693, Loss: 1073297630.7486563\n",
      "Iteration 3694, Loss: 1247556826.1464186\n",
      "Iteration 3695, Loss: 1291280363.9313717\n",
      "Iteration 3696, Loss: 1363286229.8988402\n",
      "Iteration 3697, Loss: 1153027843.291194\n",
      "Iteration 3698, Loss: 1274809916.7220428\n",
      "Iteration 3699, Loss: 1258582132.9315658\n",
      "Iteration 3700, Loss: 1286125510.342024\n",
      "Iteration 3701, Loss: 1116894905.5321872\n",
      "Iteration 3702, Loss: 1168639211.3513126\n",
      "Iteration 3703, Loss: 1082476258.4146538\n",
      "Iteration 3704, Loss: 1233378945.5090137\n",
      "Iteration 3705, Loss: 1212219415.696026\n",
      "Iteration 3706, Loss: 1117537351.4883683\n",
      "Iteration 3707, Loss: 1199174802.3995476\n",
      "Iteration 3708, Loss: 1180406345.1822433\n",
      "Iteration 3709, Loss: 1292866089.1707728\n",
      "Iteration 3710, Loss: 1257291876.200897\n",
      "Iteration 3711, Loss: 1227359203.9485242\n",
      "Iteration 3712, Loss: 1200777499.4599328\n",
      "Iteration 3713, Loss: 1260219662.971086\n",
      "Iteration 3714, Loss: 1136418187.5155666\n",
      "Iteration 3715, Loss: 1118658054.543084\n",
      "Iteration 3716, Loss: 1127720037.0142174\n",
      "Iteration 3717, Loss: 1109365600.6041973\n",
      "Iteration 3718, Loss: 1251293275.7206638\n",
      "Iteration 3719, Loss: 1189917944.6213355\n",
      "Iteration 3720, Loss: 1208137894.183133\n",
      "Iteration 3721, Loss: 1192827591.6073182\n",
      "Iteration 3722, Loss: 1050406718.7702249\n",
      "Iteration 3723, Loss: 1087268966.7879963\n",
      "Iteration 3724, Loss: 1007613606.6345245\n",
      "Iteration 3725, Loss: 2729508377.1687446\n",
      "Iteration 3726, Loss: 1327886285.0990543\n",
      "Iteration 3727, Loss: 1035801909.5386267\n",
      "Iteration 3728, Loss: 1023858366.343111\n",
      "Iteration 3729, Loss: 1024263355.5701852\n",
      "Iteration 3730, Loss: 1009982052.0814006\n",
      "Iteration 3731, Loss: 1993675059.1308742\n",
      "Iteration 3732, Loss: 1794306733.4665208\n",
      "Iteration 3733, Loss: 1832607896.9469967\n",
      "Iteration 3734, Loss: 1035857758.7890872\n",
      "Iteration 3735, Loss: 1376089837.1414297\n",
      "Iteration 3736, Loss: 1351947519.315503\n",
      "Iteration 3737, Loss: 1150027962.4274611\n",
      "Iteration 3738, Loss: 1133610358.9828403\n",
      "Iteration 3739, Loss: 1134148080.784636\n",
      "Iteration 3740, Loss: 1171163811.1930084\n",
      "Iteration 3741, Loss: 1174854981.3837357\n",
      "Iteration 3742, Loss: 1290813779.0245168\n",
      "Iteration 3743, Loss: 1068872068.5427142\n",
      "Iteration 3744, Loss: 1106894795.9437814\n",
      "Iteration 3745, Loss: 1212545813.5078547\n",
      "Iteration 3746, Loss: 1585056752.9703717\n",
      "Iteration 3747, Loss: 1640887736.1870046\n",
      "Iteration 3748, Loss: 1069552124.6031762\n",
      "Iteration 3749, Loss: 1283934967.6794329\n",
      "Iteration 3750, Loss: 1126081299.1349802\n",
      "Iteration 3751, Loss: 1262508920.2139075\n",
      "Iteration 3752, Loss: 1340058718.1573334\n",
      "Iteration 3753, Loss: 1354935611.9060616\n",
      "Iteration 3754, Loss: 1328494832.0221727\n",
      "Iteration 3755, Loss: 1346609446.635706\n",
      "Iteration 3756, Loss: 1087215404.5740104\n",
      "Iteration 3757, Loss: 1077565139.308291\n",
      "Iteration 3758, Loss: 1288726931.0328588\n",
      "Iteration 3759, Loss: 1110885353.667102\n",
      "Iteration 3760, Loss: 1330723934.9325922\n",
      "Iteration 3761, Loss: 1173261582.28269\n",
      "Iteration 3762, Loss: 1213096973.1533635\n",
      "Iteration 3763, Loss: 1201913728.7765262\n",
      "Iteration 3764, Loss: 1190564654.9726021\n",
      "Iteration 3765, Loss: 1220205020.930355\n",
      "Iteration 3766, Loss: 1233477043.526943\n",
      "Iteration 3767, Loss: 1240176937.604007\n",
      "Iteration 3768, Loss: 1286690424.840871\n",
      "Iteration 3769, Loss: 1360074739.111788\n",
      "Iteration 3770, Loss: 1345875211.8217564\n",
      "Iteration 3771, Loss: 1330052745.8319218\n",
      "Iteration 3772, Loss: 1237778252.6156852\n",
      "Iteration 3773, Loss: 1220581704.4929843\n",
      "Iteration 3774, Loss: 1147107146.346025\n",
      "Iteration 3775, Loss: 1167574464.9100769\n",
      "Iteration 3776, Loss: 1084129145.6539981\n",
      "Iteration 3777, Loss: 1000965244.4484385\n",
      "Iteration 3778, Loss: 2527748056.0968184\n",
      "Iteration 3779, Loss: 1560526952.646579\n",
      "Iteration 3780, Loss: 1520213714.6036139\n",
      "Iteration 3781, Loss: 1085790386.384674\n",
      "Iteration 3782, Loss: 1069042940.8762523\n",
      "Iteration 3783, Loss: 1024414178.2151648\n",
      "Iteration 3784, Loss: 1016577840.9371397\n",
      "Iteration 3785, Loss: 1100209068.5544817\n",
      "Iteration 3786, Loss: 1243491267.1229544\n",
      "Iteration 3787, Loss: 1265717504.298199\n",
      "Iteration 3788, Loss: 1217264529.8404472\n",
      "Iteration 3789, Loss: 1190829041.984431\n",
      "Iteration 3790, Loss: 1162947361.2715425\n",
      "Iteration 3791, Loss: 1218935443.950772\n",
      "Iteration 3792, Loss: 1229426115.4901366\n",
      "Iteration 3793, Loss: 1203679279.593414\n",
      "Iteration 3794, Loss: 1114427577.1537414\n",
      "Iteration 3795, Loss: 1049295339.5997043\n",
      "Iteration 3796, Loss: 1886563036.6150968\n",
      "Iteration 3797, Loss: 1279821079.8891292\n",
      "Iteration 3798, Loss: 1053902365.6577064\n",
      "Iteration 3799, Loss: 1244711742.7501433\n",
      "Iteration 3800, Loss: 1224263095.4861948\n",
      "Iteration 3801, Loss: 1005051925.9815661\n",
      "Iteration 3802, Loss: 994110914.5210443\n",
      "Iteration 3803, Loss: 1229997765.1694698\n",
      "Iteration 3804, Loss: 1163312133.4660943\n",
      "Iteration 3805, Loss: 1159750568.1027672\n",
      "Iteration 3806, Loss: 1054284399.2436101\n",
      "Iteration 3807, Loss: 1146841744.127237\n",
      "Iteration 3808, Loss: 1176290817.5426228\n",
      "Iteration 3809, Loss: 1543456853.6790292\n",
      "Iteration 3810, Loss: 1500280231.1450944\n",
      "Iteration 3811, Loss: 1339441453.5000763\n",
      "Iteration 3812, Loss: 1070633246.5996121\n",
      "Iteration 3813, Loss: 1106122883.4769533\n",
      "Iteration 3814, Loss: 1079914201.7751915\n",
      "Iteration 3815, Loss: 974983276.639881\n",
      "Iteration 3816, Loss: 965805885.7123369\n",
      "Iteration 3817, Loss: 983124635.8266374\n",
      "Iteration 3818, Loss: 1057097186.7502258\n",
      "Iteration 3819, Loss: 952541317.1239691\n",
      "Iteration 3820, Loss: 1209200372.7179918\n",
      "Iteration 3821, Loss: 1024904926.635998\n",
      "Iteration 3822, Loss: 1123336567.0958781\n",
      "Iteration 3823, Loss: 1088247935.9812317\n",
      "Iteration 3824, Loss: 1099942029.6110005\n",
      "Iteration 3825, Loss: 1060447995.190499\n",
      "Iteration 3826, Loss: 1050819191.30252\n",
      "Iteration 3827, Loss: 1117087550.167257\n",
      "Iteration 3828, Loss: 1035263635.4904449\n",
      "Iteration 3829, Loss: 1052845870.0651517\n",
      "Iteration 3830, Loss: 1062917592.9212862\n",
      "Iteration 3831, Loss: 987161973.7839805\n",
      "Iteration 3832, Loss: 965546803.3803211\n",
      "Iteration 3833, Loss: 1143018097.4948292\n",
      "Iteration 3834, Loss: 1046441440.4037992\n",
      "Iteration 3835, Loss: 1748924739.5347297\n",
      "Iteration 3836, Loss: 1012732263.0558152\n",
      "Iteration 3837, Loss: 1102550101.0231614\n",
      "Iteration 3838, Loss: 1011800164.1205821\n",
      "Iteration 3839, Loss: 1333115330.0749733\n",
      "Iteration 3840, Loss: 1041753391.9227204\n",
      "Iteration 3841, Loss: 1031047909.77517\n",
      "Iteration 3842, Loss: 1062935394.7766021\n",
      "Iteration 3843, Loss: 1137494085.7341852\n",
      "Iteration 3844, Loss: 1010786617.2297368\n",
      "Iteration 3845, Loss: 997952209.6174598\n",
      "Iteration 3846, Loss: 1184774799.570193\n",
      "Iteration 3847, Loss: 1165235411.9720478\n",
      "Iteration 3848, Loss: 1086664174.8943045\n",
      "Iteration 3849, Loss: 1194734956.8842456\n",
      "Iteration 3850, Loss: 1067712212.0284351\n",
      "Iteration 3851, Loss: 1425390182.0007193\n",
      "Iteration 3852, Loss: 1367486785.7543902\n",
      "Iteration 3853, Loss: 1339135040.7100573\n",
      "Iteration 3854, Loss: 1294770273.7840683\n",
      "Iteration 3855, Loss: 1130877701.320872\n",
      "Iteration 3856, Loss: 1137696399.4583943\n",
      "Iteration 3857, Loss: 1181340986.9202158\n",
      "Iteration 3858, Loss: 1204838186.5036085\n",
      "Iteration 3859, Loss: 1220733783.2126184\n",
      "Iteration 3860, Loss: 1200095231.6290748\n",
      "Iteration 3861, Loss: 1008268561.6319566\n",
      "Iteration 3862, Loss: 1365768204.8353047\n",
      "Iteration 3863, Loss: 1153218571.139164\n",
      "Iteration 3864, Loss: 1127604494.5832977\n",
      "Iteration 3865, Loss: 1110720911.8512907\n",
      "Iteration 3866, Loss: 1082727425.1414487\n",
      "Iteration 3867, Loss: 1101232181.6723788\n",
      "Iteration 3868, Loss: 1071273185.1154294\n",
      "Iteration 3869, Loss: 1044187673.1898447\n",
      "Iteration 3870, Loss: 1233008994.9782596\n",
      "Iteration 3871, Loss: 1369591354.3891077\n",
      "Iteration 3872, Loss: 1315412099.6137788\n",
      "Iteration 3873, Loss: 1006277940.3219953\n",
      "Iteration 3874, Loss: 1031200905.8070836\n",
      "Iteration 3875, Loss: 986218201.8529307\n",
      "Iteration 3876, Loss: 966659298.5109562\n",
      "Iteration 3877, Loss: 955813920.8877456\n",
      "Iteration 3878, Loss: 3073273501.2637506\n",
      "Iteration 3879, Loss: 1087551712.710359\n",
      "Iteration 3880, Loss: 972644450.6978137\n",
      "Iteration 3881, Loss: 3007135675.5160527\n",
      "Iteration 3882, Loss: 3549915350.289352\n",
      "Iteration 3883, Loss: 3747895078.4943914\n",
      "Iteration 3884, Loss: 1454557455.7293808\n",
      "Iteration 3885, Loss: 1054145023.0087913\n",
      "Iteration 3886, Loss: 1049430525.8939462\n",
      "Iteration 3887, Loss: 1010565116.8420005\n",
      "Iteration 3888, Loss: 1004595562.6161991\n",
      "Iteration 3889, Loss: 2140286199.4445198\n",
      "Iteration 3890, Loss: 1812809345.7346275\n",
      "Iteration 3891, Loss: 1021328285.012347\n",
      "Iteration 3892, Loss: 1015855290.2115816\n",
      "Iteration 3893, Loss: 1004817797.9938254\n",
      "Iteration 3894, Loss: 2159327616.956449\n",
      "Iteration 3895, Loss: 1033976940.7068062\n",
      "Iteration 3896, Loss: 1441354623.0295672\n",
      "Iteration 3897, Loss: 1312839443.5070457\n",
      "Iteration 3898, Loss: 1312272642.4461904\n",
      "Iteration 3899, Loss: 1289219251.8163157\n",
      "Iteration 3900, Loss: 1153405830.7821906\n",
      "Iteration 3901, Loss: 1175670060.392183\n",
      "Iteration 3902, Loss: 1111361443.5923939\n",
      "Iteration 3903, Loss: 1148639087.7298996\n",
      "Iteration 3904, Loss: 1413392355.8069756\n",
      "Iteration 3905, Loss: 1163524062.5709662\n",
      "Iteration 3906, Loss: 1154890180.8479922\n",
      "Iteration 3907, Loss: 1450074449.9900348\n",
      "Iteration 3908, Loss: 1452354027.1973796\n",
      "Iteration 3909, Loss: 1408441617.5768018\n",
      "Iteration 3910, Loss: 1395480124.9111078\n",
      "Iteration 3911, Loss: 1379393972.2530808\n",
      "Iteration 3912, Loss: 1254026581.749302\n",
      "Iteration 3913, Loss: 1302064933.7715352\n",
      "Iteration 3914, Loss: 1113035107.9036868\n",
      "Iteration 3915, Loss: 1216860971.4888084\n",
      "Iteration 3916, Loss: 1050754044.7137017\n",
      "Iteration 3917, Loss: 1272427720.1876483\n",
      "Iteration 3918, Loss: 1168678860.9481266\n",
      "Iteration 3919, Loss: 1196524086.9742844\n",
      "Iteration 3920, Loss: 1134352819.58166\n",
      "Iteration 3921, Loss: 1178243338.1044142\n",
      "Iteration 3922, Loss: 1669523318.9928045\n",
      "Iteration 3923, Loss: 1603220353.9385996\n",
      "Iteration 3924, Loss: 1548047018.9975355\n",
      "Iteration 3925, Loss: 1328060721.0528204\n",
      "Iteration 3926, Loss: 1369162487.3560662\n",
      "Iteration 3927, Loss: 1172722485.738573\n",
      "Iteration 3928, Loss: 1264514746.4545617\n",
      "Iteration 3929, Loss: 1240003976.456417\n",
      "Iteration 3930, Loss: 1039806675.9560955\n",
      "Iteration 3931, Loss: 1139859428.7362409\n",
      "Iteration 3932, Loss: 1175007301.1123629\n",
      "Iteration 3933, Loss: 1141868100.8298235\n",
      "Iteration 3934, Loss: 1070187430.151646\n",
      "Iteration 3935, Loss: 1063458508.6339402\n",
      "Iteration 3936, Loss: 1043373159.2900146\n",
      "Iteration 3937, Loss: 1660348559.2671077\n",
      "Iteration 3938, Loss: 1275959502.123105\n",
      "Iteration 3939, Loss: 1308144199.7647524\n",
      "Iteration 3940, Loss: 1275912426.4576864\n",
      "Iteration 3941, Loss: 1106026745.103126\n",
      "Iteration 3942, Loss: 1212807382.4403121\n",
      "Iteration 3943, Loss: 1542725259.8467405\n",
      "Iteration 3944, Loss: 1329394844.778954\n",
      "Iteration 3945, Loss: 1081498263.7755399\n",
      "Iteration 3946, Loss: 1101696772.540159\n",
      "Iteration 3947, Loss: 1095678289.6596556\n",
      "Iteration 3948, Loss: 1093231371.4741538\n",
      "Iteration 3949, Loss: 1087854069.812744\n",
      "Iteration 3950, Loss: 1136238142.4981644\n",
      "Iteration 3951, Loss: 1119135752.4086566\n",
      "Iteration 3952, Loss: 1224128591.054201\n",
      "Iteration 3953, Loss: 1193353800.071579\n",
      "Iteration 3954, Loss: 1226353845.2063167\n",
      "Iteration 3955, Loss: 1226554648.4791899\n",
      "Iteration 3956, Loss: 1140157872.6018028\n",
      "Iteration 3957, Loss: 1205413970.113814\n",
      "Iteration 3958, Loss: 1114080919.042584\n",
      "Iteration 3959, Loss: 2019355484.1342804\n",
      "Iteration 3960, Loss: 1708461616.4566674\n",
      "Iteration 3961, Loss: 1671587626.5636535\n",
      "Iteration 3962, Loss: 1073647792.7120658\n",
      "Iteration 3963, Loss: 1760258441.700756\n",
      "Iteration 3964, Loss: 1744908734.7947598\n",
      "Iteration 3965, Loss: 1141368609.2148254\n",
      "Iteration 3966, Loss: 1139181192.0053046\n",
      "Iteration 3967, Loss: 1214862224.040218\n",
      "Iteration 3968, Loss: 1287934058.2142663\n",
      "Iteration 3969, Loss: 1349946680.0417302\n",
      "Iteration 3970, Loss: 1100006855.459233\n",
      "Iteration 3971, Loss: 1524817868.2432823\n",
      "Iteration 3972, Loss: 1502577711.6023319\n",
      "Iteration 3973, Loss: 1208404555.1689317\n",
      "Iteration 3974, Loss: 1331838684.2136447\n",
      "Iteration 3975, Loss: 1324386208.2814825\n",
      "Iteration 3976, Loss: 1384792843.26051\n",
      "Iteration 3977, Loss: 1418822796.2870674\n",
      "Iteration 3978, Loss: 1269434462.4477632\n",
      "Iteration 3979, Loss: 1176837143.783304\n",
      "Iteration 3980, Loss: 1318569038.395215\n",
      "Iteration 3981, Loss: 1228654527.8534722\n",
      "Iteration 3982, Loss: 1299974826.5967054\n",
      "Iteration 3983, Loss: 1106823033.0618353\n",
      "Iteration 3984, Loss: 1373316965.6101453\n",
      "Iteration 3985, Loss: 1410140263.2945664\n",
      "Iteration 3986, Loss: 1403195762.5009103\n",
      "Iteration 3987, Loss: 1425284638.1459904\n",
      "Iteration 3988, Loss: 1154417417.6652267\n",
      "Iteration 3989, Loss: 1243990232.8127954\n",
      "Iteration 3990, Loss: 1351594096.505578\n",
      "Iteration 3991, Loss: 1216000448.0846682\n",
      "Iteration 3992, Loss: 1320163154.3764045\n",
      "Iteration 3993, Loss: 1255514702.980748\n",
      "Iteration 3994, Loss: 1280878320.8973134\n",
      "Iteration 3995, Loss: 1343058242.5899625\n",
      "Iteration 3996, Loss: 1410487970.58131\n",
      "Iteration 3997, Loss: 1353338615.6828287\n",
      "Iteration 3998, Loss: 1369795356.126737\n",
      "Iteration 3999, Loss: 1380565190.66694\n",
      "Iteration 4000, Loss: 1440589309.5491827\n",
      "Iteration 4001, Loss: 1406739053.1913817\n",
      "Iteration 4002, Loss: 1166995026.0429406\n",
      "Iteration 4003, Loss: 1497621765.9681313\n",
      "Iteration 4004, Loss: 1458991266.6616817\n",
      "Iteration 4005, Loss: 1547374750.3587296\n",
      "Iteration 4006, Loss: 1366363038.4490976\n",
      "Iteration 4007, Loss: 1475361841.4636676\n",
      "Iteration 4008, Loss: 1519047264.8468382\n",
      "Iteration 4009, Loss: 1484177127.903661\n",
      "Iteration 4010, Loss: 1518080440.8808434\n",
      "Iteration 4011, Loss: 1372211018.6378572\n",
      "Iteration 4012, Loss: 1283584441.9627407\n",
      "Iteration 4013, Loss: 1144358392.8803244\n",
      "Iteration 4014, Loss: 1396241428.2320635\n",
      "Iteration 4015, Loss: 1169621556.7557213\n",
      "Iteration 4016, Loss: 1152655808.2023108\n",
      "Iteration 4017, Loss: 1171339878.3962903\n",
      "Iteration 4018, Loss: 1273476987.8883295\n",
      "Iteration 4019, Loss: 1317285344.4847338\n",
      "Iteration 4020, Loss: 1403254549.8759658\n",
      "Iteration 4021, Loss: 1155097484.7211971\n",
      "Iteration 4022, Loss: 1202501873.3545697\n",
      "Iteration 4023, Loss: 1422933837.1402538\n",
      "Iteration 4024, Loss: 1294878255.5183012\n",
      "Iteration 4025, Loss: 1295037245.4371967\n",
      "Iteration 4026, Loss: 1366037009.4561188\n",
      "Iteration 4027, Loss: 1361712939.0830321\n",
      "Iteration 4028, Loss: 1148411443.6903574\n",
      "Iteration 4029, Loss: 1350954937.156363\n",
      "Iteration 4030, Loss: 1354417027.9066663\n",
      "Iteration 4031, Loss: 1193853818.8195128\n",
      "Iteration 4032, Loss: 1284530455.4890373\n",
      "Iteration 4033, Loss: 1289784445.7203345\n",
      "Iteration 4034, Loss: 1128674538.665347\n",
      "Iteration 4035, Loss: 1134403334.024057\n",
      "Iteration 4036, Loss: 1353787985.8477077\n",
      "Iteration 4037, Loss: 1131687363.0793226\n",
      "Iteration 4038, Loss: 1120321257.706068\n",
      "Iteration 4039, Loss: 2588596876.908801\n",
      "Iteration 4040, Loss: 2420041265.081417\n",
      "Iteration 4041, Loss: 2365462610.402325\n",
      "Iteration 4042, Loss: 2253050607.4688344\n",
      "Iteration 4043, Loss: 1730600097.6267622\n",
      "Iteration 4044, Loss: 1486863932.3653882\n",
      "Iteration 4045, Loss: 1533937606.7264657\n",
      "Iteration 4046, Loss: 1536966806.0577831\n",
      "Iteration 4047, Loss: 1495895569.4454021\n",
      "Iteration 4048, Loss: 1140287151.0446174\n",
      "Iteration 4049, Loss: 1197253402.2324626\n",
      "Iteration 4050, Loss: 1198641396.2951784\n",
      "Iteration 4051, Loss: 1256493099.9422903\n",
      "Iteration 4052, Loss: 1352711813.2621267\n",
      "Iteration 4053, Loss: 1134569046.83634\n",
      "Iteration 4054, Loss: 1496555243.8375757\n",
      "Iteration 4055, Loss: 1281822349.6665757\n",
      "Iteration 4056, Loss: 1283442064.6082869\n",
      "Iteration 4057, Loss: 1191391932.337423\n",
      "Iteration 4058, Loss: 1246440250.2151504\n",
      "Iteration 4059, Loss: 1284592594.4445324\n",
      "Iteration 4060, Loss: 1126871600.946819\n",
      "Iteration 4061, Loss: 1322725788.9421465\n",
      "Iteration 4062, Loss: 1407767637.0758917\n",
      "Iteration 4063, Loss: 1365517910.2289042\n",
      "Iteration 4064, Loss: 1192251519.1742926\n",
      "Iteration 4065, Loss: 1328482010.7892144\n",
      "Iteration 4066, Loss: 1231974956.7850904\n",
      "Iteration 4067, Loss: 1300458379.3128572\n",
      "Iteration 4068, Loss: 1128665095.5793855\n",
      "Iteration 4069, Loss: 1126634188.3727438\n",
      "Iteration 4070, Loss: 1187235893.7113738\n",
      "Iteration 4071, Loss: 1370581508.312766\n",
      "Iteration 4072, Loss: 1131268446.5304334\n",
      "Iteration 4073, Loss: 1106057046.1436005\n",
      "Iteration 4074, Loss: 3453642974.3353834\n",
      "Iteration 4075, Loss: 2191545056.3436484\n",
      "Iteration 4076, Loss: 1964990666.4266942\n",
      "Iteration 4077, Loss: 1861608722.8141007\n",
      "Iteration 4078, Loss: 1836256462.7179601\n",
      "Iteration 4079, Loss: 1825503868.4099557\n",
      "Iteration 4080, Loss: 1789350599.3298602\n",
      "Iteration 4081, Loss: 1619841186.5285985\n",
      "Iteration 4082, Loss: 1557388541.2469342\n",
      "Iteration 4083, Loss: 1489323410.374361\n",
      "Iteration 4084, Loss: 1213050603.9711227\n",
      "Iteration 4085, Loss: 1156584213.419237\n",
      "Iteration 4086, Loss: 1147278540.3987231\n",
      "Iteration 4087, Loss: 1497799925.3003998\n",
      "Iteration 4088, Loss: 1468182744.7814496\n",
      "Iteration 4089, Loss: 1157257345.8573723\n",
      "Iteration 4090, Loss: 1265639034.2332447\n",
      "Iteration 4091, Loss: 1246507283.6090343\n",
      "Iteration 4092, Loss: 1238160816.0254948\n",
      "Iteration 4093, Loss: 1289751353.3351564\n",
      "Iteration 4094, Loss: 1317047063.55069\n",
      "Iteration 4095, Loss: 1249207610.8809063\n",
      "Iteration 4096, Loss: 1292803610.141741\n",
      "Iteration 4097, Loss: 1225874701.339793\n",
      "Iteration 4098, Loss: 1216408885.4413247\n",
      "Iteration 4099, Loss: 1232535507.1663225\n",
      "Iteration 4100, Loss: 1198978344.9660125\n",
      "Iteration 4101, Loss: 1201085535.2774963\n",
      "Iteration 4102, Loss: 1051811938.4632959\n",
      "Iteration 4103, Loss: 1026291426.3415438\n",
      "Iteration 4104, Loss: 1010470862.0776632\n",
      "Iteration 4105, Loss: 1220363580.127328\n",
      "Iteration 4106, Loss: 1086023560.8022027\n",
      "Iteration 4107, Loss: 1099828149.5997925\n",
      "Iteration 4108, Loss: 1246394571.152154\n",
      "Iteration 4109, Loss: 1005509457.8805032\n",
      "Iteration 4110, Loss: 995132296.5535913\n",
      "Iteration 4111, Loss: 984866848.6375413\n",
      "Iteration 4112, Loss: 1154901120.9689\n",
      "Iteration 4113, Loss: 1071818095.5280997\n",
      "Iteration 4114, Loss: 1166274166.8961008\n",
      "Iteration 4115, Loss: 1162211836.909798\n",
      "Iteration 4116, Loss: 979787630.9769613\n",
      "Iteration 4117, Loss: 1793406309.005024\n",
      "Iteration 4118, Loss: 1144281461.8292105\n",
      "Iteration 4119, Loss: 1189915004.4839137\n",
      "Iteration 4120, Loss: 1529610543.2800555\n",
      "Iteration 4121, Loss: 1456101689.6997268\n",
      "Iteration 4122, Loss: 1104851218.976063\n",
      "Iteration 4123, Loss: 1146395178.1398106\n",
      "Iteration 4124, Loss: 1228763739.2203126\n",
      "Iteration 4125, Loss: 1214721323.5396864\n",
      "Iteration 4126, Loss: 1178321654.3666203\n",
      "Iteration 4127, Loss: 1222743763.0689852\n",
      "Iteration 4128, Loss: 1190842630.6051183\n",
      "Iteration 4129, Loss: 1064798422.1790612\n",
      "Iteration 4130, Loss: 1294413150.1067488\n",
      "Iteration 4131, Loss: 1023795229.959548\n",
      "Iteration 4132, Loss: 1000349704.4174029\n",
      "Iteration 4133, Loss: 981147882.8902221\n",
      "Iteration 4134, Loss: 1455705742.643381\n",
      "Iteration 4135, Loss: 1379976303.8483741\n",
      "Iteration 4136, Loss: 1067450497.0854831\n",
      "Iteration 4137, Loss: 1196079336.003947\n",
      "Iteration 4138, Loss: 1179200520.7572753\n",
      "Iteration 4139, Loss: 1146920860.2617168\n",
      "Iteration 4140, Loss: 1215687035.9139266\n",
      "Iteration 4141, Loss: 1252728763.2235408\n",
      "Iteration 4142, Loss: 1277457767.3269832\n",
      "Iteration 4143, Loss: 1225906162.455981\n",
      "Iteration 4144, Loss: 1182559218.036905\n",
      "Iteration 4145, Loss: 1146469001.8808105\n",
      "Iteration 4146, Loss: 1056653973.1847124\n",
      "Iteration 4147, Loss: 1145238853.346021\n",
      "Iteration 4148, Loss: 1144752674.4107459\n",
      "Iteration 4149, Loss: 1061294184.4530499\n",
      "Iteration 4150, Loss: 1142511340.0971167\n",
      "Iteration 4151, Loss: 1108288057.5638955\n",
      "Iteration 4152, Loss: 1087825182.0503516\n",
      "Iteration 4153, Loss: 1100704495.512902\n",
      "Iteration 4154, Loss: 1142184704.0592148\n",
      "Iteration 4155, Loss: 1029674999.7357051\n",
      "Iteration 4156, Loss: 1000708417.1667714\n",
      "Iteration 4157, Loss: 1250586398.2453685\n",
      "Iteration 4158, Loss: 955946496.186782\n",
      "Iteration 4159, Loss: 935091055.6509056\n",
      "Iteration 4160, Loss: 925003137.9166337\n",
      "Iteration 4161, Loss: 915440404.259172\n",
      "Iteration 4162, Loss: 1250060434.845287\n",
      "Iteration 4163, Loss: 1215457637.013903\n",
      "Iteration 4164, Loss: 942625485.8316835\n",
      "Iteration 4165, Loss: 1079630714.022765\n",
      "Iteration 4166, Loss: 1728306678.8238704\n",
      "Iteration 4167, Loss: 974976048.8264151\n",
      "Iteration 4168, Loss: 961861937.3363677\n",
      "Iteration 4169, Loss: 956055697.7252448\n",
      "Iteration 4170, Loss: 944015750.6698335\n",
      "Iteration 4171, Loss: 2036632846.6505888\n",
      "Iteration 4172, Loss: 2048421005.6550152\n",
      "Iteration 4173, Loss: 2054575177.6951413\n",
      "Iteration 4174, Loss: 1601130584.169933\n",
      "Iteration 4175, Loss: 1398877075.9167743\n",
      "Iteration 4176, Loss: 1026956339.4364845\n",
      "Iteration 4177, Loss: 1962858274.5875976\n",
      "Iteration 4178, Loss: 974792179.0926864\n",
      "Iteration 4179, Loss: 1189574300.3259356\n",
      "Iteration 4180, Loss: 1142321063.591932\n",
      "Iteration 4181, Loss: 1041355620.8013866\n",
      "Iteration 4182, Loss: 1063022580.1868032\n",
      "Iteration 4183, Loss: 1086758408.7662055\n",
      "Iteration 4184, Loss: 1158751017.2733567\n",
      "Iteration 4185, Loss: 1085357425.785003\n",
      "Iteration 4186, Loss: 1211666426.3007507\n",
      "Iteration 4187, Loss: 1226726419.583287\n",
      "Iteration 4188, Loss: 977356465.3091276\n",
      "Iteration 4189, Loss: 1530003927.6564555\n",
      "Iteration 4190, Loss: 1098782853.071043\n",
      "Iteration 4191, Loss: 1064255394.4972521\n",
      "Iteration 4192, Loss: 974046569.1174501\n",
      "Iteration 4193, Loss: 1410206393.6474075\n",
      "Iteration 4194, Loss: 1437832477.2237248\n",
      "Iteration 4195, Loss: 1146218472.807608\n",
      "Iteration 4196, Loss: 1187599046.2469506\n",
      "Iteration 4197, Loss: 1204774299.1724505\n",
      "Iteration 4198, Loss: 1152324768.517298\n",
      "Iteration 4199, Loss: 1147859593.2461004\n",
      "Iteration 4200, Loss: 1182018659.9369667\n",
      "Iteration 4201, Loss: 1164185731.6190128\n",
      "Iteration 4202, Loss: 1103071676.1314259\n",
      "Iteration 4203, Loss: 1062475773.3821368\n",
      "Iteration 4204, Loss: 1801888726.2589173\n",
      "Iteration 4205, Loss: 1593487558.557301\n",
      "Iteration 4206, Loss: 1494077508.297818\n",
      "Iteration 4207, Loss: 1033074575.7451844\n",
      "Iteration 4208, Loss: 1171201435.6184657\n",
      "Iteration 4209, Loss: 1043225875.2601854\n",
      "Iteration 4210, Loss: 1243109272.03743\n",
      "Iteration 4211, Loss: 968902674.2559289\n",
      "Iteration 4212, Loss: 967964447.8523254\n",
      "Iteration 4213, Loss: 965312741.0491413\n",
      "Iteration 4214, Loss: 954292105.3183322\n",
      "Iteration 4215, Loss: 1013313181.2062385\n",
      "Iteration 4216, Loss: 992939207.8165803\n",
      "Iteration 4217, Loss: 1068658140.6027164\n",
      "Iteration 4218, Loss: 1135663520.3485723\n",
      "Iteration 4219, Loss: 1035113940.2342014\n",
      "Iteration 4220, Loss: 992260659.5226835\n",
      "Iteration 4221, Loss: 960599222.3982819\n",
      "Iteration 4222, Loss: 1164575552.1578085\n",
      "Iteration 4223, Loss: 942326180.6074978\n",
      "Iteration 4224, Loss: 947553890.7411475\n",
      "Iteration 4225, Loss: 930318430.4576794\n",
      "Iteration 4226, Loss: 917690420.3640765\n",
      "Iteration 4227, Loss: 1194983125.5856576\n",
      "Iteration 4228, Loss: 1165215974.20238\n",
      "Iteration 4229, Loss: 1011914492.4497676\n",
      "Iteration 4230, Loss: 978647198.1245803\n",
      "Iteration 4231, Loss: 928608787.4185731\n",
      "Iteration 4232, Loss: 914099885.2579688\n",
      "Iteration 4233, Loss: 904903608.0972968\n",
      "Iteration 4234, Loss: 890746327.421686\n",
      "Iteration 4235, Loss: 890108447.7079426\n",
      "Iteration 4236, Loss: 3548672783.183938\n",
      "Iteration 4237, Loss: 6557386056.862539\n",
      "Iteration 4238, Loss: 50522811328.07369\n",
      "Iteration 4239, Loss: 14975506935.49095\n",
      "Iteration 4240, Loss: 14428038866.659847\n",
      "Iteration 4241, Loss: 6221647007.504213\n",
      "Iteration 4242, Loss: 8025156228.745224\n",
      "Iteration 4243, Loss: 5831137714.405954\n",
      "Iteration 4244, Loss: 4268219738.7468596\n",
      "Iteration 4245, Loss: 3353640554.599791\n",
      "Iteration 4246, Loss: 2168082352.240602\n",
      "Iteration 4247, Loss: 1920509663.4714637\n",
      "Iteration 4248, Loss: 1002027301.4211471\n",
      "Iteration 4249, Loss: 963594740.7940125\n",
      "Iteration 4250, Loss: 1310603519.0033221\n",
      "Iteration 4251, Loss: 1014654724.8508438\n",
      "Iteration 4252, Loss: 1021318553.7585999\n",
      "Iteration 4253, Loss: 1041123797.5459684\n",
      "Iteration 4254, Loss: 1267110793.1372538\n",
      "Iteration 4255, Loss: 997237910.0582492\n",
      "Iteration 4256, Loss: 1286020742.5975373\n",
      "Iteration 4257, Loss: 1186817577.5620096\n",
      "Iteration 4258, Loss: 1194566883.3831546\n",
      "Iteration 4259, Loss: 1005913093.048105\n",
      "Iteration 4260, Loss: 1095533431.1968493\n",
      "Iteration 4261, Loss: 1076671850.6026251\n",
      "Iteration 4262, Loss: 1172173480.035359\n",
      "Iteration 4263, Loss: 1196304295.2178519\n",
      "Iteration 4264, Loss: 1184431703.2657785\n",
      "Iteration 4265, Loss: 1216681066.9770875\n",
      "Iteration 4266, Loss: 1224118691.35367\n",
      "Iteration 4267, Loss: 1051219098.6407658\n",
      "Iteration 4268, Loss: 1143234707.6736393\n",
      "Iteration 4269, Loss: 1113311269.5684028\n",
      "Iteration 4270, Loss: 1204495132.199702\n",
      "Iteration 4271, Loss: 1050768967.9547298\n",
      "Iteration 4272, Loss: 1075921614.8826056\n",
      "Iteration 4273, Loss: 1128331370.8409207\n",
      "Iteration 4274, Loss: 1393070084.9055703\n",
      "Iteration 4275, Loss: 1361594013.9033341\n",
      "Iteration 4276, Loss: 1148310242.7415807\n",
      "Iteration 4277, Loss: 1101290312.5828106\n",
      "Iteration 4278, Loss: 1184844961.3223245\n",
      "Iteration 4279, Loss: 1054319395.1827086\n",
      "Iteration 4280, Loss: 1142778491.342809\n",
      "Iteration 4281, Loss: 1128248260.6904566\n",
      "Iteration 4282, Loss: 1111584097.0578134\n",
      "Iteration 4283, Loss: 1128065694.0317507\n",
      "Iteration 4284, Loss: 1096622572.4528465\n",
      "Iteration 4285, Loss: 1066289557.1093404\n",
      "Iteration 4286, Loss: 1162382969.3556468\n",
      "Iteration 4287, Loss: 1234017875.5193398\n",
      "Iteration 4288, Loss: 1229605652.566234\n",
      "Iteration 4289, Loss: 1073515567.3474482\n",
      "Iteration 4290, Loss: 1035527382.4348882\n",
      "Iteration 4291, Loss: 1594322292.1483798\n",
      "Iteration 4292, Loss: 1489373040.3221173\n",
      "Iteration 4293, Loss: 1036461859.2912295\n",
      "Iteration 4294, Loss: 1085069578.71231\n",
      "Iteration 4295, Loss: 1169311135.1576555\n",
      "Iteration 4296, Loss: 1283501620.6501064\n",
      "Iteration 4297, Loss: 1288763183.728606\n",
      "Iteration 4298, Loss: 1313334605.52333\n",
      "Iteration 4299, Loss: 1173331342.6918936\n",
      "Iteration 4300, Loss: 1128529246.4631896\n",
      "Iteration 4301, Loss: 1121145101.663324\n",
      "Iteration 4302, Loss: 1573768140.811396\n",
      "Iteration 4303, Loss: 1532351817.0089338\n",
      "Iteration 4304, Loss: 1241232490.8490808\n",
      "Iteration 4305, Loss: 1158094432.7881882\n",
      "Iteration 4306, Loss: 1121603512.4502082\n",
      "Iteration 4307, Loss: 1243633121.9452052\n",
      "Iteration 4308, Loss: 1238243855.5862715\n",
      "Iteration 4309, Loss: 1290104511.5115218\n",
      "Iteration 4310, Loss: 1108554793.1013944\n",
      "Iteration 4311, Loss: 1162459863.2552645\n",
      "Iteration 4312, Loss: 1267890605.8402796\n",
      "Iteration 4313, Loss: 1245911482.2718227\n",
      "Iteration 4314, Loss: 1229491614.9210417\n",
      "Iteration 4315, Loss: 1123580197.1651485\n",
      "Iteration 4316, Loss: 1120025249.6240628\n",
      "Iteration 4317, Loss: 1111864327.8502133\n",
      "Iteration 4318, Loss: 1277813033.6821396\n",
      "Iteration 4319, Loss: 1242769581.4589088\n",
      "Iteration 4320, Loss: 1265154827.536774\n",
      "Iteration 4321, Loss: 1190497406.900941\n",
      "Iteration 4322, Loss: 1012769487.1125531\n",
      "Iteration 4323, Loss: 1265315192.8305972\n",
      "Iteration 4324, Loss: 1018090153.1809558\n",
      "Iteration 4325, Loss: 1252927474.1116712\n",
      "Iteration 4326, Loss: 1225158813.5870693\n",
      "Iteration 4327, Loss: 1192126769.460012\n",
      "Iteration 4328, Loss: 1221452991.9597166\n",
      "Iteration 4329, Loss: 1077485974.3807392\n",
      "Iteration 4330, Loss: 1333143083.9814055\n",
      "Iteration 4331, Loss: 1199415463.2093923\n",
      "Iteration 4332, Loss: 1110410511.1722825\n",
      "Iteration 4333, Loss: 1188579516.1305485\n",
      "Iteration 4334, Loss: 1132596943.2552752\n",
      "Iteration 4335, Loss: 1239021666.4657426\n",
      "Iteration 4336, Loss: 1218430896.4874403\n",
      "Iteration 4337, Loss: 1076488005.853378\n",
      "Iteration 4338, Loss: 1678978827.452909\n",
      "Iteration 4339, Loss: 1087390338.7993522\n",
      "Iteration 4340, Loss: 1203476521.4507077\n",
      "Iteration 4341, Loss: 1185931624.969818\n",
      "Iteration 4342, Loss: 1186428344.9546568\n",
      "Iteration 4343, Loss: 1228834610.9147718\n",
      "Iteration 4344, Loss: 1547682390.196722\n",
      "Iteration 4345, Loss: 1490044246.1844552\n",
      "Iteration 4346, Loss: 1151801533.82885\n",
      "Iteration 4347, Loss: 1120441577.547395\n",
      "Iteration 4348, Loss: 1074078319.5922863\n",
      "Iteration 4349, Loss: 1041440260.08916\n",
      "Iteration 4350, Loss: 1023654735.6544509\n",
      "Iteration 4351, Loss: 1021500810.0939124\n",
      "Iteration 4352, Loss: 1256866972.2704127\n",
      "Iteration 4353, Loss: 1226799996.9335124\n",
      "Iteration 4354, Loss: 1253954945.5501647\n",
      "Iteration 4355, Loss: 1056137914.7089521\n",
      "Iteration 4356, Loss: 1040951508.4687042\n",
      "Iteration 4357, Loss: 2066271028.6795225\n",
      "Iteration 4358, Loss: 1963359993.8210363\n",
      "Iteration 4359, Loss: 995664210.3995246\n",
      "Iteration 4360, Loss: 984266780.232696\n",
      "Iteration 4361, Loss: 3198568525.5434275\n",
      "Iteration 4362, Loss: 1731257200.295112\n",
      "Iteration 4363, Loss: 1061552677.377608\n",
      "Iteration 4364, Loss: 1043305182.6157471\n",
      "Iteration 4365, Loss: 1028795808.1137668\n",
      "Iteration 4366, Loss: 1061463931.2727474\n",
      "Iteration 4367, Loss: 1169638184.2475429\n",
      "Iteration 4368, Loss: 1102226423.442548\n",
      "Iteration 4369, Loss: 1089016583.3162515\n",
      "Iteration 4370, Loss: 1105620761.7921238\n",
      "Iteration 4371, Loss: 1084121468.8117661\n",
      "Iteration 4372, Loss: 1105992779.573834\n",
      "Iteration 4373, Loss: 1077644397.4794784\n",
      "Iteration 4374, Loss: 1083135340.18116\n",
      "Iteration 4375, Loss: 1142534539.7819211\n",
      "Iteration 4376, Loss: 1069768976.1503906\n",
      "Iteration 4377, Loss: 982424584.8027252\n",
      "Iteration 4378, Loss: 968984114.8878014\n",
      "Iteration 4379, Loss: 1323235025.7977295\n",
      "Iteration 4380, Loss: 1225714323.3332803\n",
      "Iteration 4381, Loss: 1057957901.3595787\n",
      "Iteration 4382, Loss: 1879838856.1899974\n",
      "Iteration 4383, Loss: 1587762769.285483\n",
      "Iteration 4384, Loss: 981436176.5451927\n",
      "Iteration 4385, Loss: 2853229565.119414\n",
      "Iteration 4386, Loss: 1027612339.0532899\n",
      "Iteration 4387, Loss: 1070316293.2943937\n",
      "Iteration 4388, Loss: 1228610070.0725844\n",
      "Iteration 4389, Loss: 1081212202.8170805\n",
      "Iteration 4390, Loss: 1242016300.2268126\n",
      "Iteration 4391, Loss: 1212089167.2692432\n",
      "Iteration 4392, Loss: 1015772770.0519549\n",
      "Iteration 4393, Loss: 1012337082.3228033\n",
      "Iteration 4394, Loss: 1040721174.6869076\n",
      "Iteration 4395, Loss: 1022860361.9905661\n",
      "Iteration 4396, Loss: 1105755063.3060791\n",
      "Iteration 4397, Loss: 1122270818.822906\n",
      "Iteration 4398, Loss: 1100597143.4411094\n",
      "Iteration 4399, Loss: 1063954316.8622535\n",
      "Iteration 4400, Loss: 1052090216.8629271\n",
      "Iteration 4401, Loss: 1099643023.1895943\n",
      "Iteration 4402, Loss: 1692789071.4555283\n",
      "Iteration 4403, Loss: 1018599989.0243589\n",
      "Iteration 4404, Loss: 1006659444.918138\n",
      "Iteration 4405, Loss: 990404639.3637645\n",
      "Iteration 4406, Loss: 2435648848.545246\n",
      "Iteration 4407, Loss: 3380193142.096105\n",
      "Iteration 4408, Loss: 1821960963.8045657\n",
      "Iteration 4409, Loss: 1743157329.862695\n",
      "Iteration 4410, Loss: 1237128041.7159734\n",
      "Iteration 4411, Loss: 5728143677.214698\n",
      "Iteration 4412, Loss: 1311590497.8778899\n",
      "Iteration 4413, Loss: 4912721016.789916\n",
      "Iteration 4414, Loss: 2927904299.1372647\n",
      "Iteration 4415, Loss: 2409256235.6207666\n",
      "Iteration 4416, Loss: 1694440274.886624\n",
      "Iteration 4417, Loss: 1058093527.6947101\n",
      "Iteration 4418, Loss: 1041143048.7684444\n",
      "Iteration 4419, Loss: 1042030331.8501871\n",
      "Iteration 4420, Loss: 1711218304.2211509\n",
      "Iteration 4421, Loss: 1487367826.199325\n",
      "Iteration 4422, Loss: 1153282414.3630607\n",
      "Iteration 4423, Loss: 1055479236.040394\n",
      "Iteration 4424, Loss: 1055933709.4515669\n",
      "Iteration 4425, Loss: 1056711750.0017306\n",
      "Iteration 4426, Loss: 1631298013.8420084\n",
      "Iteration 4427, Loss: 1081174068.3990855\n",
      "Iteration 4428, Loss: 1185690243.4071848\n",
      "Iteration 4429, Loss: 1177934316.7342787\n",
      "Iteration 4430, Loss: 1694300049.5472715\n",
      "Iteration 4431, Loss: 1214374494.9692085\n",
      "Iteration 4432, Loss: 1202708084.474611\n",
      "Iteration 4433, Loss: 1190555399.3111289\n",
      "Iteration 4434, Loss: 1047755810.0264484\n",
      "Iteration 4435, Loss: 1049245693.8759353\n",
      "Iteration 4436, Loss: 1037197257.9351218\n",
      "Iteration 4437, Loss: 1259955599.1468592\n",
      "Iteration 4438, Loss: 1299272850.0877032\n",
      "Iteration 4439, Loss: 1279184151.6841784\n",
      "Iteration 4440, Loss: 1236973759.7937527\n",
      "Iteration 4441, Loss: 1069047019.671875\n",
      "Iteration 4442, Loss: 1061910212.2491579\n",
      "Iteration 4443, Loss: 1030073034.8744382\n",
      "Iteration 4444, Loss: 1018670514.2489288\n",
      "Iteration 4445, Loss: 1012419165.5356822\n",
      "Iteration 4446, Loss: 1244444717.880654\n",
      "Iteration 4447, Loss: 1087336243.9344447\n",
      "Iteration 4448, Loss: 1929029495.8425462\n",
      "Iteration 4449, Loss: 1962820090.8461192\n",
      "Iteration 4450, Loss: 1086199079.5476935\n",
      "Iteration 4451, Loss: 1262387099.085828\n",
      "Iteration 4452, Loss: 1163292992.684687\n",
      "Iteration 4453, Loss: 1291674490.0728002\n",
      "Iteration 4454, Loss: 1106497596.5616682\n",
      "Iteration 4455, Loss: 1057462507.5464587\n",
      "Iteration 4456, Loss: 2423270136.3425746\n",
      "Iteration 4457, Loss: 1725085225.2250936\n",
      "Iteration 4458, Loss: 1341948095.4758167\n",
      "Iteration 4459, Loss: 1318468729.520772\n",
      "Iteration 4460, Loss: 1077783000.49933\n",
      "Iteration 4461, Loss: 1052736236.7957155\n",
      "Iteration 4462, Loss: 1349196790.713593\n",
      "Iteration 4463, Loss: 1101854320.1452186\n",
      "Iteration 4464, Loss: 1243409528.7436604\n",
      "Iteration 4465, Loss: 1332253131.5197392\n",
      "Iteration 4466, Loss: 1385331472.2768192\n",
      "Iteration 4467, Loss: 1287993513.301236\n",
      "Iteration 4468, Loss: 1151583155.0725758\n",
      "Iteration 4469, Loss: 1148261598.5163293\n",
      "Iteration 4470, Loss: 1069813841.5884854\n",
      "Iteration 4471, Loss: 1093503010.9435315\n",
      "Iteration 4472, Loss: 1056569261.4475892\n",
      "Iteration 4473, Loss: 1212060965.4601572\n",
      "Iteration 4474, Loss: 1200771749.5802333\n",
      "Iteration 4475, Loss: 1196129658.1602652\n",
      "Iteration 4476, Loss: 1249204878.1238954\n",
      "Iteration 4477, Loss: 1241690384.4106066\n",
      "Iteration 4478, Loss: 1217670234.178749\n",
      "Iteration 4479, Loss: 1169038704.7064395\n",
      "Iteration 4480, Loss: 1123360932.9997604\n",
      "Iteration 4481, Loss: 1106616702.6653714\n",
      "Iteration 4482, Loss: 1072222781.8998127\n",
      "Iteration 4483, Loss: 1104059596.2313907\n",
      "Iteration 4484, Loss: 1210697739.1357758\n",
      "Iteration 4485, Loss: 1217453873.4767942\n",
      "Iteration 4486, Loss: 1212420063.8261075\n",
      "Iteration 4487, Loss: 1127142909.4993422\n",
      "Iteration 4488, Loss: 1125073108.450534\n",
      "Iteration 4489, Loss: 1119943891.6124518\n",
      "Iteration 4490, Loss: 1110373141.9857306\n",
      "Iteration 4491, Loss: 1205918636.5928278\n",
      "Iteration 4492, Loss: 1068834528.3743142\n",
      "Iteration 4493, Loss: 1056417776.7418166\n",
      "Iteration 4494, Loss: 1497483369.1024656\n",
      "Iteration 4495, Loss: 1430772018.0447505\n",
      "Iteration 4496, Loss: 1377845818.150036\n",
      "Iteration 4497, Loss: 1067323545.4267081\n",
      "Iteration 4498, Loss: 1272518382.194249\n",
      "Iteration 4499, Loss: 1064692604.8978662\n",
      "Iteration 4500, Loss: 1066277623.2983277\n",
      "Iteration 4501, Loss: 1062994164.9048758\n",
      "Iteration 4502, Loss: 2325461132.410778\n",
      "Iteration 4503, Loss: 2099690976.6025543\n",
      "Iteration 4504, Loss: 2044079557.2209609\n",
      "Iteration 4505, Loss: 1345672644.4013276\n",
      "Iteration 4506, Loss: 1337149873.3853495\n",
      "Iteration 4507, Loss: 1217764581.2290642\n",
      "Iteration 4508, Loss: 1286848278.8983393\n",
      "Iteration 4509, Loss: 1064047396.077469\n",
      "Iteration 4510, Loss: 1349417778.581576\n",
      "Iteration 4511, Loss: 1096093965.917517\n",
      "Iteration 4512, Loss: 1148943453.8318555\n",
      "Iteration 4513, Loss: 1237225580.2158113\n",
      "Iteration 4514, Loss: 1341026431.2119343\n",
      "Iteration 4515, Loss: 1199775488.2989798\n",
      "Iteration 4516, Loss: 1270914701.4846027\n",
      "Iteration 4517, Loss: 1171045557.2861373\n",
      "Iteration 4518, Loss: 1238264163.0323539\n",
      "Iteration 4519, Loss: 1353596006.11911\n",
      "Iteration 4520, Loss: 1420203696.4995794\n",
      "Iteration 4521, Loss: 1437571795.7183404\n",
      "Iteration 4522, Loss: 1411170236.380059\n",
      "Iteration 4523, Loss: 1169252153.7148554\n",
      "Iteration 4524, Loss: 1355502730.2809107\n",
      "Iteration 4525, Loss: 1247321230.0903113\n",
      "Iteration 4526, Loss: 1245515929.3139648\n",
      "Iteration 4527, Loss: 1205933648.0934193\n",
      "Iteration 4528, Loss: 1397799245.0778441\n",
      "Iteration 4529, Loss: 1184731244.8785627\n",
      "Iteration 4530, Loss: 1320833370.3930986\n",
      "Iteration 4531, Loss: 1485433811.4123151\n",
      "Iteration 4532, Loss: 1557532454.913833\n",
      "Iteration 4533, Loss: 1375644020.617285\n",
      "Iteration 4534, Loss: 1198298350.9720092\n",
      "Iteration 4535, Loss: 1225734985.2497666\n",
      "Iteration 4536, Loss: 1220786157.5138996\n",
      "Iteration 4537, Loss: 1221775806.1265464\n",
      "Iteration 4538, Loss: 1256954188.4033978\n",
      "Iteration 4539, Loss: 1351554415.729168\n",
      "Iteration 4540, Loss: 1460128138.1103942\n",
      "Iteration 4541, Loss: 1335158782.7009656\n",
      "Iteration 4542, Loss: 1418611289.667962\n",
      "Iteration 4543, Loss: 1216642458.1257277\n",
      "Iteration 4544, Loss: 1582220211.3006778\n",
      "Iteration 4545, Loss: 1421040429.1830497\n",
      "Iteration 4546, Loss: 1268081284.2895298\n",
      "Iteration 4547, Loss: 1253167681.7633266\n",
      "Iteration 4548, Loss: 1484556549.56634\n",
      "Iteration 4549, Loss: 1309074696.6571732\n",
      "Iteration 4550, Loss: 1462071058.6159778\n",
      "Iteration 4551, Loss: 1442183331.721105\n",
      "Iteration 4552, Loss: 1549617090.9207227\n",
      "Iteration 4553, Loss: 1280910046.3011143\n",
      "Iteration 4554, Loss: 1438037461.80992\n",
      "Iteration 4555, Loss: 1456369986.998215\n",
      "Iteration 4556, Loss: 1499164920.5615332\n",
      "Iteration 4557, Loss: 1508024009.4692957\n",
      "Iteration 4558, Loss: 1550439924.1821144\n",
      "Iteration 4559, Loss: 1202647428.8084512\n",
      "Iteration 4560, Loss: 1220107252.4525054\n",
      "Iteration 4561, Loss: 1856135586.9263806\n",
      "Iteration 4562, Loss: 1191480123.1087043\n",
      "Iteration 4563, Loss: 1196947913.3346617\n",
      "Iteration 4564, Loss: 1192325279.006478\n",
      "Iteration 4565, Loss: 1636421651.7785108\n",
      "Iteration 4566, Loss: 1240341665.26914\n",
      "Iteration 4567, Loss: 1252036080.0562472\n",
      "Iteration 4568, Loss: 1400868835.2828257\n",
      "Iteration 4569, Loss: 1182061175.0977397\n",
      "Iteration 4570, Loss: 1158336969.7605639\n",
      "Iteration 4571, Loss: 1940696149.858226\n",
      "Iteration 4572, Loss: 1902316349.0108519\n",
      "Iteration 4573, Loss: 1643704737.2888312\n",
      "Iteration 4574, Loss: 1711869762.5131066\n",
      "Iteration 4575, Loss: 1232649226.5056589\n",
      "Iteration 4576, Loss: 1253159368.1673336\n",
      "Iteration 4577, Loss: 1248917270.7393198\n",
      "Iteration 4578, Loss: 1208378749.845083\n",
      "Iteration 4579, Loss: 1216357280.5270205\n",
      "Iteration 4580, Loss: 1309756256.969561\n",
      "Iteration 4581, Loss: 1183905550.4733646\n",
      "Iteration 4582, Loss: 1194939218.6788561\n",
      "Iteration 4583, Loss: 1207510756.3376505\n",
      "Iteration 4584, Loss: 1237079462.6407328\n",
      "Iteration 4585, Loss: 1392466223.8231137\n",
      "Iteration 4586, Loss: 1169745010.9045138\n",
      "Iteration 4587, Loss: 1177225075.9130232\n",
      "Iteration 4588, Loss: 1390978313.0379825\n",
      "Iteration 4589, Loss: 1448886577.4694343\n",
      "Iteration 4590, Loss: 1336706943.1268551\n",
      "Iteration 4591, Loss: 1291439849.7018678\n",
      "Iteration 4592, Loss: 1171205813.8245811\n",
      "Iteration 4593, Loss: 2337865897.11716\n",
      "Iteration 4594, Loss: 2229814309.1305094\n",
      "Iteration 4595, Loss: 2241468080.6178694\n",
      "Iteration 4596, Loss: 1788708107.8368149\n",
      "Iteration 4597, Loss: 1816439197.2596087\n",
      "Iteration 4598, Loss: 1195032699.545797\n",
      "Iteration 4599, Loss: 1164264158.4276052\n",
      "Iteration 4600, Loss: 1257017993.8076656\n",
      "Iteration 4601, Loss: 1277017341.9487147\n",
      "Iteration 4602, Loss: 1337909470.6275594\n",
      "Iteration 4603, Loss: 1349603816.0167499\n",
      "Iteration 4604, Loss: 1305675023.1172378\n",
      "Iteration 4605, Loss: 1427537666.3406546\n",
      "Iteration 4606, Loss: 1154072070.747136\n",
      "Iteration 4607, Loss: 1145310435.4667108\n",
      "Iteration 4608, Loss: 1464668357.7387552\n",
      "Iteration 4609, Loss: 1410413365.9590018\n",
      "Iteration 4610, Loss: 1480916621.6913042\n",
      "Iteration 4611, Loss: 1410218152.0889199\n",
      "Iteration 4612, Loss: 1186658628.3702877\n",
      "Iteration 4613, Loss: 1213401715.4486606\n",
      "Iteration 4614, Loss: 1215871004.083242\n",
      "Iteration 4615, Loss: 1316667288.4369156\n",
      "Iteration 4616, Loss: 1310685448.7785258\n",
      "Iteration 4617, Loss: 1214445654.792614\n",
      "Iteration 4618, Loss: 1188943772.7002752\n",
      "Iteration 4619, Loss: 1219385670.994349\n",
      "Iteration 4620, Loss: 1338215197.8344514\n",
      "Iteration 4621, Loss: 1387810328.646433\n",
      "Iteration 4622, Loss: 1440047258.5913982\n",
      "Iteration 4623, Loss: 1418080333.2093964\n",
      "Iteration 4624, Loss: 1231657512.7710552\n",
      "Iteration 4625, Loss: 1068164110.6451588\n",
      "Iteration 4626, Loss: 1501624462.1583424\n",
      "Iteration 4627, Loss: 1178283229.5121937\n",
      "Iteration 4628, Loss: 1115758960.8526413\n",
      "Iteration 4629, Loss: 1063559945.5494671\n",
      "Iteration 4630, Loss: 1070932333.4608846\n",
      "Iteration 4631, Loss: 1451464020.3560452\n",
      "Iteration 4632, Loss: 1284517760.4147437\n",
      "Iteration 4633, Loss: 1306188079.119161\n",
      "Iteration 4634, Loss: 1223734758.4856071\n",
      "Iteration 4635, Loss: 1269956646.752865\n",
      "Iteration 4636, Loss: 1253974107.9010143\n",
      "Iteration 4637, Loss: 1206346657.5499883\n",
      "Iteration 4638, Loss: 1283214395.2327068\n",
      "Iteration 4639, Loss: 1311727541.4576285\n",
      "Iteration 4640, Loss: 1318801982.9871576\n",
      "Iteration 4641, Loss: 1256141666.2458298\n",
      "Iteration 4642, Loss: 1102924691.7126188\n",
      "Iteration 4643, Loss: 1766304428.1738088\n",
      "Iteration 4644, Loss: 1584245079.7382445\n",
      "Iteration 4645, Loss: 1123518159.0415442\n",
      "Iteration 4646, Loss: 1193967169.6941109\n",
      "Iteration 4647, Loss: 1192626598.7327332\n",
      "Iteration 4648, Loss: 1053781707.0917697\n",
      "Iteration 4649, Loss: 1237525410.1300347\n",
      "Iteration 4650, Loss: 1273107110.9884753\n",
      "Iteration 4651, Loss: 1149919278.480257\n",
      "Iteration 4652, Loss: 1134782373.8820386\n",
      "Iteration 4653, Loss: 1819968398.7522347\n",
      "Iteration 4654, Loss: 1483355632.5890408\n",
      "Iteration 4655, Loss: 1213787692.5689964\n",
      "Iteration 4656, Loss: 1264714034.326001\n",
      "Iteration 4657, Loss: 1162246604.8606539\n",
      "Iteration 4658, Loss: 1836943147.078866\n",
      "Iteration 4659, Loss: 1118327477.2344334\n",
      "Iteration 4660, Loss: 1315077393.0692813\n",
      "Iteration 4661, Loss: 1279474229.206016\n",
      "Iteration 4662, Loss: 1372076584.5695336\n",
      "Iteration 4663, Loss: 1113047751.848642\n",
      "Iteration 4664, Loss: 1124508176.2916102\n",
      "Iteration 4665, Loss: 1251635791.4017067\n",
      "Iteration 4666, Loss: 1354585971.3136423\n",
      "Iteration 4667, Loss: 1345033072.5336769\n",
      "Iteration 4668, Loss: 1295802760.1773129\n",
      "Iteration 4669, Loss: 1339332319.6626644\n",
      "Iteration 4670, Loss: 1099030706.0005774\n",
      "Iteration 4671, Loss: 1300927283.9041302\n",
      "Iteration 4672, Loss: 1329114539.459331\n",
      "Iteration 4673, Loss: 1271793031.1871657\n",
      "Iteration 4674, Loss: 1254403777.998918\n",
      "Iteration 4675, Loss: 1149693030.3978617\n",
      "Iteration 4676, Loss: 1258830221.7719967\n",
      "Iteration 4677, Loss: 1241633764.5318096\n",
      "Iteration 4678, Loss: 1313914676.5611715\n",
      "Iteration 4679, Loss: 1292149470.3087623\n",
      "Iteration 4680, Loss: 1260346167.096137\n",
      "Iteration 4681, Loss: 1264007570.6887896\n",
      "Iteration 4682, Loss: 1228092233.3292918\n",
      "Iteration 4683, Loss: 1087035485.9072015\n",
      "Iteration 4684, Loss: 1130114807.1441774\n",
      "Iteration 4685, Loss: 1261315580.1751506\n",
      "Iteration 4686, Loss: 1094401325.5643935\n",
      "Iteration 4687, Loss: 1086759898.308344\n",
      "Iteration 4688, Loss: 1106642395.568887\n",
      "Iteration 4689, Loss: 1087199242.0611563\n",
      "Iteration 4690, Loss: 1268155320.2262936\n",
      "Iteration 4691, Loss: 1162710265.5719702\n",
      "Iteration 4692, Loss: 1136955464.0334892\n",
      "Iteration 4693, Loss: 1110762002.3181672\n",
      "Iteration 4694, Loss: 1102595162.6194732\n",
      "Iteration 4695, Loss: 1260173046.9051838\n",
      "Iteration 4696, Loss: 1071061574.5626945\n",
      "Iteration 4697, Loss: 1162939917.4723427\n",
      "Iteration 4698, Loss: 1141015736.7206593\n",
      "Iteration 4699, Loss: 1117618108.1622245\n",
      "Iteration 4700, Loss: 1093136319.9029164\n",
      "Iteration 4701, Loss: 1162800260.762897\n",
      "Iteration 4702, Loss: 1065124817.3027147\n",
      "Iteration 4703, Loss: 1710814789.377298\n",
      "Iteration 4704, Loss: 1254887538.802643\n",
      "Iteration 4705, Loss: 1039257152.3597633\n",
      "Iteration 4706, Loss: 1021770715.9321462\n",
      "Iteration 4707, Loss: 1248851582.6444445\n",
      "Iteration 4708, Loss: 1187200461.414671\n",
      "Iteration 4709, Loss: 1215842009.6069233\n",
      "Iteration 4710, Loss: 1236295399.9862182\n",
      "Iteration 4711, Loss: 1099771414.763414\n",
      "Iteration 4712, Loss: 1025473561.110861\n",
      "Iteration 4713, Loss: 1268096253.1539297\n",
      "Iteration 4714, Loss: 1150831815.3994014\n",
      "Iteration 4715, Loss: 1017372162.7467009\n",
      "Iteration 4716, Loss: 1008201319.0311648\n",
      "Iteration 4717, Loss: 1000391988.4590998\n",
      "Iteration 4718, Loss: 998336551.7260582\n",
      "Iteration 4719, Loss: 1033111285.3271016\n",
      "Iteration 4720, Loss: 2127172492.7284744\n",
      "Iteration 4721, Loss: 2459290448.356561\n",
      "Iteration 4722, Loss: 11641575374.191841\n",
      "Iteration 4723, Loss: 8196843388.115204\n",
      "Iteration 4724, Loss: 1080077355.8782847\n",
      "Iteration 4725, Loss: 4927304997.9090395\n",
      "Iteration 4726, Loss: 1723643582.778509\n",
      "Iteration 4727, Loss: 1102872945.169811\n",
      "Iteration 4728, Loss: 1147533228.7500942\n",
      "Iteration 4729, Loss: 1137347456.3634565\n",
      "Iteration 4730, Loss: 1121626267.0312283\n",
      "Iteration 4731, Loss: 1789850953.2089558\n",
      "Iteration 4732, Loss: 1389580738.2803307\n",
      "Iteration 4733, Loss: 1390090110.0053854\n",
      "Iteration 4734, Loss: 1371349438.158192\n",
      "Iteration 4735, Loss: 1086409236.132678\n",
      "Iteration 4736, Loss: 1132135638.5667675\n",
      "Iteration 4737, Loss: 1244837462.379899\n",
      "Iteration 4738, Loss: 1327295343.9275155\n",
      "Iteration 4739, Loss: 1378595874.6569731\n",
      "Iteration 4740, Loss: 1299738626.4038014\n",
      "Iteration 4741, Loss: 1107926587.20805\n",
      "Iteration 4742, Loss: 1155186017.787167\n",
      "Iteration 4743, Loss: 1152022952.7881908\n",
      "Iteration 4744, Loss: 1137935601.9628026\n",
      "Iteration 4745, Loss: 1150902066.2581606\n",
      "Iteration 4746, Loss: 1029273340.7052853\n",
      "Iteration 4747, Loss: 1024962224.9804333\n",
      "Iteration 4748, Loss: 1325516807.3979735\n",
      "Iteration 4749, Loss: 1244159178.127568\n",
      "Iteration 4750, Loss: 1127098860.320703\n",
      "Iteration 4751, Loss: 1174331335.6234353\n",
      "Iteration 4752, Loss: 1156965299.3886201\n",
      "Iteration 4753, Loss: 1149382812.9111242\n",
      "Iteration 4754, Loss: 1236959131.8882267\n",
      "Iteration 4755, Loss: 1213036662.7236698\n",
      "Iteration 4756, Loss: 1209375899.570143\n",
      "Iteration 4757, Loss: 1183873582.3910818\n",
      "Iteration 4758, Loss: 1245278394.3851318\n",
      "Iteration 4759, Loss: 1065383189.2671603\n",
      "Iteration 4760, Loss: 1190603526.4424357\n",
      "Iteration 4761, Loss: 1128544864.302317\n",
      "Iteration 4762, Loss: 1533092157.8377178\n",
      "Iteration 4763, Loss: 1490773057.1405766\n",
      "Iteration 4764, Loss: 1369749893.0324109\n",
      "Iteration 4765, Loss: 1118728608.6286898\n",
      "Iteration 4766, Loss: 1109947373.437899\n",
      "Iteration 4767, Loss: 1149497277.2748516\n",
      "Iteration 4768, Loss: 1131543441.469244\n",
      "Iteration 4769, Loss: 1109726682.0296094\n",
      "Iteration 4770, Loss: 1170537512.9195528\n",
      "Iteration 4771, Loss: 1110879492.985376\n",
      "Iteration 4772, Loss: 1278759446.4449785\n",
      "Iteration 4773, Loss: 1312804089.066125\n",
      "Iteration 4774, Loss: 1296199097.3736157\n",
      "Iteration 4775, Loss: 1053031594.7373332\n",
      "Iteration 4776, Loss: 1262734821.7791839\n",
      "Iteration 4777, Loss: 1045499880.208297\n",
      "Iteration 4778, Loss: 1278873000.0439968\n",
      "Iteration 4779, Loss: 1251286516.25821\n",
      "Iteration 4780, Loss: 1306127918.1189272\n",
      "Iteration 4781, Loss: 1267841763.7721767\n",
      "Iteration 4782, Loss: 1025814763.8597704\n",
      "Iteration 4783, Loss: 1333027102.9349895\n",
      "Iteration 4784, Loss: 1047888954.0766498\n",
      "Iteration 4785, Loss: 1036292385.406989\n",
      "Iteration 4786, Loss: 1042924036.418759\n",
      "Iteration 4787, Loss: 1046006355.2271274\n",
      "Iteration 4788, Loss: 1045194551.7383281\n",
      "Iteration 4789, Loss: 1171271032.2612984\n",
      "Iteration 4790, Loss: 1214360833.7682056\n",
      "Iteration 4791, Loss: 1153956737.7996109\n",
      "Iteration 4792, Loss: 1105508072.4204693\n",
      "Iteration 4793, Loss: 1085811211.7071233\n",
      "Iteration 4794, Loss: 1037763205.4893564\n",
      "Iteration 4795, Loss: 1212617943.59482\n",
      "Iteration 4796, Loss: 1024729689.7784142\n",
      "Iteration 4797, Loss: 1144446453.342889\n",
      "Iteration 4798, Loss: 1172522652.9131615\n",
      "Iteration 4799, Loss: 1190935639.4211023\n",
      "Iteration 4800, Loss: 1211672971.9342906\n",
      "Iteration 4801, Loss: 1197171325.909267\n",
      "Iteration 4802, Loss: 1168828255.3798568\n",
      "Iteration 4803, Loss: 1182443846.9446075\n",
      "Iteration 4804, Loss: 1149386606.5180964\n",
      "Iteration 4805, Loss: 1081799500.372146\n",
      "Iteration 4806, Loss: 1069830404.8438863\n",
      "Iteration 4807, Loss: 1228996942.534373\n",
      "Iteration 4808, Loss: 1127442735.4791448\n",
      "Iteration 4809, Loss: 1118553315.116966\n",
      "Iteration 4810, Loss: 1067233424.0617617\n",
      "Iteration 4811, Loss: 1025622105.6840304\n",
      "Iteration 4812, Loss: 1012162569.6390071\n",
      "Iteration 4813, Loss: 1096285551.0091317\n",
      "Iteration 4814, Loss: 1817975426.9408336\n",
      "Iteration 4815, Loss: 1306742206.2760162\n",
      "Iteration 4816, Loss: 1097776380.5740595\n",
      "Iteration 4817, Loss: 1031316663.082507\n",
      "Iteration 4818, Loss: 1120564063.5262053\n",
      "Iteration 4819, Loss: 1074754731.0818715\n",
      "Iteration 4820, Loss: 1142857926.2256057\n",
      "Iteration 4821, Loss: 1020483619.2716142\n",
      "Iteration 4822, Loss: 1480535221.3600774\n",
      "Iteration 4823, Loss: 1074938901.5877113\n",
      "Iteration 4824, Loss: 1062628571.2583894\n",
      "Iteration 4825, Loss: 1056577761.0800476\n",
      "Iteration 4826, Loss: 1037571007.5782763\n",
      "Iteration 4827, Loss: 1030406463.1479434\n",
      "Iteration 4828, Loss: 1013763885.6343609\n",
      "Iteration 4829, Loss: 1107338481.3322701\n",
      "Iteration 4830, Loss: 1178776805.6736896\n",
      "Iteration 4831, Loss: 1093800395.4608972\n",
      "Iteration 4832, Loss: 1070391966.4332862\n",
      "Iteration 4833, Loss: 1008174669.9887561\n",
      "Iteration 4834, Loss: 1097381666.4146714\n",
      "Iteration 4835, Loss: 1033622035.5801613\n",
      "Iteration 4836, Loss: 990315889.1477851\n",
      "Iteration 4837, Loss: 980106587.3322765\n",
      "Iteration 4838, Loss: 963423420.9074576\n",
      "Iteration 4839, Loss: 1195874717.9155364\n",
      "Iteration 4840, Loss: 1184275417.9052532\n",
      "Iteration 4841, Loss: 1158807920.3340683\n",
      "Iteration 4842, Loss: 1052493844.7025183\n",
      "Iteration 4843, Loss: 1146041689.6211786\n",
      "Iteration 4844, Loss: 1083860565.4182007\n",
      "Iteration 4845, Loss: 1178310425.2649791\n",
      "Iteration 4846, Loss: 1154473132.8736405\n",
      "Iteration 4847, Loss: 1075808556.557953\n",
      "Iteration 4848, Loss: 1028416317.8798507\n",
      "Iteration 4849, Loss: 1109348174.4770193\n",
      "Iteration 4850, Loss: 1109116311.8439102\n",
      "Iteration 4851, Loss: 1229424067.1968622\n",
      "Iteration 4852, Loss: 1212839564.707553\n",
      "Iteration 4853, Loss: 1090416484.5136333\n",
      "Iteration 4854, Loss: 1251387714.4593995\n",
      "Iteration 4855, Loss: 1239281279.1769783\n",
      "Iteration 4856, Loss: 1190223991.519212\n",
      "Iteration 4857, Loss: 1162959505.2060378\n",
      "Iteration 4858, Loss: 1090557158.213238\n",
      "Iteration 4859, Loss: 1004305204.91389\n",
      "Iteration 4860, Loss: 992945591.0112398\n",
      "Iteration 4861, Loss: 1002177217.5405042\n",
      "Iteration 4862, Loss: 1127640770.6021369\n",
      "Iteration 4863, Loss: 1053336549.3552152\n",
      "Iteration 4864, Loss: 2010033487.9082794\n",
      "Iteration 4865, Loss: 1045070337.7162907\n",
      "Iteration 4866, Loss: 1038306949.7950336\n",
      "Iteration 4867, Loss: 1017205586.6621697\n",
      "Iteration 4868, Loss: 1003180530.896808\n",
      "Iteration 4869, Loss: 1084562842.9028635\n",
      "Iteration 4870, Loss: 1031522854.7592952\n",
      "Iteration 4871, Loss: 1290451468.0993598\n",
      "Iteration 4872, Loss: 995731581.5207167\n",
      "Iteration 4873, Loss: 986311676.0890169\n",
      "Iteration 4874, Loss: 1014804229.1187714\n",
      "Iteration 4875, Loss: 998982471.8036762\n",
      "Iteration 4876, Loss: 1013636198.1248386\n",
      "Iteration 4877, Loss: 1038342997.6340399\n",
      "Iteration 4878, Loss: 1080077523.6043177\n",
      "Iteration 4879, Loss: 1178266533.2058425\n",
      "Iteration 4880, Loss: 1130240430.7685456\n",
      "Iteration 4881, Loss: 1073676554.2170063\n",
      "Iteration 4882, Loss: 1105178756.5799813\n",
      "Iteration 4883, Loss: 1107366283.416025\n",
      "Iteration 4884, Loss: 1105581421.59712\n",
      "Iteration 4885, Loss: 1089204563.3754134\n",
      "Iteration 4886, Loss: 1218950113.9014382\n",
      "Iteration 4887, Loss: 1226353534.5845234\n",
      "Iteration 4888, Loss: 1231068524.4358366\n",
      "Iteration 4889, Loss: 1101575870.5677824\n",
      "Iteration 4890, Loss: 1185338127.6637654\n",
      "Iteration 4891, Loss: 1154551883.707913\n",
      "Iteration 4892, Loss: 1126862767.3778715\n",
      "Iteration 4893, Loss: 1145692752.4981968\n",
      "Iteration 4894, Loss: 1388430687.5738828\n",
      "Iteration 4895, Loss: 1342883682.9877958\n",
      "Iteration 4896, Loss: 1039280105.273215\n",
      "Iteration 4897, Loss: 1054905984.2071354\n",
      "Iteration 4898, Loss: 1228961871.4477003\n",
      "Iteration 4899, Loss: 1175896631.5276275\n",
      "Iteration 4900, Loss: 1253237533.042866\n",
      "Iteration 4901, Loss: 1277638594.2585785\n",
      "Iteration 4902, Loss: 1308298234.0591996\n",
      "Iteration 4903, Loss: 1056953825.6042582\n",
      "Iteration 4904, Loss: 1049404558.6285747\n",
      "Iteration 4905, Loss: 1232502010.8222804\n",
      "Iteration 4906, Loss: 1504615796.6405795\n",
      "Iteration 4907, Loss: 1131118259.4365902\n",
      "Iteration 4908, Loss: 1078812679.172272\n",
      "Iteration 4909, Loss: 1227304873.475719\n",
      "Iteration 4910, Loss: 1076924013.796196\n",
      "Iteration 4911, Loss: 1632956561.4199004\n",
      "Iteration 4912, Loss: 1575984518.7022443\n",
      "Iteration 4913, Loss: 1543494133.9851\n",
      "Iteration 4914, Loss: 1490710546.3621213\n",
      "Iteration 4915, Loss: 1484924054.1272042\n",
      "Iteration 4916, Loss: 1479585116.6222782\n",
      "Iteration 4917, Loss: 1082218444.7889419\n",
      "Iteration 4918, Loss: 2385357125.1106806\n",
      "Iteration 4919, Loss: 1189364141.1683154\n",
      "Iteration 4920, Loss: 1181070627.5205877\n",
      "Iteration 4921, Loss: 1193597109.74431\n",
      "Iteration 4922, Loss: 1167593449.3931484\n",
      "Iteration 4923, Loss: 1238532211.2580829\n",
      "Iteration 4924, Loss: 1283738070.5304744\n",
      "Iteration 4925, Loss: 1218266161.123374\n",
      "Iteration 4926, Loss: 1270767052.1116102\n",
      "Iteration 4927, Loss: 1256411952.0670722\n",
      "Iteration 4928, Loss: 1335976986.2605584\n",
      "Iteration 4929, Loss: 1379424227.433473\n",
      "Iteration 4930, Loss: 1200858449.036716\n",
      "Iteration 4931, Loss: 1185377160.3563032\n",
      "Iteration 4932, Loss: 1183576182.1719587\n",
      "Iteration 4933, Loss: 1124353983.7082853\n",
      "Iteration 4934, Loss: 1117893827.7078378\n",
      "Iteration 4935, Loss: 1312779172.02745\n",
      "Iteration 4936, Loss: 1283704797.608427\n",
      "Iteration 4937, Loss: 1253924464.4309475\n",
      "Iteration 4938, Loss: 1286661158.1924233\n",
      "Iteration 4939, Loss: 1306559815.8589635\n",
      "Iteration 4940, Loss: 1021810441.417736\n",
      "Iteration 4941, Loss: 1013100450.0216687\n",
      "Iteration 4942, Loss: 1005305364.5613939\n",
      "Iteration 4943, Loss: 994136732.0917809\n",
      "Iteration 4944, Loss: 984452261.0264695\n",
      "Iteration 4945, Loss: 979387065.8850293\n",
      "Iteration 4946, Loss: 963423538.6261699\n",
      "Iteration 4947, Loss: 1032071280.8361129\n",
      "Iteration 4948, Loss: 1098780413.979981\n",
      "Iteration 4949, Loss: 1034194485.3766061\n",
      "Iteration 4950, Loss: 979516159.3959447\n",
      "Iteration 4951, Loss: 952601292.1708041\n",
      "Iteration 4952, Loss: 1018926815.2010432\n",
      "Iteration 4953, Loss: 987505830.9285038\n",
      "Iteration 4954, Loss: 959753169.761553\n",
      "Iteration 4955, Loss: 974095032.699834\n",
      "Iteration 4956, Loss: 1025737438.9356204\n",
      "Iteration 4957, Loss: 1166786711.1453764\n",
      "Iteration 4958, Loss: 978702680.7390316\n",
      "Iteration 4959, Loss: 937366303.3323433\n",
      "Iteration 4960, Loss: 1136303825.080839\n",
      "Iteration 4961, Loss: 1098452802.9858196\n",
      "Iteration 4962, Loss: 1107745722.3285599\n",
      "Iteration 4963, Loss: 981436286.3405291\n",
      "Iteration 4964, Loss: 2050490582.16993\n",
      "Iteration 4965, Loss: 1641398294.7024753\n",
      "Iteration 4966, Loss: 1245988155.0342667\n",
      "Iteration 4967, Loss: 1191843721.721503\n",
      "Iteration 4968, Loss: 1148331696.3789613\n",
      "Iteration 4969, Loss: 916082597.615989\n",
      "Iteration 4970, Loss: 1073874553.6305993\n",
      "Iteration 4971, Loss: 1026366445.6665207\n",
      "Iteration 4972, Loss: 946552243.8119854\n",
      "Iteration 4973, Loss: 1066133034.0411291\n",
      "Iteration 4974, Loss: 1738665125.4845934\n",
      "Iteration 4975, Loss: 1076946844.0653465\n",
      "Iteration 4976, Loss: 1067327591.6890244\n",
      "Iteration 4977, Loss: 1028839329.4825344\n",
      "Iteration 4978, Loss: 998862331.6466511\n",
      "Iteration 4979, Loss: 971534256.9420358\n",
      "Iteration 4980, Loss: 946511329.4823627\n",
      "Iteration 4981, Loss: 957084315.7060425\n",
      "Iteration 4982, Loss: 1026781649.6705005\n",
      "Iteration 4983, Loss: 963386621.3435844\n",
      "Iteration 4984, Loss: 933979423.0891405\n",
      "Iteration 4985, Loss: 1518790779.0533118\n",
      "Iteration 4986, Loss: 1425379178.287423\n",
      "Iteration 4987, Loss: 1059376012.5874541\n",
      "Iteration 4988, Loss: 1018667713.9811902\n",
      "Iteration 4989, Loss: 1765212486.4652476\n",
      "Iteration 4990, Loss: 1512647446.2597651\n",
      "Iteration 4991, Loss: 948264042.1074709\n",
      "Iteration 4992, Loss: 936009377.5837792\n",
      "Iteration 4993, Loss: 1168766523.8168628\n",
      "Iteration 4994, Loss: 979924719.3348436\n",
      "Iteration 4995, Loss: 964995339.4668962\n",
      "Iteration 4996, Loss: 945496286.5051517\n",
      "Iteration 4997, Loss: 1039228258.8328325\n",
      "Iteration 4998, Loss: 970367486.999765\n",
      "Iteration 4999, Loss: 1284696090.99098\n",
      "Iteration 5000, Loss: 1073808432.7308106\n",
      "Iteration 5001, Loss: 1055023866.9418992\n",
      "Iteration 5002, Loss: 1023880044.8322285\n",
      "Iteration 5003, Loss: 1232646330.681625\n",
      "Iteration 5004, Loss: 1000144642.0577075\n",
      "Iteration 5005, Loss: 1112569945.4469414\n",
      "Iteration 5006, Loss: 1054891325.0663354\n",
      "Iteration 5007, Loss: 1025377206.6918302\n",
      "Iteration 5008, Loss: 1090364537.7689977\n",
      "Iteration 5009, Loss: 1062240648.3275237\n",
      "Iteration 5010, Loss: 1048808874.3816261\n",
      "Iteration 5011, Loss: 1089308838.6511493\n",
      "Iteration 5012, Loss: 1154113585.1207037\n",
      "Iteration 5013, Loss: 1162251493.7698097\n",
      "Iteration 5014, Loss: 1150128708.5020232\n",
      "Iteration 5015, Loss: 1109100070.4100308\n",
      "Iteration 5016, Loss: 1117080142.6606328\n",
      "Iteration 5017, Loss: 1046908650.2491838\n",
      "Iteration 5018, Loss: 977849205.1607829\n",
      "Iteration 5019, Loss: 931906234.9971983\n",
      "Iteration 5020, Loss: 1959735078.8422508\n",
      "Iteration 5021, Loss: 1620098602.1099436\n",
      "Iteration 5022, Loss: 1219480041.3709288\n",
      "Iteration 5023, Loss: 1043132017.7864989\n",
      "Iteration 5024, Loss: 953884120.1184448\n",
      "Iteration 5025, Loss: 952476986.259107\n",
      "Iteration 5026, Loss: 943279235.1721864\n",
      "Iteration 5027, Loss: 1156134518.2905571\n",
      "Iteration 5028, Loss: 1105541242.6372113\n",
      "Iteration 5029, Loss: 1088473505.4549596\n",
      "Iteration 5030, Loss: 1598236820.7843657\n",
      "Iteration 5031, Loss: 1162538139.0855072\n",
      "Iteration 5032, Loss: 1106765465.9238918\n",
      "Iteration 5033, Loss: 1093168515.2742648\n",
      "Iteration 5034, Loss: 1354298828.544304\n",
      "Iteration 5035, Loss: 1035730408.7683706\n",
      "Iteration 5036, Loss: 1188259119.4318163\n",
      "Iteration 5037, Loss: 1012249662.2500179\n",
      "Iteration 5038, Loss: 1027675845.5219471\n",
      "Iteration 5039, Loss: 1027855902.4258046\n",
      "Iteration 5040, Loss: 1395124857.6242204\n",
      "Iteration 5041, Loss: 1339850768.852952\n",
      "Iteration 5042, Loss: 1353435923.5706143\n",
      "Iteration 5043, Loss: 1050577617.2775186\n",
      "Iteration 5044, Loss: 1055365123.3499073\n",
      "Iteration 5045, Loss: 1060550514.7954335\n",
      "Iteration 5046, Loss: 1038724513.8136337\n",
      "Iteration 5047, Loss: 1047469225.966214\n",
      "Iteration 5048, Loss: 1201087841.5232465\n",
      "Iteration 5049, Loss: 987611450.7999921\n",
      "Iteration 5050, Loss: 1190663827.33961\n",
      "Iteration 5051, Loss: 1140949427.4186683\n",
      "Iteration 5052, Loss: 1155003826.6959026\n",
      "Iteration 5053, Loss: 1166903624.0072377\n",
      "Iteration 5054, Loss: 1140127812.3243592\n",
      "Iteration 5055, Loss: 1051742929.0858407\n",
      "Iteration 5056, Loss: 1160900339.020182\n",
      "Iteration 5057, Loss: 1111658595.8193345\n",
      "Iteration 5058, Loss: 1156596723.2525315\n",
      "Iteration 5059, Loss: 1021484858.5875754\n",
      "Iteration 5060, Loss: 989786124.2820065\n",
      "Iteration 5061, Loss: 992301910.5042933\n",
      "Iteration 5062, Loss: 935487651.8353912\n",
      "Iteration 5063, Loss: 922190296.3406831\n",
      "Iteration 5064, Loss: 1167494478.900275\n",
      "Iteration 5065, Loss: 986449162.0248746\n",
      "Iteration 5066, Loss: 1047569699.039285\n",
      "Iteration 5067, Loss: 1011710537.9883337\n",
      "Iteration 5068, Loss: 1408013466.524393\n",
      "Iteration 5069, Loss: 1340713256.6471014\n",
      "Iteration 5070, Loss: 1245495836.2436519\n",
      "Iteration 5071, Loss: 1102430456.679826\n",
      "Iteration 5072, Loss: 1097258089.3871827\n",
      "Iteration 5073, Loss: 1106411940.4826956\n",
      "Iteration 5074, Loss: 1156801234.9016957\n",
      "Iteration 5075, Loss: 1116073450.8716447\n",
      "Iteration 5076, Loss: 1150171678.4562004\n",
      "Iteration 5077, Loss: 1064990788.1209891\n",
      "Iteration 5078, Loss: 972857455.9665769\n",
      "Iteration 5079, Loss: 2224958280.6223\n",
      "Iteration 5080, Loss: 952276449.0670378\n",
      "Iteration 5081, Loss: 1175192272.9083865\n",
      "Iteration 5082, Loss: 1145162210.526371\n",
      "Iteration 5083, Loss: 938429584.9340665\n",
      "Iteration 5084, Loss: 2924825782.6266522\n",
      "Iteration 5085, Loss: 1020759889.0100615\n",
      "Iteration 5086, Loss: 1097895829.9422667\n",
      "Iteration 5087, Loss: 1064958699.3105456\n",
      "Iteration 5088, Loss: 1223357208.7524748\n",
      "Iteration 5089, Loss: 1022105540.9172608\n",
      "Iteration 5090, Loss: 981961827.2107762\n",
      "Iteration 5091, Loss: 1065500671.4373827\n",
      "Iteration 5092, Loss: 1140116926.048162\n",
      "Iteration 5093, Loss: 1030044591.6629057\n",
      "Iteration 5094, Loss: 986754096.8142304\n",
      "Iteration 5095, Loss: 1179778484.649497\n",
      "Iteration 5096, Loss: 1025158521.2613932\n",
      "Iteration 5097, Loss: 1964057348.6875856\n",
      "Iteration 5098, Loss: 1417198378.6977131\n",
      "Iteration 5099, Loss: 1337420697.9447749\n",
      "Iteration 5100, Loss: 985818267.5536522\n",
      "Iteration 5101, Loss: 1017647200.9499743\n",
      "Iteration 5102, Loss: 1024677197.5749868\n",
      "Iteration 5103, Loss: 1158217610.3453994\n",
      "Iteration 5104, Loss: 1083792928.3184898\n",
      "Iteration 5105, Loss: 1022446968.6762384\n",
      "Iteration 5106, Loss: 1338548443.7924845\n",
      "Iteration 5107, Loss: 1055128981.5298672\n",
      "Iteration 5108, Loss: 1143932619.5812805\n",
      "Iteration 5109, Loss: 1074683110.0252306\n",
      "Iteration 5110, Loss: 1931878871.0855277\n",
      "Iteration 5111, Loss: 1741275084.920902\n",
      "Iteration 5112, Loss: 1653552501.395438\n",
      "Iteration 5113, Loss: 1237511713.7044086\n",
      "Iteration 5114, Loss: 9195665874.968943\n",
      "Iteration 5115, Loss: 2093267831.5219712\n",
      "Iteration 5116, Loss: 9120174191.52255\n",
      "Iteration 5117, Loss: 2298485480.839617\n",
      "Iteration 5118, Loss: 2128195092.9644864\n",
      "Iteration 5119, Loss: 1154864330.1871045\n",
      "Iteration 5120, Loss: 1072859015.158642\n",
      "Iteration 5121, Loss: 1103778059.616491\n",
      "Iteration 5122, Loss: 1118217450.9427903\n",
      "Iteration 5123, Loss: 1753718374.1021078\n",
      "Iteration 5124, Loss: 1128283796.9715314\n",
      "Iteration 5125, Loss: 1246047484.6959147\n",
      "Iteration 5126, Loss: 1194062266.2426732\n",
      "Iteration 5127, Loss: 1772070926.558837\n",
      "Iteration 5128, Loss: 1506943145.1277065\n",
      "Iteration 5129, Loss: 1115623751.430877\n",
      "Iteration 5130, Loss: 1100518604.6114326\n",
      "Iteration 5131, Loss: 1194976095.7950683\n",
      "Iteration 5132, Loss: 1253815112.1533353\n",
      "Iteration 5133, Loss: 1176926664.1890366\n",
      "Iteration 5134, Loss: 1361548572.7085917\n",
      "Iteration 5135, Loss: 1114693859.6042945\n",
      "Iteration 5136, Loss: 2310755261.3952255\n",
      "Iteration 5137, Loss: 1308176572.0742257\n",
      "Iteration 5138, Loss: 1242606312.1859689\n",
      "Iteration 5139, Loss: 1118143841.4197304\n",
      "Iteration 5140, Loss: 1314020618.1583655\n",
      "Iteration 5141, Loss: 1241782302.7907243\n",
      "Iteration 5142, Loss: 1361457219.4396467\n",
      "Iteration 5143, Loss: 1443317808.3912752\n",
      "Iteration 5144, Loss: 1378618636.8625257\n",
      "Iteration 5145, Loss: 1415834543.152259\n",
      "Iteration 5146, Loss: 1397940924.8292716\n",
      "Iteration 5147, Loss: 1181465355.5230165\n",
      "Iteration 5148, Loss: 1207695903.7045498\n",
      "Iteration 5149, Loss: 1310357524.3183835\n",
      "Iteration 5150, Loss: 1208069933.82479\n",
      "Iteration 5151, Loss: 1759517604.7635047\n",
      "Iteration 5152, Loss: 1176761557.5331073\n",
      "Iteration 5153, Loss: 1443916228.208224\n",
      "Iteration 5154, Loss: 1223150506.918635\n",
      "Iteration 5155, Loss: 3890489281.816399\n",
      "Iteration 5156, Loss: 1197081810.8664157\n",
      "Iteration 5157, Loss: 1221190638.9894345\n",
      "Iteration 5158, Loss: 1194808860.674001\n",
      "Iteration 5159, Loss: 1454543513.2195067\n",
      "Iteration 5160, Loss: 1295916423.2405744\n",
      "Iteration 5161, Loss: 1491567881.9921\n",
      "Iteration 5162, Loss: 1523727393.1369233\n",
      "Iteration 5163, Loss: 1196794407.9256368\n",
      "Iteration 5164, Loss: 1252555662.844831\n",
      "Iteration 5165, Loss: 1450727499.6519613\n",
      "Iteration 5166, Loss: 1566137848.9496639\n",
      "Iteration 5167, Loss: 1318662583.8285651\n",
      "Iteration 5168, Loss: 1464477678.6610699\n",
      "Iteration 5169, Loss: 1487229328.7989936\n",
      "Iteration 5170, Loss: 1256405656.0815866\n",
      "Iteration 5171, Loss: 3825607774.794721\n",
      "Iteration 5172, Loss: 1747777867.771673\n",
      "Iteration 5173, Loss: 1758393025.3186707\n",
      "Iteration 5174, Loss: 1231125416.0435073\n",
      "Iteration 5175, Loss: 1304287803.1418788\n",
      "Iteration 5176, Loss: 1331442781.9579046\n",
      "Iteration 5177, Loss: 1469309504.805346\n",
      "Iteration 5178, Loss: 1488318020.7020044\n",
      "Iteration 5179, Loss: 1552622458.947749\n",
      "Iteration 5180, Loss: 1614627400.3670754\n",
      "Iteration 5181, Loss: 1456571557.1034424\n",
      "Iteration 5182, Loss: 1179904689.4887168\n",
      "Iteration 5183, Loss: 1355330843.7133179\n",
      "Iteration 5184, Loss: 1433924268.109423\n",
      "Iteration 5185, Loss: 1306242137.067403\n",
      "Iteration 5186, Loss: 1184177601.0607827\n",
      "Iteration 5187, Loss: 1543014470.1598988\n",
      "Iteration 5188, Loss: 1289577673.6542463\n",
      "Iteration 5189, Loss: 1297745956.1138427\n",
      "Iteration 5190, Loss: 1419301239.7258658\n",
      "Iteration 5191, Loss: 1527866885.5919695\n",
      "Iteration 5192, Loss: 1426584992.8685129\n",
      "Iteration 5193, Loss: 1342066986.5156856\n",
      "Iteration 5194, Loss: 1397470796.0283306\n",
      "Iteration 5195, Loss: 1194190909.0402973\n",
      "Iteration 5196, Loss: 1395666712.1299942\n",
      "Iteration 5197, Loss: 1294550234.685706\n",
      "Iteration 5198, Loss: 1424684861.3404202\n",
      "Iteration 5199, Loss: 1544692577.4052725\n",
      "Iteration 5200, Loss: 1299826086.7619743\n",
      "Iteration 5201, Loss: 3990148568.3070135\n",
      "Iteration 5202, Loss: 3334045403.733141\n",
      "Iteration 5203, Loss: 3096799433.9692016\n",
      "Iteration 5204, Loss: 1973907710.1534288\n",
      "Iteration 5205, Loss: 2447178988.1154475\n",
      "Iteration 5206, Loss: 1952551246.6781375\n",
      "Iteration 5207, Loss: 1582873855.0046656\n",
      "Iteration 5208, Loss: 1206289533.4756129\n",
      "Iteration 5209, Loss: 1225775334.3215384\n",
      "Iteration 5210, Loss: 1222761597.7876537\n",
      "Iteration 5211, Loss: 1328320058.2343876\n",
      "Iteration 5212, Loss: 1274682935.2826633\n",
      "Iteration 5213, Loss: 1285826901.2293239\n",
      "Iteration 5214, Loss: 1386623541.3362389\n",
      "Iteration 5215, Loss: 1511486892.7522593\n",
      "Iteration 5216, Loss: 1280518021.0159693\n",
      "Iteration 5217, Loss: 1199623554.5226865\n",
      "Iteration 5218, Loss: 1209555886.9528785\n",
      "Iteration 5219, Loss: 1285120796.7130752\n",
      "Iteration 5220, Loss: 1391487228.6028903\n",
      "Iteration 5221, Loss: 1212771678.9450026\n",
      "Iteration 5222, Loss: 1438938988.5465164\n",
      "Iteration 5223, Loss: 1555739735.538772\n",
      "Iteration 5224, Loss: 1607561201.9917867\n",
      "Iteration 5225, Loss: 1315805903.8264437\n",
      "Iteration 5226, Loss: 1177780208.8160782\n",
      "Iteration 5227, Loss: 1297309253.4182518\n",
      "Iteration 5228, Loss: 1238932627.1898658\n",
      "Iteration 5229, Loss: 1242212141.9434886\n",
      "Iteration 5230, Loss: 1328759151.5095487\n",
      "Iteration 5231, Loss: 1212225247.271206\n",
      "Iteration 5232, Loss: 1293449977.8721867\n",
      "Iteration 5233, Loss: 1247894465.2317038\n",
      "Iteration 5234, Loss: 1192518555.5824661\n",
      "Iteration 5235, Loss: 1250681009.7646053\n",
      "Iteration 5236, Loss: 1297553864.6956468\n",
      "Iteration 5237, Loss: 1399124096.1417258\n",
      "Iteration 5238, Loss: 1437527210.5538511\n",
      "Iteration 5239, Loss: 1396666912.1236794\n",
      "Iteration 5240, Loss: 1479351688.5545642\n",
      "Iteration 5241, Loss: 1466893314.271588\n",
      "Iteration 5242, Loss: 1267951183.4219651\n",
      "Iteration 5243, Loss: 1256958527.5018249\n",
      "Iteration 5244, Loss: 1144251919.5046144\n",
      "Iteration 5245, Loss: 1170203932.1869228\n",
      "Iteration 5246, Loss: 1179222052.212443\n",
      "Iteration 5247, Loss: 1293607966.780927\n",
      "Iteration 5248, Loss: 1235469566.6043427\n",
      "Iteration 5249, Loss: 1231874169.7182589\n",
      "Iteration 5250, Loss: 1230277156.669746\n",
      "Iteration 5251, Loss: 1226218276.6211596\n",
      "Iteration 5252, Loss: 1280527366.5752804\n",
      "Iteration 5253, Loss: 1361356083.4112666\n",
      "Iteration 5254, Loss: 1418069789.5107112\n",
      "Iteration 5255, Loss: 1390332731.75472\n",
      "Iteration 5256, Loss: 1382370093.580093\n",
      "Iteration 5257, Loss: 1172152070.1982396\n",
      "Iteration 5258, Loss: 1168400834.4616926\n",
      "Iteration 5259, Loss: 1195510799.0788245\n",
      "Iteration 5260, Loss: 1121936826.044622\n",
      "Iteration 5261, Loss: 1071709443.7950393\n",
      "Iteration 5262, Loss: 1068004519.5079148\n",
      "Iteration 5263, Loss: 1308216021.10333\n",
      "Iteration 5264, Loss: 1158403231.671262\n",
      "Iteration 5265, Loss: 1193636131.073454\n",
      "Iteration 5266, Loss: 1656456557.4144983\n",
      "Iteration 5267, Loss: 1319337661.90986\n",
      "Iteration 5268, Loss: 1753454087.4537916\n",
      "Iteration 5269, Loss: 1633908969.7327142\n",
      "Iteration 5270, Loss: 1397678581.3996606\n",
      "Iteration 5271, Loss: 1284629165.017326\n",
      "Iteration 5272, Loss: 4188916139.4220552\n",
      "Iteration 5273, Loss: 2181608363.1398344\n",
      "Iteration 5274, Loss: 2083038577.361354\n",
      "Iteration 5275, Loss: 1112741472.0047376\n",
      "Iteration 5276, Loss: 1502047028.6875677\n",
      "Iteration 5277, Loss: 1274458376.9606555\n",
      "Iteration 5278, Loss: 1507176825.1518483\n",
      "Iteration 5279, Loss: 1258440931.7848594\n",
      "Iteration 5280, Loss: 1397942867.6543\n",
      "Iteration 5281, Loss: 1198063028.1301932\n",
      "Iteration 5282, Loss: 2887255944.3226147\n",
      "Iteration 5283, Loss: 2399849040.86583\n",
      "Iteration 5284, Loss: 1444581169.0765028\n",
      "Iteration 5285, Loss: 1516863731.861717\n",
      "Iteration 5286, Loss: 1212758518.599723\n",
      "Iteration 5287, Loss: 1213354481.3331149\n",
      "Iteration 5288, Loss: 1218090744.9175086\n",
      "Iteration 5289, Loss: 1477574599.0661602\n",
      "Iteration 5290, Loss: 1256268775.7306206\n",
      "Iteration 5291, Loss: 1495304733.958875\n",
      "Iteration 5292, Loss: 1244944362.6385648\n",
      "Iteration 5293, Loss: 1311128929.3754401\n",
      "Iteration 5294, Loss: 1437853892.1730795\n",
      "Iteration 5295, Loss: 1533275549.0631812\n",
      "Iteration 5296, Loss: 1302691438.2355428\n",
      "Iteration 5297, Loss: 1217122537.5033224\n",
      "Iteration 5298, Loss: 2821298131.4300838\n",
      "Iteration 5299, Loss: 2540479142.8859406\n",
      "Iteration 5300, Loss: 1983124722.9768836\n",
      "Iteration 5301, Loss: 1868186366.3742583\n",
      "Iteration 5302, Loss: 2175660895.956029\n",
      "Iteration 5303, Loss: 1246989306.8317385\n",
      "Iteration 5304, Loss: 1251161728.577287\n",
      "Iteration 5305, Loss: 1393421795.5227923\n",
      "Iteration 5306, Loss: 1366293395.7803187\n",
      "Iteration 5307, Loss: 1237446558.9101477\n",
      "Iteration 5308, Loss: 1515344820.2855284\n",
      "Iteration 5309, Loss: 1256416702.8966038\n",
      "Iteration 5310, Loss: 1275880704.637559\n",
      "Iteration 5311, Loss: 1273054236.8486722\n",
      "Iteration 5312, Loss: 1290976152.3610651\n",
      "Iteration 5313, Loss: 1319927774.308053\n",
      "Iteration 5314, Loss: 1197059345.200527\n",
      "Iteration 5315, Loss: 1550734109.599227\n",
      "Iteration 5316, Loss: 1503535640.8695595\n",
      "Iteration 5317, Loss: 1267694505.4024584\n",
      "Iteration 5318, Loss: 1216125463.5415847\n",
      "Iteration 5319, Loss: 1276080071.7885826\n",
      "Iteration 5320, Loss: 1279585276.123048\n",
      "Iteration 5321, Loss: 1293823581.3156035\n",
      "Iteration 5322, Loss: 1311543774.1527245\n",
      "Iteration 5323, Loss: 1317750163.5263228\n",
      "Iteration 5324, Loss: 1495114185.8936\n",
      "Iteration 5325, Loss: 1504928270.983845\n",
      "Iteration 5326, Loss: 1516030515.5929425\n",
      "Iteration 5327, Loss: 1523409031.2100701\n",
      "Iteration 5328, Loss: 1460271513.9125862\n",
      "Iteration 5329, Loss: 1269348453.5593493\n",
      "Iteration 5330, Loss: 1285288973.5792181\n",
      "Iteration 5331, Loss: 1626106714.2614045\n",
      "Iteration 5332, Loss: 1305908274.135227\n",
      "Iteration 5333, Loss: 1249505155.0848725\n",
      "Iteration 5334, Loss: 1316154844.2525156\n",
      "Iteration 5335, Loss: 1251641667.0588698\n",
      "Iteration 5336, Loss: 1257168114.2224026\n",
      "Iteration 5337, Loss: 1287077943.355922\n",
      "Iteration 5338, Loss: 1290568365.729266\n",
      "Iteration 5339, Loss: 1308771682.2408533\n",
      "Iteration 5340, Loss: 1272541374.4489965\n",
      "Iteration 5341, Loss: 1344840600.4766126\n",
      "Iteration 5342, Loss: 1193233709.3862278\n",
      "Iteration 5343, Loss: 1393726402.808071\n",
      "Iteration 5344, Loss: 1207719143.7727919\n",
      "Iteration 5345, Loss: 1218890889.7439425\n",
      "Iteration 5346, Loss: 1540602177.401686\n",
      "Iteration 5347, Loss: 1318419223.411683\n",
      "Iteration 5348, Loss: 1217280951.1154788\n",
      "Iteration 5349, Loss: 1244384365.5147405\n",
      "Iteration 5350, Loss: 1332600001.5883632\n",
      "Iteration 5351, Loss: 1361361059.432028\n",
      "Iteration 5352, Loss: 1370603180.0058415\n",
      "Iteration 5353, Loss: 1288109453.3753228\n",
      "Iteration 5354, Loss: 1380720998.9256837\n",
      "Iteration 5355, Loss: 1487488701.3532305\n",
      "Iteration 5356, Loss: 1467390735.637878\n",
      "Iteration 5357, Loss: 1555007382.193362\n",
      "Iteration 5358, Loss: 1201934633.3335109\n",
      "Iteration 5359, Loss: 1272167356.11083\n",
      "Iteration 5360, Loss: 1336626236.5017097\n",
      "Iteration 5361, Loss: 1327657960.6308508\n",
      "Iteration 5362, Loss: 1353323516.585707\n",
      "Iteration 5363, Loss: 1211942989.2098546\n",
      "Iteration 5364, Loss: 1477279986.8176086\n",
      "Iteration 5365, Loss: 1562279576.1256406\n",
      "Iteration 5366, Loss: 1347532387.9308739\n",
      "Iteration 5367, Loss: 1215294092.7458444\n",
      "Iteration 5368, Loss: 1547113490.008415\n",
      "Iteration 5369, Loss: 1530027096.7437303\n",
      "Iteration 5370, Loss: 1339200277.1935227\n",
      "Iteration 5371, Loss: 1299259576.0819397\n",
      "Iteration 5372, Loss: 1380206650.3757985\n",
      "Iteration 5373, Loss: 1264150013.1225781\n",
      "Iteration 5374, Loss: 1375405859.8498702\n",
      "Iteration 5375, Loss: 1443442516.138299\n",
      "Iteration 5376, Loss: 1288876785.3632193\n",
      "Iteration 5377, Loss: 1394559113.7976224\n",
      "Iteration 5378, Loss: 1262601725.539966\n",
      "Iteration 5379, Loss: 1356001926.5347707\n",
      "Iteration 5380, Loss: 1372974378.7635298\n",
      "Iteration 5381, Loss: 1235522953.679566\n",
      "Iteration 5382, Loss: 2116407605.6228585\n",
      "Iteration 5383, Loss: 2086921477.056794\n",
      "Iteration 5384, Loss: 2089676098.2613232\n",
      "Iteration 5385, Loss: 1294270993.0027823\n",
      "Iteration 5386, Loss: 1514833905.3448436\n",
      "Iteration 5387, Loss: 1261600570.3398159\n",
      "Iteration 5388, Loss: 1678934812.671227\n",
      "Iteration 5389, Loss: 1455165104.8030567\n",
      "Iteration 5390, Loss: 1484423018.7512789\n",
      "Iteration 5391, Loss: 1299523683.0757253\n",
      "Iteration 5392, Loss: 1615567656.2725754\n",
      "Iteration 5393, Loss: 1406655454.1830173\n",
      "Iteration 5394, Loss: 1304404916.1828506\n",
      "Iteration 5395, Loss: 2080058877.689189\n",
      "Iteration 5396, Loss: 2025333547.4954042\n",
      "Iteration 5397, Loss: 2724468692.468923\n",
      "Iteration 5398, Loss: 2336100703.342012\n",
      "Iteration 5399, Loss: 1327859201.4364254\n",
      "Iteration 5400, Loss: 1372772964.4488285\n",
      "Iteration 5401, Loss: 1481147430.6351202\n",
      "Iteration 5402, Loss: 1408414263.8361905\n",
      "Iteration 5403, Loss: 1463644251.7305362\n",
      "Iteration 5404, Loss: 1533125319.1891718\n",
      "Iteration 5405, Loss: 1381373013.2801785\n",
      "Iteration 5406, Loss: 1920711424.4301503\n",
      "Iteration 5407, Loss: 1928614140.9856548\n",
      "Iteration 5408, Loss: 1395115472.9622135\n",
      "Iteration 5409, Loss: 1520807790.5835023\n",
      "Iteration 5410, Loss: 1690248263.828798\n",
      "Iteration 5411, Loss: 1757606046.51596\n",
      "Iteration 5412, Loss: 1819968940.1811295\n",
      "Iteration 5413, Loss: 1353218022.8303423\n",
      "Iteration 5414, Loss: 1465941415.2054045\n",
      "Iteration 5415, Loss: 1398741284.3957732\n",
      "Iteration 5416, Loss: 1431266097.8799548\n",
      "Iteration 5417, Loss: 1459345381.7359622\n",
      "Iteration 5418, Loss: 1601514710.6288173\n",
      "Iteration 5419, Loss: 1415958115.5451603\n",
      "Iteration 5420, Loss: 1453730713.2088525\n",
      "Iteration 5421, Loss: 1586204295.048274\n",
      "Iteration 5422, Loss: 1344739374.8211792\n",
      "Iteration 5423, Loss: 1434155989.2550466\n",
      "Iteration 5424, Loss: 1476417126.9452581\n",
      "Iteration 5425, Loss: 1259767364.4593744\n",
      "Iteration 5426, Loss: 1293044470.765049\n",
      "Iteration 5427, Loss: 1481795995.668151\n",
      "Iteration 5428, Loss: 1579920168.6697738\n",
      "Iteration 5429, Loss: 1334276987.6318529\n",
      "Iteration 5430, Loss: 1439172997.0583858\n",
      "Iteration 5431, Loss: 1250507039.3433821\n",
      "Iteration 5432, Loss: 1441177022.3266754\n",
      "Iteration 5433, Loss: 1294447415.7842577\n",
      "Iteration 5434, Loss: 1501712859.0883822\n",
      "Iteration 5435, Loss: 1357146191.4290893\n",
      "Iteration 5436, Loss: 1454981667.5225136\n",
      "Iteration 5437, Loss: 1594868491.4908164\n",
      "Iteration 5438, Loss: 1491960280.938976\n",
      "Iteration 5439, Loss: 1601136107.4816656\n",
      "Iteration 5440, Loss: 1651933828.5357554\n",
      "Iteration 5441, Loss: 1335548200.5599587\n",
      "Iteration 5442, Loss: 1354536298.132247\n",
      "Iteration 5443, Loss: 1384521065.9441612\n",
      "Iteration 5444, Loss: 1431580038.0506296\n",
      "Iteration 5445, Loss: 1444759417.951878\n",
      "Iteration 5446, Loss: 1455089511.0064116\n",
      "Iteration 5447, Loss: 1467462799.442007\n",
      "Iteration 5448, Loss: 1475596749.4486454\n",
      "Iteration 5449, Loss: 1268483923.5156505\n",
      "Iteration 5450, Loss: 1218833821.1627858\n",
      "Iteration 5451, Loss: 1141688339.8330688\n",
      "Iteration 5452, Loss: 1250802810.8861873\n",
      "Iteration 5453, Loss: 1309502467.2963939\n",
      "Iteration 5454, Loss: 1247097860.5419548\n",
      "Iteration 5455, Loss: 1687911257.697123\n",
      "Iteration 5456, Loss: 1184492087.4280326\n",
      "Iteration 5457, Loss: 1256791636.2339814\n",
      "Iteration 5458, Loss: 1216605442.24689\n",
      "Iteration 5459, Loss: 1288680972.2006612\n",
      "Iteration 5460, Loss: 1352897906.5833664\n",
      "Iteration 5461, Loss: 1370256303.9321961\n",
      "Iteration 5462, Loss: 1147518586.860283\n",
      "Iteration 5463, Loss: 1657494660.7230792\n",
      "Iteration 5464, Loss: 1315708386.854459\n",
      "Iteration 5465, Loss: 1190634760.4281971\n",
      "Iteration 5466, Loss: 1402850979.4540598\n",
      "Iteration 5467, Loss: 1223859578.253819\n",
      "Iteration 5468, Loss: 3356539034.9466004\n",
      "Iteration 5469, Loss: 4977274651.500174\n",
      "Iteration 5470, Loss: 1427815981.2713292\n",
      "Iteration 5471, Loss: 1474958832.0993822\n",
      "Iteration 5472, Loss: 1595260335.531055\n",
      "Iteration 5473, Loss: 1552833647.8848283\n",
      "Iteration 5474, Loss: 1565655917.8806505\n",
      "Iteration 5475, Loss: 1662966705.930475\n",
      "Iteration 5476, Loss: 1580470653.7408636\n",
      "Iteration 5477, Loss: 1584872581.2091596\n",
      "Iteration 5478, Loss: 1254356163.1932125\n",
      "Iteration 5479, Loss: 1347718106.1880388\n",
      "Iteration 5480, Loss: 1343701872.4187758\n",
      "Iteration 5481, Loss: 1550219808.9147217\n",
      "Iteration 5482, Loss: 1294938668.2962544\n",
      "Iteration 5483, Loss: 1227711840.6874478\n",
      "Iteration 5484, Loss: 1168429443.456218\n",
      "Iteration 5485, Loss: 1189872994.693654\n",
      "Iteration 5486, Loss: 1295191794.7362137\n",
      "Iteration 5487, Loss: 1653516025.591806\n",
      "Iteration 5488, Loss: 1231497819.6955104\n",
      "Iteration 5489, Loss: 1199513492.1512525\n",
      "Iteration 5490, Loss: 1336016781.3271465\n",
      "Iteration 5491, Loss: 1345755243.5395274\n",
      "Iteration 5492, Loss: 1347802360.8310165\n",
      "Iteration 5493, Loss: 1416536537.522586\n",
      "Iteration 5494, Loss: 1395120785.261572\n",
      "Iteration 5495, Loss: 1291412679.7461615\n",
      "Iteration 5496, Loss: 1300639650.4312146\n",
      "Iteration 5497, Loss: 1562760757.6938827\n",
      "Iteration 5498, Loss: 1593781044.3242433\n",
      "Iteration 5499, Loss: 1216116996.9150233\n",
      "Iteration 5500, Loss: 1411982198.1524267\n",
      "Iteration 5501, Loss: 1362665220.5794802\n",
      "Iteration 5502, Loss: 1383647268.5444033\n",
      "Iteration 5503, Loss: 1386253003.014244\n",
      "Iteration 5504, Loss: 1313114098.372734\n",
      "Iteration 5505, Loss: 1445437137.311163\n",
      "Iteration 5506, Loss: 1296733633.2128866\n",
      "Iteration 5507, Loss: 1271174066.818646\n",
      "Iteration 5508, Loss: 1233363282.5092719\n",
      "Iteration 5509, Loss: 1231987199.9335911\n",
      "Iteration 5510, Loss: 1265406249.5834997\n",
      "Iteration 5511, Loss: 1399486809.6973827\n",
      "Iteration 5512, Loss: 1302019563.4896932\n",
      "Iteration 5513, Loss: 1265847849.7717695\n",
      "Iteration 5514, Loss: 1253588942.0672264\n",
      "Iteration 5515, Loss: 1268079779.9439912\n",
      "Iteration 5516, Loss: 1358108308.1237307\n",
      "Iteration 5517, Loss: 1364884234.9982316\n",
      "Iteration 5518, Loss: 1367933758.7165155\n",
      "Iteration 5519, Loss: 1364774220.9874904\n",
      "Iteration 5520, Loss: 1407175500.433323\n",
      "Iteration 5521, Loss: 1263651769.6132927\n",
      "Iteration 5522, Loss: 1305600891.5869462\n",
      "Iteration 5523, Loss: 1267737798.5397875\n",
      "Iteration 5524, Loss: 1274013805.2223625\n",
      "Iteration 5525, Loss: 1152777615.5627654\n",
      "Iteration 5526, Loss: 1102296200.298426\n",
      "Iteration 5527, Loss: 1098133843.5923629\n",
      "Iteration 5528, Loss: 1218007383.0107563\n",
      "Iteration 5529, Loss: 1209355054.875168\n",
      "Iteration 5530, Loss: 1199831386.8971345\n",
      "Iteration 5531, Loss: 1189436856.1313848\n",
      "Iteration 5532, Loss: 1162696090.8949792\n",
      "Iteration 5533, Loss: 1260975132.2068012\n",
      "Iteration 5534, Loss: 1296967537.7121186\n",
      "Iteration 5535, Loss: 1177229537.7356987\n",
      "Iteration 5536, Loss: 1158223045.453352\n",
      "Iteration 5537, Loss: 1251714370.7840006\n",
      "Iteration 5538, Loss: 1128972251.1652834\n",
      "Iteration 5539, Loss: 1203173224.0934572\n",
      "Iteration 5540, Loss: 1286530593.244194\n",
      "Iteration 5541, Loss: 1070202414.4856999\n",
      "Iteration 5542, Loss: 1246902727.2671666\n",
      "Iteration 5543, Loss: 1223525139.1212714\n",
      "Iteration 5544, Loss: 1152613135.417684\n",
      "Iteration 5545, Loss: 1144481350.9229968\n",
      "Iteration 5546, Loss: 1146916189.8018596\n",
      "Iteration 5547, Loss: 1079153009.3765082\n",
      "Iteration 5548, Loss: 1229658349.3639278\n",
      "Iteration 5549, Loss: 1192389850.3181896\n",
      "Iteration 5550, Loss: 1755669471.7039118\n",
      "Iteration 5551, Loss: 1692856513.6486824\n",
      "Iteration 5552, Loss: 1155235207.6805317\n",
      "Iteration 5553, Loss: 1108801007.747798\n",
      "Iteration 5554, Loss: 1104283581.1063201\n",
      "Iteration 5555, Loss: 1161209434.0023031\n",
      "Iteration 5556, Loss: 1099892852.3004243\n",
      "Iteration 5557, Loss: 1337512131.1454234\n",
      "Iteration 5558, Loss: 1124654989.5400295\n",
      "Iteration 5559, Loss: 1148869465.0668945\n",
      "Iteration 5560, Loss: 1149074217.9217947\n",
      "Iteration 5561, Loss: 1407185955.8598611\n",
      "Iteration 5562, Loss: 1201205104.9085422\n",
      "Iteration 5563, Loss: 1109932897.4779205\n",
      "Iteration 5564, Loss: 1268633013.2194662\n",
      "Iteration 5565, Loss: 1311336425.2310152\n",
      "Iteration 5566, Loss: 1409126805.2147775\n",
      "Iteration 5567, Loss: 1416775363.6209073\n",
      "Iteration 5568, Loss: 1141721159.882157\n",
      "Iteration 5569, Loss: 1505907764.3711765\n",
      "Iteration 5570, Loss: 1532791771.215032\n",
      "Iteration 5571, Loss: 1592475792.590242\n",
      "Iteration 5572, Loss: 1578031002.3067505\n",
      "Iteration 5573, Loss: 1544779971.1329732\n",
      "Iteration 5574, Loss: 1202588890.470496\n",
      "Iteration 5575, Loss: 1187986453.2843802\n",
      "Iteration 5576, Loss: 1376452153.5698688\n",
      "Iteration 5577, Loss: 1152415280.4145472\n",
      "Iteration 5578, Loss: 1113528290.891215\n",
      "Iteration 5579, Loss: 1376645533.1697059\n",
      "Iteration 5580, Loss: 1416158158.972228\n",
      "Iteration 5581, Loss: 1183788971.3385723\n",
      "Iteration 5582, Loss: 1144457573.2562637\n",
      "Iteration 5583, Loss: 1505403350.3926535\n",
      "Iteration 5584, Loss: 1228378890.8969982\n",
      "Iteration 5585, Loss: 1831303884.8055449\n",
      "Iteration 5586, Loss: 1615070132.043254\n",
      "Iteration 5587, Loss: 1198729786.3488922\n",
      "Iteration 5588, Loss: 1385344211.660241\n",
      "Iteration 5589, Loss: 1408843452.1052904\n",
      "Iteration 5590, Loss: 1422506478.237772\n",
      "Iteration 5591, Loss: 1423304298.4961507\n",
      "Iteration 5592, Loss: 1211711408.0712092\n",
      "Iteration 5593, Loss: 1604558586.5641537\n",
      "Iteration 5594, Loss: 1493792550.9221418\n",
      "Iteration 5595, Loss: 2202743007.556898\n",
      "Iteration 5596, Loss: 1220301466.7056339\n",
      "Iteration 5597, Loss: 1422004999.496148\n",
      "Iteration 5598, Loss: 1529956620.0332317\n",
      "Iteration 5599, Loss: 1382425068.6864984\n",
      "Iteration 5600, Loss: 1223422255.6205509\n",
      "Iteration 5601, Loss: 1504646968.9875164\n",
      "Iteration 5602, Loss: 1543291492.4422326\n",
      "Iteration 5603, Loss: 1554879399.415139\n",
      "Iteration 5604, Loss: 1410429950.2246478\n",
      "Iteration 5605, Loss: 2099914173.124748\n",
      "Iteration 5606, Loss: 1964236260.8727\n",
      "Iteration 5607, Loss: 1548348986.84254\n",
      "Iteration 5608, Loss: 1655910420.2595408\n",
      "Iteration 5609, Loss: 1719953829.0562165\n",
      "Iteration 5610, Loss: 1820920749.8594153\n",
      "Iteration 5611, Loss: 1249143375.80391\n",
      "Iteration 5612, Loss: 1364628889.041257\n",
      "Iteration 5613, Loss: 1394906794.10664\n",
      "Iteration 5614, Loss: 1244822280.3667548\n",
      "Iteration 5615, Loss: 1264699516.7386696\n",
      "Iteration 5616, Loss: 1266353636.051385\n",
      "Iteration 5617, Loss: 1424103367.6309414\n",
      "Iteration 5618, Loss: 1412766980.0329347\n",
      "Iteration 5619, Loss: 1436950363.335429\n",
      "Iteration 5620, Loss: 1486855637.5746827\n",
      "Iteration 5621, Loss: 1253719213.4539292\n",
      "Iteration 5622, Loss: 1207707168.3001845\n",
      "Iteration 5623, Loss: 1340278406.2407477\n",
      "Iteration 5624, Loss: 1369388267.4459755\n",
      "Iteration 5625, Loss: 1367714716.9675806\n",
      "Iteration 5626, Loss: 1427515658.560664\n",
      "Iteration 5627, Loss: 1494725816.1795673\n",
      "Iteration 5628, Loss: 1385179150.3395715\n",
      "Iteration 5629, Loss: 1300023715.4048579\n",
      "Iteration 5630, Loss: 1437428217.106428\n",
      "Iteration 5631, Loss: 1502963300.2428412\n",
      "Iteration 5632, Loss: 1433806886.2305746\n",
      "Iteration 5633, Loss: 1434097697.292201\n",
      "Iteration 5634, Loss: 1495738064.7083683\n",
      "Iteration 5635, Loss: 1448260585.1101189\n",
      "Iteration 5636, Loss: 1484109812.2472835\n",
      "Iteration 5637, Loss: 1139814936.8490806\n",
      "Iteration 5638, Loss: 1278221302.5583124\n",
      "Iteration 5639, Loss: 1232623944.829706\n",
      "Iteration 5640, Loss: 1362527183.6835763\n",
      "Iteration 5641, Loss: 1150029666.7221618\n",
      "Iteration 5642, Loss: 1224197610.2458615\n",
      "Iteration 5643, Loss: 1236839221.7398148\n",
      "Iteration 5644, Loss: 1242084291.6869664\n",
      "Iteration 5645, Loss: 1239287817.853263\n",
      "Iteration 5646, Loss: 1191137693.0392983\n",
      "Iteration 5647, Loss: 1141484733.0499082\n",
      "Iteration 5648, Loss: 1267544562.063706\n",
      "Iteration 5649, Loss: 1261586076.8054051\n",
      "Iteration 5650, Loss: 1084360219.1586704\n",
      "Iteration 5651, Loss: 1233525842.3523757\n",
      "Iteration 5652, Loss: 1199360680.7843065\n",
      "Iteration 5653, Loss: 1151677862.3246984\n",
      "Iteration 5654, Loss: 1183446738.0091414\n",
      "Iteration 5655, Loss: 1072329168.2916195\n",
      "Iteration 5656, Loss: 1116909919.6429055\n",
      "Iteration 5657, Loss: 2058068185.636464\n",
      "Iteration 5658, Loss: 1790731538.8529856\n",
      "Iteration 5659, Loss: 1621985850.724684\n",
      "Iteration 5660, Loss: 1104510732.267637\n",
      "Iteration 5661, Loss: 1088457767.962108\n",
      "Iteration 5662, Loss: 1089950216.082259\n",
      "Iteration 5663, Loss: 1089985795.3909016\n",
      "Iteration 5664, Loss: 1206538739.4012628\n",
      "Iteration 5665, Loss: 1288831408.1171885\n",
      "Iteration 5666, Loss: 1188051998.225203\n",
      "Iteration 5667, Loss: 1073874222.6860685\n",
      "Iteration 5668, Loss: 1246242877.2763376\n",
      "Iteration 5669, Loss: 1239897864.0458424\n",
      "Iteration 5670, Loss: 1153155668.7405503\n",
      "Iteration 5671, Loss: 1094209365.6941087\n",
      "Iteration 5672, Loss: 1367296385.7738025\n",
      "Iteration 5673, Loss: 1182454881.8051214\n",
      "Iteration 5674, Loss: 1277530328.0613742\n",
      "Iteration 5675, Loss: 1281423483.5901392\n",
      "Iteration 5676, Loss: 1271842330.8600261\n",
      "Iteration 5677, Loss: 1275328700.2339854\n",
      "Iteration 5678, Loss: 1275339300.5094943\n",
      "Iteration 5679, Loss: 1137968291.6897292\n",
      "Iteration 5680, Loss: 1660715069.935949\n",
      "Iteration 5681, Loss: 1232348617.626137\n",
      "Iteration 5682, Loss: 1181481874.5694695\n",
      "Iteration 5683, Loss: 1239790791.3534\n",
      "Iteration 5684, Loss: 1231440355.160669\n",
      "Iteration 5685, Loss: 1221571819.1514132\n",
      "Iteration 5686, Loss: 1223695633.9605794\n",
      "Iteration 5687, Loss: 1209228499.4342058\n",
      "Iteration 5688, Loss: 1128133668.11899\n",
      "Iteration 5689, Loss: 1211107594.3574138\n",
      "Iteration 5690, Loss: 1242170977.029806\n",
      "Iteration 5691, Loss: 1124751520.070014\n",
      "Iteration 5692, Loss: 1238672586.5058365\n",
      "Iteration 5693, Loss: 1224003561.5388505\n",
      "Iteration 5694, Loss: 1266327540.557648\n",
      "Iteration 5695, Loss: 1252851398.1905813\n",
      "Iteration 5696, Loss: 1238456663.6140459\n",
      "Iteration 5697, Loss: 1312493811.6478896\n",
      "Iteration 5698, Loss: 1369251645.7881877\n",
      "Iteration 5699, Loss: 1092632972.4070115\n",
      "Iteration 5700, Loss: 1146569463.7375455\n",
      "Iteration 5701, Loss: 1333448179.772377\n",
      "Iteration 5702, Loss: 1307254312.9440002\n",
      "Iteration 5703, Loss: 1096279456.8287203\n",
      "Iteration 5704, Loss: 1095574938.5853794\n",
      "Iteration 5705, Loss: 1095203286.2384672\n",
      "Iteration 5706, Loss: 1128430283.851231\n",
      "Iteration 5707, Loss: 1186254040.5151968\n",
      "Iteration 5708, Loss: 1177190454.189585\n",
      "Iteration 5709, Loss: 1165814723.8271797\n",
      "Iteration 5710, Loss: 1295090467.149513\n",
      "Iteration 5711, Loss: 1106676984.7459853\n",
      "Iteration 5712, Loss: 1215579123.9506693\n",
      "Iteration 5713, Loss: 1313031953.6052625\n",
      "Iteration 5714, Loss: 1364380180.2641532\n",
      "Iteration 5715, Loss: 1204057793.339383\n",
      "Iteration 5716, Loss: 1669650252.189857\n",
      "Iteration 5717, Loss: 1551963147.9592955\n",
      "Iteration 5718, Loss: 2216069063.4642878\n",
      "Iteration 5719, Loss: 3058827156.4542665\n",
      "Iteration 5720, Loss: 1239856494.736007\n",
      "Iteration 5721, Loss: 1111110498.7040675\n",
      "Iteration 5722, Loss: 1092118912.022265\n",
      "Iteration 5723, Loss: 1105348487.3686278\n",
      "Iteration 5724, Loss: 1112836393.2739778\n",
      "Iteration 5725, Loss: 1226646485.200137\n",
      "Iteration 5726, Loss: 1174193590.2730963\n",
      "Iteration 5727, Loss: 1195206112.6338468\n",
      "Iteration 5728, Loss: 1170998616.9352233\n",
      "Iteration 5729, Loss: 1143235170.124392\n",
      "Iteration 5730, Loss: 1300529008.513642\n",
      "Iteration 5731, Loss: 1358109375.423864\n",
      "Iteration 5732, Loss: 1335676991.8429236\n",
      "Iteration 5733, Loss: 1238736858.527687\n",
      "Iteration 5734, Loss: 1071101889.7529755\n",
      "Iteration 5735, Loss: 2475635902.1476483\n",
      "Iteration 5736, Loss: 1254586578.280181\n",
      "Iteration 5737, Loss: 1688831312.6169722\n",
      "Iteration 5738, Loss: 1620835996.4234304\n",
      "Iteration 5739, Loss: 1568524913.2491803\n",
      "Iteration 5740, Loss: 1230670628.3278031\n",
      "Iteration 5741, Loss: 1980099935.7946649\n",
      "Iteration 5742, Loss: 1128618868.3579402\n",
      "Iteration 5743, Loss: 1121231605.3587244\n",
      "Iteration 5744, Loss: 1180640092.849614\n",
      "Iteration 5745, Loss: 1248435775.255082\n",
      "Iteration 5746, Loss: 1239257143.4840696\n",
      "Iteration 5747, Loss: 1242707691.1735525\n",
      "Iteration 5748, Loss: 1278063519.4941585\n",
      "Iteration 5749, Loss: 1275875168.4605675\n",
      "Iteration 5750, Loss: 1190676230.9845903\n",
      "Iteration 5751, Loss: 1383610624.2405515\n",
      "Iteration 5752, Loss: 1448195891.0658557\n",
      "Iteration 5753, Loss: 1495580624.6286566\n",
      "Iteration 5754, Loss: 1399578353.248384\n",
      "Iteration 5755, Loss: 1430481739.2772088\n",
      "Iteration 5756, Loss: 1413048293.7931511\n",
      "Iteration 5757, Loss: 1401691615.0925348\n",
      "Iteration 5758, Loss: 1134859404.9109507\n",
      "Iteration 5759, Loss: 1289837915.9503038\n",
      "Iteration 5760, Loss: 1256720628.4971247\n",
      "Iteration 5761, Loss: 1177695522.4272497\n",
      "Iteration 5762, Loss: 1164334278.790587\n",
      "Iteration 5763, Loss: 1145638882.0800495\n",
      "Iteration 5764, Loss: 1142232467.0155337\n",
      "Iteration 5765, Loss: 1176962220.9940372\n",
      "Iteration 5766, Loss: 1184360609.4269345\n",
      "Iteration 5767, Loss: 1469283801.2779968\n",
      "Iteration 5768, Loss: 1449836672.4783678\n",
      "Iteration 5769, Loss: 1500737260.3852148\n",
      "Iteration 5770, Loss: 1497849971.8093686\n",
      "Iteration 5771, Loss: 1482092110.297232\n",
      "Iteration 5772, Loss: 1273102018.2535377\n",
      "Iteration 5773, Loss: 1232735139.8384695\n",
      "Iteration 5774, Loss: 1274707431.7717757\n",
      "Iteration 5775, Loss: 1246942739.5759916\n",
      "Iteration 5776, Loss: 1277739960.822455\n",
      "Iteration 5777, Loss: 1255459472.715694\n",
      "Iteration 5778, Loss: 1104387465.7626762\n",
      "Iteration 5779, Loss: 1138185043.016163\n",
      "Iteration 5780, Loss: 1383179076.84239\n",
      "Iteration 5781, Loss: 1132288107.440502\n",
      "Iteration 5782, Loss: 1131582008.7188342\n",
      "Iteration 5783, Loss: 1872566745.8914254\n",
      "Iteration 5784, Loss: 1119474939.366649\n",
      "Iteration 5785, Loss: 1216842380.951446\n",
      "Iteration 5786, Loss: 1205389816.7639945\n",
      "Iteration 5787, Loss: 1212733883.602644\n",
      "Iteration 5788, Loss: 1220198478.8500452\n",
      "Iteration 5789, Loss: 1225963033.0506244\n",
      "Iteration 5790, Loss: 1329368799.4338617\n",
      "Iteration 5791, Loss: 1185022323.942361\n",
      "Iteration 5792, Loss: 1301890531.7018535\n",
      "Iteration 5793, Loss: 1362867216.2407904\n",
      "Iteration 5794, Loss: 1096860954.7682374\n",
      "Iteration 5795, Loss: 1091446712.6742396\n",
      "Iteration 5796, Loss: 1479795737.678887\n",
      "Iteration 5797, Loss: 1398489152.978898\n",
      "Iteration 5798, Loss: 1380049939.6020203\n",
      "Iteration 5799, Loss: 1088340198.7174122\n",
      "Iteration 5800, Loss: 1076006060.3879375\n",
      "Iteration 5801, Loss: 1072024198.8759407\n",
      "Iteration 5802, Loss: 2498803149.0827456\n",
      "Iteration 5803, Loss: 2299399875.461751\n",
      "Iteration 5804, Loss: 1989645581.3591127\n",
      "Iteration 5805, Loss: 1540089237.8839896\n",
      "Iteration 5806, Loss: 1084224532.7313528\n",
      "Iteration 5807, Loss: 1091671594.7439864\n",
      "Iteration 5808, Loss: 1396843945.586298\n",
      "Iteration 5809, Loss: 1194282059.2037673\n",
      "Iteration 5810, Loss: 1300871630.9049542\n",
      "Iteration 5811, Loss: 1135644410.13134\n",
      "Iteration 5812, Loss: 1314667377.8567178\n",
      "Iteration 5813, Loss: 1228909780.6978657\n",
      "Iteration 5814, Loss: 1216138110.5421891\n",
      "Iteration 5815, Loss: 1180742962.6091216\n",
      "Iteration 5816, Loss: 1230804841.7153027\n",
      "Iteration 5817, Loss: 1317353183.8193724\n",
      "Iteration 5818, Loss: 1296724533.4905627\n",
      "Iteration 5819, Loss: 1349223667.6051593\n",
      "Iteration 5820, Loss: 1112541316.1470184\n",
      "Iteration 5821, Loss: 1208814475.531854\n",
      "Iteration 5822, Loss: 1300537565.6653037\n",
      "Iteration 5823, Loss: 1130753279.242545\n",
      "Iteration 5824, Loss: 1118220435.8578486\n",
      "Iteration 5825, Loss: 1171124849.7901692\n",
      "Iteration 5826, Loss: 1158828396.7476473\n",
      "Iteration 5827, Loss: 1706857681.3924417\n",
      "Iteration 5828, Loss: 1195886473.0573108\n",
      "Iteration 5829, Loss: 1145220116.1875658\n",
      "Iteration 5830, Loss: 1164355935.8043802\n",
      "Iteration 5831, Loss: 1276760819.7456632\n",
      "Iteration 5832, Loss: 1137962370.4133883\n",
      "Iteration 5833, Loss: 1292636649.5865088\n",
      "Iteration 5834, Loss: 1138082727.189388\n",
      "Iteration 5835, Loss: 2063391296.5835695\n",
      "Iteration 5836, Loss: 1132676427.5747313\n",
      "Iteration 5837, Loss: 1815754253.4532113\n",
      "Iteration 5838, Loss: 1758727009.0698981\n",
      "Iteration 5839, Loss: 1152641903.5617127\n",
      "Iteration 5840, Loss: 1171162720.7891672\n",
      "Iteration 5841, Loss: 1175506941.421323\n",
      "Iteration 5842, Loss: 1168784091.185162\n",
      "Iteration 5843, Loss: 1946064579.8940098\n",
      "Iteration 5844, Loss: 1917779367.0916562\n",
      "Iteration 5845, Loss: 1750491060.406387\n",
      "Iteration 5846, Loss: 1516477634.8393204\n",
      "Iteration 5847, Loss: 1520298996.085684\n",
      "Iteration 5848, Loss: 1515623599.8619874\n",
      "Iteration 5849, Loss: 1137932881.2110796\n",
      "Iteration 5850, Loss: 1408637336.685831\n",
      "Iteration 5851, Loss: 1465366452.3827646\n",
      "Iteration 5852, Loss: 1154112690.8088715\n",
      "Iteration 5853, Loss: 1374327822.8407521\n",
      "Iteration 5854, Loss: 1371308805.3636663\n",
      "Iteration 5855, Loss: 1180960160.518396\n",
      "Iteration 5856, Loss: 1280897920.843316\n",
      "Iteration 5857, Loss: 1325856020.3141723\n",
      "Iteration 5858, Loss: 1172013638.6520152\n",
      "Iteration 5859, Loss: 1174697702.4401488\n",
      "Iteration 5860, Loss: 1182608319.29822\n",
      "Iteration 5861, Loss: 1220874009.5275393\n",
      "Iteration 5862, Loss: 1351737761.2884455\n",
      "Iteration 5863, Loss: 1282558156.2562275\n",
      "Iteration 5864, Loss: 1246223923.6348639\n",
      "Iteration 5865, Loss: 1285774417.7656643\n",
      "Iteration 5866, Loss: 1141553038.630336\n",
      "Iteration 5867, Loss: 1154442935.944768\n",
      "Iteration 5868, Loss: 1173942035.2090673\n",
      "Iteration 5869, Loss: 1383906392.4358754\n",
      "Iteration 5870, Loss: 1481777236.6796432\n",
      "Iteration 5871, Loss: 1216045265.4423232\n",
      "Iteration 5872, Loss: 2952464298.7060876\n",
      "Iteration 5873, Loss: 2198612577.468367\n",
      "Iteration 5874, Loss: 1746802435.5439758\n",
      "Iteration 5875, Loss: 1521052268.7353282\n",
      "Iteration 5876, Loss: 4773787378.555616\n",
      "Iteration 5877, Loss: 2977509747.702953\n",
      "Iteration 5878, Loss: 2803942856.7354746\n",
      "Iteration 5879, Loss: 1547630803.3968632\n",
      "Iteration 5880, Loss: 1404722098.1850374\n",
      "Iteration 5881, Loss: 1531995185.7583146\n",
      "Iteration 5882, Loss: 1364514592.5648065\n",
      "Iteration 5883, Loss: 1452605844.0881567\n",
      "Iteration 5884, Loss: 1523388754.4913661\n",
      "Iteration 5885, Loss: 1278496214.910505\n",
      "Iteration 5886, Loss: 1187017204.2895176\n",
      "Iteration 5887, Loss: 1548869315.2377915\n",
      "Iteration 5888, Loss: 1623223354.4799693\n",
      "Iteration 5889, Loss: 1268572612.148654\n",
      "Iteration 5890, Loss: 1275370548.3009417\n",
      "Iteration 5891, Loss: 1308530033.060747\n",
      "Iteration 5892, Loss: 1656980372.4039853\n",
      "Iteration 5893, Loss: 1224774058.6294258\n",
      "Iteration 5894, Loss: 1244069872.1988854\n",
      "Iteration 5895, Loss: 1240020692.6474667\n",
      "Iteration 5896, Loss: 1251870333.470953\n",
      "Iteration 5897, Loss: 1506964659.2247615\n",
      "Iteration 5898, Loss: 1262301711.3391132\n",
      "Iteration 5899, Loss: 1422910671.4290757\n",
      "Iteration 5900, Loss: 1450492790.6818757\n",
      "Iteration 5901, Loss: 1405351559.2315655\n",
      "Iteration 5902, Loss: 1377463467.9662848\n",
      "Iteration 5903, Loss: 1524517200.6796508\n",
      "Iteration 5904, Loss: 1218327728.1362712\n",
      "Iteration 5905, Loss: 1374426924.8636131\n",
      "Iteration 5906, Loss: 1404858620.1919103\n",
      "Iteration 5907, Loss: 1438445208.5293498\n",
      "Iteration 5908, Loss: 1222108865.3063846\n",
      "Iteration 5909, Loss: 1394849676.8303018\n",
      "Iteration 5910, Loss: 1459261540.7386\n",
      "Iteration 5911, Loss: 1231178000.5056884\n",
      "Iteration 5912, Loss: 1589065008.1403058\n",
      "Iteration 5913, Loss: 1666790750.1918654\n",
      "Iteration 5914, Loss: 1708503344.7902231\n",
      "Iteration 5915, Loss: 1749712087.1118376\n",
      "Iteration 5916, Loss: 1683922009.6494763\n",
      "Iteration 5917, Loss: 1695796668.8894122\n",
      "Iteration 5918, Loss: 1721272108.4106696\n",
      "Iteration 5919, Loss: 1703947267.0669756\n",
      "Iteration 5920, Loss: 1706050092.7791157\n",
      "Iteration 5921, Loss: 1613572660.5718277\n",
      "Iteration 5922, Loss: 1609491585.4791908\n",
      "Iteration 5923, Loss: 1266031319.9807575\n",
      "Iteration 5924, Loss: 1226093010.354052\n",
      "Iteration 5925, Loss: 1237205861.9845073\n",
      "Iteration 5926, Loss: 1679156706.8249907\n",
      "Iteration 5927, Loss: 1434967518.3839397\n",
      "Iteration 5928, Loss: 1208514815.063405\n",
      "Iteration 5929, Loss: 1604364914.4425912\n",
      "Iteration 5930, Loss: 1246982891.0690522\n",
      "Iteration 5931, Loss: 1281804299.052266\n",
      "Iteration 5932, Loss: 1305126015.7961576\n",
      "Iteration 5933, Loss: 1616279780.491466\n",
      "Iteration 5934, Loss: 1620143941.6202843\n",
      "Iteration 5935, Loss: 1641677457.7907846\n",
      "Iteration 5936, Loss: 1370696972.5945888\n",
      "Iteration 5937, Loss: 1302230149.599158\n",
      "Iteration 5938, Loss: 6910382743.454869\n",
      "Iteration 5939, Loss: 2097150276.360017\n",
      "Iteration 5940, Loss: 1408534974.9800487\n",
      "Iteration 5941, Loss: 2114848373.9337234\n",
      "Iteration 5942, Loss: 2105445098.2619805\n",
      "Iteration 5943, Loss: 1190343880.4691558\n",
      "Iteration 5944, Loss: 1403249560.0362391\n",
      "Iteration 5945, Loss: 1507319912.6198928\n",
      "Iteration 5946, Loss: 1297298951.8794484\n",
      "Iteration 5947, Loss: 1279965752.8892221\n",
      "Iteration 5948, Loss: 1206592327.670122\n",
      "Iteration 5949, Loss: 1212078991.3748646\n",
      "Iteration 5950, Loss: 1360943028.8921988\n",
      "Iteration 5951, Loss: 1253077845.8467977\n",
      "Iteration 5952, Loss: 1338304011.0580661\n",
      "Iteration 5953, Loss: 1670806228.2275975\n",
      "Iteration 5954, Loss: 1286863698.239047\n",
      "Iteration 5955, Loss: 1314045950.3396456\n",
      "Iteration 5956, Loss: 1397685930.7724345\n",
      "Iteration 5957, Loss: 1415487205.2198186\n",
      "Iteration 5958, Loss: 1371721303.7860787\n",
      "Iteration 5959, Loss: 1521954308.9234118\n",
      "Iteration 5960, Loss: 1311612125.4827635\n",
      "Iteration 5961, Loss: 1479183626.6400428\n",
      "Iteration 5962, Loss: 1507160205.8795695\n",
      "Iteration 5963, Loss: 1339876793.1599088\n",
      "Iteration 5964, Loss: 1598046219.6110458\n",
      "Iteration 5965, Loss: 1282339167.9997907\n",
      "Iteration 5966, Loss: 1374015465.3522234\n",
      "Iteration 5967, Loss: 1310064883.1878808\n",
      "Iteration 5968, Loss: 1333669030.4940808\n",
      "Iteration 5969, Loss: 1499848308.5816147\n",
      "Iteration 5970, Loss: 1627153861.9701526\n",
      "Iteration 5971, Loss: 1685283132.149109\n",
      "Iteration 5972, Loss: 1699989227.7049391\n",
      "Iteration 5973, Loss: 1642176165.9582834\n",
      "Iteration 5974, Loss: 1641343145.5618842\n",
      "Iteration 5975, Loss: 1188480000.819894\n",
      "Iteration 5976, Loss: 1171110866.7774518\n",
      "Iteration 5977, Loss: 1283095654.6452894\n",
      "Iteration 5978, Loss: 1338986572.3727322\n",
      "Iteration 5979, Loss: 1274396842.4508393\n",
      "Iteration 5980, Loss: 1257641821.7686484\n",
      "Iteration 5981, Loss: 1488269995.6398358\n",
      "Iteration 5982, Loss: 1219705265.4705997\n",
      "Iteration 5983, Loss: 1497668331.0869231\n",
      "Iteration 5984, Loss: 1281136630.278971\n",
      "Iteration 5985, Loss: 1224227967.1384897\n",
      "Iteration 5986, Loss: 1202802723.3024712\n",
      "Iteration 5987, Loss: 1245453288.8453596\n",
      "Iteration 5988, Loss: 1245257098.8048162\n",
      "Iteration 5989, Loss: 1326920410.55358\n",
      "Iteration 5990, Loss: 1403514811.537615\n",
      "Iteration 5991, Loss: 1224972598.5364733\n",
      "Iteration 5992, Loss: 1226768519.9608307\n",
      "Iteration 5993, Loss: 1193148378.394299\n",
      "Iteration 5994, Loss: 1308866440.668777\n",
      "Iteration 5995, Loss: 1347723633.5214713\n",
      "Iteration 5996, Loss: 1224178287.3662062\n",
      "Iteration 5997, Loss: 1449031610.7545173\n",
      "Iteration 5998, Loss: 1308031943.4818547\n",
      "Iteration 5999, Loss: 1397376167.2565708\n",
      "Iteration 6000, Loss: 1217290701.1722639\n",
      "Iteration 6001, Loss: 1420668255.5047464\n",
      "Iteration 6002, Loss: 1447063779.9243608\n",
      "Iteration 6003, Loss: 1520439741.8530548\n",
      "Iteration 6004, Loss: 1429933931.1468325\n",
      "Iteration 6005, Loss: 1264240757.3675847\n",
      "Iteration 6006, Loss: 1208527236.399755\n",
      "Iteration 6007, Loss: 1392238442.4958234\n",
      "Iteration 6008, Loss: 1370209546.270409\n",
      "Iteration 6009, Loss: 1289534922.7399542\n",
      "Iteration 6010, Loss: 1312229498.538733\n",
      "Iteration 6011, Loss: 1214527743.405399\n",
      "Iteration 6012, Loss: 1402066011.1427228\n",
      "Iteration 6013, Loss: 1528794711.1792011\n",
      "Iteration 6014, Loss: 1350696460.6129217\n",
      "Iteration 6015, Loss: 1542201723.0729723\n",
      "Iteration 6016, Loss: 1651925165.1147165\n",
      "Iteration 6017, Loss: 1679284829.8112173\n",
      "Iteration 6018, Loss: 1698854822.2968276\n",
      "Iteration 6019, Loss: 1699506651.4149897\n",
      "Iteration 6020, Loss: 1238462705.7084703\n",
      "Iteration 6021, Loss: 1454639969.9434886\n",
      "Iteration 6022, Loss: 1580629294.2686124\n",
      "Iteration 6023, Loss: 1615089021.5958676\n",
      "Iteration 6024, Loss: 1620643025.9403405\n",
      "Iteration 6025, Loss: 1460531993.9193954\n",
      "Iteration 6026, Loss: 1185314559.955547\n",
      "Iteration 6027, Loss: 1553702607.4250505\n",
      "Iteration 6028, Loss: 1286234543.8690746\n",
      "Iteration 6029, Loss: 1282089953.5921452\n",
      "Iteration 6030, Loss: 1377214542.2910235\n",
      "Iteration 6031, Loss: 1410103020.6251752\n",
      "Iteration 6032, Loss: 1497654404.3838735\n",
      "Iteration 6033, Loss: 1248428061.5131006\n",
      "Iteration 6034, Loss: 1499104884.003142\n",
      "Iteration 6035, Loss: 1516914631.2432692\n",
      "Iteration 6036, Loss: 1328207666.3902678\n",
      "Iteration 6037, Loss: 1335419613.9916575\n",
      "Iteration 6038, Loss: 1355273890.442252\n",
      "Iteration 6039, Loss: 1467903357.6755393\n",
      "Iteration 6040, Loss: 1314902266.8905776\n",
      "Iteration 6041, Loss: 1375231288.360413\n",
      "Iteration 6042, Loss: 1338244482.370086\n",
      "Iteration 6043, Loss: 1393043137.5526648\n",
      "Iteration 6044, Loss: 1398600323.5355499\n",
      "Iteration 6045, Loss: 1406703624.257258\n",
      "Iteration 6046, Loss: 1409974261.6835403\n",
      "Iteration 6047, Loss: 1456725070.9739542\n",
      "Iteration 6048, Loss: 1290888834.1860337\n",
      "Iteration 6049, Loss: 1304435391.2335472\n",
      "Iteration 6050, Loss: 1335594541.1555326\n",
      "Iteration 6051, Loss: 1347653065.0313344\n",
      "Iteration 6052, Loss: 1195108526.6752267\n",
      "Iteration 6053, Loss: 1182647855.257312\n",
      "Iteration 6054, Loss: 2030494663.83847\n",
      "Iteration 6055, Loss: 1282922577.4971838\n",
      "Iteration 6056, Loss: 1295180414.0859377\n",
      "Iteration 6057, Loss: 1395927227.4875224\n",
      "Iteration 6058, Loss: 1406273662.4650621\n",
      "Iteration 6059, Loss: 1418394366.5788455\n",
      "Iteration 6060, Loss: 1203951074.025188\n",
      "Iteration 6061, Loss: 1181605841.840181\n",
      "Iteration 6062, Loss: 2039992176.3601875\n",
      "Iteration 6063, Loss: 2608437583.8702927\n",
      "Iteration 6064, Loss: 1405155718.3281317\n",
      "Iteration 6065, Loss: 1201698037.4996738\n",
      "Iteration 6066, Loss: 1231357483.0234883\n",
      "Iteration 6067, Loss: 1466376097.6033573\n",
      "Iteration 6068, Loss: 1482543929.1030786\n",
      "Iteration 6069, Loss: 1570194935.693826\n",
      "Iteration 6070, Loss: 1336324687.9146407\n",
      "Iteration 6071, Loss: 1204106221.4192026\n",
      "Iteration 6072, Loss: 1217539965.2153218\n",
      "Iteration 6073, Loss: 1223257423.2463472\n",
      "Iteration 6074, Loss: 1310853923.3483243\n",
      "Iteration 6075, Loss: 1289032140.9303637\n",
      "Iteration 6076, Loss: 1309037713.3909805\n",
      "Iteration 6077, Loss: 1439457372.0808187\n",
      "Iteration 6078, Loss: 1488818930.735255\n",
      "Iteration 6079, Loss: 1257605559.7542453\n",
      "Iteration 6080, Loss: 1317250905.5636802\n",
      "Iteration 6081, Loss: 1294709443.3262515\n",
      "Iteration 6082, Loss: 1402136336.120512\n",
      "Iteration 6083, Loss: 1396136513.3717022\n",
      "Iteration 6084, Loss: 1171731008.2871902\n",
      "Iteration 6085, Loss: 1311421586.546465\n",
      "Iteration 6086, Loss: 1530852823.887214\n",
      "Iteration 6087, Loss: 1266371577.46214\n",
      "Iteration 6088, Loss: 1165628517.280275\n",
      "Iteration 6089, Loss: 1175266545.5343878\n",
      "Iteration 6090, Loss: 2063641627.8856373\n",
      "Iteration 6091, Loss: 1195296116.0146503\n",
      "Iteration 6092, Loss: 1189592961.368025\n",
      "Iteration 6093, Loss: 1457511189.8129985\n",
      "Iteration 6094, Loss: 1524391127.8376482\n",
      "Iteration 6095, Loss: 1270667209.913971\n",
      "Iteration 6096, Loss: 1644900680.760652\n",
      "Iteration 6097, Loss: 1680393544.4596229\n",
      "Iteration 6098, Loss: 1685247435.890066\n",
      "Iteration 6099, Loss: 1668889703.8380487\n",
      "Iteration 6100, Loss: 1732233082.2139323\n",
      "Iteration 6101, Loss: 1757660090.3586602\n",
      "Iteration 6102, Loss: 9072030983.642504\n",
      "Iteration 6103, Loss: 4189402796.6461563\n",
      "Iteration 6104, Loss: 1595299395.7408724\n",
      "Iteration 6105, Loss: 1329208774.4581351\n",
      "Iteration 6106, Loss: 1241507692.672924\n",
      "Iteration 6107, Loss: 1234899582.4059541\n",
      "Iteration 6108, Loss: 1379446720.4300725\n",
      "Iteration 6109, Loss: 1321669666.683289\n",
      "Iteration 6110, Loss: 1353316221.9699194\n",
      "Iteration 6111, Loss: 1334916275.25184\n",
      "Iteration 6112, Loss: 1363473054.9081125\n",
      "Iteration 6113, Loss: 1388269399.4712987\n",
      "Iteration 6114, Loss: 1468256326.945548\n",
      "Iteration 6115, Loss: 1475161210.8155434\n",
      "Iteration 6116, Loss: 1533134930.5193775\n",
      "Iteration 6117, Loss: 1342994237.346917\n",
      "Iteration 6118, Loss: 1365160721.827034\n",
      "Iteration 6119, Loss: 1381497969.867422\n",
      "Iteration 6120, Loss: 1184616381.242743\n",
      "Iteration 6121, Loss: 1149644788.9624918\n",
      "Iteration 6122, Loss: 1131205622.9547787\n",
      "Iteration 6123, Loss: 1306599172.9135594\n",
      "Iteration 6124, Loss: 1215977112.182197\n",
      "Iteration 6125, Loss: 1153114270.6095579\n",
      "Iteration 6126, Loss: 1143338680.4930289\n",
      "Iteration 6127, Loss: 1143284567.6220777\n",
      "Iteration 6128, Loss: 1144750974.6587617\n",
      "Iteration 6129, Loss: 1140988813.647051\n",
      "Iteration 6130, Loss: 1268965995.5153666\n",
      "Iteration 6131, Loss: 1275365178.7027557\n",
      "Iteration 6132, Loss: 1197497763.322003\n",
      "Iteration 6133, Loss: 1310883354.1924024\n",
      "Iteration 6134, Loss: 1265849507.1148546\n",
      "Iteration 6135, Loss: 1183689629.5176492\n",
      "Iteration 6136, Loss: 1295795684.6944075\n",
      "Iteration 6137, Loss: 1272696514.7990873\n",
      "Iteration 6138, Loss: 1328866991.3106997\n",
      "Iteration 6139, Loss: 1378522671.3679237\n",
      "Iteration 6140, Loss: 1243029824.9257746\n",
      "Iteration 6141, Loss: 1274988837.661235\n",
      "Iteration 6142, Loss: 1226217237.983111\n",
      "Iteration 6143, Loss: 1222571952.088183\n",
      "Iteration 6144, Loss: 1058176470.397564\n",
      "Iteration 6145, Loss: 1046618192.7878097\n",
      "Iteration 6146, Loss: 1322262939.6497953\n",
      "Iteration 6147, Loss: 1087228367.700026\n",
      "Iteration 6148, Loss: 2382346344.589179\n",
      "Iteration 6149, Loss: 1957396527.0395074\n",
      "Iteration 6150, Loss: 1107260005.2046366\n",
      "Iteration 6151, Loss: 1091394105.0877957\n",
      "Iteration 6152, Loss: 1082941766.1064913\n",
      "Iteration 6153, Loss: 1072226694.6155225\n",
      "Iteration 6154, Loss: 1632186183.654351\n",
      "Iteration 6155, Loss: 1105408917.0937743\n",
      "Iteration 6156, Loss: 1090883328.7128615\n",
      "Iteration 6157, Loss: 1491493203.814734\n",
      "Iteration 6158, Loss: 1429181628.7483494\n",
      "Iteration 6159, Loss: 1152989025.820465\n",
      "Iteration 6160, Loss: 1139366381.8107455\n",
      "Iteration 6161, Loss: 1387119352.5452154\n",
      "Iteration 6162, Loss: 1430465395.4329975\n",
      "Iteration 6163, Loss: 1206073948.5593488\n",
      "Iteration 6164, Loss: 1343834663.4661481\n",
      "Iteration 6165, Loss: 1223003117.3595867\n",
      "Iteration 6166, Loss: 1351489471.634697\n",
      "Iteration 6167, Loss: 1441094954.448558\n",
      "Iteration 6168, Loss: 1175668657.6867719\n",
      "Iteration 6169, Loss: 1178927203.7135363\n",
      "Iteration 6170, Loss: 1244373560.2816389\n",
      "Iteration 6171, Loss: 1319957555.7387269\n",
      "Iteration 6172, Loss: 1313003362.220436\n",
      "Iteration 6173, Loss: 1163592357.1279883\n",
      "Iteration 6174, Loss: 1279393612.340272\n",
      "Iteration 6175, Loss: 1268932453.5458477\n",
      "Iteration 6176, Loss: 1099045293.6480696\n",
      "Iteration 6177, Loss: 1240855306.5850923\n",
      "Iteration 6178, Loss: 1279246103.6567886\n",
      "Iteration 6179, Loss: 1358446300.2966285\n",
      "Iteration 6180, Loss: 1226229246.2705889\n",
      "Iteration 6181, Loss: 1303486342.5721853\n",
      "Iteration 6182, Loss: 1271982730.4899619\n",
      "Iteration 6183, Loss: 1131291845.620216\n",
      "Iteration 6184, Loss: 1326010047.3020053\n",
      "Iteration 6185, Loss: 1307012786.8861907\n",
      "Iteration 6186, Loss: 1373281756.8922756\n",
      "Iteration 6187, Loss: 1367257472.8841503\n",
      "Iteration 6188, Loss: 1323496871.072816\n",
      "Iteration 6189, Loss: 1112019836.7391071\n",
      "Iteration 6190, Loss: 1475912288.4392366\n",
      "Iteration 6191, Loss: 1451060382.240297\n",
      "Iteration 6192, Loss: 1387076509.680106\n",
      "Iteration 6193, Loss: 1142716052.0845263\n",
      "Iteration 6194, Loss: 1339055155.2939496\n",
      "Iteration 6195, Loss: 1138554525.1406236\n",
      "Iteration 6196, Loss: 1124296172.1435308\n",
      "Iteration 6197, Loss: 1389741131.477412\n",
      "Iteration 6198, Loss: 1314737580.5044153\n",
      "Iteration 6199, Loss: 1207845669.972951\n",
      "Iteration 6200, Loss: 1857585099.7621562\n",
      "Iteration 6201, Loss: 1191668108.4927974\n",
      "Iteration 6202, Loss: 1265145544.0443528\n",
      "Iteration 6203, Loss: 1160397189.4747672\n",
      "Iteration 6204, Loss: 1160406191.4800985\n",
      "Iteration 6205, Loss: 1176422868.04323\n",
      "Iteration 6206, Loss: 1267924790.1729252\n",
      "Iteration 6207, Loss: 1411106392.8597493\n",
      "Iteration 6208, Loss: 1153289065.3096669\n",
      "Iteration 6209, Loss: 1149005609.8101945\n",
      "Iteration 6210, Loss: 1153412855.3426528\n",
      "Iteration 6211, Loss: 1198859039.1215641\n",
      "Iteration 6212, Loss: 1340790124.103593\n",
      "Iteration 6213, Loss: 1350994534.3575277\n",
      "Iteration 6214, Loss: 1343066143.5276961\n",
      "Iteration 6215, Loss: 1291243776.6326895\n",
      "Iteration 6216, Loss: 1275904351.2353005\n",
      "Iteration 6217, Loss: 1306817491.8547392\n",
      "Iteration 6218, Loss: 1326894880.6024032\n",
      "Iteration 6219, Loss: 1118500041.24957\n",
      "Iteration 6220, Loss: 1174718912.9458258\n",
      "Iteration 6221, Loss: 1185720806.0377488\n",
      "Iteration 6222, Loss: 1113637507.4928417\n",
      "Iteration 6223, Loss: 1229902063.345583\n",
      "Iteration 6224, Loss: 1256683682.6650276\n",
      "Iteration 6225, Loss: 1243896943.2206995\n",
      "Iteration 6226, Loss: 1157552445.2377543\n",
      "Iteration 6227, Loss: 1194665262.8917034\n",
      "Iteration 6228, Loss: 1065439174.3277304\n",
      "Iteration 6229, Loss: 1057403208.257092\n",
      "Iteration 6230, Loss: 1292699441.1954682\n",
      "Iteration 6231, Loss: 1306937576.3354864\n",
      "Iteration 6232, Loss: 1229492853.8725855\n",
      "Iteration 6233, Loss: 1225360212.450351\n",
      "Iteration 6234, Loss: 1209484525.1134522\n",
      "Iteration 6235, Loss: 1201910514.5220332\n",
      "Iteration 6236, Loss: 1030231337.6227717\n",
      "Iteration 6237, Loss: 3633188070.802356\n",
      "Iteration 6238, Loss: 2140057263.9745305\n",
      "Iteration 6239, Loss: 1612856350.6386347\n",
      "Iteration 6240, Loss: 1131001892.1558328\n",
      "Iteration 6241, Loss: 1356415100.4011595\n",
      "Iteration 6242, Loss: 1339927340.9089594\n",
      "Iteration 6243, Loss: 1284376911.951974\n",
      "Iteration 6244, Loss: 1109870999.9169943\n",
      "Iteration 6245, Loss: 1931618542.3177433\n",
      "Iteration 6246, Loss: 1802943100.2838192\n",
      "Iteration 6247, Loss: 1109204766.5721052\n",
      "Iteration 6248, Loss: 1116943208.4846373\n",
      "Iteration 6249, Loss: 1109341182.7263858\n",
      "Iteration 6250, Loss: 1340049014.3038766\n",
      "Iteration 6251, Loss: 1239612478.1296353\n",
      "Iteration 6252, Loss: 1232215641.659447\n",
      "Iteration 6253, Loss: 1326791779.7284384\n",
      "Iteration 6254, Loss: 1083368963.6811438\n",
      "Iteration 6255, Loss: 2450844464.341986\n",
      "Iteration 6256, Loss: 2118231029.7339149\n",
      "Iteration 6257, Loss: 2672410734.7864447\n",
      "Iteration 6258, Loss: 1100697155.1562867\n",
      "Iteration 6259, Loss: 1125732425.3832595\n",
      "Iteration 6260, Loss: 1141458514.570027\n",
      "Iteration 6261, Loss: 1163914945.616957\n",
      "Iteration 6262, Loss: 1222909905.401924\n",
      "Iteration 6263, Loss: 1062195678.2789845\n",
      "Iteration 6264, Loss: 1577486880.1849778\n",
      "Iteration 6265, Loss: 1347714861.6661806\n",
      "Iteration 6266, Loss: 1138655495.2905977\n",
      "Iteration 6267, Loss: 1160807324.4765751\n",
      "Iteration 6268, Loss: 1264134586.0359704\n",
      "Iteration 6269, Loss: 1166132975.1233857\n",
      "Iteration 6270, Loss: 1057829557.4880936\n",
      "Iteration 6271, Loss: 1051336265.5232573\n",
      "Iteration 6272, Loss: 1042762799.1955553\n",
      "Iteration 6273, Loss: 1073156598.2599342\n",
      "Iteration 6274, Loss: 2022411452.0804136\n",
      "Iteration 6275, Loss: 1850429057.8235257\n",
      "Iteration 6276, Loss: 1361838100.2353642\n",
      "Iteration 6277, Loss: 1053784522.333299\n",
      "Iteration 6278, Loss: 1449539443.115058\n",
      "Iteration 6279, Loss: 1417974859.6938388\n",
      "Iteration 6280, Loss: 1344580774.1572719\n",
      "Iteration 6281, Loss: 1310154958.8063142\n",
      "Iteration 6282, Loss: 1040771531.917871\n",
      "Iteration 6283, Loss: 1518756420.9166698\n",
      "Iteration 6284, Loss: 1482692722.5091102\n",
      "Iteration 6285, Loss: 1145401398.699626\n",
      "Iteration 6286, Loss: 1100605350.6814578\n",
      "Iteration 6287, Loss: 1063026497.9261209\n",
      "Iteration 6288, Loss: 1056895622.8881887\n",
      "Iteration 6289, Loss: 1446813568.4000156\n",
      "Iteration 6290, Loss: 1067988133.2991527\n",
      "Iteration 6291, Loss: 2324641518.187822\n",
      "Iteration 6292, Loss: 2105083628.2473106\n",
      "Iteration 6293, Loss: 1834353520.5224376\n",
      "Iteration 6294, Loss: 1705855547.8265595\n",
      "Iteration 6295, Loss: 1354527318.7295437\n",
      "Iteration 6296, Loss: 1362791968.0367346\n",
      "Iteration 6297, Loss: 1334450211.2814543\n",
      "Iteration 6298, Loss: 1084724641.5627701\n",
      "Iteration 6299, Loss: 3473462031.906272\n",
      "Iteration 6300, Loss: 3000354087.460408\n",
      "Iteration 6301, Loss: 3784401607.4645596\n",
      "Iteration 6302, Loss: 1092096048.2527363\n",
      "Iteration 6303, Loss: 1332482808.781942\n",
      "Iteration 6304, Loss: 1281784638.2648153\n",
      "Iteration 6305, Loss: 1204261706.3229225\n",
      "Iteration 6306, Loss: 1169394821.70818\n",
      "Iteration 6307, Loss: 1245701659.44534\n",
      "Iteration 6308, Loss: 1223720857.148325\n",
      "Iteration 6309, Loss: 1220820331.008868\n",
      "Iteration 6310, Loss: 1211778511.4300382\n",
      "Iteration 6311, Loss: 1345892217.5638838\n",
      "Iteration 6312, Loss: 1271577999.0986905\n",
      "Iteration 6313, Loss: 1207136576.467692\n",
      "Iteration 6314, Loss: 1274391729.1124864\n",
      "Iteration 6315, Loss: 1283636321.0713358\n",
      "Iteration 6316, Loss: 1377566677.0951817\n",
      "Iteration 6317, Loss: 1165250607.757904\n",
      "Iteration 6318, Loss: 1224309539.0040448\n",
      "Iteration 6319, Loss: 1216418209.9424381\n",
      "Iteration 6320, Loss: 1223299305.8487587\n",
      "Iteration 6321, Loss: 1352671992.081744\n",
      "Iteration 6322, Loss: 1276079741.481004\n",
      "Iteration 6323, Loss: 1147691851.6634862\n",
      "Iteration 6324, Loss: 1158953491.9788406\n",
      "Iteration 6325, Loss: 1159964731.2563293\n",
      "Iteration 6326, Loss: 1163590226.5958846\n",
      "Iteration 6327, Loss: 1273156202.5284379\n",
      "Iteration 6328, Loss: 1316507291.4764256\n",
      "Iteration 6329, Loss: 1132969770.866076\n",
      "Iteration 6330, Loss: 1610128519.1114695\n",
      "Iteration 6331, Loss: 1423132315.4808598\n",
      "Iteration 6332, Loss: 1312913556.618994\n",
      "Iteration 6333, Loss: 1330352931.922354\n",
      "Iteration 6334, Loss: 1387863011.8637145\n",
      "Iteration 6335, Loss: 1446491456.1290762\n",
      "Iteration 6336, Loss: 1157072125.804765\n",
      "Iteration 6337, Loss: 1159763833.8804739\n",
      "Iteration 6338, Loss: 1168690026.0301514\n",
      "Iteration 6339, Loss: 1264731725.2508056\n",
      "Iteration 6340, Loss: 1268160046.7683592\n",
      "Iteration 6341, Loss: 1271515442.1342921\n",
      "Iteration 6342, Loss: 1261409774.5393763\n",
      "Iteration 6343, Loss: 1264409507.1851883\n",
      "Iteration 6344, Loss: 1253802612.385085\n",
      "Iteration 6345, Loss: 1261044794.6229868\n",
      "Iteration 6346, Loss: 1300369258.6054618\n",
      "Iteration 6347, Loss: 1341259413.4596636\n",
      "Iteration 6348, Loss: 1080903254.225313\n",
      "Iteration 6349, Loss: 1050848131.7530931\n",
      "Iteration 6350, Loss: 1324355907.4265711\n",
      "Iteration 6351, Loss: 1154933151.4858725\n",
      "Iteration 6352, Loss: 1189966551.3695312\n",
      "Iteration 6353, Loss: 1138680007.574911\n",
      "Iteration 6354, Loss: 1131237027.8031976\n",
      "Iteration 6355, Loss: 1229367618.9921343\n",
      "Iteration 6356, Loss: 1110886482.6696558\n",
      "Iteration 6357, Loss: 1087285343.7316291\n",
      "Iteration 6358, Loss: 1193679662.767649\n",
      "Iteration 6359, Loss: 1597840490.9244812\n",
      "Iteration 6360, Loss: 1333518692.8848476\n",
      "Iteration 6361, Loss: 1328302410.7563252\n",
      "Iteration 6362, Loss: 1215146520.4641516\n",
      "Iteration 6363, Loss: 1146967737.291912\n",
      "Iteration 6364, Loss: 1148564726.5385196\n",
      "Iteration 6365, Loss: 1897602875.1945229\n",
      "Iteration 6366, Loss: 1126431797.067643\n",
      "Iteration 6367, Loss: 1094820816.5411189\n",
      "Iteration 6368, Loss: 1069525706.972791\n",
      "Iteration 6369, Loss: 2559895503.758469\n",
      "Iteration 6370, Loss: 1145700103.9718366\n",
      "Iteration 6371, Loss: 1116194028.5584116\n",
      "Iteration 6372, Loss: 1697310733.229759\n",
      "Iteration 6373, Loss: 1643477295.3485472\n",
      "Iteration 6374, Loss: 1498757904.9296682\n",
      "Iteration 6375, Loss: 1304176249.7367492\n",
      "Iteration 6376, Loss: 1182866716.9874592\n",
      "Iteration 6377, Loss: 1828957391.90221\n",
      "Iteration 6378, Loss: 1675327049.7430356\n",
      "Iteration 6379, Loss: 1537073460.7247267\n",
      "Iteration 6380, Loss: 1082555096.1388924\n",
      "Iteration 6381, Loss: 1108839289.3279588\n",
      "Iteration 6382, Loss: 1275237450.0294764\n",
      "Iteration 6383, Loss: 1317159223.638008\n",
      "Iteration 6384, Loss: 1409148899.6138208\n",
      "Iteration 6385, Loss: 1401709182.4885714\n",
      "Iteration 6386, Loss: 1188744054.668121\n",
      "Iteration 6387, Loss: 1256925509.9731112\n",
      "Iteration 6388, Loss: 1250543555.698725\n",
      "Iteration 6389, Loss: 1173687281.8293545\n",
      "Iteration 6390, Loss: 1163708846.7752225\n",
      "Iteration 6391, Loss: 1264851066.5250504\n",
      "Iteration 6392, Loss: 1234790527.9937806\n",
      "Iteration 6393, Loss: 1167787754.0084648\n",
      "Iteration 6394, Loss: 1267767340.3017309\n",
      "Iteration 6395, Loss: 1292651041.1901312\n",
      "Iteration 6396, Loss: 1166096337.716961\n",
      "Iteration 6397, Loss: 1255892495.7440107\n",
      "Iteration 6398, Loss: 1107777712.1164424\n",
      "Iteration 6399, Loss: 1991460515.5284562\n",
      "Iteration 6400, Loss: 1835589827.8260686\n",
      "Iteration 6401, Loss: 1490320989.3118417\n",
      "Iteration 6402, Loss: 1463997443.6464565\n",
      "Iteration 6403, Loss: 1098273302.1183176\n",
      "Iteration 6404, Loss: 2100262558.3584015\n",
      "Iteration 6405, Loss: 1556737807.1061215\n",
      "Iteration 6406, Loss: 1502096085.5610764\n",
      "Iteration 6407, Loss: 1089328925.811109\n",
      "Iteration 6408, Loss: 1155888018.6393292\n",
      "Iteration 6409, Loss: 1220314713.9659746\n",
      "Iteration 6410, Loss: 1326142237.4964187\n",
      "Iteration 6411, Loss: 1191954527.4239295\n",
      "Iteration 6412, Loss: 1167368620.7039022\n",
      "Iteration 6413, Loss: 1165060235.332712\n",
      "Iteration 6414, Loss: 1216607195.5885983\n",
      "Iteration 6415, Loss: 1158689991.0738878\n",
      "Iteration 6416, Loss: 1114141716.6376524\n",
      "Iteration 6417, Loss: 1131293170.433799\n",
      "Iteration 6418, Loss: 1280525611.872588\n",
      "Iteration 6419, Loss: 1227980908.5818477\n",
      "Iteration 6420, Loss: 1057949256.7831736\n",
      "Iteration 6421, Loss: 1045489042.4624922\n",
      "Iteration 6422, Loss: 2804843764.3869476\n",
      "Iteration 6423, Loss: 1387941327.2847226\n",
      "Iteration 6424, Loss: 1374409929.9519112\n",
      "Iteration 6425, Loss: 1218022458.6104398\n",
      "Iteration 6426, Loss: 1665192071.2378633\n",
      "Iteration 6427, Loss: 1123935372.4316604\n",
      "Iteration 6428, Loss: 1130086119.9920783\n",
      "Iteration 6429, Loss: 1132337671.3651116\n",
      "Iteration 6430, Loss: 1325744192.7691743\n",
      "Iteration 6431, Loss: 1312415569.8763833\n",
      "Iteration 6432, Loss: 1298962860.6095412\n",
      "Iteration 6433, Loss: 1340151427.4116771\n",
      "Iteration 6434, Loss: 1342457289.3108304\n",
      "Iteration 6435, Loss: 1077331289.9639888\n",
      "Iteration 6436, Loss: 1040886285.0595309\n",
      "Iteration 6437, Loss: 1038180238.81828\n",
      "Iteration 6438, Loss: 1068598171.1397614\n",
      "Iteration 6439, Loss: 1074676393.3334303\n",
      "Iteration 6440, Loss: 1087568768.3662374\n",
      "Iteration 6441, Loss: 1048587823.4070659\n",
      "Iteration 6442, Loss: 1150531408.0278473\n",
      "Iteration 6443, Loss: 1129129025.3651106\n",
      "Iteration 6444, Loss: 1107304441.7400253\n",
      "Iteration 6445, Loss: 1119359821.810993\n",
      "Iteration 6446, Loss: 1092157627.7391193\n",
      "Iteration 6447, Loss: 1159577390.448487\n",
      "Iteration 6448, Loss: 1181054472.5124385\n",
      "Iteration 6449, Loss: 1216557008.5775924\n",
      "Iteration 6450, Loss: 1239160439.7913275\n",
      "Iteration 6451, Loss: 1221379912.2700922\n",
      "Iteration 6452, Loss: 1206409264.879692\n",
      "Iteration 6453, Loss: 1186155636.062801\n",
      "Iteration 6454, Loss: 1065918754.927144\n",
      "Iteration 6455, Loss: 1106405943.2153172\n",
      "Iteration 6456, Loss: 1119781570.5482893\n",
      "Iteration 6457, Loss: 1168319129.3724258\n",
      "Iteration 6458, Loss: 1125209083.6495872\n",
      "Iteration 6459, Loss: 1369359586.9710286\n",
      "Iteration 6460, Loss: 1070448825.4888482\n",
      "Iteration 6461, Loss: 1211756622.5813878\n",
      "Iteration 6462, Loss: 1192381347.3706675\n",
      "Iteration 6463, Loss: 1185909248.3949778\n",
      "Iteration 6464, Loss: 1222309266.9233713\n",
      "Iteration 6465, Loss: 1191138352.9597118\n",
      "Iteration 6466, Loss: 1216578113.6212578\n",
      "Iteration 6467, Loss: 1173288153.112248\n",
      "Iteration 6468, Loss: 1176135757.549462\n",
      "Iteration 6469, Loss: 1224961407.806724\n",
      "Iteration 6470, Loss: 1189627869.5084684\n",
      "Iteration 6471, Loss: 1206005177.916252\n",
      "Iteration 6472, Loss: 1026147700.381801\n",
      "Iteration 6473, Loss: 1056378420.682804\n",
      "Iteration 6474, Loss: 988085397.6734068\n",
      "Iteration 6475, Loss: 977827300.7809669\n",
      "Iteration 6476, Loss: 966854444.1437817\n",
      "Iteration 6477, Loss: 1419370153.2381673\n",
      "Iteration 6478, Loss: 1062379210.5433422\n",
      "Iteration 6479, Loss: 1110184954.1619916\n",
      "Iteration 6480, Loss: 1172897635.3847947\n",
      "Iteration 6481, Loss: 1130536607.5423143\n",
      "Iteration 6482, Loss: 1097802107.7099843\n",
      "Iteration 6483, Loss: 1110941289.5274205\n",
      "Iteration 6484, Loss: 1068010984.6249144\n",
      "Iteration 6485, Loss: 1005618157.5664412\n",
      "Iteration 6486, Loss: 1230117432.7686136\n",
      "Iteration 6487, Loss: 1059179869.6907341\n",
      "Iteration 6488, Loss: 1113154279.3881586\n",
      "Iteration 6489, Loss: 997413585.111002\n",
      "Iteration 6490, Loss: 945548759.3964365\n",
      "Iteration 6491, Loss: 933045301.092076\n",
      "Iteration 6492, Loss: 920883247.571659\n",
      "Iteration 6493, Loss: 905517477.5876969\n",
      "Iteration 6494, Loss: 893668055.7683007\n",
      "Iteration 6495, Loss: 1208981257.1878474\n",
      "Iteration 6496, Loss: 1135633972.6546972\n",
      "Iteration 6497, Loss: 1008060375.5626384\n",
      "Iteration 6498, Loss: 971204436.1302027\n",
      "Iteration 6499, Loss: 2172926396.081806\n",
      "Iteration 6500, Loss: 1455340034.0797434\n",
      "Iteration 6501, Loss: 959638530.3022357\n",
      "Iteration 6502, Loss: 1234317570.408746\n",
      "Iteration 6503, Loss: 949331962.936416\n",
      "Iteration 6504, Loss: 1361185526.9441576\n",
      "Iteration 6505, Loss: 1308245968.4358604\n",
      "Iteration 6506, Loss: 1237335279.4922166\n",
      "Iteration 6507, Loss: 1184303364.6593077\n",
      "Iteration 6508, Loss: 1170924039.995048\n",
      "Iteration 6509, Loss: 984162249.9912376\n",
      "Iteration 6510, Loss: 963452208.0086817\n",
      "Iteration 6511, Loss: 2612084577.3848133\n",
      "Iteration 6512, Loss: 2663712780.5943923\n",
      "Iteration 6513, Loss: 2442817519.973735\n",
      "Iteration 6514, Loss: 2083104761.8818467\n",
      "Iteration 6515, Loss: 1919385745.4257612\n",
      "Iteration 6516, Loss: 20423725958.056435\n",
      "Iteration 6517, Loss: 12191792845.71639\n",
      "Iteration 6518, Loss: 44005742666.81175\n",
      "Iteration 6519, Loss: 12363359444.766315\n",
      "Iteration 6520, Loss: 13956695560.573847\n",
      "Iteration 6521, Loss: 1359534905.0956876\n",
      "Iteration 6522, Loss: 1187844793.603953\n",
      "Iteration 6523, Loss: 1127422836.4436944\n",
      "Iteration 6524, Loss: 1014843905.2190145\n",
      "Iteration 6525, Loss: 1018078030.2977744\n",
      "Iteration 6526, Loss: 1082530972.8180366\n",
      "Iteration 6527, Loss: 1064644905.1601217\n",
      "Iteration 6528, Loss: 988334020.3268105\n",
      "Iteration 6529, Loss: 955159710.6772039\n",
      "Iteration 6530, Loss: 940567488.617111\n",
      "Iteration 6531, Loss: 963274716.9775339\n",
      "Iteration 6532, Loss: 1274559068.6130288\n",
      "Iteration 6533, Loss: 976315146.1657392\n",
      "Iteration 6534, Loss: 992767804.2201865\n",
      "Iteration 6535, Loss: 1011744055.2436967\n",
      "Iteration 6536, Loss: 924206670.5202231\n",
      "Iteration 6537, Loss: 2906851673.070691\n",
      "Iteration 6538, Loss: 1242853573.7410002\n",
      "Iteration 6539, Loss: 959265505.9405526\n",
      "Iteration 6540, Loss: 986997727.4042577\n",
      "Iteration 6541, Loss: 971119338.8011729\n",
      "Iteration 6542, Loss: 951559422.1828065\n",
      "Iteration 6543, Loss: 968065601.7561526\n",
      "Iteration 6544, Loss: 946013550.3443652\n",
      "Iteration 6545, Loss: 1318452722.8941255\n",
      "Iteration 6546, Loss: 1197588408.7217743\n",
      "Iteration 6547, Loss: 1183247320.650555\n",
      "Iteration 6548, Loss: 1429885036.1437416\n",
      "Iteration 6549, Loss: 1245658597.5759635\n",
      "Iteration 6550, Loss: 1015389290.8056358\n",
      "Iteration 6551, Loss: 1089703536.694115\n",
      "Iteration 6552, Loss: 1208322325.0339308\n",
      "Iteration 6553, Loss: 1214328067.2287128\n",
      "Iteration 6554, Loss: 1137908648.764687\n",
      "Iteration 6555, Loss: 1183249006.3571882\n",
      "Iteration 6556, Loss: 1044534743.7771981\n",
      "Iteration 6557, Loss: 1025018627.3078598\n",
      "Iteration 6558, Loss: 2075974131.224291\n",
      "Iteration 6559, Loss: 1813811243.039166\n",
      "Iteration 6560, Loss: 1250685190.2502613\n",
      "Iteration 6561, Loss: 998618836.1516523\n",
      "Iteration 6562, Loss: 1000162277.2488198\n",
      "Iteration 6563, Loss: 1001069327.6513879\n",
      "Iteration 6564, Loss: 2376650099.63366\n",
      "Iteration 6565, Loss: 1001807070.7591993\n",
      "Iteration 6566, Loss: 1447075949.038028\n",
      "Iteration 6567, Loss: 1281602416.6011145\n",
      "Iteration 6568, Loss: 1260793159.045728\n",
      "Iteration 6569, Loss: 1281601306.3043983\n",
      "Iteration 6570, Loss: 1114120090.319815\n",
      "Iteration 6571, Loss: 1142842209.9448013\n",
      "Iteration 6572, Loss: 1130418122.2394147\n",
      "Iteration 6573, Loss: 1198950312.3988037\n",
      "Iteration 6574, Loss: 1070711448.1256601\n",
      "Iteration 6575, Loss: 1270049416.0688727\n",
      "Iteration 6576, Loss: 1293700435.5887685\n",
      "Iteration 6577, Loss: 1271244865.1053576\n",
      "Iteration 6578, Loss: 1000715728.0623841\n",
      "Iteration 6579, Loss: 1255489265.933543\n",
      "Iteration 6580, Loss: 1110726372.0375824\n",
      "Iteration 6581, Loss: 1118245360.6987686\n",
      "Iteration 6582, Loss: 1099027978.0287766\n",
      "Iteration 6583, Loss: 1049621175.1767583\n",
      "Iteration 6584, Loss: 1185881744.5349185\n",
      "Iteration 6585, Loss: 1168443696.364462\n",
      "Iteration 6586, Loss: 1257582033.8841388\n",
      "Iteration 6587, Loss: 1028762040.5756882\n",
      "Iteration 6588, Loss: 1023390212.2721847\n",
      "Iteration 6589, Loss: 1456154657.843057\n",
      "Iteration 6590, Loss: 1249845826.4535065\n",
      "Iteration 6591, Loss: 1083973908.7881367\n",
      "Iteration 6592, Loss: 2085640952.1573453\n",
      "Iteration 6593, Loss: 1960598906.054127\n",
      "Iteration 6594, Loss: 1090144509.6277478\n",
      "Iteration 6595, Loss: 1067621760.9242238\n",
      "Iteration 6596, Loss: 5094981100.701575\n",
      "Iteration 6597, Loss: 3964803302.5077543\n",
      "Iteration 6598, Loss: 1902700459.9636025\n",
      "Iteration 6599, Loss: 1422991958.0619648\n",
      "Iteration 6600, Loss: 1168257253.4329462\n",
      "Iteration 6601, Loss: 1074086827.1539772\n",
      "Iteration 6602, Loss: 1176315436.6219466\n",
      "Iteration 6603, Loss: 1207062100.382444\n",
      "Iteration 6604, Loss: 1190748913.852154\n",
      "Iteration 6605, Loss: 1172097887.18211\n",
      "Iteration 6606, Loss: 1114738058.443016\n",
      "Iteration 6607, Loss: 1099731016.5973237\n",
      "Iteration 6608, Loss: 1303099042.4499748\n",
      "Iteration 6609, Loss: 1328810140.8697467\n",
      "Iteration 6610, Loss: 1308251420.7942646\n",
      "Iteration 6611, Loss: 1269210675.7369266\n",
      "Iteration 6612, Loss: 1384239865.8507304\n",
      "Iteration 6613, Loss: 1326483221.8389282\n",
      "Iteration 6614, Loss: 1339945278.056593\n",
      "Iteration 6615, Loss: 1294799527.0662446\n",
      "Iteration 6616, Loss: 1152185924.880263\n",
      "Iteration 6617, Loss: 1100510421.2275758\n",
      "Iteration 6618, Loss: 1113139782.0893927\n",
      "Iteration 6619, Loss: 1233785504.1752207\n",
      "Iteration 6620, Loss: 1186861137.6084661\n",
      "Iteration 6621, Loss: 1144108130.608004\n",
      "Iteration 6622, Loss: 1106752276.1074786\n",
      "Iteration 6623, Loss: 1157962523.713071\n",
      "Iteration 6624, Loss: 962119042.8120074\n",
      "Iteration 6625, Loss: 958858420.9860302\n",
      "Iteration 6626, Loss: 985807504.3322953\n",
      "Iteration 6627, Loss: 964160152.6836824\n",
      "Iteration 6628, Loss: 947938877.0432954\n",
      "Iteration 6629, Loss: 1609747341.5397334\n",
      "Iteration 6630, Loss: 1222359518.1503055\n",
      "Iteration 6631, Loss: 1000716307.3969128\n",
      "Iteration 6632, Loss: 1072660606.121146\n",
      "Iteration 6633, Loss: 1039944696.7395607\n",
      "Iteration 6634, Loss: 1337480646.9138787\n",
      "Iteration 6635, Loss: 970993517.6005102\n",
      "Iteration 6636, Loss: 958318245.7751085\n",
      "Iteration 6637, Loss: 1147776941.6630514\n",
      "Iteration 6638, Loss: 1576407072.75459\n",
      "Iteration 6639, Loss: 1255716775.7430892\n",
      "Iteration 6640, Loss: 1144026787.1555285\n",
      "Iteration 6641, Loss: 1201724792.9480252\n",
      "Iteration 6642, Loss: 1478132024.9149632\n",
      "Iteration 6643, Loss: 1099615288.4561157\n",
      "Iteration 6644, Loss: 1054651673.2502872\n",
      "Iteration 6645, Loss: 991786120.1795505\n",
      "Iteration 6646, Loss: 978617145.196797\n",
      "Iteration 6647, Loss: 1276737892.4904182\n",
      "Iteration 6648, Loss: 1178479141.5236528\n",
      "Iteration 6649, Loss: 1175937107.8949513\n",
      "Iteration 6650, Loss: 976139834.0101852\n",
      "Iteration 6651, Loss: 1056904096.5314634\n",
      "Iteration 6652, Loss: 1065260261.9087403\n",
      "Iteration 6653, Loss: 1202714067.8879604\n",
      "Iteration 6654, Loss: 1019044722.8858336\n",
      "Iteration 6655, Loss: 999273984.7258774\n",
      "Iteration 6656, Loss: 1289128095.2400146\n",
      "Iteration 6657, Loss: 974093301.8761057\n",
      "Iteration 6658, Loss: 2776404178.688007\n",
      "Iteration 6659, Loss: 2615219985.0169473\n",
      "Iteration 6660, Loss: 1686979727.4805746\n",
      "Iteration 6661, Loss: 1533570427.5955667\n",
      "Iteration 6662, Loss: 1095711612.5865679\n",
      "Iteration 6663, Loss: 999024578.8827965\n",
      "Iteration 6664, Loss: 989311041.1631109\n",
      "Iteration 6665, Loss: 1304239873.758087\n",
      "Iteration 6666, Loss: 1274427902.627691\n",
      "Iteration 6667, Loss: 1230476434.5101573\n",
      "Iteration 6668, Loss: 1146385668.2300916\n",
      "Iteration 6669, Loss: 1102513988.171605\n",
      "Iteration 6670, Loss: 1069500819.6573678\n",
      "Iteration 6671, Loss: 1131540997.3831716\n",
      "Iteration 6672, Loss: 1146507398.857509\n",
      "Iteration 6673, Loss: 1090379692.513074\n",
      "Iteration 6674, Loss: 1064638329.3878402\n",
      "Iteration 6675, Loss: 1072597843.8436514\n",
      "Iteration 6676, Loss: 1074266148.02402\n",
      "Iteration 6677, Loss: 1185488783.6181076\n",
      "Iteration 6678, Loss: 1157668694.2213147\n",
      "Iteration 6679, Loss: 1020613208.1663582\n",
      "Iteration 6680, Loss: 1231332302.0443506\n",
      "Iteration 6681, Loss: 1184979198.9895394\n",
      "Iteration 6682, Loss: 1049051116.2945112\n",
      "Iteration 6683, Loss: 1857519689.8816903\n",
      "Iteration 6684, Loss: 1634670460.599962\n",
      "Iteration 6685, Loss: 1032936249.4873607\n",
      "Iteration 6686, Loss: 1488523841.7681484\n",
      "Iteration 6687, Loss: 1286475577.5707626\n",
      "Iteration 6688, Loss: 986913947.9341522\n",
      "Iteration 6689, Loss: 1167642930.145272\n",
      "Iteration 6690, Loss: 1102685385.242378\n",
      "Iteration 6691, Loss: 1079806988.006679\n",
      "Iteration 6692, Loss: 1171776733.861011\n",
      "Iteration 6693, Loss: 1151435382.6530647\n",
      "Iteration 6694, Loss: 1117857541.2530217\n",
      "Iteration 6695, Loss: 1193118523.1382859\n",
      "Iteration 6696, Loss: 1071468589.6802608\n",
      "Iteration 6697, Loss: 975986186.9708446\n",
      "Iteration 6698, Loss: 2342988804.2921777\n",
      "Iteration 6699, Loss: 1455275789.8168738\n",
      "Iteration 6700, Loss: 1068394323.0318849\n",
      "Iteration 6701, Loss: 1830642415.9384515\n",
      "Iteration 6702, Loss: 1702582480.9619956\n",
      "Iteration 6703, Loss: 1082384642.6342037\n",
      "Iteration 6704, Loss: 1276576303.8541496\n",
      "Iteration 6705, Loss: 1247172644.1580315\n",
      "Iteration 6706, Loss: 1295386810.5545423\n",
      "Iteration 6707, Loss: 1142722594.5307784\n",
      "Iteration 6708, Loss: 1148655765.7479339\n",
      "Iteration 6709, Loss: 1269940918.229796\n",
      "Iteration 6710, Loss: 1204615908.4309943\n",
      "Iteration 6711, Loss: 1124547233.1250596\n",
      "Iteration 6712, Loss: 1206596841.8199515\n",
      "Iteration 6713, Loss: 1123554586.985267\n",
      "Iteration 6714, Loss: 1198645383.5915885\n",
      "Iteration 6715, Loss: 1349372250.0753467\n",
      "Iteration 6716, Loss: 1333059626.5984087\n",
      "Iteration 6717, Loss: 1150300500.1590774\n",
      "Iteration 6718, Loss: 1332971605.4542134\n",
      "Iteration 6719, Loss: 1290408681.8554995\n",
      "Iteration 6720, Loss: 1343976460.5549946\n",
      "Iteration 6721, Loss: 1357687226.591085\n",
      "Iteration 6722, Loss: 1334555067.57927\n",
      "Iteration 6723, Loss: 1302088546.6835556\n",
      "Iteration 6724, Loss: 1075741461.2018116\n",
      "Iteration 6725, Loss: 1295582434.4359696\n",
      "Iteration 6726, Loss: 1145241210.3473365\n",
      "Iteration 6727, Loss: 1124610456.2730124\n",
      "Iteration 6728, Loss: 1167182062.0359542\n",
      "Iteration 6729, Loss: 1140185290.2661698\n",
      "Iteration 6730, Loss: 1320130640.942392\n",
      "Iteration 6731, Loss: 1215435074.736099\n",
      "Iteration 6732, Loss: 1224752447.7617745\n",
      "Iteration 6733, Loss: 1303442733.0994205\n",
      "Iteration 6734, Loss: 1410091254.0883722\n",
      "Iteration 6735, Loss: 1383123963.2351708\n",
      "Iteration 6736, Loss: 1312686994.718715\n",
      "Iteration 6737, Loss: 1402252318.0096505\n",
      "Iteration 6738, Loss: 1336515097.2157216\n",
      "Iteration 6739, Loss: 1402363075.2368\n",
      "Iteration 6740, Loss: 1140548262.5874279\n",
      "Iteration 6741, Loss: 1142616767.0392635\n",
      "Iteration 6742, Loss: 1230647462.7477138\n",
      "Iteration 6743, Loss: 1299647814.3022265\n",
      "Iteration 6744, Loss: 1381215966.0606115\n",
      "Iteration 6745, Loss: 1401108285.2137415\n",
      "Iteration 6746, Loss: 1093542012.3308637\n",
      "Iteration 6747, Loss: 1210832845.8614008\n",
      "Iteration 6748, Loss: 1198229682.0604951\n",
      "Iteration 6749, Loss: 1166971711.8373652\n",
      "Iteration 6750, Loss: 1150475184.385087\n",
      "Iteration 6751, Loss: 1202074459.9283602\n",
      "Iteration 6752, Loss: 1186282051.871193\n",
      "Iteration 6753, Loss: 1285704346.5990667\n",
      "Iteration 6754, Loss: 1270385161.301194\n",
      "Iteration 6755, Loss: 1280659351.2719646\n",
      "Iteration 6756, Loss: 1370317196.7022822\n",
      "Iteration 6757, Loss: 1114862907.1551507\n",
      "Iteration 6758, Loss: 1236522819.016264\n",
      "Iteration 6759, Loss: 1280975516.4245057\n",
      "Iteration 6760, Loss: 1187691948.721828\n",
      "Iteration 6761, Loss: 1166385872.871243\n",
      "Iteration 6762, Loss: 1109681025.194818\n",
      "Iteration 6763, Loss: 1324951699.8436117\n",
      "Iteration 6764, Loss: 1319650380.6719077\n",
      "Iteration 6765, Loss: 1141789618.6492379\n",
      "Iteration 6766, Loss: 1275672599.9278855\n",
      "Iteration 6767, Loss: 1275578850.201266\n",
      "Iteration 6768, Loss: 1126111165.3880405\n",
      "Iteration 6769, Loss: 1176971825.3322325\n",
      "Iteration 6770, Loss: 1242127696.247421\n",
      "Iteration 6771, Loss: 1239138657.0785887\n",
      "Iteration 6772, Loss: 1294970061.0976608\n",
      "Iteration 6773, Loss: 1355874841.1495917\n",
      "Iteration 6774, Loss: 1170459535.4415305\n",
      "Iteration 6775, Loss: 1175355486.5076644\n",
      "Iteration 6776, Loss: 1279772268.5551035\n",
      "Iteration 6777, Loss: 1229467111.9818716\n",
      "Iteration 6778, Loss: 1236919369.2937548\n",
      "Iteration 6779, Loss: 1337374848.1040502\n",
      "Iteration 6780, Loss: 1336019991.6558003\n",
      "Iteration 6781, Loss: 1148011974.5935376\n",
      "Iteration 6782, Loss: 1409156126.6377437\n",
      "Iteration 6783, Loss: 1450381235.083918\n",
      "Iteration 6784, Loss: 1508645389.2291071\n",
      "Iteration 6785, Loss: 1500203536.456965\n",
      "Iteration 6786, Loss: 1562511723.8222594\n",
      "Iteration 6787, Loss: 1358369098.0786183\n",
      "Iteration 6788, Loss: 1154666255.3494346\n",
      "Iteration 6789, Loss: 1260254862.2691855\n",
      "Iteration 6790, Loss: 1257059284.51163\n",
      "Iteration 6791, Loss: 1223636353.3125315\n",
      "Iteration 6792, Loss: 1212602808.9761426\n",
      "Iteration 6793, Loss: 1268411539.2388682\n",
      "Iteration 6794, Loss: 1349263642.7353046\n",
      "Iteration 6795, Loss: 1422649749.074061\n",
      "Iteration 6796, Loss: 1307386410.5116296\n",
      "Iteration 6797, Loss: 1313771333.926516\n",
      "Iteration 6798, Loss: 1492497855.6122992\n",
      "Iteration 6799, Loss: 1208754362.1197886\n",
      "Iteration 6800, Loss: 1161115558.12149\n",
      "Iteration 6801, Loss: 1300608794.3846872\n",
      "Iteration 6802, Loss: 1272661211.592059\n",
      "Iteration 6803, Loss: 1195388046.1563358\n",
      "Iteration 6804, Loss: 1335302742.0252175\n",
      "Iteration 6805, Loss: 1185207759.3081193\n",
      "Iteration 6806, Loss: 1681580335.692022\n",
      "Iteration 6807, Loss: 1203454970.8001451\n",
      "Iteration 6808, Loss: 1483994951.1637084\n",
      "Iteration 6809, Loss: 1248917039.8866048\n",
      "Iteration 6810, Loss: 1374566926.7755177\n",
      "Iteration 6811, Loss: 1418959126.7992885\n",
      "Iteration 6812, Loss: 1439070934.9877653\n",
      "Iteration 6813, Loss: 1314697840.8563862\n",
      "Iteration 6814, Loss: 1311414204.0156744\n",
      "Iteration 6815, Loss: 1272257483.9294453\n",
      "Iteration 6816, Loss: 1300496247.1503687\n",
      "Iteration 6817, Loss: 1193868258.3000958\n",
      "Iteration 6818, Loss: 1178680421.9413593\n",
      "Iteration 6819, Loss: 1416789015.0092015\n",
      "Iteration 6820, Loss: 1363173385.280242\n",
      "Iteration 6821, Loss: 1387043946.8883088\n",
      "Iteration 6822, Loss: 1464455534.4143114\n",
      "Iteration 6823, Loss: 1539184623.2219489\n",
      "Iteration 6824, Loss: 1487346746.1333642\n",
      "Iteration 6825, Loss: 1244313542.1053731\n",
      "Iteration 6826, Loss: 1282063451.8862486\n",
      "Iteration 6827, Loss: 1406664574.2738338\n",
      "Iteration 6828, Loss: 1427380817.3447454\n",
      "Iteration 6829, Loss: 1153682212.714794\n",
      "Iteration 6830, Loss: 1213030391.3823833\n",
      "Iteration 6831, Loss: 1222950328.0407944\n",
      "Iteration 6832, Loss: 1220266135.0852182\n",
      "Iteration 6833, Loss: 1278828383.596669\n",
      "Iteration 6834, Loss: 1273568842.8090532\n",
      "Iteration 6835, Loss: 1356644602.969502\n",
      "Iteration 6836, Loss: 1100736220.3188412\n",
      "Iteration 6837, Loss: 2082177229.0870817\n",
      "Iteration 6838, Loss: 2625480005.331085\n",
      "Iteration 6839, Loss: 3881794928.5824547\n",
      "Iteration 6840, Loss: 1512271250.3231983\n",
      "Iteration 6841, Loss: 1173172953.3359768\n",
      "Iteration 6842, Loss: 1346398286.1008558\n",
      "Iteration 6843, Loss: 1445972492.6429253\n",
      "Iteration 6844, Loss: 1454683531.0410478\n",
      "Iteration 6845, Loss: 1240073170.703682\n",
      "Iteration 6846, Loss: 1163501205.3397572\n",
      "Iteration 6847, Loss: 1174046152.4149966\n",
      "Iteration 6848, Loss: 1632580519.4542172\n",
      "Iteration 6849, Loss: 1634744625.0220854\n",
      "Iteration 6850, Loss: 1171339898.8227723\n",
      "Iteration 6851, Loss: 2774855988.867253\n",
      "Iteration 6852, Loss: 1466712212.613244\n",
      "Iteration 6853, Loss: 2167344134.300938\n",
      "Iteration 6854, Loss: 2124565472.2461233\n",
      "Iteration 6855, Loss: 1258059337.352284\n",
      "Iteration 6856, Loss: 1220647177.8752685\n",
      "Iteration 6857, Loss: 1407732905.1207576\n",
      "Iteration 6858, Loss: 1301249604.6435978\n",
      "Iteration 6859, Loss: 1379416827.984219\n",
      "Iteration 6860, Loss: 1519896301.1455886\n",
      "Iteration 6861, Loss: 1502771275.285181\n",
      "Iteration 6862, Loss: 1332641892.7616892\n",
      "Iteration 6863, Loss: 1225835616.0638824\n",
      "Iteration 6864, Loss: 1477509889.9498081\n",
      "Iteration 6865, Loss: 1550777382.2164757\n",
      "Iteration 6866, Loss: 1622012176.2172415\n",
      "Iteration 6867, Loss: 1226465560.9743116\n",
      "Iteration 6868, Loss: 1684950330.4471626\n",
      "Iteration 6869, Loss: 1787509588.4044487\n",
      "Iteration 6870, Loss: 1547167884.7390058\n",
      "Iteration 6871, Loss: 1347911626.7276893\n",
      "Iteration 6872, Loss: 1483300040.4137106\n",
      "Iteration 6873, Loss: 1527376602.6772692\n",
      "Iteration 6874, Loss: 1299613704.528802\n",
      "Iteration 6875, Loss: 1494411011.6187735\n",
      "Iteration 6876, Loss: 1587690753.8145409\n",
      "Iteration 6877, Loss: 1348915613.8837087\n",
      "Iteration 6878, Loss: 1377740043.4972224\n",
      "Iteration 6879, Loss: 1311537776.387096\n",
      "Iteration 6880, Loss: 1357678409.3578374\n",
      "Iteration 6881, Loss: 1326980957.2216597\n",
      "Iteration 6882, Loss: 1441623489.2909825\n",
      "Iteration 6883, Loss: 1483411942.6032367\n",
      "Iteration 6884, Loss: 1499232581.067595\n",
      "Iteration 6885, Loss: 1252493286.8418398\n",
      "Iteration 6886, Loss: 1255696289.9861565\n",
      "Iteration 6887, Loss: 1253887097.7198822\n",
      "Iteration 6888, Loss: 1341257424.529452\n",
      "Iteration 6889, Loss: 1211103344.1519482\n",
      "Iteration 6890, Loss: 1910981035.9428396\n",
      "Iteration 6891, Loss: 1247869783.8342934\n",
      "Iteration 6892, Loss: 1241505350.4282625\n",
      "Iteration 6893, Loss: 1248182065.7172644\n",
      "Iteration 6894, Loss: 1337643170.1729386\n",
      "Iteration 6895, Loss: 1328934616.093266\n",
      "Iteration 6896, Loss: 1325423045.4335623\n",
      "Iteration 6897, Loss: 1457697982.123694\n",
      "Iteration 6898, Loss: 1492406502.584272\n",
      "Iteration 6899, Loss: 1579587480.9482548\n",
      "Iteration 6900, Loss: 1220226089.1892266\n",
      "Iteration 6901, Loss: 1220110599.2880826\n",
      "Iteration 6902, Loss: 2213331580.6887436\n",
      "Iteration 6903, Loss: 1274850992.013558\n",
      "Iteration 6904, Loss: 1307321702.8086662\n",
      "Iteration 6905, Loss: 1274549589.2729313\n",
      "Iteration 6906, Loss: 1539090767.5371752\n",
      "Iteration 6907, Loss: 1328350614.1271496\n",
      "Iteration 6908, Loss: 1909127194.798\n",
      "Iteration 6909, Loss: 1938912771.9812138\n",
      "Iteration 6910, Loss: 1958987520.2574663\n",
      "Iteration 6911, Loss: 1575875020.186962\n",
      "Iteration 6912, Loss: 1394819099.4754674\n",
      "Iteration 6913, Loss: 4207053825.212304\n",
      "Iteration 6914, Loss: 3046769932.178718\n",
      "Iteration 6915, Loss: 1355053988.2795575\n",
      "Iteration 6916, Loss: 1708906542.852178\n",
      "Iteration 6917, Loss: 1739313217.593785\n",
      "Iteration 6918, Loss: 1895927543.7242608\n",
      "Iteration 6919, Loss: 1968512317.9060614\n",
      "Iteration 6920, Loss: 1918498889.1542451\n",
      "Iteration 6921, Loss: 2028716687.3274076\n",
      "Iteration 6922, Loss: 1982960546.6256638\n",
      "Iteration 6923, Loss: 2020541013.7341948\n",
      "Iteration 6924, Loss: 2055078309.780672\n",
      "Iteration 6925, Loss: 1800424245.827371\n",
      "Iteration 6926, Loss: 1239332330.1026573\n",
      "Iteration 6927, Loss: 1680295745.8288221\n",
      "Iteration 6928, Loss: 1711170271.073066\n",
      "Iteration 6929, Loss: 1265441287.4506874\n",
      "Iteration 6930, Loss: 1290340987.2507496\n",
      "Iteration 6931, Loss: 1506157655.2371457\n",
      "Iteration 6932, Loss: 1429349307.9536448\n",
      "Iteration 6933, Loss: 1582936284.6414683\n",
      "Iteration 6934, Loss: 1606718321.5794804\n",
      "Iteration 6935, Loss: 1622240133.7575548\n",
      "Iteration 6936, Loss: 1386685098.5215535\n",
      "Iteration 6937, Loss: 1289422764.8308427\n",
      "Iteration 6938, Loss: 1451743589.011131\n",
      "Iteration 6939, Loss: 1312799572.3875978\n",
      "Iteration 6940, Loss: 1720380796.0868795\n",
      "Iteration 6941, Loss: 1729807613.5241642\n",
      "Iteration 6942, Loss: 1256961196.6946175\n",
      "Iteration 6943, Loss: 1354718934.2364411\n",
      "Iteration 6944, Loss: 1387656829.9904747\n",
      "Iteration 6945, Loss: 1281465349.5256722\n",
      "Iteration 6946, Loss: 1231244754.238738\n",
      "Iteration 6947, Loss: 2271332083.8405533\n",
      "Iteration 6948, Loss: 1407968153.1426215\n",
      "Iteration 6949, Loss: 1345116803.1229427\n",
      "Iteration 6950, Loss: 1563846324.498858\n",
      "Iteration 6951, Loss: 1450229419.3777108\n",
      "Iteration 6952, Loss: 1524496379.621188\n",
      "Iteration 6953, Loss: 1316223810.502888\n",
      "Iteration 6954, Loss: 1498175579.4685378\n",
      "Iteration 6955, Loss: 1280244898.2469394\n",
      "Iteration 6956, Loss: 1443915170.946412\n",
      "Iteration 6957, Loss: 1304451559.4598846\n",
      "Iteration 6958, Loss: 1296181694.373947\n",
      "Iteration 6959, Loss: 1558496861.6537716\n",
      "Iteration 6960, Loss: 1440261435.988437\n",
      "Iteration 6961, Loss: 1473082383.4086177\n",
      "Iteration 6962, Loss: 1601593803.510294\n",
      "Iteration 6963, Loss: 1706758937.6055722\n",
      "Iteration 6964, Loss: 1747738403.048252\n",
      "Iteration 6965, Loss: 1307533443.3509817\n",
      "Iteration 6966, Loss: 1565304918.7679203\n",
      "Iteration 6967, Loss: 1592125815.750793\n",
      "Iteration 6968, Loss: 1697923978.63918\n",
      "Iteration 6969, Loss: 1650719821.1244347\n",
      "Iteration 6970, Loss: 1462316909.283061\n",
      "Iteration 6971, Loss: 1313359093.7072303\n",
      "Iteration 6972, Loss: 1345253590.7886205\n",
      "Iteration 6973, Loss: 1352563777.8946521\n",
      "Iteration 6974, Loss: 1396460627.3052106\n",
      "Iteration 6975, Loss: 1445022128.9256675\n",
      "Iteration 6976, Loss: 1278548758.4895449\n",
      "Iteration 6977, Loss: 1253232873.461277\n",
      "Iteration 6978, Loss: 1260238454.7161088\n",
      "Iteration 6979, Loss: 1274206871.0940244\n",
      "Iteration 6980, Loss: 1465054486.6508121\n",
      "Iteration 6981, Loss: 1479943167.402646\n",
      "Iteration 6982, Loss: 1491632300.9596004\n",
      "Iteration 6983, Loss: 1505088843.9996376\n",
      "Iteration 6984, Loss: 1558518239.0479796\n",
      "Iteration 6985, Loss: 1598012491.317351\n",
      "Iteration 6986, Loss: 1531885868.11761\n",
      "Iteration 6987, Loss: 1173060743.8959217\n",
      "Iteration 6988, Loss: 1372327556.2814064\n",
      "Iteration 6989, Loss: 1459834423.6354036\n",
      "Iteration 6990, Loss: 1521393155.7371194\n",
      "Iteration 6991, Loss: 1335709193.6939921\n",
      "Iteration 6992, Loss: 1360647514.9862125\n",
      "Iteration 6993, Loss: 1376118247.8771584\n",
      "Iteration 6994, Loss: 1332258970.2285213\n",
      "Iteration 6995, Loss: 1386031724.009968\n",
      "Iteration 6996, Loss: 1476051981.1591458\n",
      "Iteration 6997, Loss: 1218876828.3831058\n",
      "Iteration 6998, Loss: 1621161885.7967007\n",
      "Iteration 6999, Loss: 1313040354.2569683\n",
      "Iteration 7000, Loss: 1423785982.5329561\n",
      "Iteration 7001, Loss: 1435830314.0682683\n",
      "Iteration 7002, Loss: 1523579866.2522652\n",
      "Iteration 7003, Loss: 1259263737.9659603\n",
      "Iteration 7004, Loss: 1752715393.7280173\n",
      "Iteration 7005, Loss: 1188376412.0931094\n",
      "Iteration 7006, Loss: 1236372718.6266172\n",
      "Iteration 7007, Loss: 1318258130.6516824\n",
      "Iteration 7008, Loss: 1251227260.5444157\n",
      "Iteration 7009, Loss: 1852786990.7624118\n",
      "Iteration 7010, Loss: 1810765594.8567464\n",
      "Iteration 7011, Loss: 1212754243.0872808\n",
      "Iteration 7012, Loss: 1218906183.8571818\n",
      "Iteration 7013, Loss: 1584287044.2955709\n",
      "Iteration 7014, Loss: 1560544535.4706795\n",
      "Iteration 7015, Loss: 1337525113.1959255\n",
      "Iteration 7016, Loss: 4049789625.5245843\n",
      "Iteration 7017, Loss: 3304798569.23836\n",
      "Iteration 7018, Loss: 1275139522.3178787\n",
      "Iteration 7019, Loss: 1468263165.6355445\n",
      "Iteration 7020, Loss: 1331440137.9560533\n",
      "Iteration 7021, Loss: 1285983594.4162776\n",
      "Iteration 7022, Loss: 2978556679.6204867\n",
      "Iteration 7023, Loss: 1403539737.2281833\n",
      "Iteration 7024, Loss: 1301312271.6343687\n",
      "Iteration 7025, Loss: 1295408249.7067058\n",
      "Iteration 7026, Loss: 1305748774.270016\n",
      "Iteration 7027, Loss: 1427114510.40506\n",
      "Iteration 7028, Loss: 1492820191.3104696\n",
      "Iteration 7029, Loss: 1476004487.4390042\n",
      "Iteration 7030, Loss: 1579913108.2777846\n",
      "Iteration 7031, Loss: 1583237784.3440561\n",
      "Iteration 7032, Loss: 1305336338.593196\n",
      "Iteration 7033, Loss: 2009267992.0015793\n",
      "Iteration 7034, Loss: 2032093129.2898543\n",
      "Iteration 7035, Loss: 1300965954.9946585\n",
      "Iteration 7036, Loss: 1375271288.1335588\n",
      "Iteration 7037, Loss: 1500451583.761335\n",
      "Iteration 7038, Loss: 1583742050.8471434\n",
      "Iteration 7039, Loss: 1606308929.7902248\n",
      "Iteration 7040, Loss: 1685880944.878782\n",
      "Iteration 7041, Loss: 1707469757.8683283\n",
      "Iteration 7042, Loss: 1716251432.3384845\n",
      "Iteration 7043, Loss: 1652381883.4527295\n",
      "Iteration 7044, Loss: 1653641879.2393198\n",
      "Iteration 7045, Loss: 1482408652.674003\n",
      "Iteration 7046, Loss: 1226239564.071641\n",
      "Iteration 7047, Loss: 1306259873.7183752\n",
      "Iteration 7048, Loss: 1294065364.3899474\n",
      "Iteration 7049, Loss: 1326704044.8350832\n",
      "Iteration 7050, Loss: 1211892505.0556698\n",
      "Iteration 7051, Loss: 1230459670.2019773\n",
      "Iteration 7052, Loss: 1238377540.1295927\n",
      "Iteration 7053, Loss: 1342278900.3289473\n",
      "Iteration 7054, Loss: 1319950606.0978816\n",
      "Iteration 7055, Loss: 1304979581.783708\n",
      "Iteration 7056, Loss: 1288088597.1055338\n",
      "Iteration 7057, Loss: 1310832298.7316413\n",
      "Iteration 7058, Loss: 1402546890.6721463\n",
      "Iteration 7059, Loss: 1527386140.1618083\n",
      "Iteration 7060, Loss: 1542326841.2860208\n",
      "Iteration 7061, Loss: 1553140166.0412726\n",
      "Iteration 7062, Loss: 1496922767.2404191\n",
      "Iteration 7063, Loss: 1245342229.1980486\n",
      "Iteration 7064, Loss: 1441510752.6791108\n",
      "Iteration 7065, Loss: 1455531365.0290082\n",
      "Iteration 7066, Loss: 1468836996.5729046\n",
      "Iteration 7067, Loss: 1375108063.3745587\n",
      "Iteration 7068, Loss: 1434337242.182087\n",
      "Iteration 7069, Loss: 1432727518.8798625\n",
      "Iteration 7070, Loss: 1170417595.9810216\n",
      "Iteration 7071, Loss: 1171928706.9798899\n",
      "Iteration 7072, Loss: 1460807095.2302492\n",
      "Iteration 7073, Loss: 1457065999.8980255\n",
      "Iteration 7074, Loss: 1521437792.5706089\n",
      "Iteration 7075, Loss: 1499314217.5110888\n",
      "Iteration 7076, Loss: 1192251961.668996\n",
      "Iteration 7077, Loss: 1245447904.4747121\n",
      "Iteration 7078, Loss: 1243796843.6172771\n",
      "Iteration 7079, Loss: 1235650962.306773\n",
      "Iteration 7080, Loss: 1209145485.8338697\n",
      "Iteration 7081, Loss: 1345900092.661736\n",
      "Iteration 7082, Loss: 1133873428.3124852\n",
      "Iteration 7083, Loss: 1123198617.230924\n",
      "Iteration 7084, Loss: 1764215058.635546\n",
      "Iteration 7085, Loss: 1147654358.3601556\n",
      "Iteration 7086, Loss: 1383073825.9050174\n",
      "Iteration 7087, Loss: 1183371119.668091\n",
      "Iteration 7088, Loss: 1737265284.080635\n",
      "Iteration 7089, Loss: 1231068406.2166305\n",
      "Iteration 7090, Loss: 1244344575.6705472\n",
      "Iteration 7091, Loss: 1289937453.5703049\n",
      "Iteration 7092, Loss: 1445729429.0755732\n",
      "Iteration 7093, Loss: 1294698764.5416276\n",
      "Iteration 7094, Loss: 1385580488.2539778\n",
      "Iteration 7095, Loss: 1271543028.9513292\n",
      "Iteration 7096, Loss: 1288138098.4559674\n",
      "Iteration 7097, Loss: 1290213696.4639215\n",
      "Iteration 7098, Loss: 1253975178.1936953\n",
      "Iteration 7099, Loss: 1399709073.4944537\n",
      "Iteration 7100, Loss: 1239299708.3435917\n",
      "Iteration 7101, Loss: 1220940912.9874983\n",
      "Iteration 7102, Loss: 1211665429.088387\n",
      "Iteration 7103, Loss: 1445062138.4321303\n",
      "Iteration 7104, Loss: 1454635683.4397616\n",
      "Iteration 7105, Loss: 1191552532.984005\n",
      "Iteration 7106, Loss: 2126716147.5558953\n",
      "Iteration 7107, Loss: 1217376278.6227736\n",
      "Iteration 7108, Loss: 1213269845.31405\n",
      "Iteration 7109, Loss: 1269631668.5082905\n",
      "Iteration 7110, Loss: 1293863162.1144624\n",
      "Iteration 7111, Loss: 1379675342.209414\n",
      "Iteration 7112, Loss: 1293523116.8532703\n",
      "Iteration 7113, Loss: 1879723627.036371\n",
      "Iteration 7114, Loss: 1322617523.5421429\n",
      "Iteration 7115, Loss: 1435870191.668388\n",
      "Iteration 7116, Loss: 1416684376.7515163\n",
      "Iteration 7117, Loss: 1472152990.4441078\n",
      "Iteration 7118, Loss: 1309416738.50929\n",
      "Iteration 7119, Loss: 1247347474.0056639\n",
      "Iteration 7120, Loss: 1769659264.493077\n",
      "Iteration 7121, Loss: 1821345478.960065\n",
      "Iteration 7122, Loss: 1967765004.4749265\n",
      "Iteration 7123, Loss: 2240156375.712111\n",
      "Iteration 7124, Loss: 2242898523.981192\n",
      "Iteration 7125, Loss: 2192505273.8686533\n",
      "Iteration 7126, Loss: 2129379093.4822605\n",
      "Iteration 7127, Loss: 1570043565.2768219\n",
      "Iteration 7128, Loss: 1600726990.2034233\n",
      "Iteration 7129, Loss: 1328596381.5885968\n",
      "Iteration 7130, Loss: 1341379909.9710054\n",
      "Iteration 7131, Loss: 1381208046.1117196\n",
      "Iteration 7132, Loss: 1446323899.7466466\n",
      "Iteration 7133, Loss: 1450651885.8246891\n",
      "Iteration 7134, Loss: 1254271705.8642747\n",
      "Iteration 7135, Loss: 1319473909.566224\n",
      "Iteration 7136, Loss: 1354090197.619605\n",
      "Iteration 7137, Loss: 1411371718.4584153\n",
      "Iteration 7138, Loss: 1352571749.91372\n",
      "Iteration 7139, Loss: 1401064198.8486757\n",
      "Iteration 7140, Loss: 1226169583.268467\n",
      "Iteration 7141, Loss: 1242646714.8564143\n",
      "Iteration 7142, Loss: 1400436202.8752122\n",
      "Iteration 7143, Loss: 1325309495.9563475\n",
      "Iteration 7144, Loss: 1358388976.621183\n",
      "Iteration 7145, Loss: 1353578929.09332\n",
      "Iteration 7146, Loss: 1311493456.953788\n",
      "Iteration 7147, Loss: 1323944966.1628156\n",
      "Iteration 7148, Loss: 1291798506.6886048\n",
      "Iteration 7149, Loss: 1456059949.8120544\n",
      "Iteration 7150, Loss: 1485170867.9979894\n",
      "Iteration 7151, Loss: 1506663200.8267965\n",
      "Iteration 7152, Loss: 1285363858.6621375\n",
      "Iteration 7153, Loss: 1439439983.49579\n",
      "Iteration 7154, Loss: 1458777312.108416\n",
      "Iteration 7155, Loss: 1431102927.671374\n",
      "Iteration 7156, Loss: 1519924960.0112114\n",
      "Iteration 7157, Loss: 1463487541.304575\n",
      "Iteration 7158, Loss: 1255865192.5730777\n",
      "Iteration 7159, Loss: 1243438972.0851564\n",
      "Iteration 7160, Loss: 1277506083.4206808\n",
      "Iteration 7161, Loss: 1264310038.2783055\n",
      "Iteration 7162, Loss: 1175331450.5075824\n",
      "Iteration 7163, Loss: 2093888580.9246707\n",
      "Iteration 7164, Loss: 1435617633.1458728\n",
      "Iteration 7165, Loss: 1505792006.268964\n",
      "Iteration 7166, Loss: 1202361151.0541015\n",
      "Iteration 7167, Loss: 1216649588.656539\n",
      "Iteration 7168, Loss: 1298034544.3178542\n",
      "Iteration 7169, Loss: 1293938229.611047\n",
      "Iteration 7170, Loss: 1426125985.1942117\n",
      "Iteration 7171, Loss: 1178930349.4311647\n",
      "Iteration 7172, Loss: 1243970260.798463\n",
      "Iteration 7173, Loss: 1379687161.2530432\n",
      "Iteration 7174, Loss: 1320632010.3998892\n",
      "Iteration 7175, Loss: 1443969456.2703068\n",
      "Iteration 7176, Loss: 1488899257.985604\n",
      "Iteration 7177, Loss: 1251087933.892292\n",
      "Iteration 7178, Loss: 1355622191.2246525\n",
      "Iteration 7179, Loss: 1252203918.4076571\n",
      "Iteration 7180, Loss: 1739718155.1084285\n",
      "Iteration 7181, Loss: 1205031435.5812495\n",
      "Iteration 7182, Loss: 1418134103.0558293\n",
      "Iteration 7183, Loss: 1438804393.8406277\n",
      "Iteration 7184, Loss: 1446681768.7063723\n",
      "Iteration 7185, Loss: 1331066813.1564262\n",
      "Iteration 7186, Loss: 1342113447.6848004\n",
      "Iteration 7187, Loss: 1297747378.6849995\n",
      "Iteration 7188, Loss: 1233307348.2214882\n",
      "Iteration 7189, Loss: 1234735903.823458\n",
      "Iteration 7190, Loss: 1393519421.1521668\n",
      "Iteration 7191, Loss: 1238390483.085609\n",
      "Iteration 7192, Loss: 1428817041.902729\n",
      "Iteration 7193, Loss: 1518049365.3915741\n",
      "Iteration 7194, Loss: 1570616288.7352645\n",
      "Iteration 7195, Loss: 1455016383.6702194\n",
      "Iteration 7196, Loss: 1248197234.8898256\n",
      "Iteration 7197, Loss: 1963331376.884493\n",
      "Iteration 7198, Loss: 2017743056.1554716\n",
      "Iteration 7199, Loss: 2009864011.3171017\n",
      "Iteration 7200, Loss: 1911487102.4221168\n",
      "Iteration 7201, Loss: 2299541729.5335565\n",
      "Iteration 7202, Loss: 1932393329.6065793\n",
      "Iteration 7203, Loss: 1612921180.9086747\n",
      "Iteration 7204, Loss: 1605009576.2360446\n",
      "Iteration 7205, Loss: 1456849347.3340404\n",
      "Iteration 7206, Loss: 1285513367.102208\n",
      "Iteration 7207, Loss: 3290077220.336967\n",
      "Iteration 7208, Loss: 1334524355.249419\n",
      "Iteration 7209, Loss: 1421993775.0196457\n",
      "Iteration 7210, Loss: 1440823201.009337\n",
      "Iteration 7211, Loss: 1380785578.935137\n",
      "Iteration 7212, Loss: 1576971974.764217\n",
      "Iteration 7213, Loss: 1682061074.6385684\n",
      "Iteration 7214, Loss: 1806725501.0540545\n",
      "Iteration 7215, Loss: 1259161802.4185262\n",
      "Iteration 7216, Loss: 1266158175.745079\n",
      "Iteration 7217, Loss: 1293847864.016732\n",
      "Iteration 7218, Loss: 1302418876.8813052\n",
      "Iteration 7219, Loss: 1390788668.3447323\n",
      "Iteration 7220, Loss: 1546755306.7871654\n",
      "Iteration 7221, Loss: 1273325094.6735156\n",
      "Iteration 7222, Loss: 1295874539.6360776\n",
      "Iteration 7223, Loss: 1324146548.1393988\n",
      "Iteration 7224, Loss: 1336129192.5694542\n",
      "Iteration 7225, Loss: 1268359556.0169637\n",
      "Iteration 7226, Loss: 1286836506.3999114\n",
      "Iteration 7227, Loss: 1446956839.5115006\n",
      "Iteration 7228, Loss: 1518717266.880765\n",
      "Iteration 7229, Loss: 1547241247.9853427\n",
      "Iteration 7230, Loss: 1212587156.9607666\n",
      "Iteration 7231, Loss: 1217336460.844173\n",
      "Iteration 7232, Loss: 1419679036.610631\n",
      "Iteration 7233, Loss: 1429691382.1484714\n",
      "Iteration 7234, Loss: 1208147458.8795362\n",
      "Iteration 7235, Loss: 1382675409.4380176\n",
      "Iteration 7236, Loss: 1312511682.1919374\n",
      "Iteration 7237, Loss: 1248884237.6684206\n",
      "Iteration 7238, Loss: 1848167234.1543884\n",
      "Iteration 7239, Loss: 1851158991.8068132\n",
      "Iteration 7240, Loss: 1584510213.8320467\n",
      "Iteration 7241, Loss: 1479932228.3478298\n",
      "Iteration 7242, Loss: 1511817030.3904252\n",
      "Iteration 7243, Loss: 1357587275.4714549\n",
      "Iteration 7244, Loss: 1302964192.9871755\n",
      "Iteration 7245, Loss: 1412864060.9958491\n",
      "Iteration 7246, Loss: 1441710271.7956662\n",
      "Iteration 7247, Loss: 1480950972.184328\n",
      "Iteration 7248, Loss: 1396545104.3656585\n",
      "Iteration 7249, Loss: 1374901766.9476922\n",
      "Iteration 7250, Loss: 1196773173.9842303\n",
      "Iteration 7251, Loss: 1202333653.7932372\n",
      "Iteration 7252, Loss: 1194872357.0687523\n",
      "Iteration 7253, Loss: 1374719032.209231\n",
      "Iteration 7254, Loss: 1484682858.4426355\n",
      "Iteration 7255, Loss: 1290346989.4741495\n",
      "Iteration 7256, Loss: 1296562824.6866937\n",
      "Iteration 7257, Loss: 1227287506.0720966\n",
      "Iteration 7258, Loss: 1358955033.0860357\n",
      "Iteration 7259, Loss: 1386346932.9777958\n",
      "Iteration 7260, Loss: 1250320465.3061075\n",
      "Iteration 7261, Loss: 1264821299.2867367\n",
      "Iteration 7262, Loss: 1414695139.222113\n",
      "Iteration 7263, Loss: 1196560841.9252086\n",
      "Iteration 7264, Loss: 1201306875.9433343\n",
      "Iteration 7265, Loss: 1411024044.5962954\n",
      "Iteration 7266, Loss: 1407382508.0150864\n",
      "Iteration 7267, Loss: 1438753214.38035\n",
      "Iteration 7268, Loss: 1424361570.6980004\n",
      "Iteration 7269, Loss: 1197326724.579854\n",
      "Iteration 7270, Loss: 1275359011.8455093\n",
      "Iteration 7271, Loss: 1748373662.393875\n",
      "Iteration 7272, Loss: 1767200520.766016\n",
      "Iteration 7273, Loss: 1569705703.4355729\n",
      "Iteration 7274, Loss: 1451894951.9231162\n",
      "Iteration 7275, Loss: 1251832762.4050672\n",
      "Iteration 7276, Loss: 2556334337.82313\n",
      "Iteration 7277, Loss: 1988618012.0520864\n",
      "Iteration 7278, Loss: 1921720703.5747538\n",
      "Iteration 7279, Loss: 1264262729.0154216\n",
      "Iteration 7280, Loss: 1743889725.9673626\n",
      "Iteration 7281, Loss: 1782733845.657406\n",
      "Iteration 7282, Loss: 1847375689.994473\n",
      "Iteration 7283, Loss: 1944400999.1965075\n",
      "Iteration 7284, Loss: 1861844766.7744374\n",
      "Iteration 7285, Loss: 1949194404.4253135\n",
      "Iteration 7286, Loss: 2413647512.0381174\n",
      "Iteration 7287, Loss: 1869411447.3658254\n",
      "Iteration 7288, Loss: 1241570301.9699664\n",
      "Iteration 7289, Loss: 1238404738.8204434\n",
      "Iteration 7290, Loss: 1332029443.6634984\n",
      "Iteration 7291, Loss: 1363318701.4657068\n",
      "Iteration 7292, Loss: 1389330980.1399639\n",
      "Iteration 7293, Loss: 1511074037.6263616\n",
      "Iteration 7294, Loss: 1586366171.7528107\n",
      "Iteration 7295, Loss: 1286534198.7452602\n",
      "Iteration 7296, Loss: 1189281843.3450177\n",
      "Iteration 7297, Loss: 4730866534.026575\n",
      "Iteration 7298, Loss: 2290305416.7071934\n",
      "Iteration 7299, Loss: 2091818948.9592166\n",
      "Iteration 7300, Loss: 1458987584.825693\n",
      "Iteration 7301, Loss: 1463412421.8884447\n",
      "Iteration 7302, Loss: 1531880681.9622724\n",
      "Iteration 7303, Loss: 1246572707.428938\n",
      "Iteration 7304, Loss: 1165795735.0290704\n",
      "Iteration 7305, Loss: 1652558567.9444368\n",
      "Iteration 7306, Loss: 1582393102.9407668\n",
      "Iteration 7307, Loss: 1311335009.0341473\n",
      "Iteration 7308, Loss: 1727071611.4949389\n",
      "Iteration 7309, Loss: 1246251087.5888128\n",
      "Iteration 7310, Loss: 1422699836.3740947\n",
      "Iteration 7311, Loss: 1348970506.6311302\n",
      "Iteration 7312, Loss: 1203562419.8492055\n",
      "Iteration 7313, Loss: 1381204260.7011607\n",
      "Iteration 7314, Loss: 1424416386.7639997\n",
      "Iteration 7315, Loss: 1295325239.7379827\n",
      "Iteration 7316, Loss: 1316249370.0633678\n",
      "Iteration 7317, Loss: 1186598791.159629\n",
      "Iteration 7318, Loss: 1671864281.8864188\n",
      "Iteration 7319, Loss: 1273496808.5814507\n",
      "Iteration 7320, Loss: 1348433557.3477783\n",
      "Iteration 7321, Loss: 1317740363.390672\n",
      "Iteration 7322, Loss: 1198410274.7970395\n",
      "Iteration 7323, Loss: 1398916725.967357\n",
      "Iteration 7324, Loss: 1502181696.9364\n",
      "Iteration 7325, Loss: 1361028886.916476\n",
      "Iteration 7326, Loss: 1527316079.1901762\n",
      "Iteration 7327, Loss: 1550087757.3357363\n",
      "Iteration 7328, Loss: 1233505323.4237022\n",
      "Iteration 7329, Loss: 1319850965.0303442\n",
      "Iteration 7330, Loss: 1477366339.7073102\n",
      "Iteration 7331, Loss: 1243898841.295434\n",
      "Iteration 7332, Loss: 1330990259.8046775\n",
      "Iteration 7333, Loss: 1298919738.7505708\n",
      "Iteration 7334, Loss: 1327840349.9559956\n",
      "Iteration 7335, Loss: 1325214094.183304\n",
      "Iteration 7336, Loss: 1479305214.1778\n",
      "Iteration 7337, Loss: 1299474907.7980485\n",
      "Iteration 7338, Loss: 1696977188.2577634\n",
      "Iteration 7339, Loss: 1634072308.5169952\n",
      "Iteration 7340, Loss: 1251679494.7190957\n",
      "Iteration 7341, Loss: 1249636272.9894228\n",
      "Iteration 7342, Loss: 1260067153.588583\n",
      "Iteration 7343, Loss: 1286492803.439263\n",
      "Iteration 7344, Loss: 1312847507.3501093\n",
      "Iteration 7345, Loss: 1283278203.2467406\n",
      "Iteration 7346, Loss: 1354083748.4569569\n",
      "Iteration 7347, Loss: 1349363421.8466878\n",
      "Iteration 7348, Loss: 1300576702.5322692\n",
      "Iteration 7349, Loss: 1350841884.3592546\n",
      "Iteration 7350, Loss: 1182231348.5833602\n",
      "Iteration 7351, Loss: 1532749091.8820453\n",
      "Iteration 7352, Loss: 1274915683.3683102\n",
      "Iteration 7353, Loss: 1441356175.0251234\n",
      "Iteration 7354, Loss: 1559681861.2631586\n",
      "Iteration 7355, Loss: 1199019466.024081\n",
      "Iteration 7356, Loss: 1248711686.6778986\n",
      "Iteration 7357, Loss: 1407173697.013152\n",
      "Iteration 7358, Loss: 1220395516.9527414\n",
      "Iteration 7359, Loss: 1220468556.6421945\n",
      "Iteration 7360, Loss: 1477214472.8380034\n",
      "Iteration 7361, Loss: 1289629853.8687098\n",
      "Iteration 7362, Loss: 1191238450.566839\n",
      "Iteration 7363, Loss: 1277214185.2147017\n",
      "Iteration 7364, Loss: 1478006029.046695\n",
      "Iteration 7365, Loss: 1362527760.7490854\n",
      "Iteration 7366, Loss: 1288360599.116842\n",
      "Iteration 7367, Loss: 1482079675.9517205\n",
      "Iteration 7368, Loss: 1374326512.7762158\n",
      "Iteration 7369, Loss: 1406697760.2605135\n",
      "Iteration 7370, Loss: 1266726493.9765003\n",
      "Iteration 7371, Loss: 1280841461.0199623\n",
      "Iteration 7372, Loss: 1877208628.3201392\n",
      "Iteration 7373, Loss: 1899640012.0597367\n",
      "Iteration 7374, Loss: 1683669563.8949625\n",
      "Iteration 7375, Loss: 1703788875.7408814\n",
      "Iteration 7376, Loss: 1398704170.7937987\n",
      "Iteration 7377, Loss: 1268085369.4022756\n",
      "Iteration 7378, Loss: 1902890538.8271704\n",
      "Iteration 7379, Loss: 1347163734.4460638\n",
      "Iteration 7380, Loss: 1330411945.655916\n",
      "Iteration 7381, Loss: 1239595235.792353\n",
      "Iteration 7382, Loss: 1296184306.427543\n",
      "Iteration 7383, Loss: 1359875507.0927954\n",
      "Iteration 7384, Loss: 1529794743.2566726\n",
      "Iteration 7385, Loss: 1366671173.667648\n",
      "Iteration 7386, Loss: 1326925020.2088158\n",
      "Iteration 7387, Loss: 1379400294.5813847\n",
      "Iteration 7388, Loss: 1396380215.9181662\n",
      "Iteration 7389, Loss: 1443105603.6256292\n",
      "Iteration 7390, Loss: 1602029789.0634983\n",
      "Iteration 7391, Loss: 1329214093.762217\n",
      "Iteration 7392, Loss: 1279191161.2755408\n",
      "Iteration 7393, Loss: 1308624923.2604244\n",
      "Iteration 7394, Loss: 1413200409.404437\n",
      "Iteration 7395, Loss: 1480163542.649422\n",
      "Iteration 7396, Loss: 1270030343.3890276\n",
      "Iteration 7397, Loss: 1354661901.0115552\n",
      "Iteration 7398, Loss: 1468395856.3572855\n",
      "Iteration 7399, Loss: 1515573125.0256302\n",
      "Iteration 7400, Loss: 1559669613.5933595\n",
      "Iteration 7401, Loss: 1588247970.1164038\n",
      "Iteration 7402, Loss: 1228047051.0060136\n",
      "Iteration 7403, Loss: 1234307210.9422774\n",
      "Iteration 7404, Loss: 1415504191.04492\n",
      "Iteration 7405, Loss: 1432945069.9682693\n",
      "Iteration 7406, Loss: 1447128971.6728396\n",
      "Iteration 7407, Loss: 1242292082.5281405\n",
      "Iteration 7408, Loss: 1311297444.9703279\n",
      "Iteration 7409, Loss: 1333980925.1584694\n",
      "Iteration 7410, Loss: 1329260698.3779464\n",
      "Iteration 7411, Loss: 1416901200.9674149\n",
      "Iteration 7412, Loss: 1504210212.116327\n",
      "Iteration 7413, Loss: 1588097857.6932814\n",
      "Iteration 7414, Loss: 1606111426.6130445\n",
      "Iteration 7415, Loss: 1613321854.2261658\n",
      "Iteration 7416, Loss: 1692273994.7903159\n",
      "Iteration 7417, Loss: 1707685467.3912277\n",
      "Iteration 7418, Loss: 1233941575.7371001\n",
      "Iteration 7419, Loss: 1236609698.7678895\n",
      "Iteration 7420, Loss: 1252363748.1794028\n",
      "Iteration 7421, Loss: 1380924661.0236146\n",
      "Iteration 7422, Loss: 1388562660.8268485\n",
      "Iteration 7423, Loss: 1377999893.3906977\n",
      "Iteration 7424, Loss: 1380714874.7256444\n",
      "Iteration 7425, Loss: 1196185916.002141\n",
      "Iteration 7426, Loss: 1279096686.7869108\n",
      "Iteration 7427, Loss: 1318930420.2875926\n",
      "Iteration 7428, Loss: 1338884697.1831794\n",
      "Iteration 7429, Loss: 1198546203.2222118\n",
      "Iteration 7430, Loss: 1197712573.502646\n",
      "Iteration 7431, Loss: 1265700615.7676985\n",
      "Iteration 7432, Loss: 1315508113.5034735\n",
      "Iteration 7433, Loss: 1312848789.7467852\n",
      "Iteration 7434, Loss: 1107178569.557153\n",
      "Iteration 7435, Loss: 1259901289.374015\n",
      "Iteration 7436, Loss: 1209006281.190352\n",
      "Iteration 7437, Loss: 1115920844.3534877\n",
      "Iteration 7438, Loss: 1104029518.2078733\n",
      "Iteration 7439, Loss: 1984460457.7992754\n",
      "Iteration 7440, Loss: 1592222337.0896904\n",
      "Iteration 7441, Loss: 1266738324.4751532\n",
      "Iteration 7442, Loss: 1126359408.8768249\n",
      "Iteration 7443, Loss: 1246809311.3770416\n",
      "Iteration 7444, Loss: 1336090985.9980733\n",
      "Iteration 7445, Loss: 1323734311.358707\n",
      "Iteration 7446, Loss: 1420097580.6757772\n",
      "Iteration 7447, Loss: 1194448087.5849214\n",
      "Iteration 7448, Loss: 1147166601.1592102\n",
      "Iteration 7449, Loss: 1136277167.8986998\n",
      "Iteration 7450, Loss: 1182728841.6853998\n",
      "Iteration 7451, Loss: 1141628445.91334\n",
      "Iteration 7452, Loss: 1148600793.5301006\n",
      "Iteration 7453, Loss: 1309303741.8033648\n",
      "Iteration 7454, Loss: 1303762007.8245873\n",
      "Iteration 7455, Loss: 1306630588.2359192\n",
      "Iteration 7456, Loss: 1350973702.5237164\n",
      "Iteration 7457, Loss: 1265984550.2341566\n",
      "Iteration 7458, Loss: 1225345029.664366\n",
      "Iteration 7459, Loss: 1223651453.9055994\n",
      "Iteration 7460, Loss: 1113029886.303321\n",
      "Iteration 7461, Loss: 1140601209.7864218\n",
      "Iteration 7462, Loss: 1145327429.4989917\n",
      "Iteration 7463, Loss: 1148688759.7188814\n",
      "Iteration 7464, Loss: 1346816628.3446743\n",
      "Iteration 7465, Loss: 1409848936.1238601\n",
      "Iteration 7466, Loss: 1113718276.1525357\n",
      "Iteration 7467, Loss: 1082996339.7892158\n",
      "Iteration 7468, Loss: 1170913409.1542525\n",
      "Iteration 7469, Loss: 1163016437.943589\n",
      "Iteration 7470, Loss: 1070062173.3339826\n",
      "Iteration 7471, Loss: 1366221709.1400146\n",
      "Iteration 7472, Loss: 1138819228.2245934\n",
      "Iteration 7473, Loss: 1178906424.7974672\n",
      "Iteration 7474, Loss: 1337673680.9390795\n",
      "Iteration 7475, Loss: 1126490596.6025264\n",
      "Iteration 7476, Loss: 2071696117.0034552\n",
      "Iteration 7477, Loss: 1122871722.48174\n",
      "Iteration 7478, Loss: 1124791933.1814077\n",
      "Iteration 7479, Loss: 1127835811.7551775\n",
      "Iteration 7480, Loss: 1213941244.8657773\n",
      "Iteration 7481, Loss: 1248565128.7333622\n",
      "Iteration 7482, Loss: 1555604223.602203\n",
      "Iteration 7483, Loss: 1544801908.1508074\n",
      "Iteration 7484, Loss: 1403558356.2716265\n",
      "Iteration 7485, Loss: 1385171392.8645327\n",
      "Iteration 7486, Loss: 1132661712.580936\n",
      "Iteration 7487, Loss: 2014417808.8416462\n",
      "Iteration 7488, Loss: 1866558443.6730583\n",
      "Iteration 7489, Loss: 1209269684.6803617\n",
      "Iteration 7490, Loss: 1646822511.7404027\n",
      "Iteration 7491, Loss: 1340658147.9171302\n",
      "Iteration 7492, Loss: 1269834410.9512477\n",
      "Iteration 7493, Loss: 7373383275.8863125\n",
      "Iteration 7494, Loss: 2153924938.9953237\n",
      "Iteration 7495, Loss: 1159014062.7546303\n",
      "Iteration 7496, Loss: 1528067223.014983\n",
      "Iteration 7497, Loss: 1510277876.575068\n",
      "Iteration 7498, Loss: 1394197871.3811948\n",
      "Iteration 7499, Loss: 1367913476.0468612\n",
      "Iteration 7500, Loss: 1374621460.4410863\n",
      "Iteration 7501, Loss: 1290189253.9478767\n",
      "Iteration 7502, Loss: 1256613262.7737935\n",
      "Iteration 7503, Loss: 1225511579.1160178\n",
      "Iteration 7504, Loss: 1362823945.2490056\n",
      "Iteration 7505, Loss: 1328327993.9699295\n",
      "Iteration 7506, Loss: 1127345870.9424114\n",
      "Iteration 7507, Loss: 1129805528.9468136\n",
      "Iteration 7508, Loss: 1444446038.3470652\n",
      "Iteration 7509, Loss: 1472536565.1186693\n",
      "Iteration 7510, Loss: 1511066112.316025\n",
      "Iteration 7511, Loss: 1515155951.751329\n",
      "Iteration 7512, Loss: 1118204486.4607778\n",
      "Iteration 7513, Loss: 1154209386.6827044\n",
      "Iteration 7514, Loss: 1297553545.7811222\n",
      "Iteration 7515, Loss: 1341872380.3341296\n",
      "Iteration 7516, Loss: 1167628716.9458828\n",
      "Iteration 7517, Loss: 1358551701.6105542\n",
      "Iteration 7518, Loss: 1344229042.2224534\n",
      "Iteration 7519, Loss: 1302041729.9945176\n",
      "Iteration 7520, Loss: 1326607515.254726\n",
      "Iteration 7521, Loss: 1383259982.449994\n",
      "Iteration 7522, Loss: 1422114169.7823806\n",
      "Iteration 7523, Loss: 1413574737.5515356\n",
      "Iteration 7524, Loss: 1069274980.5026568\n",
      "Iteration 7525, Loss: 1314396821.37019\n",
      "Iteration 7526, Loss: 1075665618.5821137\n",
      "Iteration 7527, Loss: 1331112773.3362308\n",
      "Iteration 7528, Loss: 1114790918.7102923\n",
      "Iteration 7529, Loss: 1104019626.4181302\n",
      "Iteration 7530, Loss: 1341416423.9393551\n",
      "Iteration 7531, Loss: 1110888490.3675785\n",
      "Iteration 7532, Loss: 1307638200.7191312\n",
      "Iteration 7533, Loss: 1301079918.8536487\n",
      "Iteration 7534, Loss: 1380662374.4029644\n",
      "Iteration 7535, Loss: 1127310235.2712896\n",
      "Iteration 7536, Loss: 1179990393.7984574\n",
      "Iteration 7537, Loss: 1240751482.1017709\n",
      "Iteration 7538, Loss: 1288698532.3115535\n",
      "Iteration 7539, Loss: 1278368288.402192\n",
      "Iteration 7540, Loss: 1109518566.4942222\n",
      "Iteration 7541, Loss: 1120991096.345514\n",
      "Iteration 7542, Loss: 2138165547.674932\n",
      "Iteration 7543, Loss: 1206325590.3504448\n",
      "Iteration 7544, Loss: 1220125342.7040236\n",
      "Iteration 7545, Loss: 1254940507.8304617\n",
      "Iteration 7546, Loss: 1252222444.2792041\n",
      "Iteration 7547, Loss: 1247265487.2539413\n",
      "Iteration 7548, Loss: 1162183733.2925334\n",
      "Iteration 7549, Loss: 1299750387.0984669\n",
      "Iteration 7550, Loss: 1305195459.9180655\n",
      "Iteration 7551, Loss: 1244340442.9953377\n",
      "Iteration 7552, Loss: 1356266746.970275\n",
      "Iteration 7553, Loss: 1431540738.4765549\n",
      "Iteration 7554, Loss: 1485662703.37512\n",
      "Iteration 7555, Loss: 1401507800.4488933\n",
      "Iteration 7556, Loss: 1133247302.939797\n",
      "Iteration 7557, Loss: 1080686970.6425202\n",
      "Iteration 7558, Loss: 1063585181.6679181\n",
      "Iteration 7559, Loss: 1048176673.9093021\n",
      "Iteration 7560, Loss: 1054371697.7305522\n",
      "Iteration 7561, Loss: 1296725775.7341952\n",
      "Iteration 7562, Loss: 1121573462.1041927\n",
      "Iteration 7563, Loss: 1174217715.854437\n",
      "Iteration 7564, Loss: 1300809526.7105908\n",
      "Iteration 7565, Loss: 1311567583.9896896\n",
      "Iteration 7566, Loss: 1313439810.152211\n",
      "Iteration 7567, Loss: 1357555076.4005277\n",
      "Iteration 7568, Loss: 1276812616.1560433\n",
      "Iteration 7569, Loss: 1215114107.2334266\n",
      "Iteration 7570, Loss: 1295796747.706052\n",
      "Iteration 7571, Loss: 1272211428.5778527\n",
      "Iteration 7572, Loss: 1085775617.1507099\n",
      "Iteration 7573, Loss: 1384972134.7258313\n",
      "Iteration 7574, Loss: 1362113257.5161932\n",
      "Iteration 7575, Loss: 1280308215.693298\n",
      "Iteration 7576, Loss: 1214632713.7176447\n",
      "Iteration 7577, Loss: 1190796612.8043702\n",
      "Iteration 7578, Loss: 1113390943.1359076\n",
      "Iteration 7579, Loss: 1827114049.724491\n",
      "Iteration 7580, Loss: 1670133324.4037452\n",
      "Iteration 7581, Loss: 1344662811.9183803\n",
      "Iteration 7582, Loss: 1190882520.2067776\n",
      "Iteration 7583, Loss: 1255245448.113232\n",
      "Iteration 7584, Loss: 1193570674.1959767\n",
      "Iteration 7585, Loss: 1183577411.0410397\n",
      "Iteration 7586, Loss: 1240484260.179992\n",
      "Iteration 7587, Loss: 1205070526.3790545\n",
      "Iteration 7588, Loss: 1238957374.524904\n",
      "Iteration 7589, Loss: 1196647152.3885126\n",
      "Iteration 7590, Loss: 1096987533.7787404\n",
      "Iteration 7591, Loss: 1815858462.6449738\n",
      "Iteration 7592, Loss: 1753276565.235218\n",
      "Iteration 7593, Loss: 1080455210.9600456\n",
      "Iteration 7594, Loss: 4560804716.321652\n",
      "Iteration 7595, Loss: 1965145650.9473615\n",
      "Iteration 7596, Loss: 1074408355.4118416\n",
      "Iteration 7597, Loss: 1054766028.5052723\n",
      "Iteration 7598, Loss: 1041019228.1431899\n",
      "Iteration 7599, Loss: 1838393952.232795\n",
      "Iteration 7600, Loss: 1228953132.1253915\n",
      "Iteration 7601, Loss: 1258767062.2108035\n",
      "Iteration 7602, Loss: 1505770322.200606\n",
      "Iteration 7603, Loss: 1322645359.3790457\n",
      "Iteration 7604, Loss: 1226757421.8191378\n",
      "Iteration 7605, Loss: 1289322122.1494155\n",
      "Iteration 7606, Loss: 1274709507.7778447\n",
      "Iteration 7607, Loss: 1178252852.181783\n",
      "Iteration 7608, Loss: 1201403417.7413235\n",
      "Iteration 7609, Loss: 1188342592.2334447\n",
      "Iteration 7610, Loss: 1095127165.5319312\n",
      "Iteration 7611, Loss: 1225971978.954653\n",
      "Iteration 7612, Loss: 1223486218.870441\n",
      "Iteration 7613, Loss: 1277020225.6742828\n",
      "Iteration 7614, Loss: 1188546683.667676\n",
      "Iteration 7615, Loss: 1152044424.5524254\n",
      "Iteration 7616, Loss: 1342493661.4415994\n",
      "Iteration 7617, Loss: 1085510348.9067087\n",
      "Iteration 7618, Loss: 1091278806.2479389\n",
      "Iteration 7619, Loss: 1209572394.4688995\n",
      "Iteration 7620, Loss: 1267555445.4211237\n",
      "Iteration 7621, Loss: 1241310128.4761238\n",
      "Iteration 7622, Loss: 1174640154.8352644\n",
      "Iteration 7623, Loss: 1308285564.6797\n",
      "Iteration 7624, Loss: 1372324811.10265\n",
      "Iteration 7625, Loss: 1091869610.2580318\n",
      "Iteration 7626, Loss: 1138863941.1854625\n",
      "Iteration 7627, Loss: 1247965970.2231364\n",
      "Iteration 7628, Loss: 1175625078.2861073\n",
      "Iteration 7629, Loss: 1133012144.1582475\n",
      "Iteration 7630, Loss: 1155625552.8935618\n",
      "Iteration 7631, Loss: 1140888339.48797\n",
      "Iteration 7632, Loss: 1125517410.00514\n",
      "Iteration 7633, Loss: 1268223814.913315\n",
      "Iteration 7634, Loss: 1304332130.651477\n",
      "Iteration 7635, Loss: 1088243329.3481643\n",
      "Iteration 7636, Loss: 1749974738.1645823\n",
      "Iteration 7637, Loss: 1428750885.75142\n",
      "Iteration 7638, Loss: 1403047293.266563\n",
      "Iteration 7639, Loss: 1050459350.236171\n",
      "Iteration 7640, Loss: 1272888489.2254102\n",
      "Iteration 7641, Loss: 1250875022.2930112\n",
      "Iteration 7642, Loss: 1309085804.2735631\n",
      "Iteration 7643, Loss: 1085743916.1796997\n",
      "Iteration 7644, Loss: 1080894611.5223606\n",
      "Iteration 7645, Loss: 1118111059.8762896\n",
      "Iteration 7646, Loss: 1088039040.930572\n",
      "Iteration 7647, Loss: 1070046777.3969027\n",
      "Iteration 7648, Loss: 1040482072.2991695\n",
      "Iteration 7649, Loss: 1071766515.8778431\n",
      "Iteration 7650, Loss: 1139316093.2333314\n",
      "Iteration 7651, Loss: 1685462277.789808\n",
      "Iteration 7652, Loss: 1046954545.413286\n",
      "Iteration 7653, Loss: 1044440279.0437413\n",
      "Iteration 7654, Loss: 1264967935.9950252\n",
      "Iteration 7655, Loss: 1037951592.2752906\n",
      "Iteration 7656, Loss: 1033802122.7075016\n",
      "Iteration 7657, Loss: 1021774108.142223\n",
      "Iteration 7658, Loss: 1406961364.3403528\n",
      "Iteration 7659, Loss: 1216256867.5040166\n",
      "Iteration 7660, Loss: 1204945421.23058\n",
      "Iteration 7661, Loss: 1280482989.285155\n",
      "Iteration 7662, Loss: 1246576163.9460626\n",
      "Iteration 7663, Loss: 1276232755.427351\n",
      "Iteration 7664, Loss: 1056613543.9606855\n",
      "Iteration 7665, Loss: 1048385771.6584923\n",
      "Iteration 7666, Loss: 1038335649.9161259\n",
      "Iteration 7667, Loss: 1014672255.8621728\n",
      "Iteration 7668, Loss: 1115453707.983368\n",
      "Iteration 7669, Loss: 1131963084.2983108\n",
      "Iteration 7670, Loss: 1108492019.9008834\n",
      "Iteration 7671, Loss: 1182464525.420682\n",
      "Iteration 7672, Loss: 1162445534.184343\n",
      "Iteration 7673, Loss: 1156057695.2107053\n",
      "Iteration 7674, Loss: 1113458731.8801754\n",
      "Iteration 7675, Loss: 1133007845.6698842\n",
      "Iteration 7676, Loss: 1072673679.8591329\n",
      "Iteration 7677, Loss: 1044145034.7793669\n",
      "Iteration 7678, Loss: 1016873448.4033331\n",
      "Iteration 7679, Loss: 962878579.6493562\n",
      "Iteration 7680, Loss: 2704992804.96306\n",
      "Iteration 7681, Loss: 1904194500.4171379\n",
      "Iteration 7682, Loss: 1920092399.7236686\n",
      "Iteration 7683, Loss: 1676967124.6960473\n",
      "Iteration 7684, Loss: 1305110508.2546105\n",
      "Iteration 7685, Loss: 1106851601.9012382\n",
      "Iteration 7686, Loss: 955408073.4124612\n",
      "Iteration 7687, Loss: 968517476.401717\n",
      "Iteration 7688, Loss: 951961838.4829787\n",
      "Iteration 7689, Loss: 1018534955.2809532\n",
      "Iteration 7690, Loss: 1145767991.0865822\n",
      "Iteration 7691, Loss: 1081788435.802203\n",
      "Iteration 7692, Loss: 1131719756.154938\n",
      "Iteration 7693, Loss: 1012005811.3113277\n",
      "Iteration 7694, Loss: 966633084.1154041\n",
      "Iteration 7695, Loss: 1069704281.2645283\n",
      "Iteration 7696, Loss: 1065077614.0156354\n",
      "Iteration 7697, Loss: 1174817754.4072168\n",
      "Iteration 7698, Loss: 1090417658.2279265\n",
      "Iteration 7699, Loss: 1099534048.9766047\n",
      "Iteration 7700, Loss: 1109499627.2546432\n",
      "Iteration 7701, Loss: 1065808533.6425952\n",
      "Iteration 7702, Loss: 1096751646.180225\n",
      "Iteration 7703, Loss: 1050001286.812039\n",
      "Iteration 7704, Loss: 985652132.2745879\n",
      "Iteration 7705, Loss: 938209405.8399718\n",
      "Iteration 7706, Loss: 1992721694.4615545\n",
      "Iteration 7707, Loss: 1736112593.2814536\n",
      "Iteration 7708, Loss: 1564579655.8725393\n",
      "Iteration 7709, Loss: 1063428321.7154138\n",
      "Iteration 7710, Loss: 918838991.8290082\n",
      "Iteration 7711, Loss: 910226961.6547502\n",
      "Iteration 7712, Loss: 903915845.901479\n",
      "Iteration 7713, Loss: 2285396902.637125\n",
      "Iteration 7714, Loss: 1193220526.3847\n",
      "Iteration 7715, Loss: 1138075313.8444724\n",
      "Iteration 7716, Loss: 999000447.5566267\n",
      "Iteration 7717, Loss: 1084468838.018699\n",
      "Iteration 7718, Loss: 1038001843.4789523\n",
      "Iteration 7719, Loss: 962423828.6899663\n",
      "Iteration 7720, Loss: 970897394.8429747\n",
      "Iteration 7721, Loss: 973381076.8913488\n",
      "Iteration 7722, Loss: 951391786.8478745\n",
      "Iteration 7723, Loss: 2594596763.451257\n",
      "Iteration 7724, Loss: 1631745491.1439085\n",
      "Iteration 7725, Loss: 945703324.5456034\n",
      "Iteration 7726, Loss: 942240043.6389099\n",
      "Iteration 7727, Loss: 931094189.0133855\n",
      "Iteration 7728, Loss: 1399874413.3766072\n",
      "Iteration 7729, Loss: 1285428678.3124418\n",
      "Iteration 7730, Loss: 976340313.5670129\n",
      "Iteration 7731, Loss: 2536397090.957173\n",
      "Iteration 7732, Loss: 1153865362.0255613\n",
      "Iteration 7733, Loss: 4323869516.611272\n",
      "Iteration 7734, Loss: 4291838422.5610466\n",
      "Iteration 7735, Loss: 3994966023.95223\n",
      "Iteration 7736, Loss: 3257083215.078525\n",
      "Iteration 7737, Loss: 1014799562.853894\n",
      "Iteration 7738, Loss: 2892478575.5952845\n",
      "Iteration 7739, Loss: 5151694060.165777\n",
      "Iteration 7740, Loss: 1099056154.6127777\n",
      "Iteration 7741, Loss: 1015244504.6590927\n",
      "Iteration 7742, Loss: 5115693531.27652\n",
      "Iteration 7743, Loss: 4308388953.675207\n",
      "Iteration 7744, Loss: 3662807814.4889565\n",
      "Iteration 7745, Loss: 1883079866.3490214\n",
      "Iteration 7746, Loss: 1625588196.980413\n",
      "Iteration 7747, Loss: 1500517436.750261\n",
      "Iteration 7748, Loss: 1145134895.1133244\n",
      "Iteration 7749, Loss: 1122145410.4095483\n",
      "Iteration 7750, Loss: 1096954795.6652615\n",
      "Iteration 7751, Loss: 1168816405.24399\n",
      "Iteration 7752, Loss: 1129963366.4025683\n",
      "Iteration 7753, Loss: 1135237425.1509116\n",
      "Iteration 7754, Loss: 1127647328.9701197\n",
      "Iteration 7755, Loss: 965575938.4490864\n",
      "Iteration 7756, Loss: 957246810.5208981\n",
      "Iteration 7757, Loss: 2870535567.9162936\n",
      "Iteration 7758, Loss: 2869349488.546449\n",
      "Iteration 7759, Loss: 983646672.9459724\n",
      "Iteration 7760, Loss: 967482948.8374857\n",
      "Iteration 7761, Loss: 954569590.2240651\n",
      "Iteration 7762, Loss: 949275694.7223545\n",
      "Iteration 7763, Loss: 937144207.3502917\n",
      "Iteration 7764, Loss: 1239872936.523808\n",
      "Iteration 7765, Loss: 1042398052.0446036\n",
      "Iteration 7766, Loss: 1118533173.8951392\n",
      "Iteration 7767, Loss: 968564910.2246825\n",
      "Iteration 7768, Loss: 974354029.079558\n",
      "Iteration 7769, Loss: 958573538.432587\n",
      "Iteration 7770, Loss: 1384960190.171067\n",
      "Iteration 7771, Loss: 1231772085.935913\n",
      "Iteration 7772, Loss: 1181824853.184425\n",
      "Iteration 7773, Loss: 990041727.5239553\n",
      "Iteration 7774, Loss: 1137564983.82416\n",
      "Iteration 7775, Loss: 987013462.0857959\n",
      "Iteration 7776, Loss: 1015320002.3671745\n",
      "Iteration 7777, Loss: 1537504266.6795392\n",
      "Iteration 7778, Loss: 1403133437.3895502\n",
      "Iteration 7779, Loss: 1141859998.9868975\n",
      "Iteration 7780, Loss: 1106469616.120536\n",
      "Iteration 7781, Loss: 1077060666.2030613\n",
      "Iteration 7782, Loss: 1076511857.6459792\n",
      "Iteration 7783, Loss: 982180271.3249723\n",
      "Iteration 7784, Loss: 1151057472.5632865\n",
      "Iteration 7785, Loss: 1035384850.9265381\n",
      "Iteration 7786, Loss: 1176356241.77627\n",
      "Iteration 7787, Loss: 1137737511.5481615\n",
      "Iteration 7788, Loss: 1578020051.0260549\n",
      "Iteration 7789, Loss: 1600769300.2300608\n",
      "Iteration 7790, Loss: 966338844.8514528\n",
      "Iteration 7791, Loss: 962083270.4194365\n",
      "Iteration 7792, Loss: 977819168.9253484\n",
      "Iteration 7793, Loss: 1001423742.8571444\n",
      "Iteration 7794, Loss: 2145493956.6636984\n",
      "Iteration 7795, Loss: 1423489635.7909288\n",
      "Iteration 7796, Loss: 1133738246.3138895\n",
      "Iteration 7797, Loss: 1005867640.5804443\n",
      "Iteration 7798, Loss: 1028581954.0365707\n",
      "Iteration 7799, Loss: 1274249014.2137094\n",
      "Iteration 7800, Loss: 1120134889.0717406\n",
      "Iteration 7801, Loss: 1120533982.9468782\n",
      "Iteration 7802, Loss: 1093475941.2161717\n",
      "Iteration 7803, Loss: 1096240223.0678835\n",
      "Iteration 7804, Loss: 1080564624.5870903\n",
      "Iteration 7805, Loss: 1053974004.2258005\n",
      "Iteration 7806, Loss: 1222956064.4864616\n",
      "Iteration 7807, Loss: 1171676241.1223102\n",
      "Iteration 7808, Loss: 1216559427.7086673\n",
      "Iteration 7809, Loss: 1042924385.734544\n",
      "Iteration 7810, Loss: 1482499684.198016\n",
      "Iteration 7811, Loss: 1391749931.9637551\n",
      "Iteration 7812, Loss: 1066269283.6687714\n",
      "Iteration 7813, Loss: 1966586114.3617191\n",
      "Iteration 7814, Loss: 1038081946.1752616\n",
      "Iteration 7815, Loss: 1330732270.1063368\n",
      "Iteration 7816, Loss: 1367525033.9537623\n",
      "Iteration 7817, Loss: 1033338126.4555926\n",
      "Iteration 7818, Loss: 1101716163.742949\n",
      "Iteration 7819, Loss: 1393425048.6828065\n",
      "Iteration 7820, Loss: 1350560254.4064264\n",
      "Iteration 7821, Loss: 1218962348.9588504\n",
      "Iteration 7822, Loss: 1295221596.695144\n",
      "Iteration 7823, Loss: 1211671968.0058613\n",
      "Iteration 7824, Loss: 1601017141.9851775\n",
      "Iteration 7825, Loss: 1079036304.8938606\n",
      "Iteration 7826, Loss: 1086966324.2317412\n",
      "Iteration 7827, Loss: 1075041035.4939384\n",
      "Iteration 7828, Loss: 1062754906.8324962\n",
      "Iteration 7829, Loss: 1066771700.1691762\n",
      "Iteration 7830, Loss: 1194048139.2784579\n",
      "Iteration 7831, Loss: 1175689556.009372\n",
      "Iteration 7832, Loss: 1256948494.0892034\n",
      "Iteration 7833, Loss: 1119213645.0766964\n",
      "Iteration 7834, Loss: 1646893005.7037842\n",
      "Iteration 7835, Loss: 1129272860.612272\n",
      "Iteration 7836, Loss: 1242270942.4201872\n",
      "Iteration 7837, Loss: 1336010684.6154602\n",
      "Iteration 7838, Loss: 1095897354.890638\n",
      "Iteration 7839, Loss: 1093587026.1983058\n",
      "Iteration 7840, Loss: 1243269011.0504377\n",
      "Iteration 7841, Loss: 1136737025.9322855\n",
      "Iteration 7842, Loss: 1127647271.1751428\n",
      "Iteration 7843, Loss: 1240858790.4228647\n",
      "Iteration 7844, Loss: 1196986791.724582\n",
      "Iteration 7845, Loss: 1309297820.9458227\n",
      "Iteration 7846, Loss: 1297593605.7607498\n",
      "Iteration 7847, Loss: 1210824648.4773197\n",
      "Iteration 7848, Loss: 1201322424.371574\n",
      "Iteration 7849, Loss: 1637510720.737897\n",
      "Iteration 7850, Loss: 1377981141.1497285\n",
      "Iteration 7851, Loss: 1209545277.047868\n",
      "Iteration 7852, Loss: 1183210619.626397\n",
      "Iteration 7853, Loss: 1076569118.820099\n",
      "Iteration 7854, Loss: 1087112000.7618327\n",
      "Iteration 7855, Loss: 1079917444.6169736\n",
      "Iteration 7856, Loss: 1124572409.6257658\n",
      "Iteration 7857, Loss: 1267768072.4253912\n",
      "Iteration 7858, Loss: 1324218377.901258\n",
      "Iteration 7859, Loss: 1137505089.56432\n",
      "Iteration 7860, Loss: 1277004120.4159446\n",
      "Iteration 7861, Loss: 1347208750.0298307\n",
      "Iteration 7862, Loss: 1372728622.666202\n",
      "Iteration 7863, Loss: 1133106170.532028\n",
      "Iteration 7864, Loss: 2108800025.3055842\n",
      "Iteration 7865, Loss: 1883434439.8942955\n",
      "Iteration 7866, Loss: 2075400501.4244504\n",
      "Iteration 7867, Loss: 1116935901.0439456\n",
      "Iteration 7868, Loss: 1130783772.6220722\n",
      "Iteration 7869, Loss: 1171109195.4671192\n",
      "Iteration 7870, Loss: 1176074198.5448716\n",
      "Iteration 7871, Loss: 1299572397.1674206\n",
      "Iteration 7872, Loss: 1181794663.479724\n",
      "Iteration 7873, Loss: 1878614170.5816343\n",
      "Iteration 7874, Loss: 1679914159.9920707\n",
      "Iteration 7875, Loss: 2673006852.900419\n",
      "Iteration 7876, Loss: 1237316763.0481958\n",
      "Iteration 7877, Loss: 1239974387.0023415\n",
      "Iteration 7878, Loss: 1377498995.3222454\n",
      "Iteration 7879, Loss: 1213615724.9234107\n",
      "Iteration 7880, Loss: 1419688075.733543\n",
      "Iteration 7881, Loss: 1471999375.946191\n",
      "Iteration 7882, Loss: 1539669438.5340116\n",
      "Iteration 7883, Loss: 1532936059.2615502\n",
      "Iteration 7884, Loss: 1582092852.7600691\n",
      "Iteration 7885, Loss: 1195015840.5098593\n",
      "Iteration 7886, Loss: 1225430060.818856\n",
      "Iteration 7887, Loss: 1203363427.7849061\n",
      "Iteration 7888, Loss: 1325741745.091019\n",
      "Iteration 7889, Loss: 1181893144.096511\n",
      "Iteration 7890, Loss: 1188389209.6615324\n",
      "Iteration 7891, Loss: 1319590317.9662278\n",
      "Iteration 7892, Loss: 1324990472.24226\n",
      "Iteration 7893, Loss: 1362653113.6974437\n",
      "Iteration 7894, Loss: 1423167645.909541\n",
      "Iteration 7895, Loss: 1123762696.6717215\n",
      "Iteration 7896, Loss: 1096132442.8258133\n",
      "Iteration 7897, Loss: 1093977788.5791054\n",
      "Iteration 7898, Loss: 1087510902.8487363\n",
      "Iteration 7899, Loss: 1140253647.1120954\n",
      "Iteration 7900, Loss: 1248707167.3973413\n",
      "Iteration 7901, Loss: 1311886978.1883445\n",
      "Iteration 7902, Loss: 1245896436.841498\n",
      "Iteration 7903, Loss: 1151357930.5149918\n",
      "Iteration 7904, Loss: 1152335982.6698062\n",
      "Iteration 7905, Loss: 1168469927.0753827\n",
      "Iteration 7906, Loss: 1189924740.2901049\n",
      "Iteration 7907, Loss: 1240425947.945145\n",
      "Iteration 7908, Loss: 1151477579.6660914\n",
      "Iteration 7909, Loss: 1079850369.678785\n",
      "Iteration 7910, Loss: 1082694952.3645415\n",
      "Iteration 7911, Loss: 1072840916.8477769\n",
      "Iteration 7912, Loss: 1072587180.2859795\n",
      "Iteration 7913, Loss: 2147203480.1649795\n",
      "Iteration 7914, Loss: 2091975619.3883898\n",
      "Iteration 7915, Loss: 2499259121.557353\n",
      "Iteration 7916, Loss: 2055681681.5650034\n",
      "Iteration 7917, Loss: 1475034063.0112174\n",
      "Iteration 7918, Loss: 1523851490.6021616\n",
      "Iteration 7919, Loss: 1135857888.4564035\n",
      "Iteration 7920, Loss: 1113007174.6098695\n",
      "Iteration 7921, Loss: 1146581832.427867\n",
      "Iteration 7922, Loss: 1261618929.7296357\n",
      "Iteration 7923, Loss: 1243295011.2811348\n",
      "Iteration 7924, Loss: 1486727283.5962188\n",
      "Iteration 7925, Loss: 1164559990.1824493\n",
      "Iteration 7926, Loss: 1265556808.4847682\n",
      "Iteration 7927, Loss: 1247324603.8756852\n",
      "Iteration 7928, Loss: 1101091762.6507027\n",
      "Iteration 7929, Loss: 1212260185.3648155\n",
      "Iteration 7930, Loss: 1258630904.0561934\n",
      "Iteration 7931, Loss: 1082567122.5634003\n",
      "Iteration 7932, Loss: 1603748808.1843066\n",
      "Iteration 7933, Loss: 1324261872.8186808\n",
      "Iteration 7934, Loss: 1416702897.8826392\n",
      "Iteration 7935, Loss: 1167905980.857072\n",
      "Iteration 7936, Loss: 1366195283.9569743\n",
      "Iteration 7937, Loss: 1379052655.9247139\n",
      "Iteration 7938, Loss: 1433865669.8684115\n",
      "Iteration 7939, Loss: 1409212501.0235698\n",
      "Iteration 7940, Loss: 1250050460.2042747\n",
      "Iteration 7941, Loss: 1145810213.8500056\n",
      "Iteration 7942, Loss: 1287344051.4014258\n",
      "Iteration 7943, Loss: 1280272298.601774\n",
      "Iteration 7944, Loss: 1497559583.7970064\n",
      "Iteration 7945, Loss: 1393481027.0610163\n",
      "Iteration 7946, Loss: 1134726409.6652427\n",
      "Iteration 7947, Loss: 1508937152.0787716\n",
      "Iteration 7948, Loss: 1371186461.3893764\n",
      "Iteration 7949, Loss: 1449277324.6709971\n",
      "Iteration 7950, Loss: 1439417134.4435012\n",
      "Iteration 7951, Loss: 1168967511.939629\n",
      "Iteration 7952, Loss: 1109481022.545484\n",
      "Iteration 7953, Loss: 1100965133.85723\n",
      "Iteration 7954, Loss: 1095599059.6920178\n",
      "Iteration 7955, Loss: 1099636118.410291\n",
      "Iteration 7956, Loss: 1099486035.5415044\n",
      "Iteration 7957, Loss: 1098952726.0498064\n",
      "Iteration 7958, Loss: 1145764327.3300972\n",
      "Iteration 7959, Loss: 1323507304.4242375\n",
      "Iteration 7960, Loss: 1374723824.9242446\n",
      "Iteration 7961, Loss: 1394697175.794318\n",
      "Iteration 7962, Loss: 1207555794.9753754\n",
      "Iteration 7963, Loss: 1315802608.1750832\n",
      "Iteration 7964, Loss: 1092315895.2659643\n",
      "Iteration 7965, Loss: 1082491408.080337\n",
      "Iteration 7966, Loss: 1339147023.9491627\n",
      "Iteration 7967, Loss: 1117071857.4317515\n",
      "Iteration 7968, Loss: 1195497714.2448823\n",
      "Iteration 7969, Loss: 1323212345.5096045\n",
      "Iteration 7970, Loss: 1164470747.067198\n",
      "Iteration 7971, Loss: 1172753034.0273979\n",
      "Iteration 7972, Loss: 1185199316.4780023\n",
      "Iteration 7973, Loss: 1332236645.1333575\n",
      "Iteration 7974, Loss: 1193467485.9004118\n",
      "Iteration 7975, Loss: 1185582668.917639\n",
      "Iteration 7976, Loss: 1344376605.961445\n",
      "Iteration 7977, Loss: 1153558509.9444287\n",
      "Iteration 7978, Loss: 1194757236.0678613\n",
      "Iteration 7979, Loss: 1192962202.8071144\n",
      "Iteration 7980, Loss: 1286505804.54741\n",
      "Iteration 7981, Loss: 1353963934.8547888\n",
      "Iteration 7982, Loss: 1292459527.9085975\n",
      "Iteration 7983, Loss: 1376881779.0991223\n",
      "Iteration 7984, Loss: 1374869148.7422643\n",
      "Iteration 7985, Loss: 1302182198.3830044\n",
      "Iteration 7986, Loss: 1303313350.8817582\n",
      "Iteration 7987, Loss: 1378490081.342911\n",
      "Iteration 7988, Loss: 1444356778.5204914\n",
      "Iteration 7989, Loss: 1117003919.1099107\n",
      "Iteration 7990, Loss: 1105778264.4472616\n",
      "Iteration 7991, Loss: 1096933685.9317572\n",
      "Iteration 7992, Loss: 1129807802.06594\n",
      "Iteration 7993, Loss: 1115442253.799684\n",
      "Iteration 7994, Loss: 2012818580.84994\n",
      "Iteration 7995, Loss: 1095136631.3021433\n",
      "Iteration 7996, Loss: 2410899613.6410995\n",
      "Iteration 7997, Loss: 2239498163.2771726\n",
      "Iteration 7998, Loss: 3093262134.73903\n",
      "Iteration 7999, Loss: 1474991954.1387453\n",
      "Iteration 8000, Loss: 1884614500.0338562\n",
      "Iteration 8001, Loss: 1439585386.5440269\n",
      "Iteration 8002, Loss: 1194296151.2222419\n",
      "Iteration 8003, Loss: 1233926877.3605735\n",
      "Iteration 8004, Loss: 1226742426.4173806\n",
      "Iteration 8005, Loss: 1774705257.9695926\n",
      "Iteration 8006, Loss: 1570507950.7417612\n",
      "Iteration 8007, Loss: 1561040577.605561\n",
      "Iteration 8008, Loss: 1574748409.1975906\n",
      "Iteration 8009, Loss: 1143426701.8112295\n",
      "Iteration 8010, Loss: 1381852893.1976533\n",
      "Iteration 8011, Loss: 1161442443.2959478\n",
      "Iteration 8012, Loss: 3923607569.832633\n",
      "Iteration 8013, Loss: 3452632828.203766\n",
      "Iteration 8014, Loss: 1350585388.4689183\n",
      "Iteration 8015, Loss: 1177029913.406336\n",
      "Iteration 8016, Loss: 1155659068.609305\n",
      "Iteration 8017, Loss: 1172088690.077254\n",
      "Iteration 8018, Loss: 1164890401.8175566\n",
      "Iteration 8019, Loss: 1213492159.0025396\n",
      "Iteration 8020, Loss: 1231487610.2160287\n",
      "Iteration 8021, Loss: 1258687980.9559157\n",
      "Iteration 8022, Loss: 1385415029.4306743\n",
      "Iteration 8023, Loss: 1383784959.9938881\n",
      "Iteration 8024, Loss: 1452932802.9561815\n",
      "Iteration 8025, Loss: 1458024380.9548984\n",
      "Iteration 8026, Loss: 1485147474.4939408\n",
      "Iteration 8027, Loss: 1200183415.8373005\n",
      "Iteration 8028, Loss: 1318092705.4250255\n",
      "Iteration 8029, Loss: 1351818448.6246076\n",
      "Iteration 8030, Loss: 1314179948.472086\n",
      "Iteration 8031, Loss: 1094026453.0836365\n",
      "Iteration 8032, Loss: 1245765093.5218709\n",
      "Iteration 8033, Loss: 1577846596.0254462\n",
      "Iteration 8034, Loss: 1539109820.2983341\n",
      "Iteration 8035, Loss: 1171244095.7606575\n",
      "Iteration 8036, Loss: 1590910689.5420344\n",
      "Iteration 8037, Loss: 1261952990.766762\n",
      "Iteration 8038, Loss: 1123624039.7146988\n",
      "Iteration 8039, Loss: 1382952379.2078362\n",
      "Iteration 8040, Loss: 1436129646.754968\n",
      "Iteration 8041, Loss: 1221721070.7294743\n",
      "Iteration 8042, Loss: 1410686867.8487754\n",
      "Iteration 8043, Loss: 1318784304.9599895\n",
      "Iteration 8044, Loss: 1143947608.110473\n",
      "Iteration 8045, Loss: 1160748385.13184\n",
      "Iteration 8046, Loss: 2224143911.534176\n",
      "Iteration 8047, Loss: 1261140757.5514324\n",
      "Iteration 8048, Loss: 1196996760.3657155\n",
      "Iteration 8049, Loss: 1272266675.388071\n",
      "Iteration 8050, Loss: 1288800973.0351949\n",
      "Iteration 8051, Loss: 1303490549.9308183\n",
      "Iteration 8052, Loss: 1205637178.527962\n",
      "Iteration 8053, Loss: 1137190852.6601589\n",
      "Iteration 8054, Loss: 1139736594.8138132\n",
      "Iteration 8055, Loss: 1457508576.0806727\n",
      "Iteration 8056, Loss: 1448428172.275096\n",
      "Iteration 8057, Loss: 1201230797.6315408\n",
      "Iteration 8058, Loss: 1241545086.328667\n",
      "Iteration 8059, Loss: 1211631344.0397446\n",
      "Iteration 8060, Loss: 1239615294.2836144\n",
      "Iteration 8061, Loss: 1349086953.834067\n",
      "Iteration 8062, Loss: 1197080214.7406147\n",
      "Iteration 8063, Loss: 1340531876.9536562\n",
      "Iteration 8064, Loss: 1449964162.8942852\n",
      "Iteration 8065, Loss: 1240280379.0289426\n",
      "Iteration 8066, Loss: 1372212955.339125\n",
      "Iteration 8067, Loss: 1295041789.1361399\n",
      "Iteration 8068, Loss: 1235499383.760147\n",
      "Iteration 8069, Loss: 1300719379.8633976\n",
      "Iteration 8070, Loss: 1380519079.5645423\n",
      "Iteration 8071, Loss: 1296389761.97466\n",
      "Iteration 8072, Loss: 1296606462.591386\n",
      "Iteration 8073, Loss: 1337320407.9960544\n",
      "Iteration 8074, Loss: 1324442478.3681304\n",
      "Iteration 8075, Loss: 1122931258.9487793\n",
      "Iteration 8076, Loss: 1137436779.0944214\n",
      "Iteration 8077, Loss: 1136802253.8896875\n",
      "Iteration 8078, Loss: 1135626855.8517234\n",
      "Iteration 8079, Loss: 1123478621.887072\n",
      "Iteration 8080, Loss: 1378993814.2691004\n",
      "Iteration 8081, Loss: 1165812215.2146034\n",
      "Iteration 8082, Loss: 1205402756.7365813\n",
      "Iteration 8083, Loss: 1207464164.8841758\n",
      "Iteration 8084, Loss: 1157035693.95436\n",
      "Iteration 8085, Loss: 1200610380.2358577\n",
      "Iteration 8086, Loss: 1693660495.9034328\n",
      "Iteration 8087, Loss: 1586105456.5518472\n",
      "Iteration 8088, Loss: 1581151153.2945368\n",
      "Iteration 8089, Loss: 1156101861.5257103\n",
      "Iteration 8090, Loss: 1158007503.3728352\n",
      "Iteration 8091, Loss: 1197095774.4098554\n",
      "Iteration 8092, Loss: 1120568440.6837902\n",
      "Iteration 8093, Loss: 1452373501.8221617\n",
      "Iteration 8094, Loss: 1457782463.5593975\n",
      "Iteration 8095, Loss: 1128285804.244624\n",
      "Iteration 8096, Loss: 1519363941.6062844\n",
      "Iteration 8097, Loss: 1211201579.7275805\n",
      "Iteration 8098, Loss: 1145097509.5172653\n",
      "Iteration 8099, Loss: 1255304196.7601464\n",
      "Iteration 8100, Loss: 1173619866.317342\n",
      "Iteration 8101, Loss: 1185251168.0409815\n",
      "Iteration 8102, Loss: 1318915139.5444453\n",
      "Iteration 8103, Loss: 1242121145.5982263\n",
      "Iteration 8104, Loss: 1238086090.4999819\n",
      "Iteration 8105, Loss: 1244597765.392253\n",
      "Iteration 8106, Loss: 1199333621.4791868\n",
      "Iteration 8107, Loss: 1335299292.796732\n",
      "Iteration 8108, Loss: 1120843575.3367584\n",
      "Iteration 8109, Loss: 1133989486.4629252\n",
      "Iteration 8110, Loss: 1302930671.4250894\n",
      "Iteration 8111, Loss: 1227499111.0032346\n",
      "Iteration 8112, Loss: 1344616441.8808482\n",
      "Iteration 8113, Loss: 1323652881.9577982\n",
      "Iteration 8114, Loss: 1390174177.9187744\n",
      "Iteration 8115, Loss: 1432124450.3530328\n",
      "Iteration 8116, Loss: 1213572885.131592\n",
      "Iteration 8117, Loss: 1330167404.880195\n",
      "Iteration 8118, Loss: 1177375983.7142663\n",
      "Iteration 8119, Loss: 1305078819.0533357\n",
      "Iteration 8120, Loss: 1396231580.2042813\n",
      "Iteration 8121, Loss: 1122597493.1195219\n",
      "Iteration 8122, Loss: 1159619549.9712842\n",
      "Iteration 8123, Loss: 1132139103.3752322\n",
      "Iteration 8124, Loss: 1336836183.5513935\n",
      "Iteration 8125, Loss: 1209287365.0688567\n",
      "Iteration 8126, Loss: 1204310883.9336636\n",
      "Iteration 8127, Loss: 1186497080.79328\n",
      "Iteration 8128, Loss: 1240816355.483955\n",
      "Iteration 8129, Loss: 1685100902.6561604\n",
      "Iteration 8130, Loss: 1211427446.4044604\n",
      "Iteration 8131, Loss: 1425606878.0704746\n",
      "Iteration 8132, Loss: 1507771957.2053978\n",
      "Iteration 8133, Loss: 1238691584.2346709\n",
      "Iteration 8134, Loss: 1645384500.951061\n",
      "Iteration 8135, Loss: 1730805918.0705283\n",
      "Iteration 8136, Loss: 1302957766.014419\n",
      "Iteration 8137, Loss: 1314896907.6860733\n",
      "Iteration 8138, Loss: 1167824832.1254923\n",
      "Iteration 8139, Loss: 2507700311.677649\n",
      "Iteration 8140, Loss: 2048847161.7083628\n",
      "Iteration 8141, Loss: 2160125777.325078\n",
      "Iteration 8142, Loss: 1883458896.4685829\n",
      "Iteration 8143, Loss: 1625679315.502837\n",
      "Iteration 8144, Loss: 1207066301.6254408\n",
      "Iteration 8145, Loss: 1205825452.3181596\n",
      "Iteration 8146, Loss: 1448374646.0600188\n",
      "Iteration 8147, Loss: 1512472008.9494336\n",
      "Iteration 8148, Loss: 1277877841.3512347\n",
      "Iteration 8149, Loss: 1275046218.0636501\n",
      "Iteration 8150, Loss: 1283042919.97674\n",
      "Iteration 8151, Loss: 1285230978.3292434\n",
      "Iteration 8152, Loss: 1302959927.6531146\n",
      "Iteration 8153, Loss: 1393334095.305746\n",
      "Iteration 8154, Loss: 1234027351.947598\n",
      "Iteration 8155, Loss: 1241410489.2397995\n",
      "Iteration 8156, Loss: 1252283953.4200883\n",
      "Iteration 8157, Loss: 1230540809.1014962\n",
      "Iteration 8158, Loss: 1386958551.945831\n",
      "Iteration 8159, Loss: 1304675036.433392\n",
      "Iteration 8160, Loss: 1433478483.7468026\n",
      "Iteration 8161, Loss: 1482836183.8318655\n",
      "Iteration 8162, Loss: 1545977306.5956843\n",
      "Iteration 8163, Loss: 1264743606.0639884\n",
      "Iteration 8164, Loss: 1271937812.91355\n",
      "Iteration 8165, Loss: 1217016035.9905446\n",
      "Iteration 8166, Loss: 1271374687.8814275\n",
      "Iteration 8167, Loss: 1382086748.0475302\n",
      "Iteration 8168, Loss: 1161003826.290317\n",
      "Iteration 8169, Loss: 1363080940.4575539\n",
      "Iteration 8170, Loss: 1334146411.0629888\n",
      "Iteration 8171, Loss: 1413676334.1471646\n",
      "Iteration 8172, Loss: 1333087052.4404383\n",
      "Iteration 8173, Loss: 1385612938.5695574\n",
      "Iteration 8174, Loss: 1191481966.0552747\n",
      "Iteration 8175, Loss: 1198708152.8728266\n",
      "Iteration 8176, Loss: 1336303640.8008618\n",
      "Iteration 8177, Loss: 1128669171.6436625\n",
      "Iteration 8178, Loss: 1392991134.9298906\n",
      "Iteration 8179, Loss: 1170466166.6777549\n",
      "Iteration 8180, Loss: 1151232379.0160384\n",
      "Iteration 8181, Loss: 1140500443.6079233\n",
      "Iteration 8182, Loss: 1391391315.982679\n",
      "Iteration 8183, Loss: 1391799441.829443\n",
      "Iteration 8184, Loss: 1159969870.9523323\n",
      "Iteration 8185, Loss: 1195063864.8284442\n",
      "Iteration 8186, Loss: 1200644781.8608174\n",
      "Iteration 8187, Loss: 1232003816.0977235\n",
      "Iteration 8188, Loss: 1191447450.2271175\n",
      "Iteration 8189, Loss: 1196329451.0449595\n",
      "Iteration 8190, Loss: 1327636485.8594928\n",
      "Iteration 8191, Loss: 1133387302.768113\n",
      "Iteration 8192, Loss: 1263348400.4388056\n",
      "Iteration 8193, Loss: 1267997901.716906\n",
      "Iteration 8194, Loss: 1379393346.062522\n",
      "Iteration 8195, Loss: 1367395133.4792926\n",
      "Iteration 8196, Loss: 1109854497.0508194\n",
      "Iteration 8197, Loss: 1160661538.9606857\n",
      "Iteration 8198, Loss: 1413092870.6759014\n",
      "Iteration 8199, Loss: 1174858530.7241771\n",
      "Iteration 8200, Loss: 1124102963.816608\n",
      "Iteration 8201, Loss: 1137944879.7870157\n",
      "Iteration 8202, Loss: 1179062317.0530725\n",
      "Iteration 8203, Loss: 1254881945.3376832\n",
      "Iteration 8204, Loss: 1372850781.3040872\n",
      "Iteration 8205, Loss: 1217125621.9885998\n",
      "Iteration 8206, Loss: 1199519467.9728792\n",
      "Iteration 8207, Loss: 1337065338.9056625\n",
      "Iteration 8208, Loss: 1156456730.6275187\n",
      "Iteration 8209, Loss: 1175553325.969841\n",
      "Iteration 8210, Loss: 1237561336.1002173\n",
      "Iteration 8211, Loss: 1152237298.6057184\n",
      "Iteration 8212, Loss: 1161850066.2053018\n",
      "Iteration 8213, Loss: 1336395749.80095\n",
      "Iteration 8214, Loss: 1382384175.7737207\n",
      "Iteration 8215, Loss: 1347338554.0510464\n",
      "Iteration 8216, Loss: 1130996091.8504112\n",
      "Iteration 8217, Loss: 1372570524.182343\n",
      "Iteration 8218, Loss: 1385589714.5805435\n",
      "Iteration 8219, Loss: 1441070903.6027296\n",
      "Iteration 8220, Loss: 1493484450.8306155\n",
      "Iteration 8221, Loss: 1196710704.3726401\n",
      "Iteration 8222, Loss: 1203020276.763232\n",
      "Iteration 8223, Loss: 1196240214.4313047\n",
      "Iteration 8224, Loss: 1080876882.509241\n",
      "Iteration 8225, Loss: 1088672918.3208573\n",
      "Iteration 8226, Loss: 1135507212.5717442\n",
      "Iteration 8227, Loss: 1192332613.663512\n",
      "Iteration 8228, Loss: 1318010190.8491802\n",
      "Iteration 8229, Loss: 1125476596.7221477\n",
      "Iteration 8230, Loss: 1109578896.1920438\n",
      "Iteration 8231, Loss: 1106823728.5131795\n",
      "Iteration 8232, Loss: 1138032125.2418175\n",
      "Iteration 8233, Loss: 1125182936.939782\n",
      "Iteration 8234, Loss: 1062022751.9125457\n",
      "Iteration 8235, Loss: 1353572989.6690524\n",
      "Iteration 8236, Loss: 1201229251.2830436\n",
      "Iteration 8237, Loss: 1268697986.2852051\n",
      "Iteration 8238, Loss: 1094108079.7327602\n",
      "Iteration 8239, Loss: 1056735694.8656873\n",
      "Iteration 8240, Loss: 1044090865.7749333\n",
      "Iteration 8241, Loss: 1157995826.9267573\n",
      "Iteration 8242, Loss: 1155628181.7932422\n",
      "Iteration 8243, Loss: 1155069925.3958244\n",
      "Iteration 8244, Loss: 1247535406.5746877\n",
      "Iteration 8245, Loss: 1098205099.2744195\n",
      "Iteration 8246, Loss: 1126371706.6019585\n",
      "Iteration 8247, Loss: 1150450564.5478673\n",
      "Iteration 8248, Loss: 1146279627.3507743\n",
      "Iteration 8249, Loss: 1135721011.2228508\n",
      "Iteration 8250, Loss: 1092906419.418101\n",
      "Iteration 8251, Loss: 1209655484.092156\n",
      "Iteration 8252, Loss: 1198112527.16901\n",
      "Iteration 8253, Loss: 1079778715.466227\n",
      "Iteration 8254, Loss: 1072693023.9191415\n",
      "Iteration 8255, Loss: 1115573257.002481\n",
      "Iteration 8256, Loss: 1096537836.6169264\n",
      "Iteration 8257, Loss: 1845245808.051514\n",
      "Iteration 8258, Loss: 1700061688.1288204\n",
      "Iteration 8259, Loss: 1489652766.3222795\n",
      "Iteration 8260, Loss: 1452781769.1698778\n",
      "Iteration 8261, Loss: 1386910327.0698385\n",
      "Iteration 8262, Loss: 1115353862.923711\n",
      "Iteration 8263, Loss: 1122066246.00129\n",
      "Iteration 8264, Loss: 1198141504.8208034\n",
      "Iteration 8265, Loss: 1167177245.1350698\n",
      "Iteration 8266, Loss: 1086845775.703789\n",
      "Iteration 8267, Loss: 1165503507.2857137\n",
      "Iteration 8268, Loss: 1235131896.1273015\n",
      "Iteration 8269, Loss: 1239812982.24407\n",
      "Iteration 8270, Loss: 1164282668.7615974\n",
      "Iteration 8271, Loss: 1132446911.182198\n",
      "Iteration 8272, Loss: 1151586766.3907766\n",
      "Iteration 8273, Loss: 1171921573.8769672\n",
      "Iteration 8274, Loss: 972068019.2131541\n",
      "Iteration 8275, Loss: 971024656.3496121\n",
      "Iteration 8276, Loss: 2195636303.1847405\n",
      "Iteration 8277, Loss: 991297763.7194835\n",
      "Iteration 8278, Loss: 1064480465.3639635\n",
      "Iteration 8279, Loss: 1030095550.2806761\n",
      "Iteration 8280, Loss: 1014260579.9293512\n",
      "Iteration 8281, Loss: 1026610620.2069416\n",
      "Iteration 8282, Loss: 1004302609.0265727\n",
      "Iteration 8283, Loss: 1028334654.8495333\n",
      "Iteration 8284, Loss: 1249301663.7043796\n",
      "Iteration 8285, Loss: 1020476572.2919853\n",
      "Iteration 8286, Loss: 1130329069.015741\n",
      "Iteration 8287, Loss: 1074396375.2670128\n",
      "Iteration 8288, Loss: 1053186461.040911\n",
      "Iteration 8289, Loss: 1033005767.3410378\n",
      "Iteration 8290, Loss: 1940679819.5817845\n",
      "Iteration 8291, Loss: 1160049041.3765004\n",
      "Iteration 8292, Loss: 1237246924.8796654\n",
      "Iteration 8293, Loss: 1163707579.7232633\n",
      "Iteration 8294, Loss: 1194448580.8787832\n",
      "Iteration 8295, Loss: 1153986855.9597156\n",
      "Iteration 8296, Loss: 1175872300.2951868\n",
      "Iteration 8297, Loss: 1094390072.5637825\n",
      "Iteration 8298, Loss: 1171694475.6216543\n",
      "Iteration 8299, Loss: 1079448353.4543316\n",
      "Iteration 8300, Loss: 1108359115.3910863\n",
      "Iteration 8301, Loss: 1090938311.875632\n",
      "Iteration 8302, Loss: 1276693011.6785288\n",
      "Iteration 8303, Loss: 1167874221.9391234\n",
      "Iteration 8304, Loss: 1145520697.7489614\n",
      "Iteration 8305, Loss: 1238127848.5191855\n",
      "Iteration 8306, Loss: 1265235413.3300037\n",
      "Iteration 8307, Loss: 1062393671.731932\n",
      "Iteration 8308, Loss: 1160749408.7187822\n",
      "Iteration 8309, Loss: 1137377405.1518316\n",
      "Iteration 8310, Loss: 1002702677.802694\n",
      "Iteration 8311, Loss: 2383326729.7676034\n",
      "Iteration 8312, Loss: 2140366570.8105953\n",
      "Iteration 8313, Loss: 2592557509.888518\n",
      "Iteration 8314, Loss: 2340096916.944158\n",
      "Iteration 8315, Loss: 2950795373.755743\n",
      "Iteration 8316, Loss: 2792885012.073471\n",
      "Iteration 8317, Loss: 2397482364.757718\n",
      "Iteration 8318, Loss: 2129717663.3929172\n",
      "Iteration 8319, Loss: 1099591145.1474566\n",
      "Iteration 8320, Loss: 1316269135.427567\n",
      "Iteration 8321, Loss: 1299922194.1203973\n",
      "Iteration 8322, Loss: 1255302713.6731992\n",
      "Iteration 8323, Loss: 1023902094.6825448\n",
      "Iteration 8324, Loss: 1019050520.7756383\n",
      "Iteration 8325, Loss: 1149634002.750976\n",
      "Iteration 8326, Loss: 1162479231.2732563\n",
      "Iteration 8327, Loss: 1118740234.9147213\n",
      "Iteration 8328, Loss: 1209521433.0600142\n",
      "Iteration 8329, Loss: 1049688467.3799658\n",
      "Iteration 8330, Loss: 1153161240.3337538\n",
      "Iteration 8331, Loss: 1129686382.2576709\n",
      "Iteration 8332, Loss: 1176970094.4580038\n",
      "Iteration 8333, Loss: 1025463461.9237896\n",
      "Iteration 8334, Loss: 2817650607.7956176\n",
      "Iteration 8335, Loss: 2073680769.6025524\n",
      "Iteration 8336, Loss: 1890316111.1024728\n",
      "Iteration 8337, Loss: 1721759897.1552951\n",
      "Iteration 8338, Loss: 2868431099.9440303\n",
      "Iteration 8339, Loss: 2918079908.9356303\n",
      "Iteration 8340, Loss: 1910965410.24011\n",
      "Iteration 8341, Loss: 1329576964.4199238\n",
      "Iteration 8342, Loss: 1263293984.9837058\n",
      "Iteration 8343, Loss: 1236003718.917211\n",
      "Iteration 8344, Loss: 1062123981.7736888\n",
      "Iteration 8345, Loss: 1170540847.1090443\n",
      "Iteration 8346, Loss: 1184259062.6387217\n",
      "Iteration 8347, Loss: 1229215593.4577625\n",
      "Iteration 8348, Loss: 1160593619.7148576\n",
      "Iteration 8349, Loss: 1116284775.3335738\n",
      "Iteration 8350, Loss: 1147218860.5572007\n",
      "Iteration 8351, Loss: 1229536196.053758\n",
      "Iteration 8352, Loss: 1299040644.3439133\n",
      "Iteration 8353, Loss: 1293036454.2929416\n",
      "Iteration 8354, Loss: 1299929940.063745\n",
      "Iteration 8355, Loss: 1083394283.6375575\n",
      "Iteration 8356, Loss: 1188767785.0987587\n",
      "Iteration 8357, Loss: 1177820596.6157942\n",
      "Iteration 8358, Loss: 1205937015.1173081\n",
      "Iteration 8359, Loss: 1189604711.8516495\n",
      "Iteration 8360, Loss: 1594383232.5211225\n",
      "Iteration 8361, Loss: 1484026874.1347108\n",
      "Iteration 8362, Loss: 1199323423.3283603\n",
      "Iteration 8363, Loss: 1202780324.912777\n",
      "Iteration 8364, Loss: 1187317365.2026465\n",
      "Iteration 8365, Loss: 1254864242.2653286\n",
      "Iteration 8366, Loss: 1233579777.1178966\n",
      "Iteration 8367, Loss: 1228204178.234062\n",
      "Iteration 8368, Loss: 1145286055.0721495\n",
      "Iteration 8369, Loss: 1463638819.5384204\n",
      "Iteration 8370, Loss: 1193676310.8623486\n",
      "Iteration 8371, Loss: 1192796579.8051102\n",
      "Iteration 8372, Loss: 1306915018.8240278\n",
      "Iteration 8373, Loss: 1337303911.1129162\n",
      "Iteration 8374, Loss: 1116600224.9290876\n",
      "Iteration 8375, Loss: 1121596558.498334\n",
      "Iteration 8376, Loss: 1498446299.7679772\n",
      "Iteration 8377, Loss: 1474378441.5281436\n",
      "Iteration 8378, Loss: 1272610057.4632504\n",
      "Iteration 8379, Loss: 1263767734.579817\n",
      "Iteration 8380, Loss: 1342223167.659283\n",
      "Iteration 8381, Loss: 1361774012.5783985\n",
      "Iteration 8382, Loss: 1367833332.8258195\n",
      "Iteration 8383, Loss: 1337481855.608621\n",
      "Iteration 8384, Loss: 1369085254.8440835\n",
      "Iteration 8385, Loss: 1061817955.784853\n",
      "Iteration 8386, Loss: 1051709830.4053963\n",
      "Iteration 8387, Loss: 1296118618.358041\n",
      "Iteration 8388, Loss: 1170844537.239512\n",
      "Iteration 8389, Loss: 1166341133.0524778\n",
      "Iteration 8390, Loss: 1165408180.54535\n",
      "Iteration 8391, Loss: 1266445572.0242827\n",
      "Iteration 8392, Loss: 1245987308.1341105\n",
      "Iteration 8393, Loss: 1153182154.8908398\n",
      "Iteration 8394, Loss: 1140524079.0521731\n",
      "Iteration 8395, Loss: 1135204160.6500874\n",
      "Iteration 8396, Loss: 1147668944.8696458\n",
      "Iteration 8397, Loss: 1158855264.3874278\n",
      "Iteration 8398, Loss: 1283461997.691376\n",
      "Iteration 8399, Loss: 1146903945.8402858\n",
      "Iteration 8400, Loss: 1232410104.4925148\n",
      "Iteration 8401, Loss: 1211428916.306478\n",
      "Iteration 8402, Loss: 1222394513.878599\n",
      "Iteration 8403, Loss: 1234507450.7826679\n",
      "Iteration 8404, Loss: 1280089392.5417128\n",
      "Iteration 8405, Loss: 1255635554.0130553\n",
      "Iteration 8406, Loss: 1119172624.1749396\n",
      "Iteration 8407, Loss: 1149745419.1076438\n",
      "Iteration 8408, Loss: 1164930134.9481652\n",
      "Iteration 8409, Loss: 1126389817.651029\n",
      "Iteration 8410, Loss: 1853751668.9294589\n",
      "Iteration 8411, Loss: 1109528653.6426551\n",
      "Iteration 8412, Loss: 1383624503.957694\n",
      "Iteration 8413, Loss: 1361625970.3250425\n",
      "Iteration 8414, Loss: 1336602402.560676\n",
      "Iteration 8415, Loss: 1122766493.94183\n",
      "Iteration 8416, Loss: 1043461454.0242542\n",
      "Iteration 8417, Loss: 1300422767.6702213\n",
      "Iteration 8418, Loss: 1096182797.057727\n",
      "Iteration 8419, Loss: 1093222423.256172\n",
      "Iteration 8420, Loss: 1200851884.2829995\n",
      "Iteration 8421, Loss: 1229688819.9748757\n",
      "Iteration 8422, Loss: 1144063655.4061391\n",
      "Iteration 8423, Loss: 1168288600.6941779\n",
      "Iteration 8424, Loss: 1250887422.6275823\n",
      "Iteration 8425, Loss: 1265572378.8163629\n",
      "Iteration 8426, Loss: 1280879417.6680624\n",
      "Iteration 8427, Loss: 1038603662.3428757\n",
      "Iteration 8428, Loss: 2271229258.968687\n",
      "Iteration 8429, Loss: 1993016055.649373\n",
      "Iteration 8430, Loss: 1883064099.2837667\n",
      "Iteration 8431, Loss: 1373687092.6322181\n",
      "Iteration 8432, Loss: 1287245448.7627823\n",
      "Iteration 8433, Loss: 1203043621.226144\n",
      "Iteration 8434, Loss: 1130601927.3061595\n",
      "Iteration 8435, Loss: 1231794235.2788367\n",
      "Iteration 8436, Loss: 1081540426.2054799\n",
      "Iteration 8437, Loss: 1195284373.8009057\n",
      "Iteration 8438, Loss: 1150982864.173502\n",
      "Iteration 8439, Loss: 1284783545.3158076\n",
      "Iteration 8440, Loss: 1099029755.511738\n",
      "Iteration 8441, Loss: 1251524560.828325\n",
      "Iteration 8442, Loss: 1292499190.6225483\n",
      "Iteration 8443, Loss: 1154538570.9154181\n",
      "Iteration 8444, Loss: 1145111440.3330421\n",
      "Iteration 8445, Loss: 1195514284.1352706\n",
      "Iteration 8446, Loss: 1192326188.3893802\n",
      "Iteration 8447, Loss: 1162103960.0745707\n",
      "Iteration 8448, Loss: 1163398391.4552257\n",
      "Iteration 8449, Loss: 1218289311.538865\n",
      "Iteration 8450, Loss: 1222252439.919131\n",
      "Iteration 8451, Loss: 1293497435.5205984\n",
      "Iteration 8452, Loss: 1333230178.6943567\n",
      "Iteration 8453, Loss: 1384931470.411789\n",
      "Iteration 8454, Loss: 1413768194.801448\n",
      "Iteration 8455, Loss: 1373802725.376741\n",
      "Iteration 8456, Loss: 1126219549.965654\n",
      "Iteration 8457, Loss: 1098764830.4075477\n",
      "Iteration 8458, Loss: 1095011664.972245\n",
      "Iteration 8459, Loss: 1146770942.223383\n",
      "Iteration 8460, Loss: 1392923162.8618178\n",
      "Iteration 8461, Loss: 1421300876.8053305\n",
      "Iteration 8462, Loss: 1151336078.636009\n",
      "Iteration 8463, Loss: 1249592607.0002856\n",
      "Iteration 8464, Loss: 1244871131.6977046\n",
      "Iteration 8465, Loss: 1304565759.9708707\n",
      "Iteration 8466, Loss: 1351347009.4239457\n",
      "Iteration 8467, Loss: 1128225533.9229422\n",
      "Iteration 8468, Loss: 1121406037.4875557\n",
      "Iteration 8469, Loss: 1408653432.6415346\n",
      "Iteration 8470, Loss: 1393841482.1060064\n",
      "Iteration 8471, Loss: 1395246602.291423\n",
      "Iteration 8472, Loss: 1379611201.5330198\n",
      "Iteration 8473, Loss: 1168081615.3836436\n",
      "Iteration 8474, Loss: 1520381746.1179054\n",
      "Iteration 8475, Loss: 1528647230.4771302\n",
      "Iteration 8476, Loss: 1197874371.8029907\n",
      "Iteration 8477, Loss: 1527414128.1167948\n",
      "Iteration 8478, Loss: 1521484514.6741493\n",
      "Iteration 8479, Loss: 1245224003.1104367\n",
      "Iteration 8480, Loss: 1239973804.3684568\n",
      "Iteration 8481, Loss: 1245498384.0782554\n",
      "Iteration 8482, Loss: 1103686715.979614\n",
      "Iteration 8483, Loss: 1296395890.4888148\n",
      "Iteration 8484, Loss: 1299043640.2858348\n",
      "Iteration 8485, Loss: 1301361367.932411\n",
      "Iteration 8486, Loss: 1347228296.793647\n",
      "Iteration 8487, Loss: 1138247361.9675188\n",
      "Iteration 8488, Loss: 1279208766.1603825\n",
      "Iteration 8489, Loss: 1126393656.724148\n",
      "Iteration 8490, Loss: 1128205347.1819024\n",
      "Iteration 8491, Loss: 1188953958.6084688\n",
      "Iteration 8492, Loss: 1371505216.239051\n",
      "Iteration 8493, Loss: 1377300662.9539957\n",
      "Iteration 8494, Loss: 1233321454.9668684\n",
      "Iteration 8495, Loss: 1210604804.6069217\n",
      "Iteration 8496, Loss: 1277662010.270515\n",
      "Iteration 8497, Loss: 1318975401.69974\n",
      "Iteration 8498, Loss: 1277439191.0146017\n",
      "Iteration 8499, Loss: 1241948442.7654078\n",
      "Iteration 8500, Loss: 1272646354.1472778\n",
      "Iteration 8501, Loss: 1334490436.3060808\n",
      "Iteration 8502, Loss: 1387197023.7232225\n",
      "Iteration 8503, Loss: 1111750826.6791809\n",
      "Iteration 8504, Loss: 1072711491.6695304\n",
      "Iteration 8505, Loss: 1091949065.186714\n",
      "Iteration 8506, Loss: 1083197819.0696032\n",
      "Iteration 8507, Loss: 1220253280.2295942\n",
      "Iteration 8508, Loss: 1214449809.7944489\n",
      "Iteration 8509, Loss: 1245020358.6539495\n",
      "Iteration 8510, Loss: 1124673367.1901748\n",
      "Iteration 8511, Loss: 1088367498.2388349\n",
      "Iteration 8512, Loss: 1073864262.0033169\n",
      "Iteration 8513, Loss: 1143597248.801544\n",
      "Iteration 8514, Loss: 1164931067.4216495\n",
      "Iteration 8515, Loss: 1182109594.1773865\n",
      "Iteration 8516, Loss: 1201876190.1940768\n",
      "Iteration 8517, Loss: 1186572985.282251\n",
      "Iteration 8518, Loss: 1053132208.7030779\n",
      "Iteration 8519, Loss: 1152948370.1280494\n",
      "Iteration 8520, Loss: 1140614628.2000074\n",
      "Iteration 8521, Loss: 1114043649.0261455\n",
      "Iteration 8522, Loss: 1109171843.2528603\n",
      "Iteration 8523, Loss: 1083140881.6600907\n",
      "Iteration 8524, Loss: 1058037373.8886966\n",
      "Iteration 8525, Loss: 1033866508.3519459\n",
      "Iteration 8526, Loss: 1017286546.9930222\n",
      "Iteration 8527, Loss: 1181451234.2474358\n",
      "Iteration 8528, Loss: 1181179718.6402512\n",
      "Iteration 8529, Loss: 947315776.0003496\n",
      "Iteration 8530, Loss: 940477122.3571463\n",
      "Iteration 8531, Loss: 942764203.0266283\n",
      "Iteration 8532, Loss: 927651098.4066232\n",
      "Iteration 8533, Loss: 924418053.8121426\n",
      "Iteration 8534, Loss: 911269580.9651401\n",
      "Iteration 8535, Loss: 1336279393.3831124\n",
      "Iteration 8536, Loss: 1230760280.970845\n",
      "Iteration 8537, Loss: 1155507142.691597\n",
      "Iteration 8538, Loss: 1016283928.8814197\n",
      "Iteration 8539, Loss: 1048807856.3674682\n",
      "Iteration 8540, Loss: 1652283753.9029646\n",
      "Iteration 8541, Loss: 965543013.9088842\n",
      "Iteration 8542, Loss: 1370457683.2554777\n",
      "Iteration 8543, Loss: 1018595880.657418\n",
      "Iteration 8544, Loss: 965631071.8916845\n",
      "Iteration 8545, Loss: 959855668.6842695\n",
      "Iteration 8546, Loss: 3148113530.2417946\n",
      "Iteration 8547, Loss: 1732000716.0442884\n",
      "Iteration 8548, Loss: 1059478837.1592497\n",
      "Iteration 8549, Loss: 976186632.6114763\n",
      "Iteration 8550, Loss: 980537003.4127448\n",
      "Iteration 8551, Loss: 1229593326.1109483\n",
      "Iteration 8552, Loss: 1219927340.0131729\n",
      "Iteration 8553, Loss: 982767945.7982594\n",
      "Iteration 8554, Loss: 969637513.51273\n",
      "Iteration 8555, Loss: 962893187.9528776\n",
      "Iteration 8556, Loss: 2873458469.307289\n",
      "Iteration 8557, Loss: 2422898018.348215\n",
      "Iteration 8558, Loss: 1456740158.045138\n",
      "Iteration 8559, Loss: 1407622399.820321\n",
      "Iteration 8560, Loss: 1219088151.189447\n",
      "Iteration 8561, Loss: 991408270.7059544\n",
      "Iteration 8562, Loss: 1648710633.5992353\n",
      "Iteration 8563, Loss: 1001382041.6193498\n",
      "Iteration 8564, Loss: 1085051214.196028\n",
      "Iteration 8565, Loss: 1100595567.6632607\n",
      "Iteration 8566, Loss: 1263838758.841196\n",
      "Iteration 8567, Loss: 1288738744.7092352\n",
      "Iteration 8568, Loss: 1196506438.7804809\n",
      "Iteration 8569, Loss: 1235946849.4246264\n",
      "Iteration 8570, Loss: 1212285345.8165586\n",
      "Iteration 8571, Loss: 1075723720.6394362\n",
      "Iteration 8572, Loss: 1659762852.7039218\n",
      "Iteration 8573, Loss: 1130300301.0224726\n",
      "Iteration 8574, Loss: 1113573295.1355977\n",
      "Iteration 8575, Loss: 1153055036.3499901\n",
      "Iteration 8576, Loss: 1043494956.0402297\n",
      "Iteration 8577, Loss: 1049827521.3811891\n",
      "Iteration 8578, Loss: 1139885118.7783148\n",
      "Iteration 8579, Loss: 1161153785.0023227\n",
      "Iteration 8580, Loss: 1127759452.5102932\n",
      "Iteration 8581, Loss: 1098081416.377975\n",
      "Iteration 8582, Loss: 1170369598.856701\n",
      "Iteration 8583, Loss: 1179891099.829346\n",
      "Iteration 8584, Loss: 1141453094.0017147\n",
      "Iteration 8585, Loss: 1075834082.1152964\n",
      "Iteration 8586, Loss: 1169609626.0989332\n",
      "Iteration 8587, Loss: 1202251722.4887397\n",
      "Iteration 8588, Loss: 1172618863.83566\n",
      "Iteration 8589, Loss: 1154245640.9213812\n",
      "Iteration 8590, Loss: 1554181465.5070531\n",
      "Iteration 8591, Loss: 1118985607.5255346\n",
      "Iteration 8592, Loss: 1091894907.3974733\n",
      "Iteration 8593, Loss: 1114653473.8440654\n",
      "Iteration 8594, Loss: 1732353206.39104\n",
      "Iteration 8595, Loss: 1053052539.5209208\n",
      "Iteration 8596, Loss: 1063799920.8190274\n",
      "Iteration 8597, Loss: 1051866238.9844218\n",
      "Iteration 8598, Loss: 1214229901.383124\n",
      "Iteration 8599, Loss: 1164376255.858059\n",
      "Iteration 8600, Loss: 995422869.9465636\n",
      "Iteration 8601, Loss: 983934937.5702796\n",
      "Iteration 8602, Loss: 970276830.4514552\n",
      "Iteration 8603, Loss: 1146138564.3398864\n",
      "Iteration 8604, Loss: 1111136440.511254\n",
      "Iteration 8605, Loss: 1080799883.9980567\n",
      "Iteration 8606, Loss: 988542724.0680884\n",
      "Iteration 8607, Loss: 976777322.4715378\n",
      "Iteration 8608, Loss: 968994037.9554483\n",
      "Iteration 8609, Loss: 963591402.7040645\n",
      "Iteration 8610, Loss: 951827101.6898029\n",
      "Iteration 8611, Loss: 946125626.5718739\n",
      "Iteration 8612, Loss: 932645418.2999756\n",
      "Iteration 8613, Loss: 1338988359.2026265\n",
      "Iteration 8614, Loss: 1241125899.8105567\n",
      "Iteration 8615, Loss: 1027278657.8443968\n",
      "Iteration 8616, Loss: 984584687.9352707\n",
      "Iteration 8617, Loss: 1088579773.299268\n",
      "Iteration 8618, Loss: 1066282753.0348291\n",
      "Iteration 8619, Loss: 1806588736.8227208\n",
      "Iteration 8620, Loss: 969736911.4789842\n",
      "Iteration 8621, Loss: 1150840338.3261104\n",
      "Iteration 8622, Loss: 1117300039.2170224\n",
      "Iteration 8623, Loss: 1041524989.3130972\n",
      "Iteration 8624, Loss: 1020721378.2302862\n",
      "Iteration 8625, Loss: 1028757230.4749345\n",
      "Iteration 8626, Loss: 1097869356.2846022\n",
      "Iteration 8627, Loss: 980531242.8985072\n",
      "Iteration 8628, Loss: 1085835412.2132022\n",
      "Iteration 8629, Loss: 1021273813.1422309\n",
      "Iteration 8630, Loss: 1104234907.783839\n",
      "Iteration 8631, Loss: 948485009.7320197\n",
      "Iteration 8632, Loss: 938848196.1577469\n",
      "Iteration 8633, Loss: 928062396.7181042\n",
      "Iteration 8634, Loss: 921793575.1322646\n",
      "Iteration 8635, Loss: 3756320441.622556\n",
      "Iteration 8636, Loss: 3125531316.479144\n",
      "Iteration 8637, Loss: 2353212940.7342167\n",
      "Iteration 8638, Loss: 2084784271.4677057\n",
      "Iteration 8639, Loss: 1544877746.0237775\n",
      "Iteration 8640, Loss: 958006648.4734055\n",
      "Iteration 8641, Loss: 1026225478.2229652\n",
      "Iteration 8642, Loss: 998093101.9019339\n",
      "Iteration 8643, Loss: 953285064.9353739\n",
      "Iteration 8644, Loss: 1025081822.8903613\n",
      "Iteration 8645, Loss: 991742727.3400025\n",
      "Iteration 8646, Loss: 2028758162.4809902\n",
      "Iteration 8647, Loss: 937406173.2476752\n",
      "Iteration 8648, Loss: 937329402.6052047\n",
      "Iteration 8649, Loss: 1085693871.4554937\n",
      "Iteration 8650, Loss: 1690235285.2146509\n",
      "Iteration 8651, Loss: 1164847441.6020956\n",
      "Iteration 8652, Loss: 981120687.4763894\n",
      "Iteration 8653, Loss: 954349277.3883198\n",
      "Iteration 8654, Loss: 944853948.146312\n",
      "Iteration 8655, Loss: 918798171.0163962\n",
      "Iteration 8656, Loss: 915796532.9954466\n",
      "Iteration 8657, Loss: 978433770.9358804\n",
      "Iteration 8658, Loss: 968835399.3376014\n",
      "Iteration 8659, Loss: 1160967906.9286532\n",
      "Iteration 8660, Loss: 1149333582.028719\n",
      "Iteration 8661, Loss: 1067216287.239451\n",
      "Iteration 8662, Loss: 1151904452.3838413\n",
      "Iteration 8663, Loss: 1014016636.0269729\n",
      "Iteration 8664, Loss: 952098578.8508776\n",
      "Iteration 8665, Loss: 1184060184.227167\n",
      "Iteration 8666, Loss: 1129133438.5977368\n",
      "Iteration 8667, Loss: 1011188290.7321693\n",
      "Iteration 8668, Loss: 1079549906.5352948\n",
      "Iteration 8669, Loss: 1027305225.0585449\n",
      "Iteration 8670, Loss: 1320560354.804027\n",
      "Iteration 8671, Loss: 1230812520.5596986\n",
      "Iteration 8672, Loss: 1181423115.7738051\n",
      "Iteration 8673, Loss: 1156084998.9616992\n",
      "Iteration 8674, Loss: 1109635066.9393728\n",
      "Iteration 8675, Loss: 1059511839.31257\n",
      "Iteration 8676, Loss: 1009073848.4834272\n",
      "Iteration 8677, Loss: 991596162.6069982\n",
      "Iteration 8678, Loss: 1115326219.7316272\n",
      "Iteration 8679, Loss: 1098474208.075108\n",
      "Iteration 8680, Loss: 965647055.2720226\n",
      "Iteration 8681, Loss: 1234757834.8978088\n",
      "Iteration 8682, Loss: 1145452592.388223\n",
      "Iteration 8683, Loss: 1193969812.611794\n",
      "Iteration 8684, Loss: 1127812164.1134737\n",
      "Iteration 8685, Loss: 1021282120.3760245\n",
      "Iteration 8686, Loss: 1218804480.566005\n",
      "Iteration 8687, Loss: 1356177491.7723453\n",
      "Iteration 8688, Loss: 1276197984.1278002\n",
      "Iteration 8689, Loss: 1192547876.32347\n",
      "Iteration 8690, Loss: 1156983801.412169\n",
      "Iteration 8691, Loss: 1017843422.2158909\n",
      "Iteration 8692, Loss: 1076948317.3166578\n",
      "Iteration 8693, Loss: 1085739732.7599537\n",
      "Iteration 8694, Loss: 1027239716.8091166\n",
      "Iteration 8695, Loss: 957984906.1443467\n",
      "Iteration 8696, Loss: 977561568.3199735\n",
      "Iteration 8697, Loss: 1156885704.523277\n",
      "Iteration 8698, Loss: 1147774098.6879172\n",
      "Iteration 8699, Loss: 1172509224.9816887\n",
      "Iteration 8700, Loss: 1105370485.1076453\n",
      "Iteration 8701, Loss: 1033983053.5442032\n",
      "Iteration 8702, Loss: 932085578.4249007\n",
      "Iteration 8703, Loss: 920942231.4605625\n",
      "Iteration 8704, Loss: 926487206.3349769\n",
      "Iteration 8705, Loss: 911885727.3786852\n",
      "Iteration 8706, Loss: 921436169.3291749\n",
      "Iteration 8707, Loss: 902358752.0417249\n",
      "Iteration 8708, Loss: 920106683.551569\n",
      "Iteration 8709, Loss: 898862858.9872423\n",
      "Iteration 8710, Loss: 1320651371.2278087\n",
      "Iteration 8711, Loss: 942424513.0858985\n",
      "Iteration 8712, Loss: 949123815.0361657\n",
      "Iteration 8713, Loss: 910888472.0603386\n",
      "Iteration 8714, Loss: 892919472.3406892\n",
      "Iteration 8715, Loss: 1035512756.270255\n",
      "Iteration 8716, Loss: 911925125.4869639\n",
      "Iteration 8717, Loss: 917913838.8567466\n",
      "Iteration 8718, Loss: 974653596.4321743\n",
      "Iteration 8719, Loss: 996056032.8986906\n",
      "Iteration 8720, Loss: 1027057305.3183659\n",
      "Iteration 8721, Loss: 1101565051.488163\n",
      "Iteration 8722, Loss: 978853007.432167\n",
      "Iteration 8723, Loss: 920980926.853476\n",
      "Iteration 8724, Loss: 894454207.1638942\n",
      "Iteration 8725, Loss: 1015402643.4249078\n",
      "Iteration 8726, Loss: 1003060200.0685582\n",
      "Iteration 8727, Loss: 895434600.2697817\n",
      "Iteration 8728, Loss: 878953566.0060611\n",
      "Iteration 8729, Loss: 933421649.7934524\n",
      "Iteration 8730, Loss: 1056790744.2705569\n",
      "Iteration 8731, Loss: 929305849.6082156\n",
      "Iteration 8732, Loss: 933777889.8569446\n",
      "Iteration 8733, Loss: 1215292671.7543664\n",
      "Iteration 8734, Loss: 1032390413.0403572\n",
      "Iteration 8735, Loss: 1132500396.9946346\n",
      "Iteration 8736, Loss: 891781118.4334196\n",
      "Iteration 8737, Loss: 881035377.9069757\n",
      "Iteration 8738, Loss: 1202578204.2608552\n",
      "Iteration 8739, Loss: 1151568159.1461234\n",
      "Iteration 8740, Loss: 950455642.9373356\n",
      "Iteration 8741, Loss: 919825735.6918435\n",
      "Iteration 8742, Loss: 1091814458.0545738\n",
      "Iteration 8743, Loss: 1041711324.2750729\n",
      "Iteration 8744, Loss: 1532050461.0468118\n",
      "Iteration 8745, Loss: 961718012.0750344\n",
      "Iteration 8746, Loss: 1076160247.3989904\n",
      "Iteration 8747, Loss: 1021496625.3214985\n",
      "Iteration 8748, Loss: 895998601.5147545\n",
      "Iteration 8749, Loss: 942421542.1954728\n",
      "Iteration 8750, Loss: 2460784495.088402\n",
      "Iteration 8751, Loss: 2274455552.476261\n",
      "Iteration 8752, Loss: 2208545330.7703815\n",
      "Iteration 8753, Loss: 2529261638.46596\n",
      "Iteration 8754, Loss: 1428843530.4330673\n",
      "Iteration 8755, Loss: 1303198444.8911684\n",
      "Iteration 8756, Loss: 981371910.6684623\n",
      "Iteration 8757, Loss: 1240238109.2519464\n",
      "Iteration 8758, Loss: 1005361308.1860962\n",
      "Iteration 8759, Loss: 975531449.9642531\n",
      "Iteration 8760, Loss: 1368708509.5893369\n",
      "Iteration 8761, Loss: 933743795.2393342\n",
      "Iteration 8762, Loss: 918954315.2712128\n",
      "Iteration 8763, Loss: 928812207.4611259\n",
      "Iteration 8764, Loss: 947525460.4622322\n",
      "Iteration 8765, Loss: 1040464190.3136295\n",
      "Iteration 8766, Loss: 1162370779.031423\n",
      "Iteration 8767, Loss: 983985387.4868292\n",
      "Iteration 8768, Loss: 1095846301.0740323\n",
      "Iteration 8769, Loss: 1074419317.0353537\n",
      "Iteration 8770, Loss: 1045835827.3918852\n",
      "Iteration 8771, Loss: 1011853451.203227\n",
      "Iteration 8772, Loss: 1063015283.7250913\n",
      "Iteration 8773, Loss: 1101155635.881712\n",
      "Iteration 8774, Loss: 1118249004.0968523\n",
      "Iteration 8775, Loss: 955131391.2785438\n",
      "Iteration 8776, Loss: 1129828396.5114143\n",
      "Iteration 8777, Loss: 1050678183.128808\n",
      "Iteration 8778, Loss: 1059116948.3768446\n",
      "Iteration 8779, Loss: 1015714231.6822948\n",
      "Iteration 8780, Loss: 983383877.8265642\n",
      "Iteration 8781, Loss: 1035309179.0062565\n",
      "Iteration 8782, Loss: 1296784536.0122085\n",
      "Iteration 8783, Loss: 1211034034.3320556\n",
      "Iteration 8784, Loss: 975219784.7742721\n",
      "Iteration 8785, Loss: 1152178101.532101\n",
      "Iteration 8786, Loss: 1002418311.2320698\n",
      "Iteration 8787, Loss: 1314807372.60537\n",
      "Iteration 8788, Loss: 1175752572.3830934\n",
      "Iteration 8789, Loss: 1187986159.5343416\n",
      "Iteration 8790, Loss: 1146898048.2456508\n",
      "Iteration 8791, Loss: 1150651074.493174\n",
      "Iteration 8792, Loss: 927804022.3358812\n",
      "Iteration 8793, Loss: 977217110.7680627\n",
      "Iteration 8794, Loss: 1061488353.0291864\n",
      "Iteration 8795, Loss: 1067803300.1075652\n",
      "Iteration 8796, Loss: 938825272.3081479\n",
      "Iteration 8797, Loss: 925539669.5451612\n",
      "Iteration 8798, Loss: 3716406334.2955127\n",
      "Iteration 8799, Loss: 987362003.4103961\n",
      "Iteration 8800, Loss: 1070287375.8746517\n",
      "Iteration 8801, Loss: 1038576757.0481998\n",
      "Iteration 8802, Loss: 1099766969.381172\n",
      "Iteration 8803, Loss: 1111212775.0233254\n",
      "Iteration 8804, Loss: 1048179641.5117623\n",
      "Iteration 8805, Loss: 1019681543.3306466\n",
      "Iteration 8806, Loss: 1084422380.3018816\n",
      "Iteration 8807, Loss: 996179782.1622527\n",
      "Iteration 8808, Loss: 969478970.6841098\n",
      "Iteration 8809, Loss: 1425511253.9053617\n",
      "Iteration 8810, Loss: 1011450435.9342085\n",
      "Iteration 8811, Loss: 1563031040.0399432\n",
      "Iteration 8812, Loss: 1018946648.5784335\n",
      "Iteration 8813, Loss: 1279912481.5293863\n",
      "Iteration 8814, Loss: 1266657805.0626705\n",
      "Iteration 8815, Loss: 1161944483.9658628\n",
      "Iteration 8816, Loss: 1536526866.5141282\n",
      "Iteration 8817, Loss: 1122239636.0617013\n",
      "Iteration 8818, Loss: 1153008462.3477159\n",
      "Iteration 8819, Loss: 1595731182.2976544\n",
      "Iteration 8820, Loss: 1008796036.8817364\n",
      "Iteration 8821, Loss: 1011410145.8495182\n",
      "Iteration 8822, Loss: 1307328957.2006428\n",
      "Iteration 8823, Loss: 1292902429.8709986\n",
      "Iteration 8824, Loss: 1265849058.7277758\n",
      "Iteration 8825, Loss: 1175213718.4010348\n",
      "Iteration 8826, Loss: 1125234706.4013314\n",
      "Iteration 8827, Loss: 1106132998.0125093\n",
      "Iteration 8828, Loss: 1142775063.9752157\n",
      "Iteration 8829, Loss: 1203550893.624594\n",
      "Iteration 8830, Loss: 1162023440.40895\n",
      "Iteration 8831, Loss: 1084370888.8479674\n",
      "Iteration 8832, Loss: 1104673140.5157576\n",
      "Iteration 8833, Loss: 1184396907.8077378\n",
      "Iteration 8834, Loss: 1172415375.3228872\n",
      "Iteration 8835, Loss: 1078141868.1186206\n",
      "Iteration 8836, Loss: 1092975526.0173483\n",
      "Iteration 8837, Loss: 1060694046.1154451\n",
      "Iteration 8838, Loss: 1471699798.4525614\n",
      "Iteration 8839, Loss: 1425139783.32171\n",
      "Iteration 8840, Loss: 1363509403.270403\n",
      "Iteration 8841, Loss: 1088467212.8806863\n",
      "Iteration 8842, Loss: 1038060652.7866722\n",
      "Iteration 8843, Loss: 1022568204.4159721\n",
      "Iteration 8844, Loss: 2472622121.2548614\n",
      "Iteration 8845, Loss: 2565881940.1393547\n",
      "Iteration 8846, Loss: 1032679275.1517369\n",
      "Iteration 8847, Loss: 1071606563.1409873\n",
      "Iteration 8848, Loss: 1546899961.4093652\n",
      "Iteration 8849, Loss: 1540945636.7855928\n",
      "Iteration 8850, Loss: 1079891009.9779563\n",
      "Iteration 8851, Loss: 1066483876.781498\n",
      "Iteration 8852, Loss: 1059079835.8456856\n",
      "Iteration 8853, Loss: 1052722730.92442\n",
      "Iteration 8854, Loss: 1179706600.7513573\n",
      "Iteration 8855, Loss: 1111147587.8645864\n",
      "Iteration 8856, Loss: 1245592044.0067668\n",
      "Iteration 8857, Loss: 1054401491.577279\n",
      "Iteration 8858, Loss: 1076848302.827729\n",
      "Iteration 8859, Loss: 1038663396.4060438\n",
      "Iteration 8860, Loss: 1745752162.9180992\n",
      "Iteration 8861, Loss: 1206291498.0197961\n",
      "Iteration 8862, Loss: 1192067937.2122471\n",
      "Iteration 8863, Loss: 1625563545.7831457\n",
      "Iteration 8864, Loss: 1104596539.8336167\n",
      "Iteration 8865, Loss: 1418617870.9928677\n",
      "Iteration 8866, Loss: 1177177934.965347\n",
      "Iteration 8867, Loss: 1221003333.7570274\n",
      "Iteration 8868, Loss: 1088251336.8895447\n",
      "Iteration 8869, Loss: 1623719574.1071913\n",
      "Iteration 8870, Loss: 1644559931.2259579\n",
      "Iteration 8871, Loss: 1617550243.171678\n",
      "Iteration 8872, Loss: 1586482123.4278114\n",
      "Iteration 8873, Loss: 1343636776.3837345\n",
      "Iteration 8874, Loss: 1222185884.2407804\n",
      "Iteration 8875, Loss: 1257366531.7976167\n",
      "Iteration 8876, Loss: 1125809088.3470325\n",
      "Iteration 8877, Loss: 1055499442.053315\n",
      "Iteration 8878, Loss: 2047676046.0460513\n",
      "Iteration 8879, Loss: 1983786152.8527734\n",
      "Iteration 8880, Loss: 1834656007.2999246\n",
      "Iteration 8881, Loss: 1642947432.3237991\n",
      "Iteration 8882, Loss: 1599677162.6414018\n",
      "Iteration 8883, Loss: 1271100646.796904\n",
      "Iteration 8884, Loss: 1306407386.772269\n",
      "Iteration 8885, Loss: 1269780839.409986\n",
      "Iteration 8886, Loss: 1268972410.2635136\n",
      "Iteration 8887, Loss: 1259054964.0787497\n",
      "Iteration 8888, Loss: 1007968428.3843298\n",
      "Iteration 8889, Loss: 2117047216.4322307\n",
      "Iteration 8890, Loss: 2016642644.1422791\n",
      "Iteration 8891, Loss: 1480284160.1977327\n",
      "Iteration 8892, Loss: 1429179547.3438234\n",
      "Iteration 8893, Loss: 1157629489.5411866\n",
      "Iteration 8894, Loss: 1019651355.4867103\n",
      "Iteration 8895, Loss: 1196429810.4373329\n",
      "Iteration 8896, Loss: 1175121849.3641646\n",
      "Iteration 8897, Loss: 1128267411.1642113\n",
      "Iteration 8898, Loss: 1173581994.881393\n",
      "Iteration 8899, Loss: 1200802594.8382576\n",
      "Iteration 8900, Loss: 1120495550.7167637\n",
      "Iteration 8901, Loss: 1018870115.8908689\n",
      "Iteration 8902, Loss: 1013671086.5347917\n",
      "Iteration 8903, Loss: 1030166348.6070074\n",
      "Iteration 8904, Loss: 1064093229.1168319\n",
      "Iteration 8905, Loss: 1238215651.2255213\n",
      "Iteration 8906, Loss: 1067546864.3967522\n",
      "Iteration 8907, Loss: 1178277839.018163\n",
      "Iteration 8908, Loss: 1092014079.1443152\n",
      "Iteration 8909, Loss: 1148011840.2387414\n",
      "Iteration 8910, Loss: 1167139783.3686242\n",
      "Iteration 8911, Loss: 1178564929.5824056\n",
      "Iteration 8912, Loss: 1142942853.8330724\n",
      "Iteration 8913, Loss: 1123054925.7596598\n",
      "Iteration 8914, Loss: 1221201862.1753113\n",
      "Iteration 8915, Loss: 1447794604.1348217\n",
      "Iteration 8916, Loss: 1091449373.55624\n",
      "Iteration 8917, Loss: 1044794955.3585556\n",
      "Iteration 8918, Loss: 4162765516.3054113\n",
      "Iteration 8919, Loss: 4162186426.6771793\n",
      "Iteration 8920, Loss: 3678878976.240203\n",
      "Iteration 8921, Loss: 3678866409.400094\n",
      "Iteration 8922, Loss: 1211445574.3730733\n",
      "Iteration 8923, Loss: 1022746738.6486942\n",
      "Iteration 8924, Loss: 1002684551.6105646\n",
      "Iteration 8925, Loss: 984407070.4092304\n",
      "Iteration 8926, Loss: 1157901144.6651154\n",
      "Iteration 8927, Loss: 1179068335.5440087\n",
      "Iteration 8928, Loss: 1520109481.5927002\n",
      "Iteration 8929, Loss: 1265032731.7278385\n",
      "Iteration 8930, Loss: 1006179626.14073\n",
      "Iteration 8931, Loss: 1007248620.7972211\n",
      "Iteration 8932, Loss: 1153412942.7200978\n",
      "Iteration 8933, Loss: 1088981221.0659332\n",
      "Iteration 8934, Loss: 1119296585.972891\n",
      "Iteration 8935, Loss: 1094984168.3688796\n",
      "Iteration 8936, Loss: 1253930140.569162\n",
      "Iteration 8937, Loss: 1216845474.7812483\n",
      "Iteration 8938, Loss: 1013037311.454071\n",
      "Iteration 8939, Loss: 1139474450.799202\n",
      "Iteration 8940, Loss: 1149740975.6089065\n",
      "Iteration 8941, Loss: 1094142915.4473977\n",
      "Iteration 8942, Loss: 986029211.6574906\n",
      "Iteration 8943, Loss: 1142419826.5306478\n",
      "Iteration 8944, Loss: 1143517486.339464\n",
      "Iteration 8945, Loss: 1120032204.4962318\n",
      "Iteration 8946, Loss: 1097396711.8293428\n",
      "Iteration 8947, Loss: 1074424139.3091552\n",
      "Iteration 8948, Loss: 1131562400.7311728\n",
      "Iteration 8949, Loss: 1232059390.0350206\n",
      "Iteration 8950, Loss: 1073087156.1882334\n",
      "Iteration 8951, Loss: 994130952.4340849\n",
      "Iteration 8952, Loss: 1117330876.9964452\n",
      "Iteration 8953, Loss: 1185982591.6702518\n",
      "Iteration 8954, Loss: 1154375594.3138504\n",
      "Iteration 8955, Loss: 1225865482.7505963\n",
      "Iteration 8956, Loss: 1037184105.2953277\n",
      "Iteration 8957, Loss: 2136229980.226037\n",
      "Iteration 8958, Loss: 1296834538.0186393\n",
      "Iteration 8959, Loss: 1065061179.4694228\n",
      "Iteration 8960, Loss: 1089871803.0722866\n",
      "Iteration 8961, Loss: 1262708904.836139\n",
      "Iteration 8962, Loss: 1270073854.7069345\n",
      "Iteration 8963, Loss: 1240062861.6086218\n",
      "Iteration 8964, Loss: 1239160427.6587968\n",
      "Iteration 8965, Loss: 1217858436.1121762\n",
      "Iteration 8966, Loss: 1172904157.9265604\n",
      "Iteration 8967, Loss: 1212370915.914699\n",
      "Iteration 8968, Loss: 1159940507.3477297\n",
      "Iteration 8969, Loss: 1074773256.562904\n",
      "Iteration 8970, Loss: 1040164379.2600113\n",
      "Iteration 8971, Loss: 1167839844.8820229\n",
      "Iteration 8972, Loss: 1217905706.783704\n",
      "Iteration 8973, Loss: 1241963124.4230409\n",
      "Iteration 8974, Loss: 1004550350.0429453\n",
      "Iteration 8975, Loss: 1004179593.7614115\n",
      "Iteration 8976, Loss: 996618671.9424766\n",
      "Iteration 8977, Loss: 1119515205.6901436\n",
      "Iteration 8978, Loss: 1182736197.3747883\n",
      "Iteration 8979, Loss: 1191548182.5408096\n",
      "Iteration 8980, Loss: 1155467788.7777016\n",
      "Iteration 8981, Loss: 1154958350.9531882\n",
      "Iteration 8982, Loss: 1171377808.4495466\n",
      "Iteration 8983, Loss: 1174476857.27475\n",
      "Iteration 8984, Loss: 1140140418.8852932\n",
      "Iteration 8985, Loss: 1066713572.9485848\n",
      "Iteration 8986, Loss: 1046025864.2459395\n",
      "Iteration 8987, Loss: 1878053319.6705086\n",
      "Iteration 8988, Loss: 1028073962.2464231\n",
      "Iteration 8989, Loss: 1033843021.7268739\n",
      "Iteration 8990, Loss: 1130266627.2462482\n",
      "Iteration 8991, Loss: 1124623541.0620139\n",
      "Iteration 8992, Loss: 1114569955.9100764\n",
      "Iteration 8993, Loss: 1133602842.37141\n",
      "Iteration 8994, Loss: 1119261516.9878628\n",
      "Iteration 8995, Loss: 1091408376.1465786\n",
      "Iteration 8996, Loss: 1043435590.5444802\n",
      "Iteration 8997, Loss: 1271746587.3095462\n",
      "Iteration 8998, Loss: 1209495681.350139\n",
      "Iteration 8999, Loss: 1061818765.7676247\n",
      "Iteration 9000, Loss: 1035210318.692772\n",
      "Iteration 9001, Loss: 989174791.3063618\n",
      "Iteration 9002, Loss: 1026369665.2917376\n",
      "Iteration 9003, Loss: 990408257.3730822\n",
      "Iteration 9004, Loss: 962999482.2686864\n",
      "Iteration 9005, Loss: 946249284.2139999\n",
      "Iteration 9006, Loss: 934663400.1217266\n",
      "Iteration 9007, Loss: 921992263.7164313\n",
      "Iteration 9008, Loss: 918104630.3097286\n",
      "Iteration 9009, Loss: 1168263727.4450436\n",
      "Iteration 9010, Loss: 1114098527.3829494\n",
      "Iteration 9011, Loss: 1116220125.6883478\n",
      "Iteration 9012, Loss: 1040688423.56396\n",
      "Iteration 9013, Loss: 1000033323.9167765\n",
      "Iteration 9014, Loss: 970221586.5822644\n",
      "Iteration 9015, Loss: 957146126.2025139\n",
      "Iteration 9016, Loss: 898412570.4869965\n",
      "Iteration 9017, Loss: 2322051417.03972\n",
      "Iteration 9018, Loss: 1960260089.3066614\n",
      "Iteration 9019, Loss: 917922914.5960555\n",
      "Iteration 9020, Loss: 1144168613.80603\n",
      "Iteration 9021, Loss: 1098031110.184296\n",
      "Iteration 9022, Loss: 1024091464.6754575\n",
      "Iteration 9023, Loss: 939787459.4558154\n",
      "Iteration 9024, Loss: 944247496.737727\n",
      "Iteration 9025, Loss: 964428020.421926\n",
      "Iteration 9026, Loss: 2195229543.115882\n",
      "Iteration 9027, Loss: 995126443.7026949\n",
      "Iteration 9028, Loss: 1446208231.8068416\n",
      "Iteration 9029, Loss: 1075599844.4757419\n",
      "Iteration 9030, Loss: 1089945900.1608155\n",
      "Iteration 9031, Loss: 1053955923.5693552\n",
      "Iteration 9032, Loss: 1048883953.1048428\n",
      "Iteration 9033, Loss: 1740193320.296243\n",
      "Iteration 9034, Loss: 1016078738.7383857\n",
      "Iteration 9035, Loss: 1462149681.9279218\n",
      "Iteration 9036, Loss: 1049998484.9209183\n",
      "Iteration 9037, Loss: 1335464365.2070236\n",
      "Iteration 9038, Loss: 1286288833.7528775\n",
      "Iteration 9039, Loss: 1038456464.5753791\n",
      "Iteration 9040, Loss: 1025542254.9793122\n",
      "Iteration 9041, Loss: 2785868716.6505837\n",
      "Iteration 9042, Loss: 2383799010.6303225\n",
      "Iteration 9043, Loss: 1076727600.6086643\n",
      "Iteration 9044, Loss: 1053566404.0626341\n",
      "Iteration 9045, Loss: 1044414422.6387877\n",
      "Iteration 9046, Loss: 1047675801.2976991\n",
      "Iteration 9047, Loss: 1485180607.4805965\n",
      "Iteration 9048, Loss: 1049089011.155351\n",
      "Iteration 9049, Loss: 1277199131.1777864\n",
      "Iteration 9050, Loss: 1158568737.8145494\n",
      "Iteration 9051, Loss: 1249072031.4940896\n",
      "Iteration 9052, Loss: 1221646104.3179457\n",
      "Iteration 9053, Loss: 1175097023.9921207\n",
      "Iteration 9054, Loss: 1296853736.935723\n",
      "Iteration 9055, Loss: 1333891276.124322\n",
      "Iteration 9056, Loss: 1301059536.683666\n",
      "Iteration 9057, Loss: 1020678655.6485059\n",
      "Iteration 9058, Loss: 1115558510.0577967\n",
      "Iteration 9059, Loss: 1045545141.4463599\n",
      "Iteration 9060, Loss: 2149694852.2397666\n",
      "Iteration 9061, Loss: 1116614520.001091\n",
      "Iteration 9062, Loss: 1036085518.2918097\n",
      "Iteration 9063, Loss: 1571934083.068018\n",
      "Iteration 9064, Loss: 1126644370.7746606\n",
      "Iteration 9065, Loss: 1338830972.9534326\n",
      "Iteration 9066, Loss: 1345904836.0805447\n",
      "Iteration 9067, Loss: 1348861211.1626484\n",
      "Iteration 9068, Loss: 1339656103.7811885\n",
      "Iteration 9069, Loss: 1285358180.4207513\n",
      "Iteration 9070, Loss: 1270388190.0931509\n",
      "Iteration 9071, Loss: 1187767516.8711376\n",
      "Iteration 9072, Loss: 1140157676.8990252\n",
      "Iteration 9073, Loss: 1180165685.0744681\n",
      "Iteration 9074, Loss: 1171002923.145162\n",
      "Iteration 9075, Loss: 1166367926.620618\n",
      "Iteration 9076, Loss: 1150643187.7056506\n",
      "Iteration 9077, Loss: 1140522847.336188\n",
      "Iteration 9078, Loss: 1186285633.2747176\n",
      "Iteration 9079, Loss: 1225010001.3043396\n",
      "Iteration 9080, Loss: 1106518524.3740697\n",
      "Iteration 9081, Loss: 1037618258.7830565\n",
      "Iteration 9082, Loss: 1173224536.2418208\n",
      "Iteration 9083, Loss: 1227861220.9327633\n",
      "Iteration 9084, Loss: 1108123806.3367279\n",
      "Iteration 9085, Loss: 1127083886.2453814\n",
      "Iteration 9086, Loss: 1094428637.6728868\n",
      "Iteration 9087, Loss: 1061602515.6180403\n",
      "Iteration 9088, Loss: 1032332155.7279061\n",
      "Iteration 9089, Loss: 1249868647.4021919\n",
      "Iteration 9090, Loss: 1250080786.0718193\n",
      "Iteration 9091, Loss: 1074343491.6558402\n",
      "Iteration 9092, Loss: 1999086710.8859656\n",
      "Iteration 9093, Loss: 1866628184.4149857\n",
      "Iteration 9094, Loss: 1748410280.7543807\n",
      "Iteration 9095, Loss: 1529676326.4141805\n",
      "Iteration 9096, Loss: 1034484976.7193037\n",
      "Iteration 9097, Loss: 1134098514.7771492\n",
      "Iteration 9098, Loss: 1155575427.334381\n",
      "Iteration 9099, Loss: 1170299656.958623\n",
      "Iteration 9100, Loss: 1221372404.6879017\n",
      "Iteration 9101, Loss: 1061847948.4897237\n",
      "Iteration 9102, Loss: 1032156209.1627514\n",
      "Iteration 9103, Loss: 1021046809.7816509\n",
      "Iteration 9104, Loss: 1131309969.5924866\n",
      "Iteration 9105, Loss: 1128042782.1060722\n",
      "Iteration 9106, Loss: 1088663282.6668255\n",
      "Iteration 9107, Loss: 1158324199.3334153\n",
      "Iteration 9108, Loss: 1150234562.8172572\n",
      "Iteration 9109, Loss: 1550926225.1079295\n",
      "Iteration 9110, Loss: 1109213409.2487497\n",
      "Iteration 9111, Loss: 1049161873.509664\n",
      "Iteration 9112, Loss: 1027800612.8142754\n",
      "Iteration 9113, Loss: 1012580132.219892\n",
      "Iteration 9114, Loss: 995751174.2219434\n",
      "Iteration 9115, Loss: 968569223.8859534\n",
      "Iteration 9116, Loss: 950669643.8377216\n",
      "Iteration 9117, Loss: 939127967.9969907\n",
      "Iteration 9118, Loss: 928836424.3494507\n",
      "Iteration 9119, Loss: 925519947.1134727\n",
      "Iteration 9120, Loss: 936984577.8461919\n",
      "Iteration 9121, Loss: 2498808738.7267294\n",
      "Iteration 9122, Loss: 2309044497.0792136\n",
      "Iteration 9123, Loss: 2290544412.7455783\n",
      "Iteration 9124, Loss: 2267139606.576451\n",
      "Iteration 9125, Loss: 1453187229.9019926\n",
      "Iteration 9126, Loss: 970567506.6965735\n",
      "Iteration 9127, Loss: 943643104.7135217\n",
      "Iteration 9128, Loss: 909980585.9213219\n",
      "Iteration 9129, Loss: 893628432.4898207\n",
      "Iteration 9130, Loss: 1360852615.2431035\n",
      "Iteration 9131, Loss: 1163364656.625385\n",
      "Iteration 9132, Loss: 1415095779.6078439\n",
      "Iteration 9133, Loss: 918714844.6875556\n",
      "Iteration 9134, Loss: 905426887.0867982\n",
      "Iteration 9135, Loss: 897383365.0995002\n",
      "Iteration 9136, Loss: 887957323.0527687\n",
      "Iteration 9137, Loss: 936333838.8864828\n",
      "Iteration 9138, Loss: 896135064.3506173\n",
      "Iteration 9139, Loss: 911393625.7309103\n",
      "Iteration 9140, Loss: 888056688.9744331\n",
      "Iteration 9141, Loss: 895435225.3049495\n",
      "Iteration 9142, Loss: 874348297.5558873\n",
      "Iteration 9143, Loss: 1319127383.9224036\n",
      "Iteration 9144, Loss: 936749759.0198758\n",
      "Iteration 9145, Loss: 995716029.9348466\n",
      "Iteration 9146, Loss: 1031521548.9889915\n",
      "Iteration 9147, Loss: 988540147.8645315\n",
      "Iteration 9148, Loss: 939957448.8810744\n",
      "Iteration 9149, Loss: 903889576.1043501\n",
      "Iteration 9150, Loss: 1075595745.3707514\n",
      "Iteration 9151, Loss: 962040155.2756383\n",
      "Iteration 9152, Loss: 924660361.355853\n",
      "Iteration 9153, Loss: 874149178.7668748\n",
      "Iteration 9154, Loss: 879364621.5858239\n",
      "Iteration 9155, Loss: 931813285.604372\n",
      "Iteration 9156, Loss: 978882426.8590078\n",
      "Iteration 9157, Loss: 1034201597.8464376\n",
      "Iteration 9158, Loss: 997311239.1408701\n",
      "Iteration 9159, Loss: 1020362091.0396566\n",
      "Iteration 9160, Loss: 962011864.7102034\n",
      "Iteration 9161, Loss: 888783508.5207872\n",
      "Iteration 9162, Loss: 864983691.5482894\n",
      "Iteration 9163, Loss: 2760462042.697767\n",
      "Iteration 9164, Loss: 1398193229.03532\n",
      "Iteration 9165, Loss: 1381445935.3432338\n",
      "Iteration 9166, Loss: 863464782.8984735\n",
      "Iteration 9167, Loss: 1151625683.0304348\n",
      "Iteration 9168, Loss: 1042948584.359291\n",
      "Iteration 9169, Loss: 968100152.7305195\n",
      "Iteration 9170, Loss: 1444701617.7043912\n",
      "Iteration 9171, Loss: 956219983.304689\n",
      "Iteration 9172, Loss: 1018275097.1218628\n",
      "Iteration 9173, Loss: 991361601.9042882\n",
      "Iteration 9174, Loss: 1028373808.0983665\n",
      "Iteration 9175, Loss: 940101252.6871065\n",
      "Iteration 9176, Loss: 1074172009.5011353\n",
      "Iteration 9177, Loss: 1018979815.397621\n",
      "Iteration 9178, Loss: 1006015435.3850986\n",
      "Iteration 9179, Loss: 1025056184.873539\n",
      "Iteration 9180, Loss: 1007693606.9345452\n",
      "Iteration 9181, Loss: 887582543.9959929\n",
      "Iteration 9182, Loss: 861296794.2554116\n",
      "Iteration 9183, Loss: 853978403.58879\n",
      "Iteration 9184, Loss: 850464593.5496714\n",
      "Iteration 9185, Loss: 1049022116.7003618\n",
      "Iteration 9186, Loss: 989257237.9377246\n",
      "Iteration 9187, Loss: 1834709199.6492317\n",
      "Iteration 9188, Loss: 961325131.3308027\n",
      "Iteration 9189, Loss: 986240112.8608341\n",
      "Iteration 9190, Loss: 966282364.1392434\n",
      "Iteration 9191, Loss: 983822601.9927565\n",
      "Iteration 9192, Loss: 947156279.5448292\n",
      "Iteration 9193, Loss: 925788745.1352148\n",
      "Iteration 9194, Loss: 887332990.1478288\n",
      "Iteration 9195, Loss: 1068933768.9797424\n",
      "Iteration 9196, Loss: 1075710579.0159407\n",
      "Iteration 9197, Loss: 893559769.5521259\n",
      "Iteration 9198, Loss: 862833246.8875148\n",
      "Iteration 9199, Loss: 847893285.0372406\n",
      "Iteration 9200, Loss: 843615820.1277508\n",
      "Iteration 9201, Loss: 2221705886.4415917\n",
      "Iteration 9202, Loss: 1364546064.9711134\n",
      "Iteration 9203, Loss: 870646980.3199391\n",
      "Iteration 9204, Loss: 2851381272.1956077\n",
      "Iteration 9205, Loss: 1908953556.0394337\n",
      "Iteration 9206, Loss: 1824654939.7173278\n",
      "Iteration 9207, Loss: 1755164678.2394\n",
      "Iteration 9208, Loss: 1244270679.9915977\n",
      "Iteration 9209, Loss: 1724591715.5662045\n",
      "Iteration 9210, Loss: 978244129.0040003\n",
      "Iteration 9211, Loss: 1409599479.324037\n",
      "Iteration 9212, Loss: 909481315.2146842\n",
      "Iteration 9213, Loss: 881083305.2950737\n",
      "Iteration 9214, Loss: 878963317.5567573\n",
      "Iteration 9215, Loss: 881679093.0670651\n",
      "Iteration 9216, Loss: 866970079.056333\n",
      "Iteration 9217, Loss: 2100690628.3050964\n",
      "Iteration 9218, Loss: 1349092675.4992018\n",
      "Iteration 9219, Loss: 1087775585.7694886\n",
      "Iteration 9220, Loss: 921191581.9198226\n",
      "Iteration 9221, Loss: 876284725.8443043\n",
      "Iteration 9222, Loss: 876719182.2877185\n",
      "Iteration 9223, Loss: 922113379.5754775\n",
      "Iteration 9224, Loss: 1430934617.5194366\n",
      "Iteration 9225, Loss: 892078688.1599525\n",
      "Iteration 9226, Loss: 875313734.780631\n",
      "Iteration 9227, Loss: 863350708.2467197\n",
      "Iteration 9228, Loss: 860356636.9618244\n",
      "Iteration 9229, Loss: 848336228.9089203\n",
      "Iteration 9230, Loss: 3254147592.528247\n",
      "Iteration 9231, Loss: 2299593274.8208413\n",
      "Iteration 9232, Loss: 2521907668.3919764\n",
      "Iteration 9233, Loss: 4187813786.994052\n",
      "Iteration 9234, Loss: 907985082.4720649\n",
      "Iteration 9235, Loss: 1144217036.506347\n",
      "Iteration 9236, Loss: 1145222804.5671277\n",
      "Iteration 9237, Loss: 1116772703.4267735\n",
      "Iteration 9238, Loss: 1057528055.576592\n",
      "Iteration 9239, Loss: 1472569109.4702718\n",
      "Iteration 9240, Loss: 935172458.0902109\n",
      "Iteration 9241, Loss: 1121601851.5426476\n",
      "Iteration 9242, Loss: 1070637140.393278\n",
      "Iteration 9243, Loss: 1085967270.6644738\n",
      "Iteration 9244, Loss: 1129427110.403876\n",
      "Iteration 9245, Loss: 1004534014.8854779\n",
      "Iteration 9246, Loss: 907834538.8605291\n",
      "Iteration 9247, Loss: 956963283.0428433\n",
      "Iteration 9248, Loss: 957756968.107624\n",
      "Iteration 9249, Loss: 1677760247.4747372\n",
      "Iteration 9250, Loss: 1387022896.1085436\n",
      "Iteration 9251, Loss: 976263311.7315861\n",
      "Iteration 9252, Loss: 972729977.5813649\n",
      "Iteration 9253, Loss: 2175072211.175419\n",
      "Iteration 9254, Loss: 1013057514.1296891\n",
      "Iteration 9255, Loss: 951875973.1408108\n",
      "Iteration 9256, Loss: 936432475.5667771\n",
      "Iteration 9257, Loss: 1081217802.8194528\n",
      "Iteration 9258, Loss: 1044666294.3558481\n",
      "Iteration 9259, Loss: 927150458.6585354\n",
      "Iteration 9260, Loss: 916970693.7127321\n",
      "Iteration 9261, Loss: 907183592.8089054\n",
      "Iteration 9262, Loss: 897001952.1632642\n",
      "Iteration 9263, Loss: 1072665443.709644\n",
      "Iteration 9264, Loss: 1661827805.0080411\n",
      "Iteration 9265, Loss: 923688811.9752702\n",
      "Iteration 9266, Loss: 924886290.0897274\n",
      "Iteration 9267, Loss: 1089447544.3477123\n",
      "Iteration 9268, Loss: 1117388692.64539\n",
      "Iteration 9269, Loss: 1071466747.847548\n",
      "Iteration 9270, Loss: 1026443832.8746291\n",
      "Iteration 9271, Loss: 950839264.5680593\n",
      "Iteration 9272, Loss: 960020763.3356546\n",
      "Iteration 9273, Loss: 914994840.4509827\n",
      "Iteration 9274, Loss: 1050487173.7021675\n",
      "Iteration 9275, Loss: 956253422.8750836\n",
      "Iteration 9276, Loss: 2201059589.3499775\n",
      "Iteration 9277, Loss: 1112176098.4117134\n",
      "Iteration 9278, Loss: 1099609981.9505777\n",
      "Iteration 9279, Loss: 1025988195.8349869\n",
      "Iteration 9280, Loss: 1221066174.964943\n",
      "Iteration 9281, Loss: 1357800645.3587162\n",
      "Iteration 9282, Loss: 1072959780.4190627\n",
      "Iteration 9283, Loss: 1166470973.8648775\n",
      "Iteration 9284, Loss: 1212998043.3990347\n",
      "Iteration 9285, Loss: 1139856128.2217562\n",
      "Iteration 9286, Loss: 1310607147.9179637\n",
      "Iteration 9287, Loss: 1246168775.2061458\n",
      "Iteration 9288, Loss: 1014639488.5586274\n",
      "Iteration 9289, Loss: 1053096762.7784736\n",
      "Iteration 9290, Loss: 1249558186.5125356\n",
      "Iteration 9291, Loss: 1206256985.8029234\n",
      "Iteration 9292, Loss: 1052351840.2812684\n",
      "Iteration 9293, Loss: 1035299964.4192233\n",
      "Iteration 9294, Loss: 1068656277.934959\n",
      "Iteration 9295, Loss: 1053675162.0393181\n",
      "Iteration 9296, Loss: 965692638.9623437\n",
      "Iteration 9297, Loss: 1080632198.8193018\n",
      "Iteration 9298, Loss: 994561139.2964544\n",
      "Iteration 9299, Loss: 981927212.1510856\n",
      "Iteration 9300, Loss: 966429872.7391287\n",
      "Iteration 9301, Loss: 1077519029.9264586\n",
      "Iteration 9302, Loss: 1055598651.7509676\n",
      "Iteration 9303, Loss: 1051274601.7904291\n",
      "Iteration 9304, Loss: 1085776182.6605585\n",
      "Iteration 9305, Loss: 1060173591.6628708\n",
      "Iteration 9306, Loss: 1198521296.9137719\n",
      "Iteration 9307, Loss: 1190016739.229227\n",
      "Iteration 9308, Loss: 1157037754.0298977\n",
      "Iteration 9309, Loss: 1110378797.2069604\n",
      "Iteration 9310, Loss: 1025737977.243088\n",
      "Iteration 9311, Loss: 965860220.3408096\n",
      "Iteration 9312, Loss: 989330658.8669811\n",
      "Iteration 9313, Loss: 1397774404.598059\n",
      "Iteration 9314, Loss: 1414582221.8387537\n",
      "Iteration 9315, Loss: 985323331.8351823\n",
      "Iteration 9316, Loss: 1159624519.9937263\n",
      "Iteration 9317, Loss: 1056745946.288211\n",
      "Iteration 9318, Loss: 1169736098.8405051\n",
      "Iteration 9319, Loss: 1042930416.1827378\n",
      "Iteration 9320, Loss: 2013762594.255803\n",
      "Iteration 9321, Loss: 1011477425.4481844\n",
      "Iteration 9322, Loss: 1999713647.256155\n",
      "Iteration 9323, Loss: 1678290562.1384845\n",
      "Iteration 9324, Loss: 1586394129.961379\n",
      "Iteration 9325, Loss: 1498057783.1134732\n",
      "Iteration 9326, Loss: 1428430110.1159804\n",
      "Iteration 9327, Loss: 1410609156.4594028\n",
      "Iteration 9328, Loss: 1103726326.1273892\n",
      "Iteration 9329, Loss: 1034886878.3690678\n",
      "Iteration 9330, Loss: 1677286368.0810416\n",
      "Iteration 9331, Loss: 1033164446.7692599\n",
      "Iteration 9332, Loss: 1028037904.9577397\n",
      "Iteration 9333, Loss: 1116829268.7543995\n",
      "Iteration 9334, Loss: 1199982786.0760343\n",
      "Iteration 9335, Loss: 1011333217.0759789\n",
      "Iteration 9336, Loss: 3831412751.8368587\n",
      "Iteration 9337, Loss: 8486393579.6871805\n",
      "Iteration 9338, Loss: 6426743367.78446\n",
      "Iteration 9339, Loss: 19325353167.62723\n",
      "Iteration 9340, Loss: 1127762906.370706\n",
      "Iteration 9341, Loss: 1156635403.8032606\n",
      "Iteration 9342, Loss: 1138647031.686079\n",
      "Iteration 9343, Loss: 1233015729.0120025\n",
      "Iteration 9344, Loss: 1102818562.1983626\n",
      "Iteration 9345, Loss: 1142903757.6047993\n",
      "Iteration 9346, Loss: 1128864616.994841\n",
      "Iteration 9347, Loss: 1084727965.4970052\n",
      "Iteration 9348, Loss: 1128042739.2542264\n",
      "Iteration 9349, Loss: 1257574948.1332238\n",
      "Iteration 9350, Loss: 1193501438.9985888\n",
      "Iteration 9351, Loss: 1171007224.2093785\n",
      "Iteration 9352, Loss: 1113363912.3291507\n",
      "Iteration 9353, Loss: 1106207012.2203708\n",
      "Iteration 9354, Loss: 1612892902.5142488\n",
      "Iteration 9355, Loss: 1452301259.2382362\n",
      "Iteration 9356, Loss: 1087568518.2270236\n",
      "Iteration 9357, Loss: 1052246954.4703045\n",
      "Iteration 9358, Loss: 1015887426.6364771\n",
      "Iteration 9359, Loss: 1086952268.663251\n",
      "Iteration 9360, Loss: 1109839255.8907347\n",
      "Iteration 9361, Loss: 1128829952.51924\n",
      "Iteration 9362, Loss: 1110668194.0715818\n",
      "Iteration 9363, Loss: 1059006212.8544464\n",
      "Iteration 9364, Loss: 1318572008.7854805\n",
      "Iteration 9365, Loss: 1223802379.9394035\n",
      "Iteration 9366, Loss: 1010174311.7835982\n",
      "Iteration 9367, Loss: 1025233811.9000431\n",
      "Iteration 9368, Loss: 1013092653.834131\n",
      "Iteration 9369, Loss: 996286949.0907598\n",
      "Iteration 9370, Loss: 1075178229.701752\n",
      "Iteration 9371, Loss: 1090993725.6867204\n",
      "Iteration 9372, Loss: 1105871074.0292838\n",
      "Iteration 9373, Loss: 1697193289.1498814\n",
      "Iteration 9374, Loss: 1614844673.4075592\n",
      "Iteration 9375, Loss: 975405557.5675838\n",
      "Iteration 9376, Loss: 1313761944.6991208\n",
      "Iteration 9377, Loss: 1252032670.6629713\n",
      "Iteration 9378, Loss: 1265231317.4500008\n",
      "Iteration 9379, Loss: 1238167555.3495734\n",
      "Iteration 9380, Loss: 1037882790.4057304\n",
      "Iteration 9381, Loss: 2098107821.9946616\n",
      "Iteration 9382, Loss: 1014747548.239621\n",
      "Iteration 9383, Loss: 2615222894.9174843\n",
      "Iteration 9384, Loss: 1241247658.9678044\n",
      "Iteration 9385, Loss: 1138742478.9906447\n",
      "Iteration 9386, Loss: 1028281659.7064508\n",
      "Iteration 9387, Loss: 3423502749.076761\n",
      "Iteration 9388, Loss: 1084795658.6277378\n",
      "Iteration 9389, Loss: 1080169684.06362\n",
      "Iteration 9390, Loss: 1069179190.6272589\n",
      "Iteration 9391, Loss: 1109596512.5449781\n",
      "Iteration 9392, Loss: 1092702262.791714\n",
      "Iteration 9393, Loss: 1061399186.4241438\n",
      "Iteration 9394, Loss: 1535531791.2690988\n",
      "Iteration 9395, Loss: 1366826391.3343134\n",
      "Iteration 9396, Loss: 1120267276.550216\n",
      "Iteration 9397, Loss: 1088717747.462498\n",
      "Iteration 9398, Loss: 1068537349.599975\n",
      "Iteration 9399, Loss: 1049273862.7672513\n",
      "Iteration 9400, Loss: 1033383319.8232341\n",
      "Iteration 9401, Loss: 1016802749.7168937\n",
      "Iteration 9402, Loss: 1001337982.4745967\n",
      "Iteration 9403, Loss: 1523226084.7528396\n",
      "Iteration 9404, Loss: 1426020612.2959647\n",
      "Iteration 9405, Loss: 1091311245.4054449\n",
      "Iteration 9406, Loss: 1305651654.1540964\n",
      "Iteration 9407, Loss: 1162851786.1552012\n",
      "Iteration 9408, Loss: 1630191909.8196702\n",
      "Iteration 9409, Loss: 1163989353.4140859\n",
      "Iteration 9410, Loss: 1266782408.8696458\n",
      "Iteration 9411, Loss: 1230285146.0661666\n",
      "Iteration 9412, Loss: 1073039216.3657428\n",
      "Iteration 9413, Loss: 1068021052.9305964\n",
      "Iteration 9414, Loss: 1111823570.6516066\n",
      "Iteration 9415, Loss: 1081245567.5319314\n",
      "Iteration 9416, Loss: 2041694804.1290774\n",
      "Iteration 9417, Loss: 1832924140.9432402\n",
      "Iteration 9418, Loss: 1279944877.3130035\n",
      "Iteration 9419, Loss: 1968576895.8444767\n",
      "Iteration 9420, Loss: 1917380650.164788\n",
      "Iteration 9421, Loss: 1737653069.1900902\n",
      "Iteration 9422, Loss: 1278449727.9590526\n",
      "Iteration 9423, Loss: 1162823267.13246\n",
      "Iteration 9424, Loss: 1133349226.8817508\n",
      "Iteration 9425, Loss: 1108385072.134784\n",
      "Iteration 9426, Loss: 1068562831.7350982\n",
      "Iteration 9427, Loss: 982364257.0578804\n",
      "Iteration 9428, Loss: 985262755.5936671\n",
      "Iteration 9429, Loss: 971352160.1857556\n",
      "Iteration 9430, Loss: 1001025668.9619975\n",
      "Iteration 9431, Loss: 988597488.0385404\n",
      "Iteration 9432, Loss: 971304695.3257668\n",
      "Iteration 9433, Loss: 1313624767.587479\n",
      "Iteration 9434, Loss: 1111475360.3161001\n",
      "Iteration 9435, Loss: 964172810.1238133\n",
      "Iteration 9436, Loss: 1192151102.894406\n",
      "Iteration 9437, Loss: 1465665828.7420247\n",
      "Iteration 9438, Loss: 1404462470.8005612\n",
      "Iteration 9439, Loss: 1344598685.6108124\n",
      "Iteration 9440, Loss: 968785027.1022415\n",
      "Iteration 9441, Loss: 1191569302.4474854\n",
      "Iteration 9442, Loss: 1155798451.3400047\n",
      "Iteration 9443, Loss: 1201564378.3086655\n",
      "Iteration 9444, Loss: 1238023632.6759934\n",
      "Iteration 9445, Loss: 1152880532.451249\n",
      "Iteration 9446, Loss: 1115487931.040475\n",
      "Iteration 9447, Loss: 979697659.6565357\n",
      "Iteration 9448, Loss: 982194197.3468776\n",
      "Iteration 9449, Loss: 1102598728.746164\n",
      "Iteration 9450, Loss: 1166197296.29332\n",
      "Iteration 9451, Loss: 1079280220.0238788\n",
      "Iteration 9452, Loss: 1150955800.354549\n",
      "Iteration 9453, Loss: 1114930631.5778549\n",
      "Iteration 9454, Loss: 1096117925.9075952\n",
      "Iteration 9455, Loss: 995036056.2011214\n",
      "Iteration 9456, Loss: 988227951.9591825\n",
      "Iteration 9457, Loss: 986111325.1165794\n",
      "Iteration 9458, Loss: 1071700461.3078514\n",
      "Iteration 9459, Loss: 1095985697.81047\n",
      "Iteration 9460, Loss: 1068619067.0887941\n",
      "Iteration 9461, Loss: 1309232110.5766735\n",
      "Iteration 9462, Loss: 1070922124.2136147\n",
      "Iteration 9463, Loss: 1052389083.5858622\n",
      "Iteration 9464, Loss: 1065106869.3554049\n",
      "Iteration 9465, Loss: 1213489311.9388115\n",
      "Iteration 9466, Loss: 1170215064.501495\n",
      "Iteration 9467, Loss: 1209842087.3162234\n",
      "Iteration 9468, Loss: 1198116460.568673\n",
      "Iteration 9469, Loss: 1138661583.36575\n",
      "Iteration 9470, Loss: 1208934765.4313326\n",
      "Iteration 9471, Loss: 1234270544.1253085\n",
      "Iteration 9472, Loss: 1164867710.3987155\n",
      "Iteration 9473, Loss: 1073355933.0948368\n",
      "Iteration 9474, Loss: 1055953421.5601783\n",
      "Iteration 9475, Loss: 1022836276.3495203\n",
      "Iteration 9476, Loss: 1053417629.2014099\n",
      "Iteration 9477, Loss: 1061232812.5382023\n",
      "Iteration 9478, Loss: 1042663088.2278444\n",
      "Iteration 9479, Loss: 1014386056.921671\n",
      "Iteration 9480, Loss: 1083305030.1370044\n",
      "Iteration 9481, Loss: 1189776422.7004056\n",
      "Iteration 9482, Loss: 1198701612.0250669\n",
      "Iteration 9483, Loss: 1103440432.2270675\n",
      "Iteration 9484, Loss: 1115520069.7717679\n",
      "Iteration 9485, Loss: 1614232219.641567\n",
      "Iteration 9486, Loss: 1172319614.0657876\n",
      "Iteration 9487, Loss: 1025841674.9604818\n",
      "Iteration 9488, Loss: 1179349008.5201905\n",
      "Iteration 9489, Loss: 1131272856.7957654\n",
      "Iteration 9490, Loss: 1065551962.9521052\n",
      "Iteration 9491, Loss: 1240709939.6921144\n",
      "Iteration 9492, Loss: 1209044347.4598475\n",
      "Iteration 9493, Loss: 1164821529.5077891\n",
      "Iteration 9494, Loss: 1145596178.899262\n",
      "Iteration 9495, Loss: 1030488727.8733106\n",
      "Iteration 9496, Loss: 976345637.2340171\n",
      "Iteration 9497, Loss: 1077144014.296683\n",
      "Iteration 9498, Loss: 1170292038.3717782\n",
      "Iteration 9499, Loss: 942867993.029691\n",
      "Iteration 9500, Loss: 1238140429.404414\n",
      "Iteration 9501, Loss: 1254305822.894908\n",
      "Iteration 9502, Loss: 1107876609.01968\n",
      "Iteration 9503, Loss: 1146433378.9507558\n",
      "Iteration 9504, Loss: 1023585418.2537738\n",
      "Iteration 9505, Loss: 1038635084.088441\n",
      "Iteration 9506, Loss: 1010346266.6600552\n",
      "Iteration 9507, Loss: 1010396230.7354325\n",
      "Iteration 9508, Loss: 958213351.9204042\n",
      "Iteration 9509, Loss: 975918086.2831485\n",
      "Iteration 9510, Loss: 1135751680.7188947\n",
      "Iteration 9511, Loss: 1122298162.750926\n",
      "Iteration 9512, Loss: 1022278700.9247777\n",
      "Iteration 9513, Loss: 993401002.3332125\n",
      "Iteration 9514, Loss: 999315052.9733647\n",
      "Iteration 9515, Loss: 997974665.7522594\n",
      "Iteration 9516, Loss: 1002080128.2274421\n",
      "Iteration 9517, Loss: 947067208.0759869\n",
      "Iteration 9518, Loss: 2660253302.085206\n",
      "Iteration 9519, Loss: 1849712924.0614033\n",
      "Iteration 9520, Loss: 1772847233.7703152\n",
      "Iteration 9521, Loss: 1578009355.5498817\n",
      "Iteration 9522, Loss: 1378925220.8050342\n",
      "Iteration 9523, Loss: 1030745665.2270256\n",
      "Iteration 9524, Loss: 1776494534.8996673\n",
      "Iteration 9525, Loss: 1671030336.2350237\n",
      "Iteration 9526, Loss: 1144904867.7430782\n",
      "Iteration 9527, Loss: 1009727881.5488133\n",
      "Iteration 9528, Loss: 983589655.8930529\n",
      "Iteration 9529, Loss: 968858977.7895885\n",
      "Iteration 9530, Loss: 916736428.8024975\n",
      "Iteration 9531, Loss: 912218545.1075081\n",
      "Iteration 9532, Loss: 1096037146.2747617\n",
      "Iteration 9533, Loss: 976427810.4134338\n",
      "Iteration 9534, Loss: 954814634.2129905\n",
      "Iteration 9535, Loss: 1019644500.4464177\n",
      "Iteration 9536, Loss: 1188589509.6394813\n",
      "Iteration 9537, Loss: 1157414600.7718632\n",
      "Iteration 9538, Loss: 1087292557.3325267\n",
      "Iteration 9539, Loss: 1052532261.5299532\n",
      "Iteration 9540, Loss: 999906700.3469794\n",
      "Iteration 9541, Loss: 975084719.0021071\n",
      "Iteration 9542, Loss: 985158280.3796152\n",
      "Iteration 9543, Loss: 1033816997.6653968\n",
      "Iteration 9544, Loss: 980987605.7589473\n",
      "Iteration 9545, Loss: 1043466694.3384448\n",
      "Iteration 9546, Loss: 998528948.0102291\n",
      "Iteration 9547, Loss: 1029334003.5216606\n",
      "Iteration 9548, Loss: 901818960.789413\n",
      "Iteration 9549, Loss: 1132774138.7524335\n",
      "Iteration 9550, Loss: 973747991.8796283\n",
      "Iteration 9551, Loss: 941594469.5994737\n",
      "Iteration 9552, Loss: 1241736185.2559834\n",
      "Iteration 9553, Loss: 1178370113.6385608\n",
      "Iteration 9554, Loss: 960660475.2346586\n",
      "Iteration 9555, Loss: 1016906599.8877788\n",
      "Iteration 9556, Loss: 904327261.8658171\n",
      "Iteration 9557, Loss: 1200865216.220479\n",
      "Iteration 9558, Loss: 1137578397.5991337\n",
      "Iteration 9559, Loss: 1140151185.535354\n",
      "Iteration 9560, Loss: 896921680.9621907\n",
      "Iteration 9561, Loss: 2330144804.390798\n",
      "Iteration 9562, Loss: 1936084026.3607404\n",
      "Iteration 9563, Loss: 1169723977.8900604\n",
      "Iteration 9564, Loss: 1068510267.0278101\n",
      "Iteration 9565, Loss: 1023051217.4295189\n",
      "Iteration 9566, Loss: 1181392286.0456865\n",
      "Iteration 9567, Loss: 991869335.9424394\n",
      "Iteration 9568, Loss: 952703654.5787444\n",
      "Iteration 9569, Loss: 1150032403.314587\n",
      "Iteration 9570, Loss: 1102085519.6937206\n",
      "Iteration 9571, Loss: 929582162.3693027\n",
      "Iteration 9572, Loss: 2839911250.428973\n",
      "Iteration 9573, Loss: 2189341596.496132\n",
      "Iteration 9574, Loss: 2186805360.0666957\n",
      "Iteration 9575, Loss: 965416255.2270333\n",
      "Iteration 9576, Loss: 1034442196.2584388\n",
      "Iteration 9577, Loss: 1041198168.9666706\n",
      "Iteration 9578, Loss: 954747474.614809\n",
      "Iteration 9579, Loss: 2869878581.1242557\n",
      "Iteration 9580, Loss: 2056331797.8625038\n",
      "Iteration 9581, Loss: 2409051286.425627\n",
      "Iteration 9582, Loss: 1121517561.5448859\n",
      "Iteration 9583, Loss: 989182576.154294\n",
      "Iteration 9584, Loss: 995008795.6140124\n",
      "Iteration 9585, Loss: 984596830.8808098\n",
      "Iteration 9586, Loss: 976566609.2361164\n",
      "Iteration 9587, Loss: 977594712.4862795\n",
      "Iteration 9588, Loss: 962726459.6483617\n",
      "Iteration 9589, Loss: 944326707.4411979\n",
      "Iteration 9590, Loss: 2149387392.0735917\n",
      "Iteration 9591, Loss: 1336194143.3963082\n",
      "Iteration 9592, Loss: 1198928258.375196\n",
      "Iteration 9593, Loss: 1181753243.1422112\n",
      "Iteration 9594, Loss: 1001910274.5958674\n",
      "Iteration 9595, Loss: 1010871134.2738122\n",
      "Iteration 9596, Loss: 990149521.3790363\n",
      "Iteration 9597, Loss: 1019092048.3468978\n",
      "Iteration 9598, Loss: 1044411860.8701669\n",
      "Iteration 9599, Loss: 1041192597.4748825\n",
      "Iteration 9600, Loss: 969455453.2890943\n",
      "Iteration 9601, Loss: 1555871344.3805416\n",
      "Iteration 9602, Loss: 1475731893.0809402\n",
      "Iteration 9603, Loss: 1048071277.7048804\n",
      "Iteration 9604, Loss: 1037608694.699734\n",
      "Iteration 9605, Loss: 936184415.4890637\n",
      "Iteration 9606, Loss: 933682669.5389864\n",
      "Iteration 9607, Loss: 1039934071.9384644\n",
      "Iteration 9608, Loss: 1327283026.3360314\n",
      "Iteration 9609, Loss: 1264605588.1982393\n",
      "Iteration 9610, Loss: 1006474135.6125672\n",
      "Iteration 9611, Loss: 1177216376.3641703\n",
      "Iteration 9612, Loss: 996441351.2746933\n",
      "Iteration 9613, Loss: 1013845927.6442515\n",
      "Iteration 9614, Loss: 988411389.5315881\n",
      "Iteration 9615, Loss: 963983638.8453209\n",
      "Iteration 9616, Loss: 937885254.8812257\n",
      "Iteration 9617, Loss: 1008576777.439571\n",
      "Iteration 9618, Loss: 1062224283.0761696\n",
      "Iteration 9619, Loss: 1725597202.7321215\n",
      "Iteration 9620, Loss: 1613791535.3318913\n",
      "Iteration 9621, Loss: 948071325.2251369\n",
      "Iteration 9622, Loss: 932295900.5203886\n",
      "Iteration 9623, Loss: 1172666085.5530987\n",
      "Iteration 9624, Loss: 1129529332.7548428\n",
      "Iteration 9625, Loss: 986388812.2873147\n",
      "Iteration 9626, Loss: 1231856764.28707\n",
      "Iteration 9627, Loss: 1022824284.8448778\n",
      "Iteration 9628, Loss: 1119566942.106437\n",
      "Iteration 9629, Loss: 1559965175.6255398\n",
      "Iteration 9630, Loss: 958600153.5529573\n",
      "Iteration 9631, Loss: 947116721.986868\n",
      "Iteration 9632, Loss: 1158513815.3521292\n",
      "Iteration 9633, Loss: 1178241141.9053514\n",
      "Iteration 9634, Loss: 942972706.3733714\n",
      "Iteration 9635, Loss: 931093123.7952807\n",
      "Iteration 9636, Loss: 1405978781.655507\n",
      "Iteration 9637, Loss: 957267370.7370596\n",
      "Iteration 9638, Loss: 1506149268.0136337\n",
      "Iteration 9639, Loss: 1397547718.6705906\n",
      "Iteration 9640, Loss: 1331084446.0881143\n",
      "Iteration 9641, Loss: 1249477405.054991\n",
      "Iteration 9642, Loss: 1203860850.5453653\n",
      "Iteration 9643, Loss: 1089467359.6781147\n",
      "Iteration 9644, Loss: 956358028.8022307\n",
      "Iteration 9645, Loss: 935220703.9955986\n",
      "Iteration 9646, Loss: 2480691226.0453186\n",
      "Iteration 9647, Loss: 1001396983.1340342\n",
      "Iteration 9648, Loss: 1064252919.5963457\n",
      "Iteration 9649, Loss: 1000781016.5833619\n",
      "Iteration 9650, Loss: 1005472115.2311673\n",
      "Iteration 9651, Loss: 973485249.3120606\n",
      "Iteration 9652, Loss: 944257409.0458539\n",
      "Iteration 9653, Loss: 917610789.9991108\n",
      "Iteration 9654, Loss: 1008252016.8863195\n",
      "Iteration 9655, Loss: 1366190055.8851092\n",
      "Iteration 9656, Loss: 1230395528.3989747\n",
      "Iteration 9657, Loss: 1139830695.9892843\n",
      "Iteration 9658, Loss: 1139945191.5103507\n",
      "Iteration 9659, Loss: 913900579.766514\n",
      "Iteration 9660, Loss: 909714060.2944417\n",
      "Iteration 9661, Loss: 902493094.1929044\n",
      "Iteration 9662, Loss: 997338986.6397672\n",
      "Iteration 9663, Loss: 1040752066.5323472\n",
      "Iteration 9664, Loss: 1127688498.2075589\n",
      "Iteration 9665, Loss: 987135063.370331\n",
      "Iteration 9666, Loss: 998899629.5952138\n",
      "Iteration 9667, Loss: 1006429496.0987504\n",
      "Iteration 9668, Loss: 1044813285.8002357\n",
      "Iteration 9669, Loss: 1039659704.3813925\n",
      "Iteration 9670, Loss: 1010550037.8186328\n",
      "Iteration 9671, Loss: 1055364936.4840192\n",
      "Iteration 9672, Loss: 1052104777.959656\n",
      "Iteration 9673, Loss: 1150228983.6988807\n",
      "Iteration 9674, Loss: 950563840.8269231\n",
      "Iteration 9675, Loss: 926146254.0534033\n",
      "Iteration 9676, Loss: 904770429.6636679\n",
      "Iteration 9677, Loss: 901382567.7645532\n",
      "Iteration 9678, Loss: 878983626.5783447\n",
      "Iteration 9679, Loss: 866381322.0817184\n",
      "Iteration 9680, Loss: 1376963279.1236842\n",
      "Iteration 9681, Loss: 1219902397.9116998\n",
      "Iteration 9682, Loss: 966132161.5727727\n",
      "Iteration 9683, Loss: 1057798119.4277586\n",
      "Iteration 9684, Loss: 974967654.9192228\n",
      "Iteration 9685, Loss: 2000352542.087713\n",
      "Iteration 9686, Loss: 1838382193.8738658\n",
      "Iteration 9687, Loss: 1284968548.270492\n",
      "Iteration 9688, Loss: 893015796.3651949\n",
      "Iteration 9689, Loss: 2619937177.8045306\n",
      "Iteration 9690, Loss: 966223694.339805\n",
      "Iteration 9691, Loss: 4374448435.983868\n",
      "Iteration 9692, Loss: 1429404199.0760756\n",
      "Iteration 9693, Loss: 1319970250.9478593\n",
      "Iteration 9694, Loss: 1063101760.8683598\n",
      "Iteration 9695, Loss: 1020852412.9007682\n",
      "Iteration 9696, Loss: 985298826.8951055\n",
      "Iteration 9697, Loss: 982838636.161535\n",
      "Iteration 9698, Loss: 1231723228.998976\n",
      "Iteration 9699, Loss: 1178323719.7185478\n",
      "Iteration 9700, Loss: 909842451.552697\n",
      "Iteration 9701, Loss: 1382575132.5920818\n",
      "Iteration 9702, Loss: 957950687.4440144\n",
      "Iteration 9703, Loss: 1142940942.9265113\n",
      "Iteration 9704, Loss: 1217647541.4436233\n",
      "Iteration 9705, Loss: 1176219811.0902512\n",
      "Iteration 9706, Loss: 1139292777.2638023\n",
      "Iteration 9707, Loss: 1030993962.674763\n",
      "Iteration 9708, Loss: 971892378.3241954\n",
      "Iteration 9709, Loss: 942429021.581\n",
      "Iteration 9710, Loss: 1236204447.595397\n",
      "Iteration 9711, Loss: 1178238648.403618\n",
      "Iteration 9712, Loss: 1142660541.647351\n",
      "Iteration 9713, Loss: 1502379996.1616974\n",
      "Iteration 9714, Loss: 1014064017.9574422\n",
      "Iteration 9715, Loss: 956274562.6145028\n",
      "Iteration 9716, Loss: 935742902.922661\n",
      "Iteration 9717, Loss: 1444119777.5414922\n",
      "Iteration 9718, Loss: 1341351808.5405254\n",
      "Iteration 9719, Loss: 961326745.4948349\n",
      "Iteration 9720, Loss: 983863662.6909395\n",
      "Iteration 9721, Loss: 992988420.0216546\n",
      "Iteration 9722, Loss: 2175699329.6209626\n",
      "Iteration 9723, Loss: 1502274667.361078\n",
      "Iteration 9724, Loss: 985906914.0667148\n",
      "Iteration 9725, Loss: 952962674.2650069\n",
      "Iteration 9726, Loss: 3015292691.346987\n",
      "Iteration 9727, Loss: 2337664329.7247033\n",
      "Iteration 9728, Loss: 995717291.4103798\n",
      "Iteration 9729, Loss: 1112813326.376399\n",
      "Iteration 9730, Loss: 1014804809.1043456\n",
      "Iteration 9731, Loss: 1102882144.706578\n",
      "Iteration 9732, Loss: 1218499308.3908813\n",
      "Iteration 9733, Loss: 1094428201.853703\n",
      "Iteration 9734, Loss: 1229948311.8621635\n",
      "Iteration 9735, Loss: 1049293538.644631\n",
      "Iteration 9736, Loss: 1022151214.5095935\n",
      "Iteration 9737, Loss: 1167891644.2929857\n",
      "Iteration 9738, Loss: 1136757634.0704067\n",
      "Iteration 9739, Loss: 1188367382.2301\n",
      "Iteration 9740, Loss: 1022452750.7971306\n",
      "Iteration 9741, Loss: 1224875048.639242\n",
      "Iteration 9742, Loss: 1092863514.8368158\n",
      "Iteration 9743, Loss: 991092826.8559524\n",
      "Iteration 9744, Loss: 3083966394.204111\n",
      "Iteration 9745, Loss: 1041827734.4814323\n",
      "Iteration 9746, Loss: 1016816756.9062791\n",
      "Iteration 9747, Loss: 1004344335.3848156\n",
      "Iteration 9748, Loss: 1076759928.0915265\n",
      "Iteration 9749, Loss: 1106984868.4108963\n",
      "Iteration 9750, Loss: 1163883897.7554388\n",
      "Iteration 9751, Loss: 1106824578.2693064\n",
      "Iteration 9752, Loss: 1087972802.6430855\n",
      "Iteration 9753, Loss: 1029076498.543985\n",
      "Iteration 9754, Loss: 2535624160.1966934\n",
      "Iteration 9755, Loss: 1058061975.7798921\n",
      "Iteration 9756, Loss: 1259027121.0949597\n",
      "Iteration 9757, Loss: 1237632024.9364512\n",
      "Iteration 9758, Loss: 1316676046.7265575\n",
      "Iteration 9759, Loss: 1206302433.354443\n",
      "Iteration 9760, Loss: 1191701537.954216\n",
      "Iteration 9761, Loss: 1157232265.5081425\n",
      "Iteration 9762, Loss: 1302774145.9389367\n",
      "Iteration 9763, Loss: 1328162885.4151843\n",
      "Iteration 9764, Loss: 1158005067.7253847\n",
      "Iteration 9765, Loss: 1151180180.2881494\n",
      "Iteration 9766, Loss: 1176464874.6535606\n",
      "Iteration 9767, Loss: 1261878913.4860282\n",
      "Iteration 9768, Loss: 1279411440.931979\n",
      "Iteration 9769, Loss: 1299143958.8447428\n",
      "Iteration 9770, Loss: 1113559017.055298\n",
      "Iteration 9771, Loss: 1068200185.1896703\n",
      "Iteration 9772, Loss: 1168832821.641405\n",
      "Iteration 9773, Loss: 1123624253.9953642\n",
      "Iteration 9774, Loss: 1222636639.5489798\n",
      "Iteration 9775, Loss: 1292735813.2860494\n",
      "Iteration 9776, Loss: 1079006853.7490025\n",
      "Iteration 9777, Loss: 1191780728.79857\n",
      "Iteration 9778, Loss: 1144354673.3241308\n",
      "Iteration 9779, Loss: 1326638635.5143619\n",
      "Iteration 9780, Loss: 1136071045.3777344\n",
      "Iteration 9781, Loss: 1235941437.089748\n",
      "Iteration 9782, Loss: 1326415509.7771325\n",
      "Iteration 9783, Loss: 1310879231.7294612\n",
      "Iteration 9784, Loss: 1295445895.4961493\n",
      "Iteration 9785, Loss: 1115166375.8647134\n",
      "Iteration 9786, Loss: 1152668326.0802758\n",
      "Iteration 9787, Loss: 1223967350.8580582\n",
      "Iteration 9788, Loss: 1149432753.4139917\n",
      "Iteration 9789, Loss: 1364578904.6392596\n",
      "Iteration 9790, Loss: 1352749444.8213696\n",
      "Iteration 9791, Loss: 1354757736.188823\n",
      "Iteration 9792, Loss: 1335546079.5532517\n",
      "Iteration 9793, Loss: 1335607637.953559\n",
      "Iteration 9794, Loss: 1360267437.042161\n",
      "Iteration 9795, Loss: 1104358831.4881275\n",
      "Iteration 9796, Loss: 1156288621.3874655\n",
      "Iteration 9797, Loss: 1123541276.943047\n",
      "Iteration 9798, Loss: 1176440773.6163368\n",
      "Iteration 9799, Loss: 1171168538.9960992\n",
      "Iteration 9800, Loss: 1211844593.10081\n",
      "Iteration 9801, Loss: 1174341307.0701687\n",
      "Iteration 9802, Loss: 1212912804.0829403\n",
      "Iteration 9803, Loss: 1219236119.613552\n",
      "Iteration 9804, Loss: 1336635973.325002\n",
      "Iteration 9805, Loss: 1377404327.7612178\n",
      "Iteration 9806, Loss: 1194122375.4475305\n",
      "Iteration 9807, Loss: 1248142524.2340624\n",
      "Iteration 9808, Loss: 1389267662.7804518\n",
      "Iteration 9809, Loss: 1152820177.0069902\n",
      "Iteration 9810, Loss: 3451259957.548822\n",
      "Iteration 9811, Loss: 2871034599.621889\n",
      "Iteration 9812, Loss: 1239592619.1844335\n",
      "Iteration 9813, Loss: 1147013176.363674\n",
      "Iteration 9814, Loss: 1190054380.6558917\n",
      "Iteration 9815, Loss: 1452679816.0567777\n",
      "Iteration 9816, Loss: 1251428916.3623087\n",
      "Iteration 9817, Loss: 1340494565.7328389\n",
      "Iteration 9818, Loss: 1394639617.7325127\n",
      "Iteration 9819, Loss: 1374887438.7567024\n",
      "Iteration 9820, Loss: 1141105156.4542968\n",
      "Iteration 9821, Loss: 1195712329.5563245\n",
      "Iteration 9822, Loss: 1290838755.056167\n",
      "Iteration 9823, Loss: 1351296356.5777416\n",
      "Iteration 9824, Loss: 1456184610.5197778\n",
      "Iteration 9825, Loss: 1407683853.193042\n",
      "Iteration 9826, Loss: 1422845035.8147836\n",
      "Iteration 9827, Loss: 1199375703.313695\n",
      "Iteration 9828, Loss: 1962264845.0085762\n",
      "Iteration 9829, Loss: 1949827413.188501\n",
      "Iteration 9830, Loss: 1278294235.748557\n",
      "Iteration 9831, Loss: 1203885533.9886627\n",
      "Iteration 9832, Loss: 1208624784.1978006\n",
      "Iteration 9833, Loss: 1209051599.1272097\n",
      "Iteration 9834, Loss: 1266707219.0330849\n",
      "Iteration 9835, Loss: 1350429091.936745\n",
      "Iteration 9836, Loss: 1138109069.5076814\n",
      "Iteration 9837, Loss: 1324680652.4455845\n",
      "Iteration 9838, Loss: 1261127515.8187518\n",
      "Iteration 9839, Loss: 1252525049.1002169\n",
      "Iteration 9840, Loss: 1401179425.8020053\n",
      "Iteration 9841, Loss: 1258724046.4250562\n",
      "Iteration 9842, Loss: 1256631552.1190016\n",
      "Iteration 9843, Loss: 1257301923.3548582\n",
      "Iteration 9844, Loss: 1420063985.5329754\n",
      "Iteration 9845, Loss: 1424172862.858728\n",
      "Iteration 9846, Loss: 1382460956.0587115\n",
      "Iteration 9847, Loss: 1193905750.8971355\n",
      "Iteration 9848, Loss: 1243652814.4547977\n",
      "Iteration 9849, Loss: 1322969162.3144083\n",
      "Iteration 9850, Loss: 1396037012.901416\n",
      "Iteration 9851, Loss: 1204579056.372103\n",
      "Iteration 9852, Loss: 1407097970.386566\n",
      "Iteration 9853, Loss: 1291937016.4581866\n",
      "Iteration 9854, Loss: 1312533117.9785151\n",
      "Iteration 9855, Loss: 1274106744.8045332\n",
      "Iteration 9856, Loss: 1475258014.7442226\n",
      "Iteration 9857, Loss: 1566277641.3249438\n",
      "Iteration 9858, Loss: 1403381953.6529539\n",
      "Iteration 9859, Loss: 1516980920.7961714\n",
      "Iteration 9860, Loss: 1296463722.4817834\n",
      "Iteration 9861, Loss: 1283205574.482762\n",
      "Iteration 9862, Loss: 1328333456.8372989\n",
      "Iteration 9863, Loss: 1399384808.636503\n",
      "Iteration 9864, Loss: 1182072061.518375\n",
      "Iteration 9865, Loss: 1380865166.1997013\n",
      "Iteration 9866, Loss: 1395148724.854799\n",
      "Iteration 9867, Loss: 1503697145.3488781\n",
      "Iteration 9868, Loss: 1502518099.9170976\n",
      "Iteration 9869, Loss: 1197483909.4540665\n",
      "Iteration 9870, Loss: 1205602187.5472188\n",
      "Iteration 9871, Loss: 1409417414.363787\n",
      "Iteration 9872, Loss: 1180075277.4513674\n",
      "Iteration 9873, Loss: 1491124432.9854865\n",
      "Iteration 9874, Loss: 1492190778.7900777\n",
      "Iteration 9875, Loss: 1438248084.640075\n",
      "Iteration 9876, Loss: 1439545417.7214007\n",
      "Iteration 9877, Loss: 1390025401.8706822\n",
      "Iteration 9878, Loss: 1158915208.5297084\n",
      "Iteration 9879, Loss: 1115261080.001742\n",
      "Iteration 9880, Loss: 1099364678.9824004\n",
      "Iteration 9881, Loss: 1200683506.1146424\n",
      "Iteration 9882, Loss: 1200134186.331481\n",
      "Iteration 9883, Loss: 1237237420.9267633\n",
      "Iteration 9884, Loss: 1248046043.8125792\n",
      "Iteration 9885, Loss: 1169881747.1271617\n",
      "Iteration 9886, Loss: 2090323373.796396\n",
      "Iteration 9887, Loss: 1884497741.9695024\n",
      "Iteration 9888, Loss: 1871771160.1186564\n",
      "Iteration 9889, Loss: 1449443304.194057\n",
      "Iteration 9890, Loss: 1207185893.3418202\n",
      "Iteration 9891, Loss: 1315660992.7331414\n",
      "Iteration 9892, Loss: 1168353165.967027\n",
      "Iteration 9893, Loss: 1167056908.181692\n",
      "Iteration 9894, Loss: 1167280360.8831942\n",
      "Iteration 9895, Loss: 1336138879.505883\n",
      "Iteration 9896, Loss: 1221313099.2880294\n",
      "Iteration 9897, Loss: 1833038244.437889\n",
      "Iteration 9898, Loss: 1813242009.4869995\n",
      "Iteration 9899, Loss: 1841534536.7014198\n",
      "Iteration 9900, Loss: 1709135511.723536\n",
      "Iteration 9901, Loss: 1683276445.3141882\n",
      "Iteration 9902, Loss: 1153577529.692861\n",
      "Iteration 9903, Loss: 1144413955.4493797\n",
      "Iteration 9904, Loss: 1268481096.6914084\n",
      "Iteration 9905, Loss: 1383758802.3746538\n",
      "Iteration 9906, Loss: 1135313381.659897\n",
      "Iteration 9907, Loss: 1423688422.3839557\n",
      "Iteration 9908, Loss: 1385129379.4934456\n",
      "Iteration 9909, Loss: 1379804980.733063\n",
      "Iteration 9910, Loss: 1148210396.1822438\n",
      "Iteration 9911, Loss: 1273660513.7852628\n",
      "Iteration 9912, Loss: 1300652541.2412837\n",
      "Iteration 9913, Loss: 1273588336.3622067\n",
      "Iteration 9914, Loss: 1264962939.0117354\n",
      "Iteration 9915, Loss: 1549565562.3678205\n",
      "Iteration 9916, Loss: 1462519803.1500356\n",
      "Iteration 9917, Loss: 1389559712.6544526\n",
      "Iteration 9918, Loss: 1454906694.7749016\n",
      "Iteration 9919, Loss: 1439456912.619475\n",
      "Iteration 9920, Loss: 1484769990.5597897\n",
      "Iteration 9921, Loss: 1091300999.6206055\n",
      "Iteration 9922, Loss: 1095308855.306004\n",
      "Iteration 9923, Loss: 1104397303.155194\n",
      "Iteration 9924, Loss: 1416781801.7337534\n",
      "Iteration 9925, Loss: 1141705713.539761\n",
      "Iteration 9926, Loss: 1098081165.835132\n",
      "Iteration 9927, Loss: 1074278980.2526643\n",
      "Iteration 9928, Loss: 1051460030.0624672\n",
      "Iteration 9929, Loss: 1054946360.9202195\n",
      "Iteration 9930, Loss: 1909725996.3454146\n",
      "Iteration 9931, Loss: 1528584334.6210387\n",
      "Iteration 9932, Loss: 1185141676.2334428\n",
      "Iteration 9933, Loss: 1143331055.9515498\n",
      "Iteration 9934, Loss: 1575404745.3173497\n",
      "Iteration 9935, Loss: 1544077445.4478261\n",
      "Iteration 9936, Loss: 1152000575.2199452\n",
      "Iteration 9937, Loss: 1153730457.3357832\n",
      "Iteration 9938, Loss: 1336566162.8568056\n",
      "Iteration 9939, Loss: 1209453121.5937366\n",
      "Iteration 9940, Loss: 1203796991.7338018\n",
      "Iteration 9941, Loss: 1282234428.9986649\n",
      "Iteration 9942, Loss: 1233582724.0535536\n",
      "Iteration 9943, Loss: 1313914818.654245\n",
      "Iteration 9944, Loss: 1071908070.9513328\n",
      "Iteration 9945, Loss: 1579275116.4687731\n",
      "Iteration 9946, Loss: 1639655208.1678562\n",
      "Iteration 9947, Loss: 1136762432.9782774\n",
      "Iteration 9948, Loss: 1185407928.703376\n",
      "Iteration 9949, Loss: 1291576448.5431411\n",
      "Iteration 9950, Loss: 1290414271.9050043\n",
      "Iteration 9951, Loss: 1268792398.4645822\n",
      "Iteration 9952, Loss: 1290569097.76828\n",
      "Iteration 9953, Loss: 1208043242.4764206\n",
      "Iteration 9954, Loss: 1189817771.091509\n",
      "Iteration 9955, Loss: 1182839296.1253126\n",
      "Iteration 9956, Loss: 1095827346.4087443\n",
      "Iteration 9957, Loss: 1847964341.2206726\n",
      "Iteration 9958, Loss: 1739385567.9285774\n",
      "Iteration 9959, Loss: 1205917997.2523458\n",
      "Iteration 9960, Loss: 1146103455.4161663\n",
      "Iteration 9961, Loss: 1132685970.641846\n",
      "Iteration 9962, Loss: 1155303394.71304\n",
      "Iteration 9963, Loss: 1203145670.6658223\n",
      "Iteration 9964, Loss: 1225019929.0215077\n",
      "Iteration 9965, Loss: 1261461662.7255168\n",
      "Iteration 9966, Loss: 1223087554.1828885\n",
      "Iteration 9967, Loss: 1189090212.6122024\n",
      "Iteration 9968, Loss: 1126358745.1668262\n",
      "Iteration 9969, Loss: 1708653205.9563627\n",
      "Iteration 9970, Loss: 1273631274.027394\n",
      "Iteration 9971, Loss: 1266993699.4319355\n",
      "Iteration 9972, Loss: 1284614038.6151497\n",
      "Iteration 9973, Loss: 1060629728.7765017\n",
      "Iteration 9974, Loss: 1042007456.0350397\n",
      "Iteration 9975, Loss: 1076740142.9338002\n",
      "Iteration 9976, Loss: 1098207920.1408155\n",
      "Iteration 9977, Loss: 1142456918.975094\n",
      "Iteration 9978, Loss: 1140001953.8572679\n",
      "Iteration 9979, Loss: 1121568888.6912243\n",
      "Iteration 9980, Loss: 1142022291.7215633\n",
      "Iteration 9981, Loss: 1120524919.8284774\n",
      "Iteration 9982, Loss: 1115480716.5253377\n",
      "Iteration 9983, Loss: 1128858618.759659\n",
      "Iteration 9984, Loss: 1131100687.1972675\n",
      "Iteration 9985, Loss: 1163298176.7582457\n",
      "Iteration 9986, Loss: 1128753551.3171504\n",
      "Iteration 9987, Loss: 1119302592.369935\n",
      "Iteration 9988, Loss: 1288369805.5331802\n",
      "Iteration 9989, Loss: 1196053157.6809804\n",
      "Iteration 9990, Loss: 1225830982.0502627\n",
      "Iteration 9991, Loss: 1218257297.828247\n",
      "Iteration 9992, Loss: 1208977615.73316\n",
      "Iteration 9993, Loss: 1182571431.7582753\n",
      "Iteration 9994, Loss: 1207853557.1296778\n",
      "Iteration 9995, Loss: 1036519338.6997962\n",
      "Iteration 9996, Loss: 1031518959.509536\n",
      "Iteration 9997, Loss: 1159773654.3311167\n",
      "Iteration 9998, Loss: 1328823263.5134342\n",
      "Iteration 9999, Loss: 1242821004.8675337\n",
      "Iteration 10000, Loss: 1244508934.434171\n",
      "Predicted values  [ 85343.15 116353.67  38368.39]\n",
      "Real values       [[ 93087.56416713]\n",
      " [125216.02455331]\n",
      " [ 38575.94680508]]\n",
      "Trained W         4323.5845841218725\n",
      "Trained b         32353.826198378225\n",
      "-29.27579067179346\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 50  # Number of data points\n",
    "X = np.random.uniform(1, 20, num_samples).reshape(-1, 1)  # Random years of experience between 1 and 20\n",
    "true_W = 5000  # True weight (salary increase per year)\n",
    "true_b = 30000  # True bias (base salary)\n",
    "noise = np.random.normal(0, 5000, num_samples).reshape(-1, 1)  # Add Gaussian noise (stddev = 5000)\n",
    "Y = true_W * X + true_b + noise  # Linear relationship with noise\n",
    "\n",
    "# Convert to DataFrame and save to CSV (optional)\n",
    "df = pd.DataFrame(np.hstack((X, Y)), columns=[\"YearsExperience\", \"Salary\"])\n",
    "df.to_csv(\"synthetic_salary_data.csv\", index=False)  # Save dataset to CSV\n",
    "\n",
    "# Splitting dataset into train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3, random_state=0)\n",
    "\n",
    "# Model training     \n",
    "model = MyLinReg(max_iter=10000) \n",
    "model.fit( X_train, Y_train ) \n",
    "    \n",
    "# Prediction on test set \n",
    "Y_pred = model.predict( X_test ) \n",
    "print( \"Predicted values \", np.round( Y_pred[:3], 2 ) )     \n",
    "print( \"Real values      \", Y_test[:3] )    \n",
    "print( \"Trained W        \",  model.W[0] )   \n",
    "print( \"Trained b        \", model.b) \n",
    "\n",
    "score = model.score(X_test,Y_test)\n",
    "print(score)\n",
    "\n",
    "# Visualization on test set     \n",
    "plt.scatter( X_test, Y_test, color = 'blue' )    \n",
    "plt.plot( X_test, Y_pred, color = 'orange' )   \n",
    "plt.title( 'Salary vs Experience' ) \n",
    "plt.xlabel( 'Years of Experience' )   \n",
    "plt.ylabel( 'Salary' ) \n",
    "plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Qd: The Journaling of Your Regressor \n",
    "\n",
    "For the journal, write a full explanation of how you implemented the linear regressor, including a code walk-through (or mini-review of the most interesting parts).\n",
    "\n",
    "### Qe: Mathematical Foundation for Training a Linear Regressor\n",
    "\n",
    "You must also include the theoretical mathematical foundation for the linear regressor using the following equations and graphs (free to include in your journal without cite/reference), and relate them directly to your code:\n",
    "\n",
    "* Design matrix of size $(n, d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "    \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands v01, remember: no newlines in defs}\n",
    "    \\rem{MACRO eq: equation <#1:lhs> <#2:rhs>}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\rem{MACRO arr: array <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\rem{MACRO ac: array column vector <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\rem{MACRO st: subscript text <#1:content>}\n",
    "    \\def\\st#1{_{\\textrm{#1}}}\n",
    "    \\rem{MACRO norm: norm caligari L <#1:content>}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\rem{MACRO obs: ??}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\rem{MACRO diff: math differetial operator <#1:content>}\n",
    "    \\def\\diff#1{\\mathrm{d}#1} \n",
    "    \\rem{MACRO half: shorthand for 1/2}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\rem{MACRO pfrac: partial fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\rem{MACRO dfrac: differetial operator fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "    \\rem{MACRO pown: power and parantesis (train/test..) <#1:content>}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\rem{MACROS powi, pown: shorthands for power (i) and (n)}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\rem{MACROS powtest, powertrain: power (test) and (train)}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\rem{MACRO boldmatrix: bold matix/vector notation} \n",
    "    \\def\\boldmatrix#1{\\mathbf{#1}} \n",
    "    \\rem{MACROS X,Z,x,y,w: bold X,Z,x etc.} \n",
    "    \\def\\bX{\\boldmatrix{X}}\n",
    "    \\def\\bZ{\\boldmatrix{Z}}\n",
    "    \\def\\bx{\\boldmatrix{x}}\n",
    "    \\def\\by{\\boldmatrix{y}}\n",
    "    \\def\\bw{\\boldmatrix{w}}\n",
    "    \\def\\bz{\\boldmatrix{z}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\rem{MACROS stpred, sttrue: shorthand for subscript 'pred' and 'true'}\n",
    "    \\def\\stpred{\\st{pred}~}\n",
    "    \\def\\sttrue{\\st{true}~}\n",
    "    \\rem{MACROS ypred, ytrue:   shorthand for scalar y 'pred' and 'true'}\n",
    "    \\def\\ytrue{y\\sttrue}\n",
    "    \\def\\ypred{y\\stpred} \n",
    "    \\rem{MACROS bypred, bytrue: shorthand for vecor y 'pred' and 'true'} \n",
    "    \\def\\bypred{\\boldmatrix{y}\\stpred}\n",
    "    \\def\\bytrue{\\boldmatrix{y}\\sttrue} \n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2} \\\\\n",
    "            \\vdots      &             &        & \\vdots      \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn   \\\\\n",
    "        } \n",
    "$$\n",
    "\n",
    "* Target ground-truth column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\bytrue =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1}\\sttrue \\\\\n",
    "     y\\pown{2}\\sttrue \\\\\n",
    "     \\vdots           \\\\\n",
    "     y\\pown{n}\\sttrue \\\\\n",
    "  } \n",
    "$$\n",
    "\n",
    "* Bias factor, and by convention in the following (prepend one)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} & \\mapsto \\bx\\powni\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Weight column vector of size $d+1$ (i.e. with bias or intercept element $w_0$ prepended)\n",
    "\n",
    "$$\n",
    "\\bw =\n",
    "    \\ac{c}{\n",
    "         w_0    \\\\\n",
    "         w_1    \\\\\n",
    "         w_2    \\\\\n",
    "         \\vdots \\\\\n",
    "         w_d    \\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "* Linear regression model hypothesis function for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  ~~~~~~~~~~~~~~~\n",
    "  h(\\bx\\powni;\\bw) &= \\ypred\\powni \\\\\n",
    "                   &= \\bw^\\top \\bx\\powni ~~~~ (\\bx\\powni~\\textrm{with bias element})\\\\ \n",
    "                   &= w_0  \\cdot 1+ w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni & \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Individual losses based on the $\\norm{2}^2$ (last part assuming one dimensional output)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || \\ypred \\powni         - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || h(\\bx\\powni;\\bw)      - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - \\ytrue\\powni~ \\right)^2 ~~~~~ \\textrm{(only for 1D output)}\n",
    "}\n",
    "$$\n",
    "\n",
    "* MSE loss function\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\textrm{MSE}(\\bX,\\bytrue;\\bw)  &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni\\sttrue \\right)^2\\\\\n",
    "                                   &= \\frac{1}{n} ||\\bX \\bw - \\bytrue||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "\n",
    "* Loss function, proportional to (R)MSE\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "   J &= \\frac{1}{2} ||\\bX \\bw - \\bytrue||_2^2\\\\\n",
    "     &  \\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "* Training: computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total loss\n",
    "\n",
    "$$\n",
    "  \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "* Visualization of $\\textrm{argmin}_\\bw$ means to the argument of $\\bw$ that minimizes the $J$ function. The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always forms a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">\n",
    "\n",
    "#### Training I: The Closed-form Solution\n",
    "\n",
    "* Finding the optimal weight in a _one-step_ analytic expression \n",
    "\n",
    "$$\n",
    "  \\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1}~ \\bX^\\top \\bytrue\n",
    "$$\n",
    "\n",
    "\n",
    "#### Training II: Numerical Optimization \n",
    "\n",
    "* The Gradient of the loss function\n",
    "\n",
    "$$   \n",
    "  \\nabla_\\bw~J = \\left[ \\frac{\\partial J}{\\partial w_1} ~~~~ \\frac{\\partial J}{\\partial w_2} ~~~~ \\ldots  ~~~~ \\frac{\\partial J}{\\partial w_d} \\right]^\\top\n",
    "$$\n",
    "\n",
    "* The Gradient for the based $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\bytrue \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "* The Gradient Decent Algorithm (GD)\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)}~ = \\bw^{(step~N)} ~ - \\eta \\nabla_{\\bw} J\n",
    "$$\n",
    "\n",
    "* Visualization of GD, showing $J$ as a function of two $w$-dimensions\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf: Smoke testing\n",
    "\n",
    "Once ready, you can test your regressor via the test stub below, or create your own _test suite_.\n",
    "\n",
    "Be aware that setting the stepsize, $\\eta$, value can be tricky, and you might want to tune `eta0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARN:  This mini smoke-test may produce false-positives and/or\n",
      "       false-negatives..\u001b[0m\n",
      "\n",
      "Iteration 1, Loss: 31.06302760003793\n",
      "Iteration 2, Loss: 29.016491812619197\n",
      "Iteration 3, Loss: 27.76867618250098\n",
      "Iteration 4, Loss: 26.36055952439296\n",
      "Iteration 5, Loss: 25.026882723101966\n",
      "Iteration 6, Loss: 23.960238686701775\n",
      "Iteration 7, Loss: 22.420891270634037\n",
      "Iteration 8, Loss: 21.470682452258348\n",
      "Iteration 9, Loss: 20.395378321178594\n",
      "Iteration 10, Loss: 19.537344826158254\n",
      "\u001b[1;35mINFO:  y_pred = [1.32102376 1.58847576 0.9533948  1.21989067]\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  SCORE = \u001b[1;34m-8.775661708009839\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;33mWARN:  your regressor has no coef_/intercept_ atrributes, trying We\\\n",
      "       ights() instead..\u001b[0m\n",
      "\n",
      "\u001b[0;31m       EXCEPTION: 'MyLinReg' object has no attribute 'coef_'\u001b[0m\n",
      "\n",
      "\u001b[1;31mERROR: having a hard time concantenating our bias and coefficients,\n",
      "       giving up!\u001b[0m\n",
      "\n",
      "\u001b[0;31m       EXCEPTION: 'MyLinReg' object has no attribute 'ndim'\u001b[0m\n",
      "\n",
      "\u001b[1;31mERROR: your regressor also has no Weights() function, giving up!\u001b[0m\n",
      "\n",
      "\u001b[0;31m       EXCEPTION: 'MyLinReg' object has no attribute 'ndim'\u001b[0m\n",
      "\n",
      "\u001b[1;31mERROR: your regressor fails during extraction of bias and weights (\\\n",
      "       but is a COULD)\u001b[0m\n",
      "\n",
      "\u001b[0;31m       EXCEPTION: 'MyLinReg' object has no attribute 'ndim'\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MyLinReg' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 259\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m Warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis mini smoke-test may produce false-positives and/or\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m false-negatives..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 259\u001b[0m TestMyLinReg()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 220\u001b[0m, in \u001b[0;36mTestMyLinReg\u001b[1;34m()\u001b[0m\n\u001b[0;32m    218\u001b[0m     Info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficients = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 220\u001b[0m     Err(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour regressor fails during extraction of bias and weights (but is a COULD)\u001b[39m\u001b[38;5;124m\"\u001b[39m, ex)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlibitmal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrintMatrix\n",
      "Cell \u001b[1;32mIn[42], line 115\u001b[0m, in \u001b[0;36mErr\u001b[1;34m(msg, ex)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mErr\u001b[39m(msg, ex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    114\u001b[0m     PrintOutput(msg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlred\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mif\u001b[39;00m ex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ex\n",
      "Cell \u001b[1;32mIn[42], line 216\u001b[0m, in \u001b[0;36mTestMyLinReg\u001b[1;34m()\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    215\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     Err(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour regressor also has no Weights() function, giving up!\u001b[39m\u001b[38;5;124m\"\u001b[39m, ex)\n\u001b[0;32m    217\u001b[0m Info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias         = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    218\u001b[0m Info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficients = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 115\u001b[0m, in \u001b[0;36mErr\u001b[1;34m(msg, ex)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mErr\u001b[39m(msg, ex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    114\u001b[0m     PrintOutput(msg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlred\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mif\u001b[39;00m ex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ex\n",
      "Cell \u001b[1;32mIn[42], line 213\u001b[0m, in \u001b[0;36mTestMyLinReg\u001b[1;34m()\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    212\u001b[0m             w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m             Err(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving a hard time concantenating our bias and coefficients, giving up!\u001b[39m\u001b[38;5;124m\"\u001b[39m, ex)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    215\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 115\u001b[0m, in \u001b[0;36mErr\u001b[1;34m(msg, ex)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mErr\u001b[39m(msg, ex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    114\u001b[0m     PrintOutput(msg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlred\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mif\u001b[39;00m ex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ex\n",
      "Cell \u001b[1;32mIn[42], line 207\u001b[0m, in \u001b[0;36mTestMyLinReg\u001b[1;34m()\u001b[0m\n\u001b[0;32m    205\u001b[0m w \u001b[38;5;241m=\u001b[39m regressor\u001b[38;5;241m.\u001b[39mWeights() \u001b[38;5;66;03m# maybe a Weigths function is avalible on you model?\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m w\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m,     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only handle vector like w\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms for now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m w\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected length of to be at least 2, that is one bias one coefficient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m     bias \u001b[38;5;241m=\u001b[39m w[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MyLinReg' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "# Mini smoke test for your linear regressor: TestMyLinReg\n",
    "\n",
    "import sys, os\n",
    "import numpy\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "### SOME NIFTY HELPER FUNS ###\n",
    "\n",
    "def isVector(y, expected_n=-1):\n",
    "    assert isinstance(y, numpy.ndarray), f\"expected type 'numpy.array' but got {type(y)}\"\n",
    "    assert y.ndim==1, f\"expected y.ndim==1 but got {y.ndim}\"\n",
    "    assert expected_n<0 or expected_n==y.shape[0], f\"expected vector of size {expected_n} but got size {y.shape}\"\n",
    "    return True\n",
    "\n",
    "def isMatrix(X, expected_m=-1, expected_n=-1):\n",
    "    assert isinstance(X, numpy.ndarray), f\"expected type 'numpy.array' but got {type(X)}\"\n",
    "    assert X.ndim==2, f\"expected X.ndim==2 but got {X.ndim}\"\n",
    "    assert expected_m<0 or expected_m==y.shape[0], f\"expected matrix of size {expected_m}x{expected_n} but got size {X.shape}\"\n",
    "    assert expected_n<0 or expected_n==y.shape[1], f\"expected vector of size {expected_m}x{expected_n} but got size {X.shape}\"\n",
    "    return True\n",
    "\n",
    "def PrintMatrix(x, label=\"\", precision=12, linewidth=60):\n",
    "    hasFancy = False\n",
    "    try:\n",
    "        # NOTE: how does multiple import behave, any performance issues?\n",
    "        from libitmal.utils import PrintMatrix as FancyPrintMatrix\n",
    "        hasFancy = True\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        FancyPrintMatrix(x, label=label, precision=precision, linewidth=linewidth)\n",
    "    else:\n",
    "        # default simple implementation\n",
    "        print(f\"{label}{' ' if len(label)>0 else ''}{x}\")\n",
    "\n",
    "def Col(color):\n",
    "    hasFancy = False\n",
    "    try:\n",
    "        from libitmal.Utils.colors import Col as FancyCol\n",
    "        hasFancy = True\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import Col from libitmal.Utils.colors, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        return FancyCol(color)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def ColEnd():\n",
    "    hasFancy = False\n",
    "    try:\n",
    "        from libitmal.Utils.colors import ColEnd as FancyColEnd\n",
    "        hasFancy = True\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import Col from libitmal.Utils.colors, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        return FancyColEnd()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def PrintOutput(msg, pre_msg, ex=None, color=\"\", filestream=sys.stdout):\n",
    "\n",
    "    def FormatTxt(txt, linewidth=60, prefix=\"\", replacetabs=True):\n",
    "        assert isinstance(txt, str)\n",
    "        assert isinstance(linewidth, int) and linewidth > 0\n",
    "        assert isinstance(prefix, str)\n",
    "\n",
    "        if replacetabs:\n",
    "            txt = txt.replace(\"\\t\",\"    \")\n",
    "\n",
    "        r = \"\"\n",
    "        n = 0\n",
    "        m = 0\n",
    "        for i in txt:\n",
    "            m += 1\n",
    "            if n >= linewidth:\n",
    "                if not i.isspace() and m < len(txt) and not txt[m].isspace():\n",
    "                    r += \"\\\\\" # add hypen\n",
    "                r += \"\\n\" + prefix\n",
    "                n = 0\n",
    "\n",
    "            if n == 0 and i.isspace():\n",
    "                continue # skip leading space\n",
    "\n",
    "            r += i\n",
    "            n += 1\n",
    "\n",
    "            if i == \"\\n\":\n",
    "                r += prefix\n",
    "                n = 0\n",
    "\n",
    "        return r\n",
    "\n",
    "    col_beg = Col(color)\n",
    "    col_end = ColEnd()\n",
    "\n",
    "    prefix = \"\".ljust(len(pre_msg)) \n",
    "    msg = FormatTxt(msg, prefix=prefix)\n",
    "    \n",
    "    print(f\"{col_beg}{pre_msg}{msg}{col_end}\\n\", file=filestream)\n",
    "\n",
    "    if ex is not None:\n",
    "        #msg += f\"\\n   EXCEPTION: {ex} ({type(ex)})\"\n",
    "        PrintOutput(str(ex), prefix + \"EXCEPTION: \", None, \"red\", filestream)\n",
    "\n",
    "\n",
    "def Warn(msg, ex=None):\n",
    "    PrintOutput(msg, \"WARN:  \", ex, \"lyellow\")\n",
    "\n",
    "\n",
    "def Err(msg, ex=None):\n",
    "    PrintOutput(msg, \"ERROR: \", ex, \"lred\" )\n",
    "    raise Exception(msg) if ex is None else ex\n",
    "\n",
    "\n",
    "def Info(msg):\n",
    "    PrintOutput(msg, \"INFO:  \", None, \"lpurple\")\n",
    "\n",
    "\n",
    "def SimpleAssertInRange(x, expected, eps):\n",
    "    #assert isinstance(x, numpy.ndarray)\n",
    "    #assert isinstance(expected, numpy.ndarray)\n",
    "    #assert x.ndim==1 and expected.ndim==1\n",
    "    #assert x.shape==expected.shape\n",
    "    assert eps>0\n",
    "    assert numpy.allclose(x, expected, eps) # should rtol or atol be set to eps?\n",
    "\n",
    "\n",
    "def GenerateData():\n",
    "    X = numpy.array([[8.34044009e-01],[1.44064899e+00],[2.28749635e-04],[6.04665145e-01]])\n",
    "    y = numpy.array([5.97396028, 7.24897834, 4.86609388, 3.51245674])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def TestMyLinReg():\n",
    "    X, y = GenerateData()\n",
    "\n",
    "    try:\n",
    "        # assume that your regressor class is named 'MyLinReg', please update/change\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor has another name, than 'MyLinReg', please change the name in this smoke test\", ex)\n",
    "\n",
    "    try:\n",
    "        regressor = MyLinReg(max_iter=200)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ for parameter 'max_iter'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(eta0=0.01)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ for parameter 'eta0'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(verbose=False)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'verbose'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(tol=1e-3)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'tol'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(n_iter_no_change=1e-3)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'n_iter_no_change'\", ex)\n",
    "\n",
    "    # create regressor with default hyperparameter values\n",
    "    # to be used for training, prediction and score..\n",
    "    try:\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ with default parameters\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        regressor.fit(X, y)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not fit\", ex)\n",
    "\n",
    "    try:\n",
    "        y_pred = regressor.predict(X)\n",
    "        Info(f\"y_pred = {y_pred}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not predict\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        score  = regressor.score(X, y)\n",
    "        Info(f\"SCORE = {Col('lblue')}{score}{ColEnd()}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails in the score call\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        w    = None # default\n",
    "        bias = None # default\n",
    "        try:\n",
    "            w = regressor.coef_\n",
    "            bias = regressor.intercept_\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Warn(\"your regressor has no coef_/intercept_ atrributes, trying Weights() instead..\", ex)\n",
    "        try:\n",
    "            if w is None:\n",
    "                w = regressor.Weights() # maybe a Weigths function is avalible on you model?\n",
    "                try:\n",
    "                    assert w.ndim == 1,     \"can only handle vector like w's for now\"\n",
    "                    assert w.shape[0] >= 2, \"expected length of to be at least 2, that is one bias one coefficient\"\n",
    "                    bias = w[0]\n",
    "                    w = w[1:]\n",
    "                except Exception as ex:\n",
    "                    w = None\n",
    "                    Err(\"having a hard time concantenating our bias and coefficients, giving up!\", ex)\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Err(\"your regressor also has no Weights() function, giving up!\", ex)\n",
    "        Info(f\"bias         = {bias}\")\n",
    "        Info(f\"coefficients = {w}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails during extraction of bias and weights (but is a COULD)\", ex)\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import PrintMatrix\n",
    "    except Exception as ex:\n",
    "        PrintMatrix = SimplePrintMatrix # fall-back\n",
    "        Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import AssertInRange\n",
    "    except Exception as ex:\n",
    "        AssertInRange = SimpleAssertInRange # fall-back\n",
    "        Warn(\"could not import AssertInRange from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        if w is not None:\n",
    "            if bias is not None:\n",
    "                w = numpy.concatenate(([bias], w)) # re-concat bias an coefficients, may be incorrect for your implementation!\n",
    "            \n",
    "            # TEST VECTOR:\n",
    "            w_expected = numpy.array([4.046879011698, 1.880121487278])\n",
    "            \n",
    "            PrintMatrix(w,          label=\"       w         =\")\n",
    "            PrintMatrix(w_expected, label=\"       w_expected=\")\n",
    "            print()\n",
    "            \n",
    "            eps = 1E-2 # somewhat big epsilon, allowing some slack..\n",
    "            AssertInRange(w, w_expected, eps)\n",
    "            Info(\"Well, good news, your w and the expected w-vector seem to be very close numerically, so the smoke-test has passed!\")\n",
    "            \n",
    "            return regressor\n",
    "        else:\n",
    "            Warn(\"cannot test due to missing w information\")\n",
    "    except Exception as ex:\n",
    "        Err(\"mini-smoketest on your regressor failed\", ex)\n",
    "    \n",
    "    return None\n",
    "\n",
    "Warn(\"This mini smoke-test may produce false-positives and/or\\n false-negatives..\")\n",
    "TestMyLinReg()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qg: [OPTIONAL] More Smoke-Testing\n",
    "\n",
    "Do you dare to compare your custom regressor with the SGD regressor in Scikit-learn on both the IRIS and MNIST datasets?\n",
    "\n",
    "Then run the next smoke-test function, but the code might requre `eta0` anb `max_iter` hyperparamter tuning).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "try:\n",
    "    from libitmal import dataloaders\n",
    "except Exception as ex:\n",
    "    Err(\"can not import dataloaders form libitmal, and then I can not run the TestAndCompareRegressors smoke-test, sorry!\", ex)\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\",  dataloaders.IRIS_GetDataSet,  1E-2),\n",
    "              (\"MNIST\", dataloaders.MNIST_GetDataSet, 1E-3)]:\n",
    "        \n",
    "        # NOTE: f-tuble is (<name>, <data-loader-function-pointer>, <eps0>)\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        Info(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        eta0 = f[2] # an adaptive learning rate is really needed here!\n",
    "        regressor0 = MyLinReg(eta0=eta0, max_iter=1000)\n",
    "        regressor1 = SGDRegressor()    \n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            Info(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            \n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            \n",
    "            PrintMatrix(y_pred_test, label=\"y_pred_test=\", precision=4)\n",
    "            print()\n",
    "            \n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            Info(f\"SCORE['{r[0]}'] = {Col('lblue')}{r2:0.3f}{ColEnd()}\")\n",
    "            \n",
    "        Info(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow...\n",
    "TestAndCompareRegressors()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qh Conclusion\n",
    "\n",
    "As always, take some time to fine-tune your regressor, perhaps just some code-refactoring, cleaning out 'bad' code, and summarize all your findings\n",
    " above. \n",
    "\n",
    "In other words, write a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2022-12-22| CEF, initial draft. \n",
    "2023-02-26| CEF, first release.\n",
    "2023-02-28| CEF, fix a few issues related to import from libitmal, added Info and color output.\n",
    "2024-09-19| CEF, major overhaul, change math/text and code snippets.\n",
    "2024-09-25| CEF, final fixes, tests, and proof-reading. Moved early stopping and learning graphs to a later excercise.\n",
    "2024-10-04| CEF, clarified Qa with respect to what-is-to-be implemented and what-is-to-be described in text only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
